text,label
"5.3 Difference Equations and Powers AkDifference Equations and Powers AkDifference equations uk+1 = Auk move forward in a ﬁnite number of ﬁnite steps. Adifferential equation takes an inﬁnite number of inﬁnitesimal steps, but the two theoriesstay absolutely in parallel. It is the same analogy between the discrete and the continuousthat appears over and over in mathematics. A good illustration is compound interest,when the time step gets shorter.Suppose you invest $1000 at 6% interest. Compounded once a year, the principal Pis multiplied by 1.06. This is a difference equation Pk+1 = APk = 1.06Pk with a time stepof one year. After 5 years, the original P0 = 1000 has been multiplied 5 times:Now suppose the time step is reduced to a month. The new difference equation is pk+1 =(1+.06/12)pk. After 5 years, or 60 months, you have $11 more:The next step is to compound every day, on 5(365) days. This only helps a little:Finally, to keep their employees really moving, banks offer continuous compounding.The interest is added on at every instant, and the difference equation breaks down. Youcan hope that the treasurer does not know calculus (which is all about limits as ∆t → 0).The bank could compound the interest N times a year, so ∆t = 1/N:1000 → e.301000 = $1349.87.Or the bank can switch to a differential equation—the limit of the difference equationpk+1 = (1+.06∆t)pk. Moving pk to the left side and dividing by ∆t,The solution is p(t) = e.06t p0. After t = 5 years, this again amounts to $1349.87. Theprincipal stays ﬁnite, even when it is compounded every instant—and the improvementover compounding every day is only four cents.The main object of this section is to solve uk+1 = Auk. That leads us to Ak and powersof matrices. Our second example is the famous Fibonacci sequence:",eigenvec_val
"(c) Why does no matrix transform (2,6) to (1,0) and (1,3) to (0,1)?37. (a) What matrix M transforms (1,0) and (0,1) to (r,t) and (s,u)?(b) What matrix N transforms (a,c) and (b,d) to (1,0) and (0,1)?(c) What condition on a, b, c, d will make part (b) impossible?38. (a) How do M and N in Problem 37 yield the matrix that transforms (a,c) to (r,t)and (b,d) to (s,u)?(b) What matrix transforms (2,5) to (1,1) and (1,3) to (0,2)?39. If you keep the same basis vectors but put them in a different order, the change-of-basis matrix M is amatrix. If you keep the basis vectors in order but changetheir lengths, M is a40. The matrix that transforms (1,0) and (0,1) to (1,4) and (1,5) is M =combination a(1,4) + b(1,5) that equals (1,0) has (a,b) = (those new coordinates of (1,0) related to M or M−1?41. What are the three equations for A, B, C if the parabola Y = A + Bx +Cx2 equals 4at x = a, 5 at x = b, and 6 at x = c? Find the determinant of the 3 by 3 matrix. Forwhich numbers a, b, c will it be impossible to ﬁnd this parabola Y?42. Suppose v1, v2, v3 are eigenvectors for T. This means T(vi) = λivi for i = 1,2,3.What is the matrix for T when the input and output bases are the v’s?43. Every invertible linear transformation can have I as its matrix. For the output basisjust choose wi = T(vi). Why must T be invertible?44. Suppose T is reﬂection across the x-axis and S is reﬂection across the y-axis. Thedomain V is the x-y plane. If v = (x,y) what is S(T(v))? Find a simpler descriptionof the product ST.45. Suppose T is reﬂection across the 45° line, and S is reﬂection across the y-axis, Ifv = (2,1) then T(v) = (1,2). Find S(T(v)) and T(S(v)). This shows that generally46. Show that the product ST of two reﬂections is a rotation. Multiply these reﬂectionmatrices to ﬁnd the rotation angle:47. The 4 by 4 Hadamard matrix is entirely +1 and −1:",vector spaces
"2.3 Linear Independence, Basis, and Dimension(b) Those vectors (are)(are not)(might be) linearly independent.(c) Any four of those vectors (are)(are not)(might be) a basis for R4.(d) If those vectors are the columns of A, then Ax = b (has) (does not have) (mightnot have) a solution.23. The columns of A are n vectors from Rm. If they are linearly independent, what isthe rank of A? If they span Rm, what is the rank? If they are a basis for Rm, what24. Find a basis for the plane x−2y+3z = 0 in R3. Then ﬁnd a basis for the intersectionof that plane with the xy-plane. Then ﬁnd a basis for all vectors perpendicular to the25. Suppose the columns of a 5 by 5 matrix A are a basis for R5.(a) The equation Ax = 0 has only the solution x = 0 because(b) If b is in R5 then Ax = b is solvable becauseConclusion: A is invertible. Its rank is 5.26. Suppose S is a ﬁve-dimensional subspace of R6. True or false?(a) Every basis for S can be extended to a basis for R6 by adding one more vector.(b) Every basis for R6 can be reduced to a basis for S by removing one vector.27. U comes from A by subtracting row 1 from row 3:Find bases for the two column spaces. Find bases for the two row spaces. Find basesfor the two nullspace.28. True or false (give a good reason)?(a) If the columns of a matrix are dependent, so are the rows.(b) The column space of a 2 by 2 matrix is the same as its row space.(c) The column space of a 2 by 2 matrix has the same dimension as its row space.(d) The columns of a matrix are a basis for the column space.29. For which numbers c and d do these matrices have rank 2?1 2 5 0 50 0 0 d",vector spaces
"Chapter 6 Positive Deﬁnite MatricesThen CTAC must have one positive and one negative eigenvalue, like A.Example 6. This application is the important one:For any symmetric matrix A, the signs of the pivots agree with the signsof the eigenvalues. The eigenvalue matrix Λ and the pivot matrix D have thesame number of positive entries, negative entries, and zero entries.We will assume that A allows the symmetric factorization A = LDLT (without row ex-changes). By the law of inertia, A has the same number of positive eigenvalues as D.But the eigenvalues of D are just its diagonal entries (the pivots). Thus the number ofpositive pivots matches the number of positive eigenvalues of A.That is both beautiful and practical. It is beautiful because it brings together (forsymmetric matrices) two parts of this book that were previously separate: pivots andeigenvalues. It is also practical, because the pivots can locate the eigenvalues:A has positive pivotsA−2I has a negative pivotA has positive eigenvalues, by our test. But we know that λmin is smaller than 2, becausesubtracting 2 dropped it below zero. The next step looks at A−I, to see if λmin < 1. (Itis, because A−I has a negative pivot.) That interval containing λ is cut in half at everystep by checking the signs of the pivots.This was almost the ﬁrst practical method of computing eigenvalues. It was dominantabout 1960, after one important improvement—to make A tridiagonal ﬁrst. Then thepivots are computed in 2n steps instead of 16n3. Elimination becomes fast, and the searchfor eigenvalues (by halving the intervals) becomes simple. The current favorite is theQR method in Chapter 7.The Generalized Eigenvalue ProblemPhysics, engineering, and statistics are usually kind enough to produce symmetric ma-trices in their eigenvalue problems. But sometimes Ax = λx is replaced by Ax = λMx.There are two matrices rather than one.An example is the motion of two unequal masses in a line of springs:dt2 +2v−w = 0dt2 −v+2w = 0When the masses were equal, m1 = m2 = 1, this was the old system u′′ +Au = 0. Nowit is Mu′′ +Au = 0, with a mass matrix M. The eigenvalue problem arises when we look",pos_def_matrices
"Chapter 2 Vector SpacesYou must notice that the word “dimensional” is used in two different ways. We speakabout a four-dimensional vector, meaning a vector in R4. Now we have deﬁned a four-dimensional subspace; an example is the set of vectors in R6 whose ﬁrst and last com-ponents are zero. The members of this four-dimensional subspace are six-dimensionalOne ﬁnal note about the language of linear algebra. We never use the terms “basis of amatrix” or “rank of a space” or “dimension of a basis.” These phrases have no meaning.It is the dimension of the column space that equals the rank of the matrix, as we provein the coming section.Problems 1–10 are about linear independence and linear dependence.1. Show that v1, v2, v3 are independent but v1, v2, v3, v4 are dependent:Solve c1v1 +···+c4v4 = 0 or Ac = 0. The v’s go in the columns of A.2. Find the largest possible number of independent vectors amongThis number is theof the space spanned by the v’s.3. Prove that if a = 0, d = 0, or f = 0 (3 cases), the columns of U are dependent:4. If a, d, f in Problem 3 are all nonzero, show that the only solution to Ux = 0 is x = 0.Then U has independent columns.5. Decide the dependence or independence of(a) the vectors (1,3,2), (2,1,3), and (3.2,1).(b) the vectors (1,−3,2), (2,1,−3), and (−3,2,1).",vector spaces
"One aim of this book is to explain the useful parts of matrix theory. In comparisonwith older texts in abstract linear algebra, the underlying theory has not been radicallychanged. One of the best things about the subject is that the theory is really essentialfor the applications. What is different is the change in emphasis which comes with anew point of view. Elimination becomes more than just a way to ﬁnd a basis for therow space, and the Gram-Schmidt process is not just a proof that every subspace has anorthonormal basis. Instead, we really need these algorithms. And we need a convenientdescription, A = LU or A = QR, of what they do.This chapter will take a few more steps in the same direction. I suppose these steps aregoverned by computational necessity, rather than by elegance, and I don’t know whetherto apologize for that; it makes them sound very superﬁcial, and that is wrong. They dealwith the oldest and most fundamental problems of the subject, Ax = b and Ax = λx, butthey are continually changing and improving. In numerical analysis there is a survivalof the ﬁttest, and we want to describe some ideas that have survived so far. They fall1. Techniques for Solving Ax = b.Elimination is a perfect algorithm, exceptwhen the particular problem has special properties—as almost every problem has. Sec-tion 7.4 will concentrate on the property of sparseness, when most of the entries in Aare zero. We develop iterative rather than direct methods for solving Ax = b. An iter-ative method is “self-correcting,” and never reaches the exact answer. The object is toget close more quickly than elimination. In some problems, that can be done; in manyothers, elimination is safer and faster if it takes advantage of the zeros. The competitionis far from over, and we will identify the spectral radius that controls the speed of con-vergence to x = A−1b.2. Techniques for Solving Ax = λx.The eigenvalue problem is one of the out-",computations
"Chapter 2 Vector SpacesThis law applies equally to AT, which has m columns. AT is just as good a matrix as A.But the dimension of its column space is also r, soThe left nullspace N(AT) has dimension m−r.The m−r solutions to yTA = 0 are hiding somewhere in elimination. The rows of Acombine to produce the m−r zero rows of U. Start from PA = LU, or L−1PA = U. Thelast m−r rows of the invertible matrix L−1P must be a basis of y’s in the left nullspace—because they multiply A to give the zero rows in U.In our 3 by 4 example, the zero row was row 3 − 2(row 2) + 5(row 1). Thereforethe components of y are 5, −2, 1. This is the same combination as in b3 −2b2 +5b1 onthe right-hand side, leading to 0 = 0 as the ﬁnal equation. That vector y is a basis forthe left nullspace, which has dimension m − r = 3 − 2 = 1. It is the last row of L−1P,and produces the zero row in U—and we can often see it without computing L−1. Whendesperate, it is always possible just to solve ATy = 0.I realize that so far in this book we have given no reason to care about N(AT). It iscorrect but not convincing if I write in italics that the left nullspace is also important. Thenext section does better by ﬁnding a physical meaning for y from Kirchhoff’s CurrentNow we know the dimensions of the four spaces. We can summarize them in a table,and it even seems fair to advertise them as theFundamental Theorem of Linear Algebra, Part I1. C(A) = column space of A; dimension r.2. N(A) = nullspace of A; dimension n−r.3. C(AT) = row space of A; dimension r.4. N(AT) = left nullspace of A; dimension m−r.Example 1. A =has m = n = 2, and rank r = 1.1. The column space contains all multiples of. The second column is in the samedirection and contributes nothing new.2. The nullspace contains all multiples of. This vector satisﬁes Ax = 0.3. The row space contains all multiples of. I write it as a column vector, sincestrictly speaking it is in the column space of AT.4. The left nullspace contains all multiples of y =. The rows of A with coefﬁ-cients −3 and 1 add to zero, so ATy = 0.",vector spaces
"6.3 Singular Value Decompositionthat A must have independent columns.) In the reverse order A = S′Q, the matrix S′ isthe symmetric positive deﬁnite square root of AAT.4. Least Squares For a rectangular system Ax = b. the least-squares solution comesfrom the normal equations ATA�x = ATb. If A has dependent columns then ATA is notinvertible and �x is not determined. Any vector in the nullspace could be added to �x. Wecan now complete Chapter 3, by choosing a “best” (shortest) �x for every Ax = b.Ax = b has two possible difﬁculties: Dependent rows or dependent columns. Withdependent rows, Ax = b may have no solution. That happens when b is outside thecolumn space of A. Instead of Ax = b. we solve ATA�x = ATb. But if A has dependentcolumns, this �x will not be unique. We have to choose a particular solution of ATA�x =ATb, and we choose the shortest.The optimal solution of Ax = b is the minimum length solution of ATA�x = ATb.That minimum length solution will be called x+. It is our preferred choice as the bestsolution to Ax = b (which had no solution), and also to ATA�x = ATb (which had toomany). We start with a diagonal example.Example 5. A is diagonal, with dependent rows and dependent columns:The columns all end with zero. In the column space, the closest vector to b = (b1,b2,b3)is p = (b1,b2,0). The best we can do with Ax = b is to solve the ﬁrst two equations,since the third equation is 0 = b3. That error cannot be reduced, but the errors in the ﬁrsttwo equations will be zero. ThenNow we face the second difﬁculty. To make �x as short as possible, we choose thetotally arbitrary �x3 and �x4 to be zero. The minimum length solution is x+:x+ = A+b is shortestThis equation ﬁnds x+, and it also displays the matrix that produces x+ from b. Thatmatrix is the pseudoinverse A+ of our diagonal A. Based on this example, we know Σ+",pos_def_matrices
"7.3 Computation of Eigenvaluessteps of the shifted algorithm, the matrix Ak looks like this:We accept the computed λ ′1 as a very close approximation to the true λ1. To ﬁnd thenext eigenvalue, the QR algorithm continues with the smaller matrix (3 by 3, in theillustration) in the upper left-hand corner. Its subdiagonal elements will be somewhatreduced by the ﬁrst QR steps, and another two steps are sufﬁcient to ﬁnd λ2. This givesa systematic procedure for ﬁnding all the eigenvalues. In fact, the QR method is nowcompletely described. It only remains to catch up on the eigenvectors—that is a singleinverse power step—and to use the zeros that Householder created.2. When A0 is tridiagonal or Hessenberg, each QR step is very fast. The Gram-Schmidtprocess (factoring into QR) takes O(n3) operations for a full matrix A. For a Hessenbergmatrix this becomes O(n2), and for a tridiagonal matrix it is O(n). Fortunately, each newAk is again in Hessenberg or tridiagonal form:∗ ∗ ∗ ∗∗ ∗ ∗ ∗0 ∗ ∗ ∗0 0 ∗ ∗∗ ∗ ∗ ∗0 ∗ ∗ ∗0 0 ∗ ∗0 0 0 ∗You can easily check that this multiplication leaves Q0 with the same three zeros as A0.Hessenberg times triangular is Hessenberg. So is triangular times Hessenberg:A1 = R0Q0 =∗ ∗ ∗ ∗0 ∗ ∗ ∗0 0 ∗ ∗0 0 0 ∗∗ ∗ ∗ ∗∗ ∗ ∗ ∗0 ∗ ∗ ∗0 0 ∗ ∗The symmetric case is even better, since A1 = Q−10 A0Q0 = QT0A0Q0 stays symmetric. Bythe reasoning just completed, A1 is also Hessenberg. So A1 must be tridiagonal. Thesame applies to A2,A3,..., and every QR step begins with a tridiagonal matrix.The last point is the factorization itself, producing the Qk and Rk from each Ak (orreally from Ak − αkI). We may use Householder again, but it is simpler to annihilateeach subdiagonal element in turn by a “plane rotation” Pij. The ﬁrst is P21:Rotation to kill a21",computations
"20. In Hilbert space, ﬁnd the length of the vector v = (1/length of the function f(x) = ex (over the interval 0 ≤ x ≤ 1). What is the innerproduct over this interval of ex and e−x?21. What is the closest function acosx + bsinx to the function f(x) = sin2x on the in-terval from −π to π? What is the closest straight line c+dx?22. By setting the derivative to zero, ﬁnd the value of b1 that minimizesCompare with the Fourier coefﬁcient b1.23. Find the Fourier coefﬁcients a0, a1, b1 of the step function y(x), which equals 1 onthe interval 0 ≤ x ≤ π and 0 on the remaining interval π < x < 2π:24. Find the fourth Legendre polynomial. It is a cubic x3+ax2+bx+c that is orthogonalto 1, x, and x2 − 13 over the interval −1 ≤ x ≤ 1.25. What is the closest straight line to the parabola y = x2 over −1 ≤ x ≤ 1?26. In the Gram-Schmidt formula (10), verify that C is orthogonal to q1 and q2.27. Find an orthonormal basis for the subspace spanned by a1 = (1,−1,0,0), a2 =(0,1,−1,0), a3 = (0,0,1,−1).28. Apply Gram-Schmidt to (1,−1,0), (0,1,−1), and (1,0,−1), to ﬁnd an orthonormalbasis on the plane x1 +x2 +x3 = 0. What is the dimension of this subspace, and howmany nonzero vectors come out of Gram-Schmidt?29. (Recommended) Find orthogonal vectors A, B, C by Gram-Schmidt from a, b, c:A, B, C and a, b, c are bases for the vectors perpendicular to d = (1,1,1,1).30. If A = QR then ATA = RTR =on A corresponds to elimination on ATA. CompareFor ATA, the pivots are 2, 33 and the multipliers are −1",orthogonality
"Chapter 6 Positive Deﬁnite MatricesThus Ak is positive deﬁnite. Its eigenvalues (not the same λ1!) must be positive. Itsdeterminant is their product, so all upper left determinants are positive.If condition III holds, so does condition IV: According to Section 4.4, the kth pivotdk is the ratio of detAk to detAk−1. If the determinants are all positive, so are the pivots.If condition IV holds, so does condition I: We are given positive pivots, and mustdeduce that xTAx > 0. This is what we did in the 2 by 2 case, by completing the square.The pivots were the numbers outside the squares. To see how that happens for symmetricmatrices of any size, we go back to elimination on a symmetric matrix: A = LDLT.Example 1. Positive pivots 2, 3I want to split xTAx into xTLDLTx:So xTAx is a sum of squares with the pivots 2, 3xTAx = (LTx)TD(LTx) = 2Those positive pivots in D multiply perfect squares to make xTAx positive. Thus condi-tion IV implies condition I, and the proof is complete.It is beautiful that elimination and completing the square are actually the same. Elim-ination removes x1 from all later equations. Similarly, the ﬁrst square accounts for allterms in xTAx involving x1. The sum of squares has the pivots outside. The multipliersℓij are inside! You can see the numbers −13 inside the squares in the example.Every diagonal entry aii must be positive. As we know from the examples, however,it is far from sufﬁcient to look only at the diagonal entries.The pivots di are not to be confused with the eigenvalues. For a typical positivedeﬁnite matrix, they are two completely different sets of positive numbers, In our 3 by 3example, probably the determinant test is the easiest:detA3 = detA = 4.The pivots are the ratios d1 = 2, d2 = 32, d3 = 43. Ordinarily the eigenvalue test is thelongest computation. For this A we know the λ’s are all positive:",pos_def_matrices
"1.5 Triangular Factors and Row ExchangesE−1 applied to F−1 applied to G−1 applied to I produces A.The order is right for the ℓ’s to fall into position. This always happens! Note thatparentheses in E−1F−1G−1 were not necessary because of the associative law.A = LU: The n by n caseThe factorization A = LU is so important that we must say more. It used to be missingin linear algebra courses when they concentrated on the abstract side. Or maybe it wasthought to be too hard—but you have got it. If the last Example 4 allows any U insteadof the particular U = I, we can see how the rule works in general. The matrix L, appliedto U, brings back A:row 1 of Urow 2 of Urow 3 of U�� = original A.The proof is to apply the steps of elimination. On the right-hand side they take A to U.On the left-hand side they reduce L to I, as in Example 4. (The ﬁrst step subtracts ℓ21times (1,0,0) from the second row, which removes ℓ21.) Both sides of (7) end up equalto the same matrix U, and the steps to get there are all reversible. Therefore (7) is correctand A = LU.A = LU is so crucial, and so beautiful, that Problem 8 at the end of this sectionsuggests a second approach. We are writing down 3 by 3 matrices, but you can see howthe arguments apply to larger matrices. Here we give one more example, and then putA = LU to use.Example 5. (A = LU, with zeros in the empty spaces)That shows how a matrix A with three diagonals has factors L and U with two diagonals.This example comes from an important problem in differential equations (Section 1.7).The second difference in A is a backward difference L times a forward difference U.",gauss elim
"4.2 Properties of the Determinant5. Subtracting a multiple of one row from another row leaves the same determinant.Rule 3 would say that there is a further term −ℓ��, but that term is zero by rule 4. Theusual elimination steps do not affect the determinant!6. If A has a row of zeros, then detA = 0.One proof is to add some other row to the zero row. The determinant is unchanged, byrule 5. Because the matrix will now have two identical rows, detA = 0 by rule 4.7. If A is triangular then detA is the product a11a22···ann of the diagonal entries. Ifthe triangular A has 1s along the diagonal, then detA = 1.Proof. Suppose the diagonal entries are nonzero. Then elimination can remove all theoff-diagonal entries, without changing the determinant (by rule 5). If A is lower triangu-lar, the steps are downward as usual. If A is upper triangular, the last column is clearedout ﬁrst—using multiples of ann. Either way we reach the diagonal matrix D:detD = a11a22···anndetI = a11a22···ann.To ﬁnd detD we patiently apply rule 3. Factoring out a11 and then a22 and ﬁnally annleaves the identity matrix. At last we have a use for rule 1: detI = 1.If a diagonal entry is zero then elimination will produce a zero row. By rule 5 theseelimination steps do not change the determinant. By rule 6 the zero row means a zerodeterminant. This means: When a triangular matrix is singular (because of a zero on themain diagonal) its determinant is zero.This is a key property. All singular matrices have a zero determinant.8. If A is singular, then detA = 0. If A is invertible, then detA ̸= 0.is not invertible if and only ifad −bc = 0.",determinants
"5. For any symmetric matrix A, compute the ratio R(x) for the special choice x =(1,...,1). How is the sum of all entries aij related to λ1 and λn?6. With A =, ﬁnd a choice of x that gives a smaller R(x) than the bound λ1 ≤ 2that comes from the diagonal entries. What is the minimum value of R(x)?7. If B is positive deﬁnite, show from the Rayleigh quotient that the smallest eigenvalueof A+B is larger than the smallest eigenvalue of A.8. If λ1 and µ1 are the smallest eigenvalues of A and B, show that the smallest eigen-value θ1 of A+B is at least as large as λ1 + µ1. (Try the corresponding eigenvectorx in the Rayleigh quotients.)Note. Problems 7 and 8 are perhaps the most typical and most important resultsthat come easily from Rayleigh’s principle, but only with great difﬁculty from the9. If B is positive deﬁnite, show from the minimax principle (12) that the second small-est eigenvalue is increased by adding B : λ2(A+B) > λ2(A).10. If you throw away two rows and columns of A, what inequalities do you expectbetween the smallest eigenvalue µ of the new matrix and the original λ’s?11. Find the minimum values of12. Prove from equation (11) that R(x) is never larger than the largest eigenvalue λn.13. The minimax principle for λj involves j-dimensional subspaces S j:Equivalent to equation (15)λ j = minx in S j R(x)(a) If λj is positive, infer that every S j contains a vector x with R(x) > 0.(b) Deduce that S j contains a vector y = C−1x with yTcTACy/yTy > 0.(c) Conclude that the jth eigenvalue of CTAC, from its minimax principle, is alsopositive—proving again the law of inertia in Section 6.2.14. Show that the smallest eigenvalue λ1 of Ax = λMx is not larger than the ratio a11/m11of the corner entries.15. Which particular subspace S2 in Problem 13 gives the minimum value λ2? In otherwords, over which S2 is the maximum of R(x) equal to λ2?",pos_def_matrices
"6.2 Tests for Positive Deﬁniteness12. In three dimensions, λ1y23 = 1 represents an ellipsoid when all λi > 0.Describe all the different kinds of surfaces that appear in the positive semideﬁnitecase when one or more of the eigenvalues is zero.13. Write down the ﬁve conditions for a 3 by 3 matrix to be negative deﬁnite (−A ispositive deﬁnite) with special attention to condition III: How is det(−A) related to14. Decide whether the following matrices are positive deﬁnite, negative deﬁnite, semidef-Is there a real solution to −x2 −5y2 −9z2 −4xy−6xz−8yz = 1?15. Suppose A is symmetric positive deﬁnite and Q is an orthogonal matrix. True or(a) QTAQ is a diagonal matrix.(b) QTAQ is symmetric positive deﬁnite.(c) QTAQ has the same eigenvalues as A.(d) e−A is symmetric positive deﬁnite.16. If A is positive deﬁnite and a11 is increased, prove from cofactors that the determinantis increased. Show by example that this can fail if A is indeﬁnite.17. From A = RTR. show for positive deﬁnite matrices that detA ≤ a11a22···ann. (Thelength squared of column j of R is a j j. Use determinant = volume.)18. (Lyapunov test for stability of M) Suppose AM + MHA = −I with positive deﬁniteA. If Mx = λx show that ReA < 0. (Hint: Multiply the ﬁrst equation by xH and x.)19. Which 3 by 3 symmetric matrices A produce these functions f = xTAx? Why is theﬁrst matrix positive deﬁnite but not the second one?(a) f = 2(x2(b) f = 2(x23 −x1x2 −x1x3 −x2x3).20. Compute the three upper left determinants to establish positive deﬁniteness. Verifythat their ratios give the second and third pivots.",pos_def_matrices
"5.2 Diagonalization of a MatrixProblems 25–28 are about the diagonalizability of A.25. True or false: If the eigenvalues of A are 2, 2, 5, then the matrix is certainly26. If the eigenvalues of A are 1 and 0, write everything you know about the matrices A27. Complete these matrices so that detA = 25. Then trace = 10, and λ = 5 is repeated!Find an eigcnvector with Ax = 5x. These matrices will nothe diagonalizabie becausethere is no second line of eigenvectors.28. The matrix A =is not diagonalizable because the rank of A − 3I isChange one entry to make A diagonalizable. Which entries could you change?Problems 29–33 are about powers of matrices.29. Ak = SΛkS−1 approaches the zero matrix as k → ∞ if and only if every λ has absolute. Does Ak → 0 or Bk → 0?30. (Recommended) Find Λ and S to diagonalize A in Problem 29. What is the limit ofΛk as k → ∞? What is the limit of SΛkS−1? In the columns of this limiting matrix31. Find Λ and S to diagonalize B in Problem 29. What is B10u0 for these u0?32. Diagonalize A and compute SΛkS−1 to prove this formula for Ak:3k +1 3k −13k −1 3k +133. Diagonalize B and compute SΛkS−1 to prove this formula for Bk:",eigenvec_val
"Chapter 2 Vector SpacesFigure 2.1: The column space C(A), a plane in three-dimensional space.These are the same three equations in two unknowns. Now the problem is: Find numbersu and v that multiply the ﬁrst and second columns to produce b. The system is solvableexactly when such coefﬁcients exist, and the vector (u,v) is the solution x.We are saying that the attainable right-hand sides b are all combinations of the columnsof A. One possible right-hand side is the ﬁrst column itself; the weights are u = 1 andv = 0. Another possibility is the second column: u = 0 and v = 1. A third is the right-hand side b = 0. With u = 0 and v = 0, the vector b = 0 will always be attainable.We can describe all combinations of the two columns geometrically: Ax = b can besolved if and only if b lies in the plane that is spanned by the two column vectors (Figure2.1). This is the thin set of attainable b. If b lies off the plane, then it is not a combinationof the two columns. In that case Ax = b has no solution.What is important is that this plane is not just a subset of R3 it is a subspace. It isthe column space of A, consisting of all combinations of the columns. It is denoted byC(A). Requirements (i) and (ii) for a subspace of Rm are easy to check:(i) Suppose b and b′ lie in the column space, so that Ax = b for some x and Ax′ = b′for some x′. Then A(x + x′) = b + b′, so that b + b′ is also a combination of thecolumns. The column space of all attainable vectors b is closed under addition.(ii) If b is in the column space C(A), so is any multiple cb. If some combinationof columns produces b (say Ax = b), then multiplying that combination by c willproduce cb. In other words, A(cx) = cb.For another matrix A, the dimensions in Figure 2.1 may be very different. The small-est possible column space (one vector only) comes from the zero matrix A = 0. The",vector spaces
"Chapter 5 Eigenvalues and EigenvectorsAt the last step, an eigenvector of the 2 by 2 matrix in the lower right-hand corner goesinto a unitary M3, which is put into the corner of U3:The product U = U1U2U3 is still a unitary matrix, and U−1AU = T.This lemma applies to all matrices, with no assumption that A is diagoalizable. Wecould use it to prove that the powers Ak approach zero when all |λi| < 1, and the expo-nentials eAt approach zero when all Reλi < 0—even without the full set of eigenvectorswhich was assumed in Sections 5.3 and 5.4.Example 2. A =has the eigenvalue λ = 1 (twice).The only line of eigenvectors goes through (1,1). After dividing by2, this is the ﬁrstcolumn of U, and the triangular U−1AU = T has the eigenvalues on its diagonal:Diagonalizing Symmetric and Hermitian MatricesThis triangular form will show that any symmetric or Hermitian matrix—whether itseigenvalues are distinct or not—has a complete set of orthonormal eigenvectors. Weneed a unitary matrix such that U−1AU is diagonal. Schur’s lemma has just found it.This triangular T must be diagonal, because it is also Hermitian when A = AH:T = T H(U−1AU)H = UHAH(U−1)H = U−1AU.The diagonal matrix U−1AU represents a key theorem in linear algebra.(Spectral Theorem) Every real symmetric A can be diagonalized by anorthogonal matrix Q. Every Hermitian matrix can be diagonalized by a unitaryThe columns of Q (or U) contain orthonormal eigenvectors of A.Remark 1. In the real symmetric case, the eigenvalues and eigenvectors are real at everystep. That produces a real unitary U—an orthogonal matrix.",eigenvec_val
"2.2 Solving Ax = 0 and Ax = b2. With free variables = 0, ﬁnd a particular solution to Axp = b and Uxp = c.3. Find the special solutions to Ax = 0 (or Ux = 0 or Rx = 0). Each free variable, inturn, is 1. Then x = xp +(any combination xn of special solutions).When the equation was Ax = 0, the particular solution was the zero vector! It ﬁts the pat-tern, but xparticular = 0 was not written in equation (2). Now xp is added to the nullspacesolutions, as in equation (4).Question: How does the reduced form R make this solution even clearer? You willsee it in our example. Subtract equation 2 from equation 1, and then divide equation 2by its pivot. On the left-hand side, this produces R, as before. On the right-hand side,these operations change c = (1,3,0) to a new vector d = (−2,1,0):1 3 0 −1Our particular solution xp, (one choice out of many) has free variables v = y = 0.Columns 2 and 4 can be ignored. Then we immediately have u = −2 and w = 1, exactlyas in equation (4). The entries of d go directly into xp. This is because the identitymatrix is sitting in the pivot columns of R!Let me summarize this section, before working a new example. Elimination revealsthe pivot variables and free variables. If there are r pivots, there are r pivot variablesand n−r free variables. That important number r will be given a name—it is the rankSuppose elimination reduces Ax = b to Ux = c and Rx = d, with r pivotrows and r pivot columns. The rank of those matrices is r. The last m − rrows of U and R are zero, so there is a solution only if the last m−r entries ofc and d are also zero.The complete solution is x = xp + xn. One particular solution xp has all freevariables zero. Its pivot variables are the ﬁrst r entries of d, so Rxp = d.The nullspace solutions xn are combinations of n − r special solutions, withone free variable equal to 1. The pivot variables in that special solution can befound in the corresponding column of R (with sign reversed).You see how the rank r is crucial. It counts the pivot rows in the “row space” and thepivot columns in the column space. There are n − r special solutions in the nullspace.There are m−r solvability conditions on b or c or d.",vector spaces
"4.4 Applications of Determinants37. A 3 by 3 determinant has three products “down to the right” and three “down to theleft” with minus signs. Compute the six terms in the ﬁgure to ﬁnd D. Then explainwithout determinants why this matrix is or is not invertible:38. For A4 in Problem 6, ﬁve of the 4! = 24 terms in the big formula (6) are nonzero.Find those ﬁve terms to show that D4 = −1.39. For the 4 by 4 tridiagonal matrix (entries −1, 2, −1), ﬁnd the ﬁve terms in the bigformula that give detA = 16−4−4−4+1.40. Find the determinant of this cyclic P by cofactors of row 1. How many exchangesreorder 4, 1, 2, 3 into 1, 2, 3, 4? Is |P2| = +1 or −1?0 0 0 11 0 0 00 1 0 00 0 1 00 0 1 00 0 0 11 0 0 00 1 0 041. A=2∗eye(n)−diag(ones(n−1, 1),1)−diag(ones(n−1, 1),−1) is the −1, 2, −1matrix. Change A(1,1) to 1 so detA = 1. Predict the entries of A−1 based on n = 3and test the prediction for n = 4.42. (MATLAB) The −1, 2, −1 matrices have determinant n + 1. Compute (n + 1)A−1for n = 3 and 4, and verify your guess for n = 5. (Inverses of tridiagonal matriceshave the rank-1 form uvT above the diagonal.)43. All Pascal matrices have determinant 1. If I subtract 1 from the n, n entry, why doesthe determinant become zero? (Use rule 3 or a cofactor.)1 4 10 20���� = 1 (known)1 4 10 19���� = 0 (explain).This section follows through on four major applications: inverse of A, solving Ax = b,volumes of boxes, and pivots. They are among the key computations in linear algebra",determinants
"6.1 Minima, Maxima, and Saddle Points9. The quadratic f(x1,x2) = 3(x1 + 2x2)2 + 4x22 is positive. Find its matrix A, factor itinto LDLT, and connect the entries in D and L to 3, 2, 4 in f.10. If R = [ p qq r ], write out R2 and check that it is positive deﬁnite unless R is singular.11. (a) If A =is Hermitian (complex b), ﬁnd its pivots and determinant.(b) Complete the square for xHAx. Now xH = [x1 x2] can be complexa|x1|2 +2Rebx1x2 +c|x2|2 = a|x1 +(b/a)x2|2 +(c) Show that a > 0 and ac > |b|2 ensure that A is positive deﬁnite.(d) Are the matrices12. Decide whether F = x2y2 − 2x − 2y has a minimum at the point x = y = 1 (aftershowing that the ﬁrst derivatives are zero at that point).13. Under what conditions on a, b, c is ax2 +2bxy+cy2 > x2 +y2 for all x, y?Problems 14–18 are about tests for positive deﬁniteness.14. Which of A1, A2, A3, A4 has two positive eigenvalues? Test a > 0 and ac > b2, don’tcompute the eigenvalues. Find an x so that xTA1x < 0.15. What is the quadratic f = ax2 +2bxy+cy2 for each of these matrices? Complete thesquare to write f as a sum of one or two squares d1(16. Show that f(x,y) = x2 + 4xy + 3y2 does not have a minimum at (0,0) even thoughit has positive coefﬁcients. Write f as a difference of squares and ﬁnd a point (x,y)where f is negative.17. (Important) If A has independent columns, then ATA is square and symmetric andinvertible (Section 4.2). Rewrite xTATAx to show why it is positive except whenx = 0. Then ATA is positive deﬁnite.18. Test to see if ATA is positive deﬁnite in each case:",pos_def_matrices
"Chapter 5 Eigenvalues and EigenvectorsThe powers 6k and 1k appear in that last matrix Ak, mixed in by the eigenvectors.For the difference equation uk+1 = Auk, we emphasize the main point. Every eigen-vector x produces a “pure solution” with powers of λ:u2 = λ 2x,...When the initial u0 is an eigenvector x, this is the solution: uk = λ kx. In general u0is not an eigenvector. But if u0 is a combination of eigenvectors, the solution uk is thesame combination of these special solutions.If u0 = c1x1 + ··· + cnxn, then after k steps uk = c1λ k1x1 + ··· + cnλ kChoose the c’s to match the starting vector u0:There was an exercise in Chapter 1, about moving in and out of California, that is worthanother look. These were the rules:10 of the people outside California move in, and10 of the peopleinside California move out. We start with y0 people outside and z0 inside.At the end of the ﬁrst year the numbers outside and inside are y1 and z1:y1 = .9y0 +.2z0z1 = .1y0 +.8z0This problem and its matrix have the two essential properties of a Markov process:1. The total number of people stays ﬁxed: Each column of the Markov matrix addsup to 1. Nobody is gained or lost.2. The numbers outside and inside can never become negative: The matrix has nonegative entries. The powers Ak are all nonnegative.3We solve this Markov difference equation using uk = SΛkS−1u0. Then we show thatthe population approaches a “steady state.” First A has to be diagonalized:det(A−λI) = λ 2 −1.7λ +.73Furthermore, history is completely disregarded; each new uk+1 depends only on the current uk. Perhaps evenour lives are examples of Markov processes, but I hope not.",eigenvec_val
"Chapter 5 Eigenvalues and EigenvectorsIf λ1 > 1, (I −A)−1 fails to be nonnegative.If λ1 = 1, (I −A)−1 fails to exist.If λ1 < 1, (I −A)−1 is a converging sum of nonnegative matrices:(I −A)−1 = I +A+A2 +A3 +··· .The 3 by 3 example has λ1 = .9, and output exceeds input. Production can go on.Those are easy to prove, once we know the main fact about a nonnegative matrix likeA: Not only is the largest eigenvalue λ1 positive, but so is the eigenvector x1. Then(I −A)−1 has the same eigenvector, with eigenvalue 1/(1−λ1).If λ1 exceeds 1, that last number is negative. The matrix (I − A)−1 will take thepositive vector x1 to a negative vector x1/(1 − λ1). In that case (I − A)−1 is deﬁnitelynot nonnegative. If λ1 = 1, then I −A is singular. The productive case is λ1 < 1, whenthe powers of A go to zero (stability) and the inﬁnite series I + A + A2 + ··· converges.Multiplying this series by I −A leaves the identity matrix—all higher powers cancel—so(I −A)−1 is a sum of nonnegative matrices, We give two examples:has λ1 = 2 and the economy is losthas λ1 = 12 and we can produce anything.The matrices (I −A)−1 in those two cases are −1Leontief’s inspiration was to ﬁnd a model that uses genuine data from the real econ-omy. The table for 1958 contained 83 industries in the United States, with a “trans-actions table” of consumption and production for each one. The theory also reachesbeyond (I −A)−1, to decide natural prices and questions of optimization. Normally la-bor is in limited supply and ought to be minimized. And, of course, the economy is notExample 3 (The prices in a closed input-output model ).The model is called “closed” when everything produced is also consumed. Nothing goesoutside the system. In that case A goes back to a Markov matrix. The columns add upto 1. We might be talking about the value of steel and food and labor, instead of thenumber of units, The vector p represents prices instead of production levels.Suppose p0 is a vector of prices. Then Ap0 multiplies prices by amounts to give thevalue of each product. That is a new set of prices which the system uses for the nextset of values A2p0. The question is whether the prices approach equilibrium. Are thereprices such that p = Ap, and does the system take us there?You recognize p as the (nonnegative) eigenvector of the Markov matrix A, with λ = 1.It is the steady state p∞, and it is approached from any starting point p0. By repeating atransaction over and over, the price tends to equilibrium.",eigenvec_val
"2.4 The Four Fundamental Subspacesobvious. It also says something about square matrices: If the rows of a square matrixare linearly independent, then so are the columns (and vice versa). Again, that does notseem self-evident (at least, not to the author).To see once more that both the row and column spaces of U have dimension r, con-sider a typical situation with rank r = 3. The echelon matrix U certainly has threeWe claim that U also has three independent columns, and no more, The columns haveonly three nonzero components. If we can show that the pivot columns—the ﬁrst, fourth,and sixth—are linearly independent, they must be a basis (for the column space of U,not A!). Suppose a combination of these pivot columns produced zero:Working upward in the usual way, c3 must be zero because the pivot d3 ̸= 0, then c2 mustbe zero because d2 ̸= 0, and ﬁnally c1 = 0. This establishes independence and completesthe proof. Since Ax = 0 if and only if Ux = 0, the ﬁrst, fourth, and sixth columns of A—whatever the original matrix A was, which we do not even know in this example—are aThe row space and column space both became clear after elimination on A. Nowcomes the fourth fundamental subspace, which has been keeping quietly out of sight.Since the ﬁrst three spaces were C(A), N(A), and C(AT), the fourth space must beN(AT), It is the nullspace of the transpose, or the left nullspace of A. ATy = 0 meansyTA = 0, and the vector appears on the left-hand side of A.4. The left nullspace of A (= the nullspace of AT) If A is an m by n matrix, thenAT is n by m. Its nullspace is a subspace of Rm; the vector y has m components. Writtenas yTA = 0, those components multiply the rows of A to produce the zero row:The dimension of this nullspace N(AT) is easy to ﬁnd, For any matrix, the numberof pivot variables plus the number of free variables must match the total number ofcolumns. For A, that was r +(n−r) = n. In other words, rank plus nullity equals n:dimension of C(A)+dimension of N(A) = number of columns.",vector spaces
"5.3 Difference Equations and Powers AkExample 1. This matrix A is certainly stable:has eigenvalues 0 and 1The λ’s are on the main diagonal because A is triangular. Starting from any u0, andfollowing the rule uk+1 = Auk, the solution must eventually approach zero:The larger eigenvalue λ = 12 governs the decay; after the ﬁrst step every uk is 1real effect of the ﬁrst step is to split u0 into the two eigenvectors of A:Positive Matrices and Applications in EconomicsBy developing the Markov ideas we can ﬁnd a small gold mine (entirely optional) ofmatrix applications in economics.Example 2 (Leontief’s input-output matrix).This is one of the ﬁrst great successes of mathematical economics. To illustrate it, weconstruct a consumption matrix—in which aij, gives the amount of product j that isneeded to create one unit of product i:The ﬁrst question is: Can we produce y1 units of steel, y2 units of food, and y3 units oflabor? We must start with larger amounts p1, p2, p3, because some part is consumedby the production itself. The amount consumed is Ap, and it leaves a net production ofTo ﬁnd a vector p such thatp = (I −A)−1y.On the surface, we are only asking if I −A is invertible. But there is a nonnegative twistto the problem. Demand and production, y and p, are nonnegative. Since p is (1−A)−1y,the real question is about the matrix that multiplies y:When is (I −A)−1 a nonnegative matrix?Roughly speaking, A cannot be too large. If production consumes too much, nothing isleft as output. The key is in the largest eigenvalue λ1 of A, which must be below 1:",eigenvec_val
"Chapter 2 Vector Spacesvector (1,1,−1) and automatically contains any multiple (c,c,−c):Nullspace is a lineThe nullspace of B is the line of all points x = c, y = c, z = −c. (The line goes throughthe origin, as any subspace must.) We want to be able, for any system Ax = b, to ﬁndC(A) and N(A): all attainable right-hand sides b and all solutions to Ax = 0.The vectors b are in the column space and the vectors x are in the nullspace. We shallcompute the dimensions of those subspaces and a convenient set of vectors to generatethem. We hope to end up by understanding all four of the subspaces that are intimatelyrelated to each other and to A—the column space of A, the nullspace of A, and their two1. Construct a subset of the x-y plane R2 that is(a) closed under vector addition and subtraction, but not scalar multiplication.(b) closed under scalar multiplication but not under vector addition.Hint: Starting with u and v, add and subtract for (a). Try cu and cv for (b).2. Which of the following subsets of R3 are actually subspaces?(a) The plane of vectors (b1,b2,b3) with ﬁrst component b1 = 0.(b) The plane of vectors b with b1 = 1.(c) The vectors b with b2b3 = 0 (this is the union of two subspaces, the plane b2 = 0and the plane b3 = 0).(d) All combinations of two given vectors (1,1,0) and (2,0,1).(e) The plane of vectors (b1,b2,b3) that satisfy b3 −b2 +3b1 = 0.3. Describe the column space and the nullspace of the matrices4. What is the smallest subspace of 3 by 3 matrices that contains all symmetric matricesand all lower triangular matrices? What is the largest subspace that is contained inboth of those subspaces?5. Addition and scalar multiplication are required to satisfy these eight rules:",vector spaces
"Chapter 5 Eigenvalues and EigenvectorsIn the differential equation, this produces the special solutions u = eλtx. They are thepure exponential solutions to du/dt = Au. Notice e−t and e2t:u(t) = eλ1tx1 = e−tu(t) = eλ2tx2 = e2tThese two special solutions give the complete solution. They can be multiplied by anynumbers c1 and c2, and they can be added together. When u1 and u2 satisfy the linearequation du/dt = Au, so does their sum u1 +u2:u(t) = c1eλ1tx1 +c2eλ2tx2This is superposition, and it applies to differential equations (homogeneous and linear)just as it applied to matrix equations Ax = 0. The nullspace is always a subspace, andcombinations of solutions are still solutions.Now we have two free parameters c1 and c2, and it is reasonable to hope that they canbe chosen to satisfy the initial condition u = u(0) at t = 0:c1x1 +c2x2 = u(0)The constants are c1 = 3 and c2 = 1, and the solution to the original equation isWriting the two components separately, we have v(0) = 8 and w(0) = 5:v(t) = 3e−t +5e2t,w(t) = 3e−t +2e2t.The key was in the eigenvalues λ and eigenvectors x. Eigenvalues are important inthemselves, and not just part of a trick for ﬁnding u. Probably the homeliest exampleis that of soldiers going over a bridge.1 Traditionally, they stop marching and just walkacross. If they happen to march at a frequency equal to one of the eigenvalues of thebridge, it would begin to oscillate. (Just as a child’s swing does; you soon notice thenatural frequency of a swing, and by matching it you make the swing go higher.) Anengineer tries to keep the natural frequencies of his bridge or rocket away from those ofthe wind or the sloshing of fuel. And at the other extreme, a stockbroker spends his lifetrying to get in line with the natural frequencies of the market. The eigenvalues are themost important feature of practically any dynamical system.To summarize, this introduction has shown how λ and x appear naturally and auto-matically when solving du/dt = Au. Such an equation has pure exponential solutions1One which I never really believed—but a bridge did crash this way in 1831.",eigenvec_val
"1.3 An Example of Gaussian EliminationAn Example of Gaussian EliminationThe way to understand elimination is by example. We begin in three dimensions:−2u + 7v + 2w =The problem is to ﬁnd the unknown values of u, v, and w, and we shall apply Gaussianelimination. (Gauss is recognized as the greatest of all mathematicians, but certainly notbecause of this invention, which probably took him ten minutes. Ironically, it is the mostfrequently used of all the ideas that bear his name.) The method starts by subtractingmultiples of the ﬁrst equation from the other equations. The goal is to eliminate u fromthe last two equations. This requires that we(a) subtract 2 times the ﬁrst equation from the second(b) subtract −1 times the ﬁrst equation from the third.− 8v − 2w = −128v + 3w =The coefﬁcient 2 is the ﬁrst pivot. Elimination is constantly dividing the pivot into thenumbers underneath it, to ﬁnd out the right multipliers.The pivot for the second stage of elimination is −8. We now ignore the ﬁrst equation.A multiple of the second equation will be subtracted from the remaining equations (inthis case there is only the third one) so as to eliminate v. We add the second equation tothe third or, in other words, we(c) subtract −1 times the second equation from the third.The elimination process is now complete, at least in the “forward” direction:− 8v − 2w = −12This system is solved backward, bottom to top. The last equation gives w = 2. Sub-stituting into the second equation, we ﬁnd v = 1. Then the ﬁrst equation gives u = 1.This process is called back-substitution.To repeat: Forward elimination produced the pivots 2, −8, 1. It subtracted multiplesof each row from the rows beneath, It reached the “triangular” system (3), which issolved in reverse order: Substitute each newly computed value into the equations that",gauss elim
"5.4 Differential Equations and eAt6. The higher order equation y′′ +y = 0 can be written as a ﬁrst-order system by intro-ducing the velocity y′ as another unknown:If this is du/dt = Au, what is the 2 by 2 matrix A? Find its eigenvalues and eigen-vectors, and compute the solution that starts from y(0) = 2, y′(0) = 0.7. Convert y′′ = 0 to a ﬁrst-order system du/dt = Au:This 2 by 2 matrix A has only one eigenvector and cannot be diagonalized. ComputeeAt from the series I +At +··· and write the solution eAtu(0) starting from y(0) = 3,y′(0) = 4. Check that your (y,y′) satisﬁes y′′ = 0.8. Suppose the rabbit population r and the wolf population w are governed bydt = 4r −2wdt = r +w.(a) Is this system stable, neutrally stable, or unstable?(b) If initially r = 300 and w = 200, what are the populations at time t?(c) After a long time, what is the proportion of rabbits to wolves?9. Decide the stability of u′ = Au for the following matrices:10. Decide on the stability or instability of dv/dt = w, dw/dt = v. Is there a solution11. From their trace and determinant, at what time t do the following matrices changebetween stable with real eigenvalues, stable with complex eigenvalues, and unstable?",eigenvec_val
"Chapter 5 Eigenvalues and Eigenvectorsor matrix powers or eigenvalues—just as elimination steps were natural for Ax = b.Elimination multiplied A on the left by L−1, but not on the right by L. So U is notsimilar to A, and the pivots are not the eigenvalues.A whole family of matrices M−1AM is similar to A, and there are two questions:1. What do these similar matrices M−1AM have in common?2. With a special choice of M, what special form can be achieved by M−1AM?The ﬁnal answer is given by the Jordan form, with which the chapter ends.These combinations M−1AM arise in a differential or difference equation, when a“change of variables” u = Mv introduces the new unknown v:The new matrix in the equation is M−1AM. In the special case M = S, the system isuncoupled because Λ = S−1AS is diagonal. The eigenvectors evolve independently. Thisis the maximum simpliﬁcation, but other M’s are also useful. We try to make M−1AMeasier to work with than A.The family of matrices M−1AM includes A itself, by choosing M = I. Any of thesesimilar matrices can appear in the differential and difference equations, by the changeu = Mv, so they ought to have something in common, and they do: Similar matricesshare the same eigenvalues.Suppose that B = M−1AM. Then A and B have the same eigenvalues.Every eigenvector x of A corresponds to an eigenvector M−1x of B.Start from Ax = λx and substitute A = MBM−1:The eigenvalue of B is still λ. The eigenvector has changed from x to M−1x.We can also check that A−λI and B−λI have the same determinant:B−λI = M−1AM −λI = M−1(A−λI)Mdet(B−λI) = detM−1det(A−λI)detM = det(A−λI).The polynomials det(A − λI) and det(B − λI) are equal. Their roots—the eigenvaluesof A and B—are the same. Here are matrices B similar to A.",eigenvec_val
"If A is symmetric positive deﬁnite, then P(x) = 12xTAx − xTb reaches itsminimum at the point where Ax = b. At that point Pmin = −1Figure 6.4: The graph of a positive quadratic P(x) is a parabolic bowl.Proof. Suppose Ax = b. For any vector y, we show that P(y) ≥ P(x):(set b = Ax)This can’t be negative since A is positive deﬁnite—and it is zero only if y−x = 0. At allother points P(y) is larger than P(x), so the minimum occurs at x.Example 1. Minimize P(x) = x21 − x1x2 + x22 − b1x1 − b2x2. The usual approach, bycalculus, is to set the partial derivatives to zero. This gives Ax = b:∂P/∂x1 = 2x1 −x2 −b1 = 0∂P/∂x2 = −x1 +2x2 −b2 = 0Linear algebra recognizes this P(x) as 12xTAx−xTb, and knows immediately that Ax = bgives the minimum. Substitute x = A−1b into P(x):2xTAx is the internal energy and −xTb is the external work. The systemautomatically goes to x = A−1b, where the total energy P(x) is a minimum.Many applications add extra equations Cx = d on top of the minimization problem.These equations are constraints. We minimize P(x) subject to the extra requirementCx = d. Usually x can’t satisfy n equations Ax = b and also ℓ extra constraints Cx = d.We have too many equations and we need ℓ more unknowns.",pos_def_matrices
"Chapter 7 Computations with Matricesellipse of all Axcircle ∥x∥ = 1∥A∥ = 1 +∥A∥2 = λmax(ATA) ≈ 2.618∥A−1∥2 = λmin(ATA) ≈ 0.382c(A) = ∥A∥∥A−1∥ ≈ (1.618)2Figure 7.1: The norms of A and A−1 come from the longest and shortest Ax.Note 4. Roundoff error also enters Ax = λx. What is the condition number of the eigen-value problem? The condition number of the diagonalizing S measures the sensitivityof the eigenvalues. If µ is an eigenvalue of A + E, then its distance from one of theeigenvalues of A is|µ −λ| ≤ ∥S∥∥S−1∥∥E∥ = c(S)∥E∥.With orthonormal eigenvectors and S = Q, the eigenvalue problem is perfectly condi-tioned: c(Q) = 1. The change δλ in the eigenvalues is no greater than the change δA.Therefore the best case is when A is symmetric, or more generally when AAT = ATA.Then A is a normal matrix; its diagonalizing S is an orthogonal Q (Section 5.6).If xk is the kth column of S and yk is the kth row of S−1, then λk changes byδλk = ykExk +terms of order ∥E∥2.In practice, ykExk is a realistic estimate of δλ. The idea in every good algorithm is tokeep the error matrix E as small as possible—usually by insisting, as in the next section,on orthogonal matrices at every step of the computation of λ.1. For an orthogonal matrix Q, show that ∥Q∥ = 1 and also c(Q) = 1. Orthogonalmatrices (and their multiples αQ) are the only perfectly conditioned matrices.2. Which “famous” inequality gives ∥(A+B)x∥ ≤ ∥Ax∥+∥Bx∥, and why does it followfrom equation (5) that ∥A+B∥ ≤ ∥A∥+∥B∥?3. Explain why ∥ABx∥ ≤ ∥A∥∥B∥∥x∥, and deduce from equation (5) that ∥AB∥ ≤ ∥A∥∥B∥.Show that this also implies c(AB) ≤ c(A)c(B).",computations
"Chapter 5 Eigenvalues and EigenvectorsRemark 3. Other matrices S will not produce a diagonal Λ. Suppose the ﬁrst columnof S is y. Then the ﬁrst column of SΛ is λ1y. If this is to agree with the ﬁrst column ofAS, which by matrix multiplication is Ay, then y must be an eigenvector: Ay = λ1y. Theorder of the eigenvectors in S and the eigenvalues in Λ is automatically the same.Remark 4. Not all matrices possess n linearly independent eigenvectors, so not all ma-trices are diagonalizable. The standard example of a “defective matrix” isIts eigenvalues are λ1 = λ2 = 0, since it is triangular with zeros on the diagonal:All eigenvectors of this A are multiples of the vector (1,0):λ = 0 is a double eigenvalue—its algebraic multiplicity is 2. But the geometric multi-plicity is 1—there is only one independent eigenvector. We can’t construct S.Here is a more direct proof that this A is not diagonalizable. Since λ1 = λ2 = 0, Λwould have to be the zero matrix, But if Λ = S−1AS = 0, then we premultiply by S andpostmultiply by S−1, to deduce falsely that A = 0. There is no invertible S.That failure of diagonalization was not a result of λ = 0. It came from λ1 = λ2:Their eigenvalues are 3, 3 and 1, 1. They are not singular! The problem is the shortageof eigenvectors—which are needed for S. That needs to be emphasized:Diagonalizability of A depends on enough eigenvectors.Invertibility of A depends on nonzero eigenvalues.There is no connection between diagonalizability (n independent eigenvector) and in-vertibility (no zero eigenvalues). The only indication given by the eigenvalues is this:Diagonalization can fail only if there are repeated eigenvalues. Even then, it does notalways fail. A = I has repeated eigenvalues 1,1,...,1 but it is already diagonal! Thereis no shortage of eigenvectors in that case.The test is to check, for an eigenvalue that is repeated p times, whether there are pindependent eigenvectors—in other words, whether A−λI has rank n− p. To completethat circle of ideas, we have to show that distinct eigenvalues present no problem.",eigenvec_val
"Chapter 5 Eigenvalues and EigenvectorsNow we have the fundamental equation of this chapter. It involves two unknownsλ and x. It is an algebra problem, and differential equations can be forgotten! Thenumber λ (lambda) is an eigenvalue of the matrix A, and the vector x is the associatedeigenvector. Our goal is to ﬁnd the eigenvalues and eigenvectors, λ’s and x’s, and to useThe Solution of Ax = λxNotice that Ax = λx is a nonlinear equation; λ multiplies x. If we could discover λ, thenthe equation for x would be linear. In fact we could write λIx in place of λx, and bringthis term over to the left side:The identity matrix keeps matrices and vectors straight; the equation (A − λ)x = 0 isshorter, but mixed up. This is the key to the problem:The vector x is in the nullspace of A−λI.The number λ is chosen so that A−λI has a nullspace.Of course every matrix has a nullspace. It was ridiculous to suggest otherwise, but yousee the point. We want a nonzero eigenvector x, The vector x = 0 always satisﬁesAx = λx, but it is useless in solving differential equations. The goal is to build u(t) outof exponentials eλtx, and we are interested only in those particular values λ for whichthere is a nonzero eigenvector x. To be of any use, the nullspace of A−λI must containvectors other than zero. In short, A−λI must be singular.For this, the determinant gives a conclusive test.The number λ is an eigenvalue of A if and only if A−λI is singular:This is the characteristic equation. Each λ is associated with eigenvectors x:In our example, we shift A by λI to make it singular:Note that λ is subtracted only from the main diagonal (because it multiplies I).λ 2 −λ −2.This is the characteristic polynomial. Its roots, where the determinant is zero, are theeigenvalues. They come from the general formula for the roots of a quadratic, or from",eigenvec_val
"Chapter 8 Linear Programming and Game TheorySome linear problems have a structure that makes their solution very quick. Band ma-trices have all nonzeros close to the main diagonal, and Ax = b is easy to solve. In linearprogramming, we are interested in the special class for which A is an incidence matrix.Its entries are −1 or +1 or (mostly) zero, and pivot steps involve only additions andsubtractions. Much larger problems than usual can be solved.Networks enter all kinds of applications. Trafﬁc through an intersection satisﬁesKirchhoff’s current law: ﬂow in equals ﬂow out. For gas and oil, network programminghas designed pipeline systems that are millions of dollars cheaper than the intuitive (notoptimized) designs. Scheduling pilots and crews and airplanes has become a signiﬁcantproblem in applied mathematics! We even solve the marriage problem—to maximizethe number of marriages when brides have a veto. That may not be the real problem, butit is the one that network programming solves.The problem in Figure 8.5 is to maximize the ﬂow from the source to the sink. Theﬂows cannot exceed the capacities marked on the edges, and the directions given bythe arrows cannot be reversed. The ﬂow on the two edges into the sink cannot exceed6+1 = 7. Is this total of 7 achievable? What is the maximal ﬂow from left to right?The unknowns are the ﬂows xij from node i to node j. The capacity constraints arexij ≤ cij. The ﬂows are nonnegative: xij ≥ 0 going with the arrows. By maximizing thereturn ﬂow x61 (dotted line), we maximize the total ﬂow into the sink.Figure 8.5: A 6-node network with edge capacities: the maximal ﬂow problem.Another constraint is still to be heard from. It is the “conservation law,” that the ﬂowinto each node equals the ﬂow out. That is Kirchhoff’s current law:x jk = 0The ﬂows xij enter node j from earlier nodes i. The ﬂows x jk leave node j to laternodes k. The balance in equation (1) can be written as Ax = 0, where A is a node-edgeincidence matrix (the transpose of Section 2.5). A has a row for every node and a +1,",linear_prog
"38. If v1,...,vn is an orthonormal basis for Cn, the matrix with those columns is amatrix. Show that any vector z equals (vH39. The functions e−ix and e−ix are orthogonal on the interval 0 ≤ x ≤ 2π because theircomplex inner product is40. The vectors v = (1,i,1), w = (i,1,0) and z =are an orthogonal basis for41. If A = R+iS is a Hermitian matrix, are the real matrices R and S symmetric?42. The (complex) dimension of Cn is. Find a nonreal basis for Cn.43. Describe all 1 by 1 matrices that are Hermitian and also unitary. Do the same for 244. How are the eigenvalues of AH (square matrix) related to the eigenvalues of A?45. If uHu = 1, show that I −2uuH is Hermitian and also unitary. The rank-1 matrix uuHis the projection onto what line in Cn?46. If A+iB is a unitary matrix (A and B are real), show that Q =47. If A+iB is a Hermitian matrix (A and B are real), show that48. Prove that the inverse of a Hermitian matrix is again a Hermitian matrix.49. Diagonalize this matrix by constructing its eigenvalue matrix Λ and its eigenvector50. A matrix with orthonormal eigenvectors has the form A = UΛU−1 = UΛUH. Provethat AAH = AHA. These are exactly the normal matrices.Virtually every step in this chapter has involved the combination S−1AS. The eigenvec-tors of A went into the columns of S, and that made S−1AS a diagonal matrix (calledΛ). When A was symmetric, we wrote Q instead of S, choosing the eigenvectors to beorthonormal. In the complex case, when A is Hermitian we write U—it is still the matrixof eigenvectors. Now we look at all combinations M−1AM—formed with any invertibleM on the right and its inverse on the left. The invertible eigenvector matrix S may fail toexist (the defective case), or we may not know it, or we may not want to use it.First a new word: The matrices A and M−1AM are “similar”. Going from one tothe other is a similarity transformation. It is the natural step for differential equations",eigenvec_val
"Linear Programming and Game TheoryAlgebra is about equations, and analysis is often about inequalities. The line betweenthem has always seemed clear. But I have realized that this chapter is a counterexam-ple: linear programming is about inequalities, but it is unquestionably a part of linearalgebra. It is also extremely useful—business decisions are more likely to involve linearprogramming than determinants or eigenvalues.There are three ways to approach the underlying mathematics: intuitively throughthe geometry, computationally through the simplex method, or algebraically throughduality. These approaches are developed in Sections 8.1, 8.2, and 8.3. Then Section8.4 is about problems (like marriage) in which the solution is an integer. Section 8.5discusses poker and other matrix games. The MIT students in Bringing Down the Housecounted high cards to win at blackjack (Las Vegas follows ﬁxed rules, and a true matrixgame involves random strategies).Section 8.3 has something new in this fourth edition. The simplex method is nowin a lively competition with a completely different way to do the computations, calledan interior point method. The excitement began when Karmarkar claimed that hisversion was 50 times faster than the simplex method. (His algorithm, outlined in 8.2,was one of the ﬁrst to be patented—something we then believed impossible, and notreally desirable.) That claim brought a burst of research into methods that approachthe solution from the “interior” where all inequalities are strict: x ≥ 0 becomes x > 0.The result is now a great way to get help from the dual problem in solving the primalOne key to this chapter is to see the geometric meaning of linear inequalities. Aninequality divides n-dimensional space into a halfspace in which the inequality is satis-ﬁed, and a halfspace in which it is not. A typical example is x +2y ≥ 4. The boundarybetween the two halfspaces is the line x+2y = 4, where the inequality is “tight.” Figure8.1 would look almost the same in three dimensions. The boundary becomes a planelike x + 2y + z = 4, and above it is the halfspace x + 2y + z ≥ 4. In n dimensions, the",linear_prog
"5.4 Differential Equations and eAtThis decays for a < 0, it is constant for a = 0, and it explodes for a > 0. The imaginarypart is producing oscillations, but the amplitude comes from the real part.The differential equation du/dt = Au isstable and eAt → 0 whenever all Reλi < 0,neutrally stable when all Reλi ≤ 0 and Reλ1 = 0, andunstable and eAt is unbounded if any eigenvalue has Reλi > 0.In some texts the condition Reλ < 0 is called asymptotic stability, because it guaranteesdecay for large times t. Our argument depended on having n pure exponential solutions,but even if A is not diagonalizable (and there are terms like teλt) the result is still true:All solutions approach zero if and only if all eigenvalues have Reλ < 0.Stability is especially easy to decide for a 2 by 2 system (which is very common inapplications). The equation isand we need to know when both eigenvalues of that matrix have negative real parts.(Note again that the eigenvalues can be complex numbers.) The stability tests areThe trace a+d must be negative.The determinant ad −bc must be positive.When the eigenvalues are real, those tests guarantee them to be negative. Their productis the determinant; it is positive when the eigenvalues have the same sign. Their sum isthe trace; it is negative when both eigenvalues are negative.When the eigenvalues are a complex pair x ± iy, the tests still succeed. The traceis their sum 2x (which is < 0) and the determinant is (x + iy)(x − iy) = x2 + y2 > 0.Figure 5.2 shows the one stable quadrant, trace < 0 and determinant > 0. It also showsthe parabolic boundary line between real and complex eigenvalues. The reason for theparabola is in the quadratic equation for the eigenvalues:= λ 2 −(trace)λ +(det) = 0.The quadratic formula for λ leads to the parabola (trace)2 = 4(det):λ1 and λ2 = 1Above the parabola, the number under the square root is negative—so λ is not real. Onthe parabola, the square root is zero and λ is repeated. Below the parabola the squareroots are real. Every symmetric matrix has real eigenvalues, since if b = c, then(trace)2 −4(det) = (a+d)2 −4(ad −b2) = (a−d)2 +4b2 ≥ 0.",eigenvec_val
"Chapter 8 Linear Programming and Game TheoryThe step ∆x is a multiple of the projection −Pc. The longer the step, the more thecost is reduced—but we cannot go out of the feasible set. The multiple of −Pc is chosenso that x1 is close to, but a little inside, the boundary at which a component of x reachesThat completes the ﬁrst idea—the projection that gives the steepest feasible descent.The second step needs a new idea. since to continue in the same direction is useless.Karmarkar’s suggestion is to transform x1 back to (1,1,...,1) at the center. Hischange of variables was nonlinear, but the simplest transformation is just a rescaling bya diagonal matrix D. Then we have room to move. The rescaling from x to X = D−1xchanges the constraint and the cost:Therefore the matrix AD takes the place of A, and the vector cTD takes the place of cT.The second step projects the new c onto the nullspace of the new A. All the work is inthis projection, to solve the weighted normal equations:The normal way to compute y is by elimination. Gram-Schmidt will orthogonalize thecolumns of DAT, which can be expensive (although it makes the rest of the calculationeasy). The favorite for large sparse problems is the conjugate gradient method, whichgives the exact answer y more slowly than elimination, but you can go part way and thenstop. In the middle of elimination you cannot stop.Like other new ideas in scientiﬁc computing, Karmarkar’s method succeeded on someproblems and not on others. The underlying idea was analyzed and improved. Newerinterior point methods (staying inside the feasible set) are a major success—mentionedin the next section. And the simplex method remains tremendously valuable. like thewhole subject of linear programming—which was discovered centuries after Ax = b, butshares the fundamental ideas of linear algebra. The most far-reaching of those ideas isduality, which comes next.1. Minimize x1 +x2 −x3, subject to2x1 −4x2 +x3 +x4Which of x1, x2, x3 should enter the basis, and which of x4, x5 should leave? Computethe new pair of basic variables, and ﬁnd the cost at the new corner.2. After the preceding simplex step, prepare for and decide on the next step.",linear_prog
"4.4 Applications of Determinants35. An n-dimensional cube has how many corners? How many edges? How many (n−1)-dimensional faces? The n-cube whose edges are the rows of 2I has volumeA hypercube computer has parallel processors at the corners with connections along36. The triangle with corners (0,0), (1,0), (0,1) has area 12. The pyramid with fourcorners (0,0,0), (1,0,0), (0,1,0), (0,0,1) has volume. The pyramid in R4with ﬁve corners at (0,0,0,0) and the rows of I has what volume?Problems 37–40 are about areas dA and volumes dV in calculus.37. Polar coordinates satisfy x = rcosθ and y = rsinθ. Polar area J dr dθ includes J:The two columns are orthogonal. Their lengths are. Thus J =38. Spherical coordinates ρ, φ, θ give x = ρ sinφ cosθ, y = ρ sinφ sinθ, z = ρ cosφ.Find the Jacobian matrix of 9 partial derivatives: ∂x/∂ρ, ∂x/∂φ, ∂x/∂θ are in row1. Simplify its determinant to J = ρ2sinφ. Then dV = ρ2sinφ dρ dφ dθ.39. The matrix that connects r, θ to x, y is in Problem 37. Invert that matrix:It is surprising that ∂r/∂x = ∂x/∂r. The product JJ−1 = I gives the chain rule40. The triangle with corners (0,0), (6,0), and (1,4) has area. When you rotate itby θ = 60° the area is. The rotation matrix has41. Let P = (1,0,−1), Q = (1,1,1), and R = (2,2,1). Choose S so that PQRS is aparallelogram, and compute its area. Choose T, U, V so that OPQRSTUV is a tiltedbox, and compute its volume.42. Suppose (x,y,z), (1,1,0), and (1,2,1) lie on a plane through the origin. What deter-minant is zero? What equation does this give for the plane?43. Suppose (x,y,z) is a linear combination of (2,3,1) and (1,2,3). What determinantis zero? What equation does this give for the plane of all combinations?",determinants
"8.3 The Dual Problem8. Verify that the vectors in the previous exercise satisfy the complementary slacknessconditions in equation (2), and ﬁnd the one slack inequality in both the primal and9. Suppose that A =, and c =. Find the optimal x and y, and verifythe complementary slackness conditions (as well as yb = cx).10. If the primal problem is constrained by equations instead of inequalities—Minimizecx subject to Ax = b and x ≥ 0—then the requirement y ≥ 0 is left out of the dual:Maximize yb subject to yA ≤ c. Show that the one-sided inequality yb ≤ cx stillholds. Why was y ≥ 0 needed in equation (1) but not here? This weak duality can becompleted to full duality.11. (a) Without the simplex method, minimize the cost 5x1 +3x2 +4x3, subject to x1 +x2 +x3 ≥ 1, x1 ≥ 0, x2 ≥ 0, x3 ≥ 0.(b) What is the shape of the feasible set?(c) What is the dual problem, and what is its solution y?12. If the primal has a unique optimal solution x∗, and then c is changed a little, explainwhy x∗ still remains the optimal solution.13. Write the dual of the following problem: Maximize x1+x2+x3 subject to 2x1+x2 ≤4, x3 ≤ 6. What are the optimal x∗ and y∗ (if they exist!)?14. If A =, describe the cone of nonnegative combinations of the columns. If b liesinside that cone, say b = (3,2), what is the feasible vector x? If b lies outside, sayb = (0,1), what vector y will satisfy the alternative?15. In three dimensions, can you ﬁnd a set of six vectors whose cone of nonnegativecombinations ﬁlls the whole space? What about four vectors?16. Use 8H to show that the following equation has no solution, because the alternative17. Use 8I to show that there is no solution x ≥ 0 (the alternative holds):18. Show that the alternatives in 8J (Ax ≥ b, x ≥ 0, yA ≥ 0, yb < 0, y ≤ 0) cannot both",linear_prog
"8.2 The Simplex Methodcolumns 1 and 3 to put basic variables before free variables:Then, elimination multiplies the ﬁrst row by −1, to give a unit pivot, and uses the secondrow to produce zeros in the second column:Fully reduced at PLook ﬁrst at r = [−1 1] in the bottom row. It has a negative entry in column 3, so thethird variable will enter the basis. The current corner P and its cost +6 are not optimal.The column above that negative entry is B−1u = (3,2); its ratios with the last column2. Since the ﬁrst ratio is smaller, the ﬁrst unknown w (and the ﬁrst column ofthe tableau) is pushed out of the basis. We move along the feasible set from corner P tocorner Q in Figure 8.3.The new tableau exchanges columns 1 and 3, and pivoting by elimination givesIn that new tableau at Q, r = [13] is positive. The stopping test is passed. The cornerx = y = 2 and its cost +4 are optimal.The Organization of a Simplex StepThe geometry of the simplex method is now expressed in algebra—“corners” are “basicfeasible solutions.” The vector r and the ratio α are decisive. Their calculation is theheart of the simplex method, and it can be organized in three different ways:1. In a tableau, as above.2. By updating B−1 when column u taken from N replaces column k of B.3. By computing B = LU, and updating these LU factors instead of B−1.This list is really a brief history of the simplex method, In some ways, the mostfascinating stage was the ﬁrst—the tableau—which dominated the subject for so manyyears. For most of us it brought an aura of mystery to linear programming, chieﬂybecause it managed to avoid matrix notation almost completely (by the skillful device of",linear_prog
"1.3 An Example of Gaussian Elimination1. What multiple ℓ of equation 1 should be subtracted from equation 2?10x + 9y = 11.After this elimination step, write down the upper triangular system and circle the twopivots. The numbers 1 and 11 have no inﬂuence on those pivots.2. Solve the triangular system of Problem 1 by back-substitution, y before x. Verifythat x times (2,10) plus y times (3,9) equals (1,11). If the right-hand side changesto (4,44), what is the new solution?3. What multiple of equation 2 should be subtracted from equation 3?−x + 5y = 0.After this elimination step, solve the triangular system. If the right-hand side changesto (−6,0), what is the new solution?4. What multiple ℓ of equation 1 should be subtracted from equation 2?ax + by =+ dy = g.The ﬁrst pivot is a (assumed nonzero). Elimination produces what formula for thesecond pivot? What is y? The second pivot is missing when ad = bc.5. Choose a right-hand side which gives no solution and another right-hand side whichgives inﬁnitely many solutions. What are two of those solutions?3x + 2y =6x + 4y =6. Choose a coefﬁcient b that makes this system singular. Then choose a right-handside g that makes it solvable. Find two solutions in that singular case.2x + by = 164x + 8y =7. For which numbers a does elimination break down (a) permanently, and (b) tem-ax + 3y = −34x + 6y =Solve for x and y after ﬁxing the second breakdown by a row exchange.",gauss elim
"Chapter 5 Eigenvalues and Eigenvectors29. The powers Ak approach zero if all |λi| < 1, and they blow up if any |λi| > 1. PeterLax gives four striking examples in his book Linear Algebra.Find the eigenvalues λ = eiθ of B and C to show that B4 = I and C3 = −I.Differential Equations and eAtWherever you ﬁnd a system of equations, rather than a single equation, matrix theoryhas a part to play. For difference equations, the solution uk = Aku0 depended on the owenof A. For differential equations, the solution u(t) = eAtu(0) depends on the exponentialof A. To deﬁne this exponential. and to understand it, we turn right away to an example:dt = Au =The ﬁrst step is always to ﬁnd the eigenvalues (ł1 and −3) and the eigenvectors:Then several approaches lead to u(t). Probably the best is to match the general solutionto the initial vector u(0) at t = 0.The general solution is a combination of pure exponential solutions. These are so-lutions of the special form ceλtx, where λ is an eigenvalue of A and x is its eigenvec-tor. These pure solutions satisfy the differential equation, since d/dt(ceλtx) = A(ceλtx).(They were our introduction to eigenvalues at the start of the chapter.) In this 2 by 2example, there are two pure exponentials to be combined:u(t) = c1eλ1tx1 +c2eλ2tx2At time zero, when the exponentials are e0 = 1, u(0) determines c1 and c2:u(0) = c1x1 +c2x2 =You recognize S, the matrix of eigenvectors. The constants c = S−1u(0) are the same asthey were for difference equations. Substituting them back into equation (2), the solution",eigenvec_val
"1.5 Triangular Factors and Row Exchangespivots. This is easy to correct. Divide out of U a diagonal pivot matrix D:In the last example all pivots were di = 1. In that case D = I. But that was very excep-tional, and normally LU is different from LDU (also written LDV).The triangular factorization can be written A = LDU, where L and U have 1s onthe diagonal and D is the diagonal matrix of pivots.Whenever you see LDU or LDV, it is understood that U or V has is on the diagonal—each row was divided by the pivot in D. Then L and U are treated evenly. An exampleof LU splitting into LDU isThat has the 1s on the diagonals of L and U, and the pivots 1 and −2 in D.Remark 2. We may have given the impression in describing each elimination step, thatthe calculations must be done in that order. This is wrong. There is some freedom, andthere is a “Crout algorithm” that arranges the calculations in a slightly different way.There is no freedom in the ﬁnal L, D, and U. That is our main point:If A = L1D1U1 and also A = L2D2U2, where the L’s are lower triangularwith unit diagonal, the U’s are upper triangular with unit diagonal, and theD’s are diagonal matrices with no zeros on the diagonal, then L1 = L2, D1 =D2, U1 = U2. The LDU factorization and the LU factorization are uniquelyThe proof is a good exercise with inverse matrices in the next section.Row Exchanges and Permutation MatricesWe now have to face a problem that has so far been avoided: The number we expect touse as a pivot might be zero. This could occur in the middle of a calculation. It willhappen at the very beginning if a11 = 0. A simple example isZero in the pivot positionThe difﬁculty is clear; no multiple of the ﬁrst equation will remove the coefﬁcient 3.",gauss elim
"Chapter 1 Matrices and Gaussian Eliminationin row i, column j of AT comes from row j, column i of A:(AT)ij = A ji.The transpose of a lower triangular matrix is upper triangular. The transpose of AT bringsus back to A.If we add two matrices and then transpose, the result is the same as ﬁrst transposingand then adding: (A+B)T is the same as AT +BT. But what is the transpose of a productAB or an inverse A−1? Those are the essential formulas of this section:(i) The transpose of AB is (AB)T = BTAT,(ii) The transpose of A−1 is (A−1)T = (AT)−1.Notice how the formula for (AB)T resembles the one for (AB)−1. In both cases wereverse the order, giving BTAT and B−1A−1. The proof for the inverse was easy, but thisone requires an unnatural patience with matrix multiplication. The ﬁrst row of (AB)T isthe ﬁrst column of AB. So the columns of A are weighted by the ﬁrst column of B. Thisamounts to the rows of AT weighted by the ﬁrst row of BT. That is exactly the ﬁrst rowof BTAT. The other rows of (AB)T and BTAT also agree.To establish the formula for (A−1)T, start from AA−1 = I and A−1A = I and take trans-poses. On one side, IT = I. On the other side, we know from part (i) the transpose of aproduct. You see how (A−1)T is the inverse of AT, proving (ii):Inverse of AT = Transpose of A−1With these rules established, we can introduce a special class of matrices, probablythe most important class of all. A symmetric matrix is a matrix that equals its owntranspose: AT = A. The matrix is necessarily square. Each entry on one side of thediagonal equals its “mirror image” on the other side: aij = a ji. Two simple examples areA and D (and also A−1):",gauss elim
"3.4 Orthogonal Bases and Gram-SchmidtSuppose you are given three independent vectors a, b, c. If they are orthonormal, life iseasy. To project a vector v onto the ﬁrst one, you compute (aTv)a. To project the samevector v onto the plane of the ﬁrst two, you just add (aTv)a + (bTv)b. To project ontothe span of a, b, c, you add three projections. All calculations require only the innerproducts aTv, bTv, and cTv. But to make this true, we are forced to say, “If they areorthonormal.” Now we propose to ﬁnd a way to make them orthonormal.The method is simple. We are given a, b, c and we want q1, q2, q3. There is noproblem with q1: it can go in the direction of a. We divide by the length, so that q1 =a/∥a∥ is a unit vector. The real problem begins with q2—which has to be orthogonalto q1. If the second vector b has any component in the direction of q1 (which is thedirection of a), that component has to be subtracted:B is orthogonal to q1. It is the part of b that goes in a new direction, and not in the a. InFigure 3.10, B is perpendicular to q1. It sets the direction for q2.Figure 3.10: The qi component of b is removed; a and B normalized to q1 and q2.At this point q1 and q2 are set. The third orthogonal direction starts with c. It willnot be in the plane of q1 and q2, which is the plane of a and b. However, it may have acomponent in that plane, and that has to be subtracted. (If the result is C = 0, this signalsthat a, b, c were not independent in the ﬁrst place) What is left is the component C wewant, the part that is in a new direction perpendicular to the plane:This is the one idea of the whole Gram-Schmidt process, to subtract from every newvector its components in the directions that are already settled. That idea is used overand over again.3 When there is a fourth vector, we subtract away its components in thedirections of q1, q2, q3.3If Gram thought of it ﬁrst, what was left for Schmidt?",orthogonality
"Chapter 1 Matrices and Gaussian EliminationRemark. One good way to write down the forward elimination steps is to include theright-hand side as an extra column. There is no need to copy u and v and w and = atevery step, so we are left with the bare minimum:0 −8 −2 −120 −8 −2 −12At the end is the triangular system, ready for back-substitution. You may prefer thisarrangement, which guarantees that operations on the left-hand side of the equations arealso done on the right-hand side—because both sides are there together.In a larger problem, forward elimination takes most of the effort. We use multiplesof the ﬁrst equation to produce zeros below the ﬁrst pivot. Then the second column iscleared out below the second pivot. The forward step is ﬁnished when the system istriangular; equation n contains only the last unknown multiplied by the last pivot. Back-substitution yields the complete solution in the opposite order—beginning with the lastunknown, then solving for the next to last, and eventually for the ﬁrst.By deﬁnition, pivots cannot be zero. We need to divide by them.The Breakdown of EliminationUnder what circumstances could the process break down? Something must go wrongin the singular case, and something might go wrong in the nonsingular case. This mayseem a little premature—after all, we have barely got the algorithm working. But thepossibility of breakdown sheds light on the method itself.The answer is: With a full set of n pivots, there is only one solution. The system isnon singular, and it is solved by forward elimination and back-substitution. But if a zeroappears in a pivot position, elimination has to stop—either temporarily or permanently.The system might or might not be singular.If the ﬁrst coefﬁcient is zero, in the upper left corner, the elimination of u from theother equations will be impossible. The same is true at every intermediate stage. Noticethat a zero can appear in a pivot position, even if the original coefﬁcient in that placewas not zero. Roughly speaking, we do not know whether a zero will appear until wetry, by actually going through the elimination process.In many cases this problem can be cured, and elimination can proceed. Such a systemstill counts as nonsingular; it is only the algorithm that needs repair. In other cases abreakdown is unavoidable. Those incurable systems are singular, they have no solutionor else inﬁnitely many, and a full set of pivots cannot be found.",gauss elim
"Chapter 2 Vector Spacesreﬂected across the 45° line. It was split into (2,2) + (2,−2) and the two parts werereﬂected separately. The same could be done for projections: split, project separately,and add the projections. These rules apply to any transformation that comes from aTheir importance has earned them a name: Transformations that obey rules (i)–(iii)are called linear transformations. The rules can be combined into one requirement:For all numbers c and d and all vectors x and y, matrix multiplicationsatisﬁes the rule of linearity:Every transformation T(x) that meets this requirement is a linear transforma-Any matrix leads immediately to a linear transformation. The more interesting questionis in the opposite direction: Does every linear transformation lead to a matrix? Theobject of this section is to ﬁnd the answer, yes. This is the foundation of an approachto linear algebra—starting with property (1) and developing its consequences—that ismuch more abstract than the main approach in this book. We preferred to begin directlywith matrices, and now we see how they represent linear transformations.A transformation need not go from Rn to the same space Rn. It is absolutely permittedto transform vectors in Rn to vectors in a different space Rm. That is exactly what is doneby an m by n matrix! The original vector x has n components, and the transformed vectorAx has m components. The rule of linearity is equally satisﬁed by rectangular matrices,so they also produce linear transformations.Having gone that far, there is no reason to stop. The operations in the linearity con-dition (1) are addition and scalar multiplication, but x and y need not be column vectorsin Rn. Those are not the only spaces. By deﬁnition, any vector space allows the com-binations cx + dy—the “vectors” are x and y, but they may actually be polynomials ormatrices or functions x(t) and y(t). As long as the transformation satisﬁes equation (1),We take as examples the spaces Pn, in which the vectors are polynomials p(t) ofdegree n. They look like p = a0 +a1t +···+antn, and the dimension of the vector spaceis n+1 (because with the constant term, there are n+1 coefﬁcients).Example 1. The operation of differentiation, A = d/dt, is linear:dt (a0 +a1t +···+antn) = a1 +···+nantn−1.The nullspace of this A is the one-dimensional space of constants: da0/dt = 0. Thecolumn space is the n-dimensional space Pn−1; the right-hand side of equation (2) isalways in that space. The sum of nullity (= 1) and rank (= n) is the dimension of the",vector spaces
"6.2 Tests for Positive DeﬁnitenessThe Law of InertiaFor elimination and eigenvalues, matrices become simpler by elementary operationsThe essential thing is to know which properties of the matrix stay unchanged. Whena multiple of one row is subtracted from another, the row space, nullspace. rant anddeterminant all remain the same. For eigenvalues, the basic operation was a similaritytransformation A → S−1AS (or A → M−1AM). The eigenvalues are unchanged (and alsothe Jordan form). Now we ask the same question for symmetric matrices: What are theelementary operations and their invariants for xTAx?The basic operation on a quadratic form is to change variables. A new vector y isrelated to x by some nonsingular matrix, x =Cy. The quadratic form becomes yTCTACy.This shows the fundamental operation on A:for some nonsingular C.The symmetry of A is preserved, since CTAC remains symmetric. The real question is,What other properties are shared by A and CTAC? The answer is given by Sylvester’s6F CTAC has the same number of positive eigenvalues, negative eigenvalues,and zero eigenvalues as A.The signs of the eigenvalues (and not the eigenvalues themselves) are preserved by acongruence transformation. In the proof, we will suppose that A is nonsingular. ThenCTAC is also nonsingular, and there are no zero eigenvalues to worry about. (Otherwisewe can work with the nonsingular A+ εI and A−εI, and at the end let ε → 0.)Proof. We want to borrow a trick from topology. Suppose C is linked to an orthogonalmatrix Q by a continuous chain of nonsingular matrices C(t). At t = 0 and t = 1, C(0) =C and C(1) = Q. Then the eigenvalues of C(t)TAC(t) will change gradually, as t goesfrom 0 to 1, from the eigenvalues of CTAC to the eigenvalues of QTAQ. Because C(t) isnever singular, none of these eigenvalues can touch zero (not to mention cross over it!).Therefore the number of eigenvalues to the right of zero, and the number to the left, isthe same for CTAC as for QTAQ. And A has exactly the same eigenvalues as the similarmatrix Q−1AQ = QTAQ.One good choice for Q is to apply Gram-Schmidt to the columns of C. Then C = QR,and the chain of matrices is C(t) = tQ+(1−t)QR. The family C(t) goes slowly throughGram-Schmidt, from QR to Q. It is invertible, because Q is invertible and the triangularfactor tI +(1−t)R has positive diagonal. That ends the proof.Example 4. Suppose A = I. Then CTAC = CTC is positive deﬁnite. Both I and CTChave n positive eigenvalues, conﬁrming the law of inertia.Example 5. If A =, then CTAC has a negative determinant:detCTAC = (detCT)(detA)(detC) = −(detC)2 < 0.",pos_def_matrices
"Chapter 2 Vector SpacesThe full picture uses elimination and pivot columns to ﬁnd the column space, nullspace,and rank. The 3 by 4 matrix A has rank 2:1. Reduce [A b] to [U c], to reach a triangular system Ux = c.2. Find the condition on b1, b2, b3 to have a solution.3. Describe the column space of A: Which plane in R3?4. Describe the nullspace of A: Which special solutions in R4?5. Find a particular solution to Ax = (0,6,−6) and the complete xp +xn.6. Reduce [U c] to [R d]: Special solutions from R and xp from d.Solution. (Notice how the right-hand side is included as an extra column!)1. The multipliers in elimination are 2 and 3 and −1, taking [A b] to [U c].2 4 8 123 6 7 130 0 −2 −21 2 3 50 0 2 20 0 0 0b3 + b2 − 5b12. The last equation shows the solvability condition b3 +b2 −5b1 = 0. Then 0 = 0.3. The column space of A is the plane containing all combinations of the pivot columnsSecond description: The column space contains all vectors with b3+b2−5b1 = 0.That makes Ax = b solvable, so b is in the column space. All columns of A pass thistest b3 +b2 −5b1 = 0. This is the equation for the plane (in the ﬁrst description of4. The special solutions in N have free variables x2 = 1, x4 = 0 and x2 = 0, x4 = 1:Special solutions to Ax = 0Back-substitution in Ux = 0Just switch signs in Rx = 0",vector spaces
"Chapter 2 Vector Spacesto show how it can be broken into simple pieces. For linear algebra, the simple piecesare matrices of rank 1:Every row is a multiple of the ﬁrst row, so the row space is one-dimensional. In fact, wecan write the whole matrix as the product of a column vector and a row vector:The product of a 4 by 1 matrix and a 1 by 3 matrix is a 4 by 3 matrix. This product hasrank 1. At the same time, the columns are all multiples of the same column vector; thecolumn space shares the dimension r = 1 and reduces to a line.Every matrix of rank 1 has the simple form A = uvT = column times row.The rows are all multiples of the same vector vT, and the columns are all multiples of u.The row space and column space are lines—the easiest case.1. True or false: If m = n, then the row space of A equals the column space. If m < n,then the nullspace has a larger dimension than2. Find the dimension and construct a basis for the four subspaces associated with each0 1 4 00 2 8 00 1 4 00 0 0 03. Find the dimension and a basis for the four fundamental subspaces for1 2 0 10 1 1 01 2 0 11 2 0 10 1 1 00 0 0 0",vector spaces
"Chapter 1 Matrices and Gaussian EliminationWe return to the fact that A is tridiagonal. What effect does this have on elimination?The ﬁrst stage of the elimination process produces zeros below the ﬁrst pivot:on A: Step 1Compared with a general 5 by 5 matrix, that step displays two major simpliﬁcations:1. There was only one nonzero entry below the pivot.2. The pivot row was very short.The multiplier ℓ21 = −12 came from one division. The new pivot 32 came from a singlemultiplication-subtraction. Furthermore, the tridiagonal pattern is preserved: Everystage of elimination admits the simpliﬁcations (a) and (b).The ﬁnal result is the LDU = LDLT factorization of A. Notice the pivots!The L and U factors of a tridiagonal matrix are bidiagonal. The three factors togetherhave the same band structure of three essential diagonals (3n−2 parameters) as A. Notetoo that L and U are transposes of one another, as expected from the symmetry. Thepivots 2/1, 3/2, 4/3, 5/4, 6/5 are all positive. Their product is the determinant of A:detA = 6. The pivots are obviously converging to 1, as n gets large. Such matrices makea computer very happy.These sparse factors L and U completely change the usual operation count. Elimina-tion on each column needs only two operations, as above, and there are n columns. Inplace of n3/3 operations we need only 2n. Tridiagonal systems Ax = b can be solvedalmost instantly. The cost of solving a tridiagonal system is proportional to n.A band matrix has aij = 0 except in the band |i − j| < w (Figure 1.8). The “halfbandwidth” is w = 1 for a diagonal matrix, w = 2 for a tridiagonal matrix, and w = nfor a full matrix. For each column, elimination requires w(w − 1) operations: a rowof length w acts on w − 1 rows below. Elimination on the n columns of a band matrixrequires about w2n operations.As w approaches n, the matrix becomes full, and the count is roughly n3. For an exactcount, the lower right-hand corner has no room for bandwidth w. The precise number ofdivisions and multiplication-subtractions that produce L, D, and U (without assuming a",gauss elim
"Chapter 5 Eigenvalues and Eigenvectorsλ1 = λ2 andboth Reλ > 0T 2 = 4Dboth λ > 0both Reλ < 0both λ < 0det < 0 gives λ1 < 0 and λ2 > 0: real and unstableFigure 5.2: Stability and instability regions for a 2 by 2 matrix.For complex eigenvalues, b and c have opposite signs and are sufﬁciently large.Example 2. One from each quadrant: only #2 is stable:On the boundaries of the second quadrant, the equation is neutrally stable. On the hori-zontal axis, one eigenvalue is zero (because the determinant is λ1λ2 = 0). On the verticalaxis above the origin, both eigenvalues are purely imaginary (because the trace is Zero).Crossing those axes are the two ways that stability is lost.The n by n case is more difﬁcult. A test for Reλi < 0 came from Routh and Hurwitz,who found a series of inequalities on the entries aij. I do not think this approach ismuch good for a large matrix; the computer can probably ﬁnd the eigenvalues with morecertainty than it can test these inequalities. Lyapunov’s idea was to ﬁnd a weightingmatrix W so that the weighted length ∥Wu(t)∥ is always decreasing. If there existssuch a W, then ∥Wu∥ will decrease steadily to zero, and after a few ups and downs umust get there too (stability). The real value of Lyapunov’s method is for a nonlinearequation—then stability can be proved without knowing a formula for u(t).Example 3. du/dt =u sends u(t) around a circle, starting from u(0) = (1,0).Since trace = 0 and det = 1, we have purely imaginary eigenvalues:= λ 2 +1 = 0λ = +i and −i.The eigenvectors are (1,−i) and (1,i). and the solution is",eigenvec_val
"4. By applying row operations to produce an upper triangular U, computeExchange rows 3 and 4 of the second matrix and recompute the pivots and determi-Note. Some readers will already know a formula for 3 by 3 determinants. It has sixterms (equation (2) of the next section), three going parallel to the main diagonal andthree others going the opposite way with minus signs. There is a similar formula for4 by 4 determinants, but it contains 4! = 24 terms (not just eight). You cannot evenbe sure that a minus sign goes with the reverse diagonal, as the next exercises show.5. Count row exchanges to ﬁnd these determinants:0 0 0 10 0 1 00 1 0 01 0 0 00 1 0 00 0 1 00 0 0 11 0 0 06. For each n, how many exchanges will put (row n, row n − 1,..., row 1) into thenormal order (row 1, ... , row n − 1, row n)? Find detP for the n by n permutationwith 1s on the reverse diagonal. Problem 5 had n = 4.7. Find the determinants of:(a) a rank one matrix(b) the upper triangular matrix4 4 8 80 1 2 20 0 2 60 0 0 2(c) the lower triangular matrix UT.(d) the inverse matrix U−1.",determinants
"7.2 Matrix Norm and Condition NumberWe must expect a violent change in the solution from ordinary changes in the data.Chapter 1 compared the equations Ax = b and Ax′ = b′:u + 1.0001v = 2u + 1.0001v = 2.0001.The right-hand sides are changed only by ∥δb∥ = .0001 = 10−4. At the same time, thesolution goes from u = 2, v = 0 to u = v = 1. This is a relative error ofWithout having made any special choice of the perturbation, there was a relatively largechange in the solution. Our x and δb make 45° angles with the worst cases, whichaccounts for the missing 2 between 2·104 and the extreme possibility c = 4·104.If A = I or even if A = I/10, its condition number is c = λmax/λmin = 1. By compari-son, the determinant is a terrible measure of ill-conditioning. It depends not only on thescaling but also on the order n; if A = I/10, then the determinant of A is 10−n. In fact,this “nearly singular” matrix is as well-conditioned as possible.Example 2. The n by n ﬁnite difference matrix A has λmax ≈ 4 and λmin ≈ π2/n2:The condition number is approximately c(A) = 12n2, and this time the dependence on theorder n is genuine. The better we approximate −u′′ = f, by increasing the number ofunknowns, the harder it is to compute the approximation. At a certain crossover point,an increase in n will actually produce a poorer answer.Fortunately for the engineer, this crossover occurs where the accuracy is alreadypretty good. Working in single precision, a typical computer might make roundoff errorsof order 10−9. With n = 100 unknowns and c = 5000, the error is ampliﬁed at most tobe of order 10−5—which is still more accurate than any ordinary measurements. Butthere will be trouble with 10,000 unknowns, or with a 1, −4, 6, −4, 1 approximation tod4u/dx4 = f(x)—for which the condition number grows as n4.1Our analysis so far has applied to symmetric matrices with positive eigenvalues. Wecould easily drop the positivity assumption, and use absolute values |λ|. But to go1The usual rule of thumb, experimentally veriﬁed, is that the computer can lose logc decimal places to theroundoff errors in Gaussian elimination.",computations
"Chapter 8 Linear Programming and Game TheoryFor linear programming, the important alternatives come when the constraints areinequalities. When is the feasible set empty (no x)?8J Ax ≥ b has a solution x ≥ 0there is a y ≤ 0 with yA ≥ 0 and yb < 0.Proof. The slack variables w = Ax−b change Ax ≥ b into an equation. Use 8I:for some y withIt is this result that leads to a “nonconstructive proof” of the duality theorem.1. What is the dual of the following problem: Minimize x1 + x2, subject to x1 ≥ 0,x2 ≥ 0, 2x1 ≥ 4, x1 +3x2 ≥ 11? Find the solution to both this problem and its dual,and verify that minimum equals maximum.2. What is the dual of the following problem: Maximize y2 subject to y1 ≥ 0, y2 ≥ 0,y1 +y2 ≤ 3? Solve both this problem and its dual.3. Suppose A is the identity matrix (so that m = n), and the vectors b and c are nonnega-tive. Explain why x∗ = b is optimal in the minimum problem, ﬁnd y∗ in the maximumproblem, and verify that the two values are the same. If the ﬁrst component of b isnegative, what are x∗ and y∗?4. Construct a 1 by 1 example in which Ax ≥ b, x ≥ 0 is unfeasible, and the dual problem5. Starting with the 2 by 2 matrix A =, choose b and c so that both of the feasiblesets Ax ≥ b, x ≥ 0 and yA ≤ c, y ≥ 0 are empty.6. If all entries of A, b, and c are positive, show that both the primal and the dual are7. Show that x = (1,1,1,0) and y = (1,1,0,1) are feasible in the primal and dual, with0 0 1 00 1 0 01 1 1 11 0 0 1Then, after computing cx and yb, explain how you know they are optimal.",linear_prog
"functions of mathematics should come together in such a graceful way. Our best answerwas to look at the power series for the exponential:eiθ = 1+iθ + (iθ)2The real part 1−θ 2/2+··· is cosθ. The imaginary part θ −θ 3/6+··· is the sine, Theformula is correct, and I wish we had sent a more beautiful proof.With this formula, we can solve wn = 1. It becomes einθ = 1, so that nθ must carryus around the unit circle and back to the start. The solution is to choose θ = 2π/n: The“primitive” nth root of unity iswn = e2πi/n = cos 2πIts nth power is e2πi, which equals 1. For n = 8, this root is (1+i)/w4 = cos πw8 = cos πThe fourth root is at θ = 90°, which is 14(360°). The other fourth roots are the powersi2 = −1, i3 = −i, and i4 = 1. The other eighth roots are the powers w2roots are equally spaced around the unit circle, at intervals of 2π/n. Note again that thesquare of w8 is w4, which will be essential in the Fast Fourier Transform. The roots addup to zero. First 1+i−1−i = 0, and thenSum of eighth rootsOne proof is to multiply the left side by w8, which leaves it unchanged. (It yields w8 +8 equals 1.) The eight points each move through 45°, but they remainthe same eight points. Since zero is the only number that is unchanged when multipliedby w8, the sum must be zero. When n is even the roots cancel in pairs (like 1 + i2 = 0and i+i3 = 0). But the three cube roots of 1 also add to zero.The Fourier Matrix and Its InverseIn the continuous case, the Fourier series can reproduce f(x) over a whole interval. Ituses inﬁnitely many sines and cosines (or exponentials). In the discrete case, with onlyn coefﬁcients c0,...,cn−1 to choose, we only ask for equality at n points. That gives nequations. We reproduce the four values y = 2,4,6,8 when Fc = y:",orthogonality
"gives a family of parallel lines. The minimum cost comes when the ﬁrst line intersectsthe feasible set. That intersection occurs at B, where x∗ = 0 and y∗ = 2; the minimumcost is 2x∗ +3y∗ = 6. The vector (0,2) is feasible because it lies in the feasible set, it isoptimal because it minimizes the cost function, and the minimum cost 6 is the value ofthe program. We denote optimal vectors by an asterisk.x + 2y ≥ 42x + 3y = 62x + 3y = 0Figure 8.2: The feasible set with ﬂat sides, and the costs 2x+3y, touching at B.The optimal vector occurs at a corner of the feasible set. This is guaranteed by thegeometry, because the lines that give the cost function (or the planes, when we get tomore unknowns) move steadily up until they intersect the feasible set. The ﬁrst contactmust occur along its boundary! The “simplex method” will go from one corner of thefeasible set to the next until it ﬁnds the corner with lowest cost. In contrast, “interiorpoint methods” approach that optimal solution from inside the feasible set.Note. With a different cost function, the intersection might not be just a single point. Ifthe cost happened to be x+2y, the whole edge between B and A would be optimal. Theminimum cost is x∗ +2y∗, which equals 4 for all these optimal vectors. On our feasibleset, the maximum problem would have no solution! The cost can go arbitrarily high andthe maximum cost is inﬁnite.Every linear programming problem falls into one of three possible categories:1. The feasible set is empty.2. The cost function is unbounded on the feasible set.3. The cost reaches its minimum (or maximum) on the feasible set: the good case.The empty and unbounded cases should be very uncommon for a genuine problem ineconomics or engineering. We expect a solution.",linear_prog
"Chapter 6 Positive Deﬁnite Matrices6A ax2 +2bxy+cy2 is positive deﬁnite if and only if a > 0 and ac > b2. Anyf(x,y) has a minimum at a point where ∂F/∂x = ∂F/∂y = 0 withTest for a maximum:Since f has a maximum whenever −f has a minimum, we justreverse the signs of a, b, and c. This actually leaves ac > b2 unchanged: The quadraticform is negative deﬁnite if and only if a < 0 and ac > b2. The same change applies fora maximum of F(x,y).Singular case ac = b2:The second term in equation (2) disappears to leave onlythe ﬁrst square—which is either positive semideﬁnite, when a > 0, or negative semidef-inite, when a < 0. The preﬁx semi allows the possibility that f can equal zero, as it willat the point x = b, y = −a. The surface z = f(x,y) degenerates from a bowl into a valley.For f = (x+y)2, the valley runs along the line x+y = 0.Saddle Point ac < b2:In one dimension, F(x) has a minimum or a maximum, orF′′ = 0. In two dimensions, a very important possibility still remains: The combinationac−b2 may be negative. This occurred in both examples, when b dominated a and c. Italso occurs if a and c have opposite signs. Then two directions give opposite results—inone direction f increases, in the other it decreases. It is useful to consider two specialSaddle points at (0,0)f2 = x2 −y2In the ﬁrst, b = 1 dominates a = c = 0. In the second, a = 1 and c = −1 have oppositesign. The saddles 2xy and x2 − y2 are practically the same; if we turn one through 45°we get the other. They are also hard to draw.These quadratic forms are indeﬁnite, because they can take either sign. So we havea stationary point that is neither a maximum or a minimum. It is called a saddle point.The surface z = x2 −y2 goes down in the direction of the y axis, where the legs ﬁt (if youstill ride a horse). In case you switched to a car, think of a road going over a mountainpass. The top of the pass is a minimum as you look along the range of mountains, but itis a maximum as you go along the road.Higher Dimensions: Linear AlgebraCalculus would be enough to ﬁnd our conditions Fxx > 0 and FxxFyy > F2xy for a minimum.But linear algebra is ready to do more, because the second derivatives ﬁt into a symmetricmatrix A. The terms ax2 and cy2 appear on the diagonal. The cross derivative 2bxy is",pos_def_matrices
"(b) If you know λ is an eigenvalue, the way to ﬁnd x is to24. What do you do to Ax = λx, in order to prove (a), (b), and (c)?(a) λ 2 is an eigenvalue of A2, as in Problem 22.(b) λ −1 is an eigenvalue of A−1, as in Problem 21.(c) λ +1 is an eigenvalue of A+I, as in Problem 20.25. From the unit vector u =, construct the rank-1 projection matrix P = uuT.(a) Show that Pu = u. Then u is an eigenvector with λ = 1.(b) If v is perpendicular to u show that Pv = zero vector. Then λ = 0.(c) Find three independent eigenvectors of P all with eigenvalue λ = 0.26. Solve det(Q−λI) = 0 by the quadratic formula, to reach λ = cosθ ±isinθ:rotates the xy-plane by the angle θ.Find the eigenvectors of Q by solving (Q−λI)x = 0. Use i2 = −1.27. Every permutation matrix leaves x = (1,1,...,1) unchanged. Then λ = 1. Find twomore λ’s for these permutations:28. If A has λ1 = 4 and λ2 = 5, then det(A−λI) = (λ −4)(λ −5) = λ 2 −9λ +20. Findthree matrices that have trace a+d = 9, determinant 20, and λ = 4,5.29. A 3 by 3 matrix B is known to have eigenvalues 0, 1, 2, This information is enoughto ﬁnd three of these:(a) the rank of B,(b) the determinant of BTB,(c) the eigenvalues of BTB, and(d) the eigenvalues of (B+I)−1.30. Choose the second row of A = [0 1∗ ∗] so that A has eigenvalues 4 and 7.31. Choose a, b, c, so that det(A−λI) = 9λ −λ 3. Then the eigenvalues are −3, 0, 3:",eigenvec_val
"7.4 Iterative Methods for Ax = b8. Show that starting from A0 =, the unshifted QR algorithm produces only themodest improvement A1 = 19. Apply to the following matrix A a single QR step with the shift α = a22—which inthis case means without shift, since a22 = 0. Show that the off-diagonal entries gofrom sinθ to −sin3θ, which is cubic convergence.10. Check that the tridiagonal A =is left unchanged by the QR algorithm. It is oneof the (rare) counterexamples to convergence (so we shift).11. Show by induction that, without shifts, (Q0Q1···Qk)(Rk ···R1R0) is exactly the QRfactorization of Ak+1. This identity connects QR to the power method and leads toan explanation of its convergence. If |λ1| > |λ2| > ··· > |λn|, these eigenvalues willgradually appear on the main diagonal.12. Choose sinθ and cosθ in the rotation P to triangularize A, and ﬁnd R:13. Choose sinθ and cosθ to make P21AP−121 triangular (same A). What are the eigen-14. When A is multiplied by Pi j (plane rotation), which entries are changed? When Pi jAis multiplied on the right by P−1i j , which entries are changed now?15. How many multiplications and how many additions are used to compute PA? (Acareful organization of all the rotations gives 23n3 multiplications and additions, thesame as for QR by reﬂectors and twice as many as for LU.)16. (Turning a robot hand) A robot produces any 3 by 3 rotation A from plane rotationsaround the x, y, and z axes. If P32P31P21A = I, the three robot turns are in A =32 . The three angles are Euler angles. Choose the ﬁrst θ so thatis zero in the (2,1) position.Iterative Methods for Ax = bIn contrast to eigenvalues, for which there was no choice, we do not absolutely needan iterative method to solve Ax = b. Gaussian elimination will reach the solution x in",computations
"Chapter 7 Computations with MatricesThe (2,1) entry in this product is a11sinθ + a21cosθ, and we choose the angle θ thatmakes this combination zero. The next rotation P32 is chosen in a similar way, to removethe (3,2) entry of P32P21Ak. After n−1 rotations, we have R0:Rk = Pn n−1···P32P21Ak.Books on numerical linear algebra give more information about this remarkable algo-rithm in scientiﬁc computing. We mention one more method—Arnoldi in ARPACK—for large sparse matrices. It orthogonalizes the Krylov sequence x,Ax,A2x,... by Gram-Schmidt. If you need the eigenvalues of a large matrix, don’t use det(A−λI)!1. For the matrix A =with eigenvalues λ1 = 1 and λ2 = 3, apply the powermethod uk+1 = Auk three times to the initial guess u0 =. What is the limiting2. For the same A and the initial guess u0 =, compare three inverse power steps toone shifted step with α = uTuk+1 = A−1uk = 1The limiting vector u∞ is now a multiple of the other eigenvector (1,1).3. Explain why |λn/λn−1| controls the convergence of the usual power method. Con-struct a matrix A for which this method does not converge.4. The Markov matrix A =has λ = 1 and .6, and the power method uk = Aku0. Find the eigenvectors of A−1. What does the inverse powermethod u−k = A−ku0 converge to (after you multiply by .6k)?5. Show that for any two different vectors of the same length, ∥x∥ = ∥y∥, the House-holder transformation with v = x−y gives Hx = y and Hy = x.6. Compute σ = ∥x∥, v = x+σz, and H = I −2vvT/vTv, Verify Hx = −σz:7. Using Problem 6, ﬁnd the tridiagonal HAH−1 that is similar to",computations
"Chapter 1 Matrices and Gaussian EliminationWhen the elimination is down to k equations, only k2 − k operations are needed toclear out the column below the pivot—by the same reasoning that applied to the ﬁrststage, when k equaled n. Altogether, the total number of operations is the sum of k2 −kover all values of k from 1 to n:(12 +···+n2)−(1+···+n) = n(n+1)(2n+1)Those are standard formulas for the sums of the ﬁrst n numbers and the ﬁrst n squares.Substituting n = 1 and n = 2 and n = 100 into the formula 1can take no steps or two steps or about a third of a million steps:If n is at all large, a good estimate for the number of operations is 1If the size is doubled, and few of the coefﬁcients are zero, the cost is multiplied by 8.Back-substitution is considerably faster. The last unknown is found in only one oper-ation (a division by the last pivot). The second to last unknown requires two operations,and so on. Then the total for back-substitution is 1+2+···+n.Forward elimination also acts on the right-hand side (subtracting the same multiplesas on the left to maintain correct equations). This starts with n − 1 subtractions of theﬁrst equation. Altogether the right-hand side is responsible for n2 operations—muchless than the n3/3 on the left. The total for forward and back isThirty years ago, almost every mathematician would have guessed that a general sys-tem of order n could not be solved with much fewer than n3/3 multiplications. (Therewere even theorems to demonstrate it, but they did not allow for all possible methods.)Astonishingly, that guess has been proved wrong. There now exists a method that re-quires only Cnlog2 7 multiplications! It depends on a simple fact: Two combinations oftwo vectors in two-dimensional space would seem to take 8 multiplications, but they canbe done in 7. That lowered the exponent from log28, which is 3, to log27 ≈ 2.8. Thisdiscovery produced tremendous activity to ﬁnd the smallest possible power of n. Theexponent ﬁnally fell (at IBM) below 2.376. Fortunately for elimination, the constant Cis so large and the coding is so awkward that the new method is largely (or entirely) oftheoretical interest. The newest problem is the cost with many processors in parallel.Problems 1–9 are about elimination on 2 by 2 systems.",gauss elim
"8.2 The Simplex MethodExample 1. The problem in Figure 8.3 has constraints x+2y ≥ 6, 2x+y ≥ 6, and costx+y. The new system has four unknowns (x, y, and two slack variables):1 1 0 0With equality constraints, the simplex method can begin. A corner is now a point wheren components of the new vector x (the old x and w) are zero. These n components of xare the free variables in Ax = b. The remaining m components are the basic variables orpivot variables. Setting the n free variables to zero, the m equations Ax = b determinethe m basic variables. This “basic solution” x will be a genuine corner if its m nonzerocomponents are positive. Then x belongs to the feasible set.8A The corners of the feasible set are the basic feasible solutions of Ax = b.A solution is basic when n of its m+n components are zero, and it is feasiblewhen it satisﬁes x ≥ 0. Phase I of the simplex method ﬁnds one basic feasiblesolution. Phase II moves step by step to the optimal x∗.The corner point P in Figure 8.3 is the intersection of x = 0 with 2x+y−6 = 0.Which corner do we go to next? We want to move along an edge to an adjacentcorner. Since the two corners are neighbors, m − 1 basic variables will remain basic.Only one of the 6s will become free (zero). At the same time, one variable will move upfrom zero to become basic. The other m−1 basic components (in this case, the other 6)will change but stay positive. The choice of edge (see Example 2 below) decides whichvariable leaves the basis and which one enters. The basic variables are computed bysolving Ax = b. The free components of x are set to zero.Example 2. An entering variable and a leaving variable move us to a new corner.+x3 +6x4 +2x5 = 8Start from the corner at which x1 = 8 and x2 = 9 are the basic variables. At that cornerx3 = x4 = x5 = 0. This is feasible, but the zero cost may not be minimal. It wouldbe foolish to make x3 positive, because its cost coefﬁcient is +7 and we are trying tolower the cost. We choose x5 because it has the most negative cost coefﬁcient −3. Theentering variable will be x5.",linear_prog
"2.1 Vector Spaces and Subspaces1. x+y = y+x.2. x+(y+z) = (x+y)+z.3. There is a unique “zero vector” such that x+0 = x for all x.4. For each x there is a unique vector −x such that x+(−x) = 0.5. 1x = x.6. (c1c2)x = c1(c2x).7. c(x+y) = cx+cy.8. (c1 +c2)x = c1x+c2x.(a) Suppose addition in R2 adds an extra 1 to each component, so that (3,1)+(5,0)equals (9,2) instead of (8,1). With scalar multiplication unchanged, which rules(b) Show that the set of all positive real numbers, with x+y and cx redeﬁned to equalthe usual xy and xc, is a vector space. What is the “zero vector”?(c) Suppose (x1,x2) + (y1,y2) is deﬁned to be (x1 + y2,x2 + y1). With the usualcx = (cx1,cx2), which of the eight conditions are not satisﬁed?6. Let P be the plane in 3-space with equation x +2y+z = 6. What is the equation ofthe plane P0 through the origin parallel to P? Are P and P0 subspaces of R3?7. Which of the following are subspaces of R∞?(a) All sequences like (1,0,1,0,...) that include inﬁnitely many zeros.(b) All sequences (x1,x2,...) with x j = 0 from some point onward.(c) All decreasing sequences: x j+1 ≤ x j for each j.(d) All convergent sequences: the x j have a limit as j → ∞.(e) All arithmetic progressions: x j+1 −x j is the same for all j.(f) All geometric progressions (x1,kx1,k2x1,...) allowing all k and x1.8. Which of the following descriptions are correct? The solutions x of",vector spaces
"Chapter 6 Positive Deﬁnite Matrices19. Find the 3 by 3 matrix A and its pivots, rank, eigenvalues, and determinant:�� = 4(x1 −x2 +2x3)2.20. For F1(x,y) = 14x4 + x2y + y2 and F2(x,y) = x3 + xy − x, ﬁnd the second derivativematrices A1 and A2:A1 is positive deﬁnite, so F1 is concave up (= convex). Find the minimum point ofF1 and the saddle point of F2 (look where ﬁrst derivatives are zero).21. The graph of z = x2 + y2 is a bowl opening upward. The graph of z = x2 − y2 is asaddle. The graph of z = −x2 − y2 is a bowl opening downward. What is a test onF(x,y) to have a saddle at (0,0)?22. Which values of c give a bowl and which give a saddle point for the graph of z =4x2 +12xy+cy2? Describe this graph at the borderline value of c.Tests for Positive DeﬁnitenessWhich symmetric matrices have the property that xTAx > 0 for all nonzero vectors x?There are four or ﬁve different ways to answer this question, and we hope to ﬁnd all ofthem. The previous section began with some hints about the signs of eigenvalues. butthat gave place to the tests on a, b, c:is positive deﬁnite whenFrom those conditions, both eigenvalues are positive. Their product λ1λ2 is determinantac − b2 > 0, so the eigenvalues are either both positive or both negative. They must bepositive because their sum is the trace a+c > 0.Looking at a and ac−b2, it is even possible to spot the appearance of the pivots. Theyturned up when we decomposed xTAx into a sum of squares:ax2 +2bxy+cy2 = aThose coefﬁcients a and (ac − b2)/a are the pivots for a 2 by 2 matrix. For largermatrices the pivots still give a simple test for positive deﬁniteness: xTAx stays positivewhen n independent squares are multiplied by positive pivots.",pos_def_matrices
"1. (a) Write the four equations for ﬁtting y = C +Dt to the dataShow that the columns are orthogonal.(b) Find the optimal straight line, draw its graph, and write E2.(c) Interpret the zero error in terms of the original system of four equations in twounknowns: The right-hand side (−4,−3,−1,0) is in the2. Project b = (0,3,0) onto each of the orthonormal vectors a1 = (23) and a2 =3), and then ﬁnd its projection p onto the plane of a1 and a2.3. Find also the projection of b = (0,3,0) onto a3 = (23), and add the three pro-jections. Why is P = a1aT3 equal to I?4. If Q1 and Q2 are orthogonal matrices, so that QTQ = I, show that Q1Q2 is alsoorthogonal. If Q1 is rotation through θ, and Q2 is rotation through φ, what is Q1Q2?Can you ﬁnd the trigonometric identities for sin(θ +φ) and cos(θ +φ) in the matrix5. If u is a unit vector, show that Q = I − 2uuT is a symmetric orthogonal matrix. (Itis a reﬂection, also known as a Householder transformation.) Compute Q when6. Find a third column so that the matrixis orthogonal. It must be a unit vector that is orthogonal to the other columns; howmuch freedom does this leave? Verify that the rows automatically become orthonor-mal at the same time.7. Show, by forming bTb directly, that Pythagoras’s law holds for any combinationb = x1q1 +···+xnqn of orthonormal vectors: ∥b∥2 = x2n. In matrix terms,b = Qx, so this again proves that lengths are preserved: ∥Qx∥2 = ∥x∥2.8. Project the vector b = (1,2) onto two vectors that are not orthogonal, a1 = (1,0)and a2 = (1,1). Show that, unlike the orthogonal case, the sum of the two one-dimensional projections does not equal b.9. If the vectors q1, q2, q3 are orthonormal, what combination of q1 and q2 is closest to",orthogonality
"Chapter 6 Positive Deﬁnite MatricesRemark 4. Eigenvectors of AAT and ATA must go into the columns of U and V:AAT = (UΣV T)(VΣTUT) = UΣΣTUTATA = VΣTΣV T.U must be the eigenvector matrix for AAT. The eigenvalue matrix in the middle is ΣΣT—which is m by m with σ2r on the diagonal.From the ATA = VΣTΣV T, the V matrix must be the eigenvector matrix for ATA. Thediagonal matrix ΣTΣ has the same σ2r , but it is n by n.Remark 5. Here is the reason that Avj = σ ju j. Start with ATAvj = σ2AATAv j = σ 2This says that Avj is an eigenvector of AAT! We just moved parentheses to (AAT)(Avj).The length of this eigenvector Avj is σ j, becausevTATAv j = σ2So the unit eigenvector is Avj/σj = u j. In other words, AV = UΣ.Example 1. This A has only one column: rank r = 1. Then Σ has only σ1 = 3:ATA is 1 by 1, whereas AAT is 3 by 3. They both have eigenvalue 9 (whose square rootis the 3 in Σ). The two zero eigenvalues of AAT leave some freedom for the eigenvectorsin columns 2 and 3 of U. We kept that matrix orthogonal.Example 2. Now A has rank 2, and AAT =with λ = 3 and 1:= UΣV T = 11. The columns of U are left singular vectors (unit eigenvectors ofAAT). The columns of V are right singular vectors (unit eigenvectors of ATA).Application of the SVDWe will pick a few important applications, after emphasizing one key point. The SVD isterriﬁc for numerically stable computations. because U and V are orthogonal matrices.They never change the length of a vector. Since ∥Ux∥2 = xTUTUx = ∥x∥2, multiplicationby U cannot destroy the scaling.",pos_def_matrices
"Chapter 6 Positive Deﬁnite Matrices16. (Recommended) From the zero submatrix decide the signs of the n eigenvalues:17. (Constrained minimum) Suppose the unconstrained minimum x = A−1b happens tosatisfy the constraint Cx = d. Verify that equation (5) correctly gives PC/min = Pmin;the correction term is zero.The Finite Element MethodThere were two main ideas in the preceding section on minimum principles:(i) Solving Ax = b is equivalent to minimizing P(x) = 1(ii) Solving Ax = λ1x is equivalent to minimizing R(x) = xTAx/xTx.Now we try to explain how these ideas can be applied.The story is a long one, because these principles have been known for more than acentury. In engineering problems like plate bending, or physics problems like the groundstate (eigenfunction) of an atom, minimization was used to get a rough approximationto the true solution. The approximations had to be rough; the computers were human.The principles (i) and (ii) were there, but they could not be implemented.Obviously the computer was going to bring about a revolution. It was the methodof ﬁnite differences that jumped ahead, because it is easy to “discretize” a differentialequation. Already in Section 1.7, derivatives were replaced by differences. The physicalregion is covered by a mesh, and u′′ = f(x) became u j+1 − 2u j + u j−1 = h2 f j. The1950s brought new ways to solve systems Au = f that are very large and very sparse—algorithms and hardware are both much faster now.What we did not fully recognize was that even ﬁnite differences become incrediblycomplicated for real engineering problems, like the stresses on an airplane. The realdifﬁculty is not to solve the equations, but to set them up. For an irregular region wepiece the mesh together from triangles or quadrilaterals or tetrahedra. Then we need asystematic way to approximate the underlying physical laws. The computer has to helpnot only in the solution of Au = f and Ax = λx, but in its formulation.You can guess what happened. The old methods came back, with a new idea and anew name. The new name is the ﬁnite element method. The new ides uses more ofthe power of the computer—in constructing a discrete approximation, solving it, anddisplaying the results—than any other technique in scientiﬁc computation2. If the basic2Please forgive this enthusiasm: I know the method may not be immortal.",pos_def_matrices
"1. For the complex numbers 3+4i and 1−i,(a) ﬁnd their positions in the complex plane.(b) ﬁnd their sum and product.(c) ﬁnd their conjugates and their absolute values.Do the original numbers lie inside or outside the unit circle?2. What can you say about(a) the sum of a complex number and its conjugate?(b) the conjugate of a number on the unit circle?(c) the product of two numbers on the unit circle?(d) the sum of two numbers on the unit circle?3. If x = 2+i and y = 1+3i, ﬁnd x, xx, 1/x, and x/y. Check that the absolute value |xy|equals |x| times |y|, and the absolute value |1/x| equals 1 divided by |x|.4. Find a and b for the complex numbers a + ib at the angles θ = 30°,60°,90° on theunit circle. Verify by direct multiplication that the square of the ﬁrst is the second,and the cube of the ﬁrst is the third.5. (a) If x = reiθ what are x2, x−1, and x in polar coordinates? Where are the complexnumbers that have x−1 = x?(b) At t = 0, the complex number e(−1+i)t equals one. Sketch its path in the complexplane as t increases from 0 to 2π.6. Find the lengths and the inner product of7. Write out the matrix AH and compute C = AHA ifWhat is the relation between C and CH? Does it hold whenever C is constructed from8. (a) With the preceding A, use elimination to solve Ax = 0.(b) Show that the nullspace you just computed is orthogonal to C(AH) and not tothe usual row space C(AT). The four fundamental spaces in the complex caseare N(A) and C(A) as before, and then N(AH) and C(AH).",eigenvec_val
"4.2 Properties of the DeterminantAgain the singular case is separate; A is singular if and only if AT is singular, and wehave 0 = 0. If A is nonsingular, then it allows the factorization PA = LDU, and we applyrule 9 for the determinant of a product:Transposing PA = LDU gives ATPT = UTDTLT, and again by rule 9,This is simpler than it looks, because L, U, LT, and UT are triangular with unit diagonal.By rule 7, their determinants all equal 1. Also, any diagonal matrix is the same as itstranspose: D = DT. We only have to show that detP = detPT.Certainly detP is 1 or −1, because P comes from I by row exchanges. Observe alsothat PPT = I. (The 1 in the ﬁrst row of P matches the 1 in the ﬁrst column of PT, andmisses the 1s in the other columns.) Therefore detPdetPT = detI = 1, and P and PTmust have the same determinant: both 1 or both −1.We conclude that the products (3) and (4) are the same, and detA = detAT. Thisfact practically doubles our list of properties, because every rule that applied to the rowscan now be applied to the columns: The determinant changes sign when two columnsare exchanged, two equal columns (or a column of zeros) produce a zero determinant,and the determinant depends linearly on each individual column. The proof is just totranspose the matrix and work with the rows.I think it is time to stop and call the list complete. It only remains to ﬁnd a deﬁniteformula for the determinant, and to put that formula to use.1. If a 4 by 4 matrix has detA = 12, ﬁnd det(2A), det(−A), det(A2), and det(A−1).2. If a 3 by 3 matrix has detA = −1, ﬁnd det(12A), det(−A), det(A2), and det(A−1).3. Row exchange: Add row 1 of A to row 2, then subtract row 2 from row 1. Thenadd row 1 to row 2 and multiply row 1 by −1 to reach B. Which rules show theThose rules could replace Rule 2 in the deﬁnition of the determinant.",determinants
"3.5 The Fast Fourier TransformThe input sequence is y = 2,4,6,8. The output sequence is c0,c1,c2,c3. The four equa-tions (6) look for a four-term Fourier series that matches the inputs at four equally spacedpoints x on the interval from 0 to 2π:c0 +c1eix +c2e2ix +c3e3ix =Those are the four equations in system (6). At x = 2π the series returns y0 = 2 andcontinues periodically. The Discrete Fourier Series is best written in this complex form,as a combination of exponentials eikx rather than sinkx and coskx.For every n, the matrix connecting y to c can be inverted. It represents n equations,requiring the ﬁnite series c0 +c1eix +··· (n terms) to agree with y (at n points). The ﬁrstagreement is at x = 0, where c0 +···+cn−1 = y0. The remaining points bring powers ofw, and the full problem is Fc = y:There stands the Fourier matrix F with entries Fjk = w jk. It is natural to number therows and columns from 0 to n − 1, instead of 1 to n. The ﬁrst row has j = 0, the ﬁrstcolumn has k = 0, and all their entries are w0 = 1.To ﬁnd the c’s we have to invert F. In the 4 by 4 case, F−1 was built from 1/i = −i.That is the general rule, that F−1 comes from the complex number w−1 = w. It lies atthe angle −2π/n, where w was at the angle +2π/n:The inverse matrix is built from the powers of w−1 = 1/w = w:Row j of F times column j of F−1 is always (1+1+···+1)/n = 1. The harder part isoff the diagonal, to show that row j of F times column k of F−1 gives zero:1·1+w jw−k +w2jw−2k +···+w(n−1)jw−(n−1)k = 0",orthogonality
"6.2 Tests for Positive DeﬁnitenessEven though it is the hardest to apply to a single matrix, eigenvalues can be the mostuseful test for theoretical purposes. Each test is enough by itself.Positive Deﬁnite Matrices and Least SquaresI hope you will allow one more test for positive deﬁniteness. It is already close. Weconnected positive deﬁnite matrices to pivots (Chapter 1), determinants (Chapter 4), andeigenvalues (Chapter 5). Now we see them in the least-squares problems in Chapter 3,coming from the rectangular matrices of Chapter 2.The rectangular matrix will be R and the least-squares problem will be Rx = b. It hasm equations with m ≥ n (square systems are included). The least-square choice �x is thesolution of RTR�x = RTb. That matrix ARTR is not only symmetric but positive deﬁnite,as we now show—provided that the n columns of R are linearly independent:The symmetric matrix A is positive deﬁnite if and only if(V) There is a matrix R with independent columns such that A = RTR.The key is to recognize xTAx as xTRTRx = (Rx)T(Rx). This squared length ∥Rx∥2 ispositive (unless x = 0), because R has independent columns. (If x is nonzero then Rx isnonzero.) Thus xTRTRx > 0 and RTR is positive deﬁnite.It remains to ﬁnd an R For which A = RTR. We have almost done this twice already:A = LDLT = (LSo take R =This Cholesky decomposition has the pivots split evenly between L and LT.A = QΛQT = (QSo take R =A third possibility is R = QΛQT, the symmetric positive deﬁnite square root of A.There are many other choices, square or rectangular, and we can see why. If you multiplyany R by a matrix Q with orthonormal columns, then (QR)T(QR) = RTQTQR = RTIR =A. Therefore QR is another choice.Applications of positive deﬁnite matrices are developed in my earlier book Intro-duction to Applied Mathematics and also the new Applied Mathematics and ScientiﬁcComputing (see www.wellesleycambridge.com). We mention that Ax = λMxarises constantly in engineering analysis. If A and M are positive deﬁnite, this general-ized problem is parallel to the familiar Ax = λx, and λ > 0. M is a mass matrix for theﬁnite element method in Section 6.4.The tests for semideﬁniteness will relax xTAx > 0, λ > 0, d > 0, and det > 0, to allowzeros to appear. The main point is to see the analogies with the positive deﬁnite case.",pos_def_matrices
"26. Project a1 = (1,0) onto a2 = (1,2). Then project the result back onto a1. Draw theseprojections and multiply the projection matrices P1P2: Is this a projection?Projections and Least SquaresUp to this point, Ax = b either has a solution or not. If b is not in the column space C(A),the system is inconsistent and Gaussian elimination fails. This failure is almost certainwhen there are several equations and only one unknown:This is solvable when b1, b2, b3 are in the ratio 2:3:4. The solution x will exist only if bis on the same line as the column a = (2,3,4).In spite of their unsolvability, inconsistent equations arise all the time in practice.They have to be solved! One possibility is to determine x from part of the system, andignore the rest; this is hard to justify if all m equations come from the same source.Rather than expecting no error in some equations and large errors in the others, it ismuch better to choose the x that minimizes an average error E in the m equations.The most convenient “average” comes from the sum of squares:E2 = (2x−b1)2 +(3x−b2)2 +(4x−b3)2.If there is an exact solution, the minimum error is E = 0. In the more likely case that bis not proportional to a, the graph of E2 will be a parabola. The minimum error is at thelowest point, where the derivative is zero:Solving for x, the least-squares solution of this model system ax = b is denoted by �x:�x = 2b1 +3b2 +4b3You recognize aTb in the numerator and aTa in the denominator.The general case is the same. We “solve” ax = b by minimizingE2 = ∥ax−b∥2 = (a1x−b1)2 +···+(amx−bm)2.The derivative of E2 is zero at the point �x, if(a1�x−b1)a1 +···+(am�x−bm)am = 0.We are minimizing the distance from b to the line through a, and calculus gives the sameanswer, �x = (a1b1 +···+ambm)/(a2m), that geometry did earlier:",orthogonality
"of one row from another. Such a transformation preserved the nullspace and row spaceof A; it normally changes the eigenvalues.Eigenvalues are actually calculated by a sequence of simple similarities. The matrixgoes gradually toward a triangular form, and the eigenvalues gradually appear on themain diagonal. (Such a sequence is described in Chapter 7.) This is much better thantrying to compute det(A−λI), whose roots should be the eigenvalues. For a large matrix,it is numerically impossible to concentrate all that information into the polynomial andthen get it out again.Triangular Forms with a Unitary MOur ﬁrst move beyond the eigenvector matrix M = S is a little bit crazy: Instead of amore general M, we go the other way and restrict M to be unitary. M−1AM can achievea triangular form T under this restriction. The columns of M = U are orthonormal (inthe real case, we would write M = Q). Unless the eigenvectors of Λ are orthogonal, adiagonal U−1AU is impossible. But “Schur’s lemma” in 5R is very useful—at least tothe theory. (The rest of this chapter is devoted more to theory than to applications. TheJordan form is independent of this triangular form.)There is a unitary matrix M = U such that U−1AU = T is triangular.The eigenvalues of A appear along the diagonal of this similar matrix T.Proof. Every matrix, say 4 by 4, has at least one eigenvalue λ1. In the worst case, itcould be repeated four times. Therefore A has at least one unit eigenvector x1, which weplace in the ﬁrst column of U. At this stage the other three columns are impossible todetermine, so we complete the matrix in any way that leaves it unitary, and call it U1.(The Gram-Schmidt process guarantees that this can be done.) Ax1 = λ1x1 column 1means that the product U−11 AU1 starts in the right form:Now work with the 3 by 3 submatrix in the lower right-hand corner. It has a uniteigenvector x2, which becomes the ﬁrst column of a unitary matrix M2:",eigenvec_val
"Chapter 8 Linear Programming and Game TheorySuppose we have four women and four men. Some of those sixteen couples are compat-ible, others regrettably are not. When is it possible to ﬁnd a complete matching, witheveryone married? If linear algebra can work in 20-dimensional space, it can certainlyhandle the trivial problem of marriage.There are two ways to present the problem—in a matrix or on a graph. The matrixcontains aij = 0 if the ith woman and jth man are not compatible, and aij = 1 if they arewilling to try. Thus row i gives the choices of the ith woman, and column j correspondsto the jth man:1 0 0 01 1 1 00 0 0 10 0 0 1has 6 compatible pairs.The left graph in Figure 8.6 shows two possible marriages. Ignoring the source s andsink t, it has four women on the left and four men on the right. The edges correspondto the 1s in the matrix, and the capacities are 1 marriage. There is no edge between theﬁrst woman and fourth man, because the matrix has a14 = 0.Figure 8.6: Two marriages on the left, three (maximum) on the right. The third is created by adding two newmarriages and one divorce (backward ﬂow).It might seem that node M2 can’t be reached by more ﬂow—but that is not so! Theextra ﬂow on the right goes backward to cancel an existing marriage. This extra ﬂowmakes 3 marriages, which is maximal. The minimal cut is crossed by 3 edges.A complete matching (if it is possible) is a set of four is in the matrix. They wouldcome from four different rows and four different columns, since bigamy is not allowed.It is like ﬁnding a permutation matrix within the nonzero entries of A. On the graph, thismeans four edges with no nodes in common. The maximal ﬂow is less than 4 exactlywhen a complete matching is impossible.In our example the maximal ﬂow is 3, not 4. The marriages 1–1, 2–2, 4–4 are allowed",linear_prog
"of the columns of Q = I:That is not the only orthonormal basis! We can rotate the axes without changing theright angles at which they meet. These rotation matrices will be examples of Q.If we have a subspace of Rn, the standard vectors ei might not lie in that subspace.But the subspace always has an orthonormal basis, and it can be constructed in a simpleway out of any basis whatsoever. This construction, which converts a skewed set of axesinto a perpendicular set, is known as Gram-Schmidt orthogonalization.To summarize, the three topics basic to this section are:1. The deﬁnition and properties of orthogonal matrices Q.2. The solution of Qx = b, either n by n or rectangular (least squares).3. The Gram-Schmidt process and its interpretation as a new factorization A = QR.If Q (square or rectangular) has orthonormal columns, then QTQ = I:1 0 · 00 1 · 00 0 · 1An orthogonal matrix is a square matrix with orthonormal columns.2 ThenQT is Q−1. For square orthogonal matrices, the transpose is the inverse.When row i of QT multiplies column j of Q, the result is qTj q j = 0. On the diagonalwhere i = j, we have qTi qi = 1. That is the normalization to unit vectors of length 1.Note that QTQ = I even if Q is rectangular. But then QT is only a left-inverse.QT = Q−1 =2Orthonormal matrix would have been a better name, but it is too late to change. Also, there is no accepted wordfor a rectangular matrix with orthonormal columns. We still write Q, but we won’t call it an “orthogonal matrix”unless it is square.",orthogonality
"2.5 Graphs and Networksﬁve currents. They are conditions of “conservation” at each node: Flow in equals ﬂowout at every node:y1 −y3 −y4 = 0y2 +y3 −y5 = 0y4 +y5 = 0Total current to node 1 is zeroThe beauty of network theory is that both A and AT have important roles.Solving ATy = 0 means ﬁnding a set of currents that do not “pile up” at any node. Thetrafﬁc keeps circulating, and the simplest solutions are currents around small loops.Our graph has two loops, and we send 1 amp of current around each loop:1 −1 1 0 00 0 1 −1 1Each loop produces a vector y in the left nullspace. The component +1 or −1 indicateswhether the current goes with or against the arrow. The combinations of y1 and y2 ﬁllthe left nullspace, so y1 and y2 are a basis (the dimension had to be m−r = 5−3 = 2).In fact y1 −y2 = (1,−1,0,1,−1) gives the big loop around the outside of the graph.The column space and left nullspace are closely related. The left nullspace containsy1 = (1,1,1,0,0), and the vectors in the column space satisfy b1 − b2 + b3 = 0. ThenyTb = 0: Vectors in the column space and left nullspace are perpendicular! That is soonto become Part Two of the “Fundamental Theorem of Linear Algebra.”The row space of A contains vectors in R4, but not all vectors. Itsdimension is the rank r = 3. Elimination will ﬁnd three independent rows, and we canalso look to the graph. The ﬁrst three rows are dependent (row 1 + row 3 = row 2, andthose edges form a loop). Rows 1,2,4 are independent because edges 1,2,4 contain noRows 1, 2, 4 are a basis for the row space. In each row the entries add to zero. Everycombination (f1, f2, f3, f4) in the row space will have that same property:f in row spacef1 + f2 + f3 + f4 = 0Again this illustrates the Fundamental Theorem: The row space is perpendicular to thenullspace. If f is in the row space and x is in the nullspace then f Tx = 0.For AT, the basic law of network theory is Kirchhoff’s Current Law. The total ﬂowinto every node is zero. The numbers f1, f2, f3, f4 are current sources into the nodes. Thesource f1 must balance −y1−y2, which is the ﬂow leaving node 1 (along edges 1 and 2).That is the ﬁrst equation in ATy = f. Similarly at the other three nodes—conservationof charge requires ﬂow in = ﬂow out. The beautiful thing is that AT is exactly the rightmatrix for the Current Law.The equations ATy = f at the nodes express Kirchhoff’s Current Law:",vector spaces
"1.6 Inverses and Transposes44. If B has the columns of A in reverse order, solve (A−B)x = 0 to show that A−B isnot invertible. An example will lead you to x.45. Find and check the inverses (assuming they exist) of these block matrices:46. Use inv(S) to invert MATLAB’s 4 by 4 symmetric matrix S = pascal(4). CreatePascal’s lower triangular A = abs(pascal(4,1)) and test inv(S) = inv(A’) ∗ inv(A).47. If A = ones(4,4) and b = rand(4,1), how does MATLAB tell you that Ax = b hasno solution? If b = ones(4,1), which solution to Ax = b is found by A\b?48. M−1 shows the change in A−1 (useful to know) when a matrix is subtracted from A.Check part 3 by carefully multiplying MM−1 to get I:M = I −uvTM−1 = I +uvT/(1−vTu).M−1 = A−1 +A−1uvTA−1/(1−vTA−1u).M = I −UVM−1 = In +U(Im −VU)−1V.M = A−UW −1VM−1 = A−1 +A−1U(W −VA−1U)−1VA−1.The four identities come from the 1, 1 block when inverting these matrices:Problems 49–55 are about the rules for transpose matrices.49. Find AT and A−1 and (A−1)T and (AT)−1 for50. Verify that (AB)T equals BTAT but those are different from ATBT:In case AB = BA (not generally true!), how do you prove that BTAT = ATBT?51. (a) The matrix(AB)−1�T comes from (A−1)T and (B−1)T. In what order?(b) If U is upper triangular then (U−1)T is52. Show that A2 = 0 is possible but ATA = 0 is not possible (unless A = zero matrix).",gauss elim
"Hint: Subtracting the last row from each of the others leavesSketch A′ = (1,6), B′ = (−2,7), C′ = (0,0) and their relation to A, B, C.6. Explain in terms of volumes why det3A = 3ndetA for an n by n matrix A.7. Predict in advance, and conﬁrm by elimination, the pivot entries of8. Find all the odd permutations of the numbers {1,2,3,4}. They come from an oddnumber of exchanges and lead to detP = −1.9. Suppose the permutation P takes (1,2,3,4,5) to (5,4,1,2,3).(a) What does P2 do to (1,2,3,4,5)?(b) What does P−1 do to (1,2,3,4,5)?10. If P is an odd permutation, explain why P2 is even but P−1 is odd.11. Prove that if you keep multiplying A by the same permutation matrix P, the ﬁrst roweventually comes back to its original place.12. If A is a 5 by 5 matrix with all |aij| ≤ 1, then detA ≤. Volumes or the bigformula or pivots should give some upper bound on the determinant.Problems 13–17 are about Cramer’s Rule for x = A−1b.13. Solve these linear equations by Cramer’s Rule x j = detB j/detA:14. Use Cramer’s Rule to solve for y (only). Call the 3 by 3 determinant D:cx + dy = 0.ax + by +gx + hy +",determinants
"Minima, Maxima, and Saddle PointsUp to now, we have hardly thought about the signs of the eigenvalues. We couldn’task whether λ was positive before it was known to be real. Chapter 5 established thatevery symmetric matrix has real eigenvalues. Now we will ﬁnd a test that can be applieddirectly to A, without computing its eigenvalues, which will guarantee that all thoseeigenvalues are positive. The test brings together three of the most basic ideas in thebook—pivots, determinants, and eigenvalues.The signs of the eigenvalues are often crucial. For stability in differential equations,we needed negative eigenvalues so that eλt would decay. The new and highly importantproblem is to recognize a minimum point. This arises throughout science and engi-neering and every problem of optimization. The mathematical problem is to move thesecond derivative test F′′ > 0 into n dimensions. Here are two examples:F(x,y) = 7+2(x+y)2 −ysiny−x3f(x,y) = 2x2 +4xy+y2.Does either F(x,y) or f(x,y) have a minimum at the point x = y = 0?Remark 3. The zero-order terms F(0,0) = 7 and f(0,0) = 0 have no effect on the an-swer. They simply raise or lower the graphs of F and f.Remark 4. The linear terms give a necessary condition: To have any chance of a mini-mum, the ﬁrst derivatives must vanish at x = y = 0:∂x = 4(x+y)−3x2 = 0∂y = 4(x+y)−ycosy−siny = 0∂x = 4x+4y = 0∂y = 4x+2y = 0.Thus (x,y) = (0,0) is a stationary point for both functions. The surface z = F(x,y) istangent to the horizontal plane z = 7, and the surface z = f(x,y) is tangent to the planez = 0. The question is whether the graphs go above those planes or not, as we moveaway from the tangency point x = y = 0.",pos_def_matrices
"Chapter 7 Computations with Matricesof λ = ω − 1. Since our goal is to make λmax as small as possible, that extremal pairspeciﬁes the best choice ωopt:λmax = ωopt −1.The splittings of the −1, 2, −1 matrix of order n yield these eigenvaluesJacobi (S = 0, 2, 0 matrix):S−1T has |λ|max = cosGauss-Seidel (S = −1, 2, 0 matrix):S−1T has |λ|max =SOR (with the best ω):This can only be appreciated by an example. Suppose A is of order 21, which is verymoderate. Then h = 122, cosπh = .99, and the Jacobi method is slow; cos2πh = .98means that even Gauss-Seidel will require a great many iterations. But since sinπh =.02 = .14, the optimal overrelaxation method will have the convergence factorωopt = 1+λmax = 1.75.The error is reduced by 25% at every step, and a single SOR step is the equivalent of30 Jacobi steps: (.99)30 = .75.That is a striking result from such a simple idea. Its real applications are not in one-dimensional problems like −uxx = f. A tridiagonal system Ax = b is already easy. It isfor partial differential equations that overrelaxation (and other ideas) will be important.Changing to −uxx −uyy = f leads to the “ﬁve-point scheme.” The entries −1, 2, −1 inthe x direction combine with −1, 2, −1 in the y direction to give a main diagonal of+4 and four off-diagonal entries of −1. The matrix A does not have a small bandwidth!There is no way to number the N2 mesh points in a square so that each point staysclose to all four of its neighbors. That is the true curse of dimensionality, and parallelcomputers will partly relieve it.If the ordering goes a row at a time, every point must wait a whole row for the neigh-bor above it to turn up. The “ﬁve-point matrix” has bandwidth N: This matrix has hadmore attention, and been attacked in more different ways, than any other linear equa-tion Ax = b. The trend now is back to direct methods, based on an idea of Golub andHockney; certain special matrices will fall apart when they are dropped the right way.(It is comparable to the Fast Fourier Transform.) Before that came the iterative methodsof alternating direction, in which the splitting separated the tridiagonal matrix in the xdirection from the one in the y direction, A recent choice is S = L0U0, in which small",computations
"Chapter 5 Eigenvalues and Eigenvectors9. (a) How is the determinant of AH related to the determinant of A?(b) Prove that the determinant of any Hermitian matrix is real.10. (a) How many degrees of freedom are there in a real symmetric matrix, a real diag-onal matrix, and a real orthogonal matrix? (The ﬁrst answer is the sum of theother two, because A = QΛQT.)(b) Show that 3 by 3 Hermitian matrices A and also unitary U have 9 real degrees offreedom (columns of U can be multiplied by any eiθ).11. Write P, Q and R in the form λ1x1xH2 of the spectral theorem:12. Give a reason if true or a counterexample if false:(a) If A is Hermitian, then A+iI is invertible.(b) If Q is orthogonal. then Q+ 1(c) If A is real, then A+iI is invertible.13. Suppose A is a symmetric 3 by 3 matrix with eigenvalues 0, 1, 2.(a) What properties can be guaranteed for the corresponding unit eigenvectors u, v,(b) In terms of u, v, w, describe the nullspace, left nullspace, row space and column(c) Find a vector x that satisﬁes Ax = v+w. Is x unique?(d) Under what conditions on b does Ax = b have a solution?(e) If u, v, w are the columns of S, what are S−1 and S−1AS?14. In the list below, which classes of matrices contain A and which contain B?0 1 0 00 0 1 00 0 0 11 0 0 01 1 1 11 1 1 11 1 1 11 1 1 1Orthogonal, invertible, projection, permutation, Hermitian, rank-1, diagonalizable,Markov. Find the eigenvalues of A and B.15. What is the dimension of the space S of all n by n real symmetric matrices? Thespectral theorem says that every symmetric matrix is a combination of n projectionmatrices. Since the dimension exceeds n, how is this difference explained?16. Write one signiﬁcant fact about the eigenvalues of each of the following.",eigenvec_val
"x + 2y = 34x + 5y = 6One solution (x, y) = (−1, 2)x + 2y = 34x + 8y = 6x + 2y = 34x + 8y = 12Whole line of solutionsFigure 1.1: The example has one solution. Singular cases have none or too many.First we have to introduce matrices and vectors and the rules for multiplication.Every matrix has a transpose AT. This matrix has an inverse A−1.3. In most cases elimination goes forward without difﬁculties. The matrix has aninverse and the system Ax = b has one solution. In exceptional cases the methodwill break down—either the equations were written in the wrong order, which iseasily ﬁxed by exchanging them, or the equations don’t have a unique solution.That singular case will appear if 8 replaces 5 in our example:1x + 2y =4x + 8y = 6.Elimination still innocently subtracts 4 times the ﬁrst equation from the second. Butlook at the result!This singular case has no solution. Other singular cases have inﬁnitely many solu-tions. (Change 6 to 12 in the example, and elimination will lead to 0 = 0. Now ycan have any value,) When elimination breaks down, we want to ﬁnd every possible4. We need a rough count of the number of elimination steps required to solve a sys-tem of size n. The computing cost often determines the accuracy in the model. Ahundred equations require a third of a million steps (multiplications and subtrac-tions). The computer can do those quickly, but not many trillions. And alreadyafter a million steps, roundoff error could be signiﬁcant. (Some problems are sen-sitive; others are not.) Without trying for full detail, we want to see large systemsthat arise in practice, and how they are actually solved.The ﬁnal result of this chapter will be an elimination algorithm that is about as efﬁ-cient as possible. It is essentially the algorithm that is in constant use in a tremendousvariety of applications. And at the same time, understanding it in terms of matrices—thecoefﬁcient matrix A, the matrices E for elimination and P for row exchanges, and the",gauss elim
"2.3 Linear Independence, Basis, and Dimension62. Construct a matrix whose column space contains (1,1,5) and (0,3.1) and whose63. Construct a matrix whose column space contains (1,1,0) and (0,1,1) and whosenullspace contains (1,0,1) and (0,0,1).64. Construct a matrix whose column space contains (1,1,1) and whose nullspace is theline of multiples of (1,1,1,1).65. Construct a 2 by 2 matrix whose nullspace equals its column space.66. Why does no 3 by 3 matrix have a nullspace that equals its column space?67. The reduced form R of a 3 by 3 matrix with randomly chosen entries is almost sure. What R is virtually certain if the random A is 4 by 3?68. Show by example that these three statements are generally false:(a) A and AT have the same nullspace.(b) A and AT have the same free variables.(c) If R is the reduced form rref(A) then RT is rref(AT).69. If the special solutions to Rx = 0 are in the columns of these N, go backward to ﬁndthe nonzero rows of the reduced matrices R:(empty 3 by 1).70. Explain why A and −A always have the same reduced echelon form R.Linear Independence, Basis, and DimensionBy themselves, the numbers m and n give an incomplete picture of the true size of alinear system. The matrix in our example had three rows and four columns, but the thirdrow was only a combination of the ﬁrst two. After elimination it became a zero row, Ithad no effect on the homogeneous problem Ax = 0. The four columns also failed to beindependent, and the column space degenerated into a two-dimensional plane.The important number that is beginning to emerge (the true size) is the rank r. Therank was introduced as the number of pivots in the elimination process. Equivalently,the ﬁnal matrix U has r nonzero rows. This deﬁnition could be given to a computer. Butit would be wrong to leave it there because the rank has a simple and intuitive meaning:The rank counts the number of genuinely independent rows in the matrix A. We wantdeﬁnitions that are mathematical rather than computational.The goal of this section is to explain and use four ideas:",vector spaces
"4.3 Formulas for the DeterminantIts determinant is the product of its pivots. The numbers 2,...,n all cancel:MATLAB computes the determinant from the pivots. But concentrating all informationinto the pivots makes it impossible to ﬁgure out how a change in one entry would affectthe determinant. We want to ﬁnd an explicit expression for the determinant in terms ofFor n = 2, we will be proving that ad − bc is correct. For n = 3, the determinantformula is again pretty well known (it has six terms):= +a11a22a33 +a12a23a31 +a13a21a32Our goal is to derive these formulas directly from the deﬁning properties 1–3 of detA. Ifwe can handle n = 2 and n = 3 in an organized way, you will see the pattern.To start, each row can be broken down into vectors in the coordinate directions:Then we apply the property of linearity, ﬁrst in row 1 and then in row 2:nn = 22 easyEvery row splits into n coordinate directions, so this expansion has nn terms. Most ofthose terms (all but n! = n factorial) will be automatically zero. When two rows are inthe same coordinate direction, one will be a multiple of the other, andWe pay attention only when the rows point in different directions. The nonzero termshave to come in different columns. Suppose the ﬁrst row has a nonzero term in columnα, the second row is nonzero in column β, and ﬁnally the nth row in column v. Thecolumn numbers α,β,...,v are all different. They are a reordering, or permutation, of",determinants
"Chapter 8 Linear Programming and Game TheoryIn calculus, everybody knows the condition for a maximum or a minimum: The ﬁrstderivatives are zero. But this is completely changed by constraints. The simplest exam-ple is the line y = x. Its derivative is never zero, calculus looks useless, and the largesty is certain to occur at the end of the interval. That is exactly the situation in linear pro-gramming! There are more variables, and an interval is replaced by a feasible set, butstill the maximum is always found at a corner of the feasible set (with only m nonzeroThe problem in linear programming is to locate that cornet For this, calculus is notcompletely helpless. Far from it, because “Lagrange multipliers” will bring back zeroderivatives at the maximum and minimum. The dual variables y are exactly the La-grange multipliers. And they answer the key question: How does the minimum costcx∗ = y∗b change, if we change b or c?This is a question in sensitivity analysis. It allows us to squeeze extra information outof the dual problem. For an economist or an executive, these questions about marginalcost are the most important.If we allow large changes in b or c, the solution behaves in a very jumpy way. Asthe price of eggs increases, there will be a point at which they disappear from the diet.The variable xegg will jump from basic to free. To follow it properly, we would haveto introduce “parametric” programming. But if the changes are small, the corner thatwas optimal remains optimal. The choice of basic variables does not change; B and Nstay the same. Geometrically, we shifted the feasible set a little (by changing b), and wetilted the planes that come up to meet it (by changing c). When these changes are small,contact occurs at the same (slightly moved) corner.At the end of the simplex method, when the right basic variables are known, thecorresponding m columns of A make up the basis matrix B. At that corner, a shift of size∆b changes the minimum cost by y∗∆b. The dual solution y∗ gives the rate of changeof minimum cost (its derivative) with respect to changes in b. The components of y∗are the shadow prices. If the requirement for a vitamin goes up by ∆, and the druggist’s1, then the diet cost (from druggist or grocer) will go up by y∗1∆. In the case1 is zero, that vitamin is a free good and the small change has no effect. The dietalready contained more than b1.We now ask a different question. Suppose we insist that the diet contain some smalledible amount of egg. The condition xegg ≥ 0 is changed to xegg ≥ δ. How does thisIf eggs were in the diet x∗, there is no change. But if x∗egg = 0, it will cost extra to addin the amount δ. The increase will not be the full price ceggδ, since we can cut down onother foods. The reduced cost of eggs is their own price, minus the price we are payingfor the equivalent in cheaper foods. To compute it we return to equation (2) of Section",linear_prog
"Chapter 1 Matrices and Gaussian EliminationOur true goal is to look beyond two or three dimensions into n dimensions. With nequations in n unknowns, there are n planes in the row picture. There are n vectors inthe column picture, plus a vector b on the right side. The equations ask for a linear com-bination of the n columns that equals b. For certain equations that will be impossible.Paradoxically, the way to understand the good case is to study the bad one. Thereforewe look at the geometry exactly when it breaks down, in the singular case.Row picture: Intersection of planesColumn picture: Combination of columnsSuppose we are again in three dimensions, and the three planes in the row picture do notintersect. What can go wrong? One possibility is that two planes may be parallel. Theequations 2u + v + w = 5 and 4u + 2v + 2w = 11 are inconsistent—and parallel planesgive no solution (Figure 1.5a shows an end view). In two dimensions, parallel linesare the only possibility for breakdown. But three planes in three dimensions can be introuble without being parallel.Figure 1.5: Singular cases: no solution for (a), (b), or (d), an inﬁnity of solutions for (c).The most common difﬁculty is shown in Figure 1.5b. From the end view the planesform a triangle. Every pair of planes intersects in a line, and those lines are parallel. Thethird plane is not parallel to the other planes, but it is parallel to their line of intersection.This corresponds to a singular system with b = (2,5,6):No solution, as in Figure 1.5b3u + v + 4w = 6.The ﬁrst two left sides add up to the third. On the right side that fails: 2+5 ̸= 6. Equation1 plus equation 2 minus equation 3 is the impossible statement 0 = 1. Thus the equationsare inconsistent, as Gaussian elimination will systematically discover.",gauss elim
"Chapter 1 Matrices and Gaussian Elimination58. In MATLAB notation, write the commands that deﬁne the matrix A and the columnvectors x and b. What command would test whether or not Ax = b?59. The MATLAB commands A = eye(3) and v = [3:5]’ produce the 3 by 3 identitymatrix and the column vector (3,4,5). What are the outputs from A ∗ v and v’ ∗ v?(Computer not needed!) If you ask for v ∗ A, what happens?60. If you multiply the 4 by 4 all-ones matrix A = ones(4,4) and the column v =ones(4,1), what is A ∗ v? (Computer not needed.) If you multiply B = eye(4)+ ones(4,4) times w = zeros(4,1) + 2 ∗ ones(4,1), what is B ∗ w?61. Invent a 3 by 3 magic matrix M with entries 1,2,...,9. All rows and columns anddiagonals add to 15. The ﬁrst row could be 8, 3, 4. What is M times (1,1,1)? Whatis the row vectorTriangular Factors and Row ExchangesWe want to look again at elimination, to see what it means in terms of matrices. Thestarting point was the model system Ax = b:Then there were three elimination steps, with multipliers 2, −1, −1:Step 1. Subtract 2 times the ﬁrst equation from the second;Step 2. Subtract −1 times the ﬁrst equation from the third;Step 3. Subtract −1 times the second equation from the third.The result was an equivalent system Ux = c, with a new coefﬁcient matrix U:This matrix U is upper triangular—all entries below the diagonal are zero.The new right side c was derived from the original vector b by the same steps thattook A into U. Forward elimination amounted to three row operations:",gauss elim

text,label
1 3 3 2,vector spaces
The second eigenvalue λ2(A) = 2 is above the lowest eigenvalue λ1(B) = 1. The lowest,pos_def_matrices
matrix Q−1AQ = QTAQ.,pos_def_matrices
i . Stability depends on the eigenvalues:,eigenvec_val
Rayleigh quotient is the fundamental frequency λ1. Its approximate minimum Λ1 will,pos_def_matrices
"permutation matrix P, and (depending on n) what is its determinant?",determinants
"x = (c,c,c,c) can be added to any particular solution of Ax = b. The complete solution",vector spaces
"derivative is zero, so column 1 is zero. The last column came from (d/dt)t3 = 3t2. Since",vector spaces
"2, which is or-",orthogonality
Tennessee thumped Miami 35-7 in the Sugar Bowl. Final AP and UPI polls,vector spaces
"direct test, look back at the matrix. Row 1 plus row 3 equals row 2. On the right-hand",vector spaces
"2. With free variables = 0, ﬁnd a particular solution to Axp = b and Uxp = c.",vector spaces
cost is reduced—but we cannot go out of the feasible set. The multiple of −Pc is chosen,linear_prog
"You see the lengths of a, B, C on the diagonal of R. The orthonormal vectors q1, q2, q3,",orthogonality
"If A can be diagonalized, A = SΛS−1, then Ak comes from Λk:",eigenvec_val
"means that only certain special numbers are eigenvalues, and only certain special",eigenvec_val
"(a) v = (2,2).",vector spaces
correct but not convincing if I write in italics that the left nullspace is also important. The,vector spaces
"team, and we want xh −xv = bi, for every game. That means a few thousand equations",vector spaces
"This unconstrained problem has n = 2, A = I, and b = 0. So the minimizing equation",pos_def_matrices
"1.9 In the vector space of 2 by 2 matrices,",vector spaces
2 4 0 1,vector spaces
Chapter 2 Vector Spaces,vector spaces
3K The least-squares solution to a problem ax = b in one unknown is �x = aTb,orthogonality
"otherwise zero), show that detA = 1 or −1 or 0.",determinants
into its frequencies is the best way to take a signal apart. The reverse process brings it,orthogonality
"33. Show that A = 4 ∗ eye(4) − ones(4,4) is not invertible: Multiply A ∗ ones(4,1).",gauss elim
space. What do they mean in terms of polynomials?,vector spaces
"(1, 2, 3) has length",orthogonality
numbers can be eigenvalues of P?,eigenvec_val
15. (a) How many entries can be chosen independently in a symmetric matrix of order,gauss elim
"of exponentials eλtx, and we are interested only in those particular values λ for which",eigenvec_val
"Remark 2. Purely out of curiosity, we might count the number of operations required",gauss elim
more important. There is an easy way to recognize when A and B share a full set of,eigenvec_val
"Column space: dimension r = n−1, any n−1 columns are independent.",vector spaces
"to apologize for that; it makes them sound very superﬁcial, and that is wrong. They deal",computations
"(a) Using those multipliers in A, show that column 1 of A and B = column 2 −",orthogonality
0 0 0 0,vector spaces
"center of the underlying theory. Introducing the dual problem is an elegant idea, and",linear_prog
40. There are six 3 by 3 permutation matrices P. What numbers can be the determinants,eigenvec_val
2 0 0 1,determinants
"3.2 Find all vectors that are perpendicular to (1,3,1) and (2,7,2), by making those the",orthogonality
fourth column in [A b]. Find all solutions when that condition holds:,vector spaces
"if A has negative eigenvalues λ1,...,λn and if ω j =",eigenvec_val
"4. (a) Find the LU factorization, the pivots, and the determinant of the 4 by 4 matrix",determinants
"pivots, or more than n. We want to prove that when r = m there is a right-inverse, and",vector spaces
"space, and it is in the left nullspace.",orthogonality
(f) What is the general solution to Ax = 0?,vector spaces
3 over the interval −1 ≤ x ≤ 1.,orthogonality
2. They span the space V (not too few vectors).,vector spaces
A band matrix has aij = 0 except in the band |i − j| < w (Figure 1.8). The “half,gauss elim
(b) If Q (3 by 2) has orthonormal columns then ∥Qx∥ always equals ∥x∥.,orthogonality
"Our ﬁrst requirement had to do with rows, and this one is concerned with columns. A",gauss elim
"it equals (IłA)−1, for the consumption matrix",eigenvec_val
zero. Another approach is to compute the determinant of the exponential:,eigenvec_val
"(I −A)−1 has the same eigenvector, with eigenvalue 1/(1−λ1).",eigenvec_val
"for j = 1,...,n.",gauss elim
We could write c = L−1b and then x = U−1c = U−1L−1b. But note that we did not,gauss elim
formula for the ﬁrst coefﬁcient c1 in terms of b and the q’s.,orthogonality
"whose columns are q1, q2, q3. What is the relation between those matrices? The matrices",orthogonality
eigenvalues equals detL = detT/detS:,computations
"The result of all three steps is GFEA = U. Note that E is the ﬁrst to multiply A,",gauss elim
"in Figure 6.6, where the horizontal distance to the ellipse (where a11x2 = 1) is between",pos_def_matrices
"The measurements b1,...,bm are given at distinct points t1,...,tm. Then",orthogonality
(a) Those vectors (do)(do not)(might not) span R4.,vector spaces
1. Techniques for Solving Ax = b.,computations
u(0) = c1x1 +c2x2 =,eigenvec_val
the same space. When are they a basis for,vector spaces
∑mijvi of the v’s.,eigenvec_val
to transform vectors in Rn to vectors in a different space Rm. That is exactly what is done,vector spaces
(a) A real symmetric matrix.,eigenvec_val
"detA ̸= 0, then A is invertible (and A−1 involves 1/detA).",determinants
(c) This is the row xT = [0 1] times the column Ay =,gauss elim
Of course the shapes of these matrices must match properly—B and C have the same,gauss elim
"of this section, that has cosθ and sinθ in a 2 by 2 block. Then we could cycle through",computations
the entries of A by 1000; then λ1 will be multiplied by 1000 and the matrix will look,computations
"In case AB = BA (not generally true!), how do you prove that BTAT = ATBT?",gauss elim
"the Fundamental Theorem, b is a combination of the columns!",orthogonality
separating hyperplane is fundamental to mathematical economics.,linear_prog
3u + 5v + 7w = 1,gauss elim
ponent of x (thereby reducing A to B).,pos_def_matrices
The total v+w still remains constant. How are the λ’s changed now that A is changed,eigenvec_val
points—what is happening in Figure 3.9b?,orthogonality
"(a+b)−1 is hard to simplify, while 1/ab splits into 1/a times 1/b. But for matrices the",gauss elim
". By averaging, the variance drops from",orthogonality
"duality, which comes next.",linear_prog
"times (1,0,0) from the second row, which removes ℓ21.) Both sides of (7) end up equal",gauss elim
the nullspace becomes more than just a line in n-dimensional space. The nullspace has,vector spaces
"The system is 8 by 8, with ﬁve currents and three potentials. Elimination of y’s reduces",vector spaces
hundred nodes and a few thousand edges—which will be given a direction by an arrow,vector spaces
space contains less than all vectors:,vector spaces
E2 = ∥ax−b∥2 = (a1x−b1)2 +···+(amx−bm)2.,orthogonality
1.6 Inverses and Transposes,gauss elim
This point x = A−1b will be a minimum if A is positive. Then the parabola P(x) opens,pos_def_matrices
functions. The combination 3 f(x)−4g(x) is the function h(x) =,vector spaces
"come from experimental data or from roundoff. We may suppose that δb is small, but",computations
"the line through a on which b was projected in the previous section! In fact, since there",orthogonality
"inner product, the transpose, the deﬁnitions of symmetric and orthogonal matrices, all",eigenvec_val
test is det(A−λI) = 0. This polynomial of degree n in λ has exactly n roots. The matrix,determinants
3.2 Cosines and Projections onto Lines,orthogonality
"Frankly, I think that is a mistake. It is certainly true that not all matrices are diagonaliz-",eigenvec_val
time genuinely important. This differential equation describes a process of diffusion.,eigenvec_val
"x1 the leaving variable. The new corner is x∗ = (0,0,0, 1",linear_prog
Example 4. What linear transformation takes x1 and x2 to Ax1 and Ax2?,vector spaces
9. The column picture for the previous exercise (singular system) is,gauss elim
"needed in New England 1500, 3000, and 3700 miles away. If shipments cost one",linear_prog
(ATA)−1AT (which exists if A has independent columns). Show that this is a left-,orthogonality
"stable if a = 0, or stable if a < 0; the factor eat approaches inﬁnity, remains bounded,",eigenvec_val
"The ﬁrst entry is 1 + 1 + 1, or c1 + c3 + c5 when C is included, because edges 1, 3, 5",vector spaces
vectors are—one entry at a time. In fact we may regard vectors as special cases of,gauss elim
"Ax = b will be inconsistent. Probably, there will not exist a choice of x that perfectly",orthogonality
"Suppose we are again in three dimensions, and the three planes in the row picture do not",gauss elim
"built-in choices for A illustrate three possibilities: 0, 1, or 2 real eigenvectors.",eigenvec_val
"that come easily from Rayleigh’s principle, but only with great difﬁculty from the",pos_def_matrices
"If you change the last entry of A from 6 to 7, all the dimensions are different. The",vector spaces
(c) Why is eΛt unitary?,eigenvec_val
1 3 3 2,vector spaces
the matrix A becomes −d2/dx2). We can write down the energy whose minimum is,pos_def_matrices
Skew-symmetric: AT = −A,eigenvec_val
"AC = I. Then B = C, according to this “proof by parentheses”:",gauss elim
directly from equation (7).,computations
cosθ +isinθ = eiθ.,orthogonality
"x = 0 and x = 1 = (n+1)h, the boundary values are u0 = 0 and un+1 = 0.",gauss elim
"In the stable case, the powers Ak approach zero and so does uk = Aku0.",eigenvec_val
. The vector c is in the row space of A when there is a solution to,vector spaces
2(column 1) and C = column 3− 2,orthogonality
The system might or might not be singular.,gauss elim
Remark 4. Suppose b is actually in the column space of A—it is a combination b = Ax,orthogonality
"6. Compute σ = ∥x∥, v = x+σz, and H = I −2vvT/vTv, Verify Hx = −σz:",computations
"AT goes in the opposite direction, from Rm to Rn and from C(A) back to C(AT).",orthogonality
"of B. The entry 8 is (4)(2)+(0)(−1), from the second row and second column.",gauss elim
"This f(x,y) behaves near (0,0) in the same way that F(x,y) behaves near (α,β).",pos_def_matrices
solution unless the vector on the right-hand side is a combination of,gauss elim
"column of U, and the triangular U−1AU = T has the eigenvalues on its diagonal:",eigenvec_val
"i xi = 0, and the normalization xT",pos_def_matrices
(ATW TWA)�xW = ATW TWb.,orthogonality
"x0. Problem: Multiply x1 = x0+y by LU, write the result as a splitting Sx1 = Tx0+b,",computations
"16. Let x be the column vector (1,0,...,0). Show that the rule (AB)x = A(Bx) forces the",gauss elim
"combination (f1, f2, f3, f4) in the row space will have that same property:",vector spaces
transpose of A is denoted by AT. Its columns are taken directly from the rows of A—the,gauss elim
0 1 0 0,determinants
"(a) If the columns of A are linearly independent, then Ax = b has exactly one solution",vector spaces
"and nothing can be done. U is upper triangular, but its pivots are not on the main diago-",vector spaces
"Normally this eigenvector would be unknown, and to approximate it we admit only the",pos_def_matrices
4.2 Properties of the Determinant,determinants
"(2,−1,2), and (2,2,−1)? Where are the other four vertices?",determinants
the plane of columns 2 and 3. Only two columns are independent.,gauss elim
"Find H−1 and write v = (7,5,3,1) as a combination of the columns of H.",vector spaces
"There is a deeper signiﬁcance to AT, Its close connection to inner products gives a new",orthogonality
a/∥a∥ is a unit vector. The real problem begins with q2—which has to be orthogonal,orthogonality
(a) The only solution to Ax =,vector spaces
���� = 1 (known),determinants
leads to three equations in two unknowns:,orthogonality
"allowed, and the goal is to make M−1AM as nearly diagonal as possible.",eigenvec_val
"This is called the Gauss-Seidel method, even though it was apparently unknown to",computations
0 0 0 1 2,vector spaces
33. Write ﬁve terms of the inﬁnite series for eAt. Take the t derivative of each term. Show,eigenvec_val
Networks enter all kinds of applications. Trafﬁc through an intersection satisﬁes,linear_prog
"21. Construct a matrix with the required property, or explain why you can’t.",vector spaces
Substituting n = 1 and n = 2 and n = 100 into the formula 1,gauss elim
"for n = 3 and 4, and verify your guess for n = 5. (Inverses of tridiagonal matrices",determinants
(a) Derive the equation L−1,gauss elim
reaches your eye as quickly as possible. Certainly there are more highbrow examples:,pos_def_matrices
"On the surface, we are only asking if I −A is invertible. But there is a nonnegative twist",eigenvec_val
"can reconstruct f(x). That is the classical case, which Fourier dreamt about, but in actual",orthogonality
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
c and d are also zero.,vector spaces
A−1 again by rounding all numbers to three ﬁgures:,computations
form is negative deﬁnite if and only if a < 0 and ac > b2. The same change applies for,pos_def_matrices
"these operations change c = (1,3,0) to a new vector d = (−2,1,0):",vector spaces
A is spanned by the columns. Their combinations produce the whole space:,vector spaces
Compare with the Fourier coefﬁcient b1.,orthogonality
", then CTAC has a negative determinant:",pos_def_matrices
"yTA = 0, and the vector appears on the left-hand side of A.",vector spaces
"The four identities come from the 1, 1 block when inverting these matrices:",gauss elim
M = [I]V to v =,eigenvec_val
us back to A.,gauss elim
"(b) Show that the set of all positive real numbers, with x+y and cx redeﬁned to equal",vector spaces
Ax = b just gives x1 = 0 and x2 = 0.,pos_def_matrices
substitution yields the complete solution in the opposite order—beginning with the last,gauss elim
", we want to discover those matrices.",vector spaces
20. True or false for M = all 3 by 3 matrices (check addition using an example)?,vector spaces
"produces approximate factors L′ and U′, it solves the equation with the wrong matrix",computations
"a2 = (2,2,−1), Multiply those projection matrices and explain why their product",orthogonality
these vectors span the space. Every vector v in V is some combination of the,vector spaces
40. True or false (with a counterexample if false and a reason if true):,gauss elim
the exponential eAt. (Remember that u(t) = eAtu(0).) That matrix of cosines and sines,eigenvec_val
but its eigenvalues and eigenvectors are complex.,eigenvec_val
"Again the problem is to choose �x so as to minimize the error, and again this mini-",orthogonality
"To check any set of vectors v1,...,vn for independence, put them in the columns of A.",vector spaces
which are many and growing.,pos_def_matrices
that such problems cannot have polynomial algorithms.,linear_prog
"By Property 1′ of unitary matrices, the length of a vector x is the same as the length",eigenvec_val
columns of A have no effect on this corner of the problem:,determinants
"Find numbers y1, y2, y3 to multiply the equations so they add to 0 = 1. You have",orthogonality
(a+ib)(a−ib) = a2 +b2 = r2.,eigenvec_val
"L([r k], 1:k-1) = L([k r], 1:k-1);",gauss elim
"elimination require, for n equations in n unknowns? If n is large, a computer is going to",gauss elim
"38. If v1,...,vn is an orthonormal basis for Cn, the matrix with those columns is a",eigenvec_val
and it was x = −A−1b. That solution was found by elimination (not by computing A−1).,vector spaces
"(b) What matrix transforms (2,5) to (1,1) and (1,3) to (0,2)?",vector spaces
"6 in the last equation becomes 7, the three equations combine to give 0 = 0. Now the",gauss elim
AB = SΛ1S−1SΛ2S−1 = SΛ1Λ2S−1,eigenvec_val
it produces the new error ek = (S−1T)ke0. The question of convergence is exactly,computations
"10. Suppose we want to minimize cx = x1 −x2, subject to",linear_prog
"Finally we connect inner products to AT. Up to now, AT is simply the reﬂection of A",orthogonality
Example 4 (Von Neumann’s model of an expanding economy ).,eigenvec_val
That component is the required y.,linear_prog
"Whenever there is a subset of k women who among them like fewer than k men, a",linear_prog
"the parabola, the square root is zero and λ is repeated. Below the parabola the square",eigenvec_val
3. Basis for a subspace (a set of vectors).,vector spaces
for the columns of A−1. They can be solved by elimination or by Gauss-Jordan. Row,gauss elim
"28. In the Cholesky factorization A =CCT, with C = L",pos_def_matrices
The other equation is C−1y + Ax = b. The potentials x are connected to the currents,vector spaces
"(b) Find the area of the triangle with sides v, w, and v+w. Draw it.",determinants
"V = W is the x-y plane, and QθQϕ is the same as QϕQθ. For a rotation and a",vector spaces
class) to making elimination more stable—why it is needed and how it is done.,gauss elim
"For any invertible matrix W, these rules deﬁne a new inner product and length:",orthogonality
"in x = (1,2,1,2,...). The laws for x+y and cx stay unchanged.",vector spaces
1000 → e.301000 = $1349.87.,eigenvec_val
"Remark 1. In the real symmetric case, the eigenvalues and eigenvectors are real at every",eigenvec_val
"denoted by N(A). It is a subspace of Rn, just as the column space was a",vector spaces
"is not a subspace, even though it contains zero and addition does leave us within the",vector spaces
since the zeros from early steps will be destroyed when later zeros are created.,computations
are called linear transformations. The rules can be combined into one requirement:,vector spaces
"The orthogonal eigenvectors are x = (1,−i) and y = (1,i). (Remember to take conjugates",eigenvec_val
R = rref(A). Of course rref(R) would give R again!,vector spaces
"are two unknowns C and D to be determined, we now project onto a two-dimensional",orthogonality
"23. Find A to change y′′ = 5y′ +4y into a vector equation for u(t) = (y(t),y′(t)):",eigenvec_val
(Consider A−1b = x instead of Ax = b.),computations
"no instability. While lengths stay the same, roundoff is under control. Orthogonalizing",orthogonality
13. If A = [3,gauss elim
sin2πx at x = 1,gauss elim
The optimal ω makes the largest eigenvalue of L (its spectral radius) as small as possible.,computations
"Kirchhoff’s current law: ﬂow in equals ﬂow out. For gas and oil, network programming",linear_prog
Example 3. A =,pos_def_matrices
Test for a minimum:,pos_def_matrices
use as a pivot might be zero. This could occur in the middle of a calculation. It will,gauss elim
= λ 2 −(trace)λ +(det) = 0.,eigenvec_val
(2) If A and B are symmetric then AB is symmetric.,gauss elim
"include the zero vector, or even all vectors.",vector spaces
"As θ → 0, we expect those optimal x and y to approach x∗ and y∗ for the original no-",linear_prog
of C. Therefore (AB)C = A(BC).,gauss elim
columns of A; it will be outside the column space.,orthogonality
"more unknowns we keep, the better will be the accuracy and the greater the expense.",gauss elim
The inverse matrix M−1 (which is here the transpose) goes from v to V. Combined with,eigenvec_val
"the usual x-y-z system, or a left-handed system like y-x-z.",determinants
0 are orthogonal subspaces.,orthogonality
The unit coordinate vectors are still in Cn; they are still independent; and they still form,eigenvec_val
"28. If A has λ1 = 4 and λ2 = 5, then det(A−λI) = (λ −4)(λ −5) = λ 2 −9λ +20. Find",eigenvec_val
∥Ax−b∥ = ∥UΣV Tx−b∥ = ∥ΣV Tx−UTb∥.,pos_def_matrices
"b = (1,10,100) and how many with b = (0,0,0)?",gauss elim
"into a perpendicular set, is known as Gram-Schmidt orthogonalization.",orthogonality
on A corresponds to elimination on ATA. Compare,orthogonality
The inverse of A is a matrix B such that BA = I and AB = I. There is at,gauss elim
"13. If a, b, c are given with a ̸= 0, choose d so that",vector spaces
"2. To ﬁnd q2, subtract from",orthogonality
"concentrations decay, and λ1 is the more important because only an exceptional set of",eigenvec_val
"Rotations Q, Projections P, and Reﬂections H",vector spaces
of B. Why can’t A and B be 3 by 3 matrices of rank 2?,orthogonality
"The ﬁrst step is to understand how eigenvalues can be useful, One of their applications",eigenvec_val
Example 1. A =,eigenvec_val
"reduce to zero; but no matter what, at least one variable must be free. This free variable",vector spaces
"Solution x1 = 3 and x2 = 0 are feasible, with cost x1 +4x2 = 3. In the dual, y1 = 1",linear_prog
eigenvalues of A is,computations
est eigenvalue is increased by adding B : λ2(A+B) > λ2(A).,pos_def_matrices
8.2 The Simplex Method,linear_prog
"Problems 24–33 use cofactors Cij = (−1)i+j detMij. Delete row i, column j.",determinants
"and that takes us to complex numbers, complex vectors, and complex matrices.",eigenvec_val
"That is a linear symmetric system, from which e has disappeared. The unknowns are the",vector spaces
"(b) If A1, A2, A3 have determinants 5, 6, 7, ﬁnd the three pivots.",determinants
(b) a symmetric matrix: ai j = a ji for all i and j.,gauss elim
23. Diagonalize A and compute SΛkS−1 to prove this formula for Ak:,eigenvec_val
"higher derivatives appear—and it is linear in the unknowns, It also has constant coefﬁ-",eigenvec_val
"independent eigenvectors—in other words, whether A−λI has rank n− p. To complete",eigenvec_val
"what the theory of optimization does in a continuous problem, where “ﬁrst derivatives",pos_def_matrices
9. Apply to the following matrix A a single QR step with the shift α = a22—which in,computations
1 0 · 0,orthogonality
"unique solution to Ax = b. If d is not zero, an exchange P13 of rows 1 and 3 will move",gauss elim
"point u = 0, v = 0, w = 0. Changing the right side moves the plane parallel to itself, and",gauss elim
"For a genuine diffusion matrix, the eigenvalues λ are all negative and the frequencies",eigenvec_val
The Rayleigh-Ritz idea—to minimize over a ﬁnite-dimensional family of V’s in place,pos_def_matrices
"I will write down A and its factors for our example, and explain them at the right",gauss elim
"vector b− p of length h is the second row b = (a21,a22), minus its projection p onto the",determinants
2. Does the product of 5 reﬂections and 8 rotations of the x-y plane produce a rotation,vector spaces
"whether this is the only way to produce zero. If so, the vectors are independent.",vector spaces
What we did not fully recognize was that even ﬁnite differences become incredibly,pos_def_matrices
0 1 4 5,vector spaces
"In contrast, the other two subspaces are in R3. The column space is the line through",orthogonality
"a = (a1, a2)",orthogonality
Figure 5.2: Stability and instability regions for a 2 by 2 matrix.,eigenvec_val
length of the function f(x) = ex (over the interval 0 ≤ x ≤ 1). What is the inner,orthogonality
12. Find a basis for the orthogonal complement of the row space of A:,orthogonality
6. The question is how to use those six numbers to solve the system.,gauss elim
y4 +y5 = 0,vector spaces
rectangular system with no solution for most b.,orthogonality
is ampliﬁed by matrix multiplication: ∥A∥ = max(∥Ax∥/∥x∥). The norm of the identity,computations
S. But there are two questions that need to be asked:,orthogonality
"could be repeated four times. Therefore A has at least one unit eigenvector x1, which we",eigenvec_val
of A+B is larger than the smallest eigenvalue of A.,pos_def_matrices
"another: Elimination changes the eigenvalues, which we don’t want.",eigenvec_val
P2 = A(ATA)−1ATA(ATA)−1AT = A(ATA)−1AT = P.,orthogonality
You can see when this example has one solution or no solution:,vector spaces
", what is the W-inner product of (1,0) with (0,1)?",orthogonality
"This example, in which the eigenvalues can be found by inspection, points to one",eigenvec_val
"26. If aij = 1 above the main diagonal and aij = 0 elsewhere, ﬁnd the Jordan form (say",eigenvec_val
"either b is in the column space, or it has a component sticking into the left nullspace.",linear_prog
The other rows of LU agree similarly with the rows of A.,gauss elim
"picks α = ∥x∥ and β = ∥y∥, so that x/α and y/β are unit vectors; the eigenvectors",eigenvec_val
A multiple of the second equation will be subtracted from the remaining equations (in,gauss elim
Fundamental theorem of orthogonality The row space is orthogonal,orthogonality
"3. What multiple of a = (1,1,1) is closest to the point b = (2,4,4)? Find also the point",orthogonality
can display all 3 by 3 permutations (there are 3! = (3)(2)(1) = 6 matrices):,gauss elim
real. They solve (A −λI)x = 0 and can be computed by elimination. But they will not,eigenvec_val
cause U lies above the diagonal. Their product is detL = (1− ω)n. (This explains why,computations
0 1 0 0,vector spaces
"Their eigenvalues are 3, 3 and 1, 1. They are not singular! The problem is the shortage",eigenvec_val
"Those new unknowns y1,...,yℓ are called Lagrange multipliers. They build the",pos_def_matrices
"produces the vector on the right side. Those vectors (2,1) and (−1,1) are represented",gauss elim
4x − 5y +,gauss elim
by projection onto the y-axis?,vector spaces
"for e j is effectively changed to (n − j)2/2. Summing over all j, the total for forward",gauss elim
Formulas for the Determinant,determinants
provided that the “scalars” are real numbers.,eigenvec_val
"The vector c on the right-hand side, which appeared after the forward elimination steps,",vector spaces
The test for b to be in the column space is Kirchhoff’s Voltage Law:,vector spaces
no multiplication is needed. This number 1,orthogonality
"15. In our method for football rankings, should the strength of the opposition be consid-",vector spaces
"orthogonal. In choosing a basis, we tend to choose an orthogonal basis.",orthogonality
the same for CTAC as for QTAQ. And A has exactly the same eigenvalues as the similar,pos_def_matrices
(b) Write its eigenvalues. If Ax = λx then eAtx =,eigenvec_val
"row 1 = (1, 0)",determinants
"and over again.3 When there is a fourth vector, we subtract away its components in the",orthogonality
There are m−r solvability conditions on b or c or d.,vector spaces
1.28 (a) If the rows of A are linearly independent (A is m by n) then the rank is,vector spaces
That reﬂection matrix is also a permutation matrix! It is alge-,vector spaces
"This is good, but the most important question is exactly the opposite: How would we",gauss elim
b in left nullspace,orthogonality
32. Find a matrix A to illustrate each of the unstable regions in Figure 5.2:,eigenvec_val
14. Show that the smallest eigenvalue λ1 of Ax = λMx is not larger than the ratio a11/m11,pos_def_matrices
"After including the n3/3 operations on A itself, the total seems to be 4n3/3.",gauss elim
Suppose A and B are linear transformations from V to W and from U,vector spaces
Obviously the computer was going to bring about a revolution. It was the method,pos_def_matrices
56. Suppose column 4 of a 3 by 5 matrix is all 0s. Then x4 is certainly a,vector spaces
its ﬁrst column comes from the ﬁrst basis vector (projected to itself). The second,vector spaces
λ1 = λ2 and,eigenvec_val
Problems 37–40 are about areas dA and volumes dV in calculus.,determinants
"Equally important, any vectors that achieve yb = cx must be optimal. At that point",linear_prog
"At the last step, we divided the rows by their pivots 2 and −8 and 1. The coefﬁcient",gauss elim
key point is that every elimination step has a meaning for the graph.,vector spaces
"21. If detA ̸= 0, at least one of the n! terms in the big formula (6) is not zero. Deduce",determinants
"solution of RTR�x = RTb. That matrix ARTR is not only symmetric but positive deﬁnite,",pos_def_matrices
(c) the new entry that replaces aij after that subtraction.,gauss elim
r nonzero entries 1/σi. All the blank spaces are zeros. Notice that (Σ+)+ is Σ again.,pos_def_matrices
"all the entries below the diagonal, choosing at each step a rotation θ that will produce a",computations
crosses the ﬁrst line at the solution.,gauss elim
Note. This proof of real eigenvalues looks correct for any real matrix:,eigenvec_val
"any of the circles, and conclude that A is nonsingular.",computations
"make Q diagonal requires complex vectors, since all real vectors are rotated.",vector spaces
"2, subtract row 1 from row 3, then subtract row 2 from row 3.",gauss elim
Decay or growth is governed by the factor eαt.,eigenvec_val
"23. Continuing Problems 21–22, ﬁnd the projection matrix P3 onto a3 = (2,−1,2). Ver-",orthogonality
"tary, factorizable into LU, factorizable into QR?",eigenvec_val
"14. For two linear equations in three unknowns x, y, z, the row picture will show (2 or 3)",gauss elim
= UΣV T = 1,pos_def_matrices
"Put y(t) from part (a) into u(t) = (y,y′). This solves Problem 6 again.",eigenvec_val
method u−k = A−ku0 converge to (after you multiply by .6k)?,computations
"ric, and tridiagonal? What are its pivots?",gauss elim
(e) All arithmetic progressions: x j+1 −x j is the same for all j.,vector spaces
grange multipliers. And they answer the key question: How does the minimum cost,linear_prog
The nullspace is the orthogonal complement of the row space in Rn.,orthogonality
(4) BA has the ﬁrst and last columns of A reversed.,gauss elim
48. Prove that the inverse of a Hermitian matrix is again a Hermitian matrix.,eigenvec_val
3. Find the dimension and a basis for the four fundamental subspaces for,vector spaces
Chapter 2 Vector Spaces,vector spaces
"In MATLAB, A([r k] :) exchanges row k with row r below it (where the kth pivot has",gauss elim
3H The projection of the vector b onto the line in the direction of a is p = �xa:,orthogonality
20. Diagonalize the 2 by 2 skew-Hermitian matrix K =,eigenvec_val
"The (2,1) entry in this product is a11sinθ + a21cosθ, and we choose the angle θ that",computations
"If λ1 is not already approximated, the shifted inverse power method has to generate its",computations
"n2 separate multiplications, coming from the n2 entries in the matrix, the matrix-vector",orthogonality
are the free variables in Ax = b. The remaining m components are the basic variables or,linear_prog
"the Vj, (2) compute A and M, and (3) solve Ay = λMy. I don’t know why that costs a",pos_def_matrices
those are closer to x2,pos_def_matrices
"the zero vector (0,0,0)?",gauss elim
real symmetric matrix A to be positive deﬁnite:,pos_def_matrices
"Strangely, to prove that the eigenvalues are real we begin with the opposite possibility—",eigenvec_val
onto line through a,orthogonality
Echelon Form U and Row Reduced Form R,vector spaces
"12. If V is the subspace spanned by (1,1,0,1) and (0,0,1,0), ﬁnd",orthogonality
5.16 By trying to solve,eigenvec_val
point in the column space that is closest to b. But the word “closest” has a new meaning,orthogonality
. Volumes or the big,determinants
"25. (Important) If A has row 1 + row 2 = row 3, show that A is not invertible:",gauss elim
"a maximum of F(x,y).",pos_def_matrices
54. Is there a 3 by 3 matrix with no zero entries for which U = R = I?,vector spaces
The next section will concentrate on solving linear programs. This is the time to describe,linear_prog
"multiply columns by rows, the matrix A becomes a combination of one-dimensional",eigenvec_val
"provided |λ1| < |λ2|. Often it is λ1 that is wanted in the applications, and then",computations
"The pictures are really striking, as more and more singular values are included. At",pos_def_matrices
a normalization for δb; our problem is to compare the relative change ∥δb∥/∥b∥ with,computations
also simpliﬁes the theory. The basic questions of existence and uniqueness—Is there,vector spaces
"y j at node j (the other V’s are zero there), so its graph is easy to draw (Figure 6.7b).",pos_def_matrices
"w−1 = e−iθ = w. Since U is unitary, its inverse is found by transposing (which changes",eigenvec_val
∗ ∗ 0 0,computations
1+w+w2 +w3 +w4 +w5?,orthogonality
"where x+2y = 4. If we add a contradictory constraint like x+2y ≤ −2, the feasible set",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
onal “columns” are the sines and cosines);,orthogonality
"Rows 1, 2, 4 are a basis for the row space. In each row the entries add to zero. Every",vector spaces
"arrangement, which guarantees that operations on the left-hand side of the equations are",gauss elim
n there is absolutely no question. Elimination wins (and this is even the best way to,gauss elim
"We are minimizing the distance from b to the line through a, and calculus gives the same",orthogonality
0 0 ∗ ∗ ∗,computations
ae+bg a f +bh,determinants
ﬁnd out what multiple (say ℓ) of the pivot equation is to be subtracted. When we do,gauss elim
The determinant of A is a combination of any row i times its cofactors:,determinants
"The diagonalization A = QΛQT leads to xTAx = xTQΛQTx = yTΛy. If A has rank r, there",pos_def_matrices
"The concentration v(t) in S1 is changing in two ways. There is diffusion into S0, and",eigenvec_val
= λ 2 −7λ −8 = (λ −8)(λ +1).,eigenvec_val
x + y = 5,gauss elim
"(a) If the vectors x1,...,xm span a subspace S, then dimS = m.",vector spaces
y = C +Dt +Et2. In the inconsistent system Ax = b that comes from the four mea-,orthogonality
(c) A is not diagonalizable.,eigenvec_val
This is exactly the way we have been solving Ux = 0. The basic example above has,vector spaces
triangular L and an upper triangular U.,gauss elim
through it—and it also shows the simplicity of the logic.,orthogonality
(a1 +···+an)2 ≤ n(a2,orthogonality
"2. If r ≥ 0, stop: the current solution is optimal. Otherwise, if ri is the most",linear_prog
"38. These Jordan matrices have eigenvalues 0, 0, 0, 0. They have two eigenvectors (ﬁnd",eigenvec_val
12. Decide whether F = x2y2 − 2x − 2y has a minimum at the point x = y = 1 (after,pos_def_matrices
"4D If A is factored into LDU, the upper left corners satisfy Ak = LkDkUk. For",determinants
(c) the product of two numbers on the unit circle?,eigenvec_val
", and c =",linear_prog
"the bottom row r, If B−1u ≤ 0, the next corner is inﬁnitely far away and the minimal",linear_prog
"years ago. Mathematics keeps changing direction! After all, a single number can tell",determinants
"By rule 7, their determinants all equal 1. Also, any diagonal matrix is the same as its",determinants
"Hermitian, skew-Hermitian, and unitary matrices are also in this class. They correspond",eigenvec_val
The ninth property is the product rule. I would say it is the most surprising.,determinants
The coefﬁcient 2 is the ﬁrst pivot. Elimination is constantly dividing the pivot into the,gauss elim
"is n, all those edges from the source and into the sink are ﬁlled—and the ﬂow produces",linear_prog
reversal on row exchanges. The determinant of a permutation matrix P was the only,determinants
2 )P2 that det(P1 +P2) = 0.,determinants
"identity matrix. There is a full set of pivots, all equal to 1, with zeros above and below.",vector spaces
"To organize the whole discussion, we take each of the four subspaces in turn. Two of",vector spaces
"If n itself is a prime, a completely different algorithm is used.",orthogonality
E−1 applied to F−1 applied to G−1 applied to I produces A.,gauss elim
To compute the minimum exactly is equivalent to solving the differential equation ex-,pos_def_matrices
"(0,1,−1,0), a3 = (0,0,1,−1).",orthogonality
"ﬁrst row. The key point is this: By rule 5, detA is unchanged when a multiple of row 1",determinants
2 governs the decay; after the ﬁrst step every uk is 1,eigenvec_val
"17. What values of α produce instability in vn+1 = α(vn +wn), wn+1 = α(vn +wn)?",eigenvec_val
"independent at t = 0, they remain linearly independent forever. If the initial vectors are",eigenvec_val
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
(a) These four vectors are dependent because,vector spaces
invertible: y+z = b1 and 2y+2z = b2 usually have no solution.,vector spaces
"eigenvector of Ak. When S diagonalizes A, it also diagonalizes Ak:",eigenvec_val
real effect of the ﬁrst step is to split u0 into the two eigenvectors of A:,eigenvec_val
"The most important application, and the reason this chapter is essential to the book,",determinants
"5.21 If M is the diagonal matrix with entries d, d2, d3, what is M−1AM? What are its",eigenvec_val
(b) What is the SVD for AT and for A−1?,pos_def_matrices
"n is wm, the two sums are",orthogonality
9. Suppose elimination fails because there is no pivot in column 3:,gauss elim
R2 = A. How many square roots will there be?,eigenvec_val
17. From A = RTR. show for positive deﬁnite matrices that detA ≤ a11a22···ann. (The,pos_def_matrices
. What R is virtually certain if the random A is 4 by 3?,vector spaces
"multiplications by Dn/2 in equation (13), which is really a factorization of Fn:",orthogonality
the minimum cost in this problem will be zero—with w∗ = 0.,linear_prog
"vectors other than zero. In short, A−λI must be singular.",eigenvec_val
bor above it to turn up. The “ﬁve-point matrix” has bandwidth N: This matrix has had,computations
2. S = triangular pail of A (Gauss-Seidel method).,computations
"2. After the preceding simplex step, prepare for and decide on the next step.",linear_prog
p = A(ATA)−1ATb = A(ATA)−10 = 0.,orthogonality
"least one of the circles C1,...,Cn, where Ci has its center at the diagonal entry aii. Its",computations
(a) What formula for x1 comes from left side = right side?,determinants
1.4 Matrix Notation and Matrix Multiplication,gauss elim
and the magnitude is,eigenvec_val
"vectors in its nullspace, the potentials x are not well determined. In the ﬁrst case x does",vector spaces
each of Lc = b and Ux = c?,gauss elim
The triangular factors have detL = detU = 1 and detD = d1···dn.,determinants
"Row space: dimension r = n−1, independent rows from any spanning tree.",vector spaces
go out from the origin. Two vectors are dependent if they lie on the same line. Three,vector spaces
F is r by n−r,vector spaces
"For the best straight line as in Figure 3.9a, ﬁnd its four heights pi and four errors ei.",orthogonality
notice that ours came directly from the calculation of ∥b− p∥2. This stays nonnegative,orthogonality
The column space looks harder to describe. Which scores ﬁt perfectly with a set of,vector spaces
a number eiθ on the unit circle: z = reiθ. That expresses z in “polar coordinates.” If,pos_def_matrices
"For n = 2, we will be proving that ad − bc is correct. For n = 3, the determinant",determinants
you try to measure the position of a particle you change its momentum.,eigenvec_val
How are the properties of a matrix reﬂected in its eigenvalues and eigenvectors? This,eigenvec_val
"extremely thin within n-dimensional space, although it would look solid to us.",gauss elim
"Finally, to keep their employees really moving, banks offer continuous compounding.",eigenvec_val
(c) ﬁnd the eigenvalues of A.,eigenvec_val
"ferences are b1,...,b6, when is it possible to assign potentials x1,...,x4 so that the",vector spaces
There is a solution when A−λM is singular. The special choice M = I brings back the,pos_def_matrices
"Remark on the calculations I think it is easier to compute the orthogonal a, B, C,",orthogonality
"x1,...,xn. Every other vector x is a combination of those particular vectors (they span",vector spaces
exactly to least-squares problems in one unknown.,orthogonality
"The next step is to compound every day, on 5(365) days. This only helps a little:",eigenvec_val
"parallel to themselves, and for b = (2,5,7) the ﬁgure is suddenly different. The lowest",gauss elim
vTx = (ATz)Tx = zTAx = zT0 = 0.,orthogonality
"60. Construct a matrix whose nullspace consists of all combinations of (2,2,1,0) and",vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
21. Explain by mathematics or economics why increasing the “consumption matrix” A,eigenvec_val
one row. The solutions to Ax = 0 lie on a,gauss elim
"45. If you take powers of a permutation, why is some Pk eventually equal to I? Find a 5",gauss elim
"The family of matrices M−1AM includes A itself, by choosing M = I. Any of these",eigenvec_val
20. Why does a complete graph with n = 6 nodes have m = 15 edges? A spanning tree,vector spaces
"(b) ∞, regardless of b.",vector spaces
6. Find the projection of b onto the column space of A:,orthogonality
Forward elimination also acts on the right-hand side (subtracting the same multiples,gauss elim
cross-section through a bowl.,pos_def_matrices
"2.3 Linear Independence, Basis, and Dimension",vector spaces
"Suppose x is an n-dimensional vector. When A multiplies x, it transforms that vector",vector spaces
"the row picture, the column picture, the coefﬁcient matrix, the solution?",gauss elim
The main object of this section is to solve uk+1 = Auk. That leads us to Ak and powers,eigenvec_val
"18. Suppose A has eigenvalues 0, 3, 5 with independent eigenvectors u, v, w.",eigenvec_val
"To decide if b is a combination of the columns, we try to solve Ax = b. To decide if",vector spaces
"(b) If ATy = c has a solution and Ax = 0, then x is perpendicular to",orthogonality
fact (L⊥)⊥ is the same as,orthogonality
This will bring us back to the product of the pivots.,determinants
pivot variables are completely determined in terms of v and y:,vector spaces
AAT). The columns of V are right singular vectors (unit eigenvectors of ATA).,pos_def_matrices
and λ2 = 5 with x2 =,eigenvec_val
the new name: A complex matrix with orthonormal columns is called a unitary matrix.,eigenvec_val
cx = (cN −cBB−1N)xN +cBB−1b,linear_prog
(b) The 4 by 4 matrix with aij = (−1)ij.,vector spaces
ﬁrst row of AB uses the numbers 2 and 3 from the ﬁrst row of A. Those numbers give,gauss elim
"4.6 Find all the cofactors, and the inverse or the nullspace, of",determinants
"tions. (Change 6 to 12 in the example, and elimination will lead to 0 = 0. Now y",gauss elim
Determinants are much further from the center of linear algebra than they were a hundred,determinants
6. The higher order equation y′′ +y = 0 can be written as a ﬁrst-order system by intro-,eigenvec_val
17. Find two ways to choose nonzeros from four different rows and columns:,determinants
3.5 The Fast Fourier Transform,orthogonality
"It is remarkable that F is so easy to invert. If that were all (and up to 1965 it was all),",orthogonality
"same applies to A2,A3,..., and every QR step begins with a tridiagonal matrix.",computations
"2nℓ, when starting with the power n = 2ℓ and going all the way to n = 1—where",orthogonality
"The constants are c1 = 3 and c2 = 1, and the solution to the original equation is",eigenvec_val
"3,−1) have opposite signs and the masses move in opposite directions. This time the",pos_def_matrices
"matrix in the left-hand half became the identity. Since A went to I, the same operations",gauss elim
1. This matrix has eigenvalues 2−,computations
T(v+w) = T(v)+T(w) by choosing w =,vector spaces
"have no solution, one solution, or inﬁnitely many solutions:",gauss elim
This vector x = 0 is orthogonal to every vector in Rn.,orthogonality
33. (Left nullspace) Add the extra column b and reduce A to echelon form:,vector spaces
(c) The solution xp with all free variables zero is the shortest solution (minimum,vector spaces
"34. Prove that if V and W are three-dimensional subspaces of R5, then V and W must",vector spaces
ﬂow graph shows the order that the c’s enter the FFT and the log2n stages that take them,orthogonality
"5. If u is a unit vector, show that Q = I − 2uuT is a symmetric orthogonal matrix. (It",orthogonality
1.7 Special Matrices and Applications,gauss elim
"around the unit circle. At the same time the screen shows Ax, in color and also moving.",eigenvec_val
"F(x,y) to have a saddle at (0,0)?",pos_def_matrices
discusses poker and other matrix games. The MIT students in Bringing Down the House,linear_prog
"and denominator are doubled, and the cosine is unchanged. Reversing the sign of b, on",orthogonality
makes computing all corners totally impractical for large m and n. It is the task of Phase,linear_prog
"4. Write out E2 = ∥Ax−b∥2 and set to zero its derivatives with respect to u and v, if",orthogonality
freedom (columns of U can be multiplied by any eiθ).,eigenvec_val
That point x = 2 and y = 3 will soon be found by “elimination.”,gauss elim
"ponents of x are zero. More than n planes pass through the corner, so a basic variable",linear_prog
∥x+δx∥ ≤ ∥A−1∥∥δA∥ = c∥δA∥,computations
matrices (and their multiples αQ) are the only perfectly conditioned matrices.,computations
"a3 = (2,−1,2). Add the three projections p1 + p2 + p3.",orthogonality
"B is orthogonal to q1. It is the part of b that goes in a new direction, and not in the a. In",orthogonality
"Proof. To show that B−1A−1 is the inverse of AB, we multiply them and use the associa-",gauss elim
Deﬁnition. A subspace of a vector space is a nonempty subset that satisﬁes the require-,vector spaces
1 1 0 0,gauss elim
the formula uk = c1λ k,eigenvec_val
"3. Find also the projection of b = (0,3,0) onto a3 = (2",orthogonality
"the same? If y = (1,2,3,4) is in the left nullspace of A, write down a vector in the",vector spaces
1 2 0 1,vector spaces
row is subtracted from another. Why is a zero eigenvalue not changed by the steps,eigenvec_val
In fact TV1 = V1 (since V1 is already on the line L) and TV2 = 0 (since V2 is perpen-,eigenvec_val
"Again the singular case is separate; A is singular if and only if AT is singular, and we",determinants
nullspace is telling us those dependencies.,vector spaces
1.3 An Example of Gaussian Elimination,gauss elim
give an example of orthogonal vectors that are not independent.,orthogonality
"the long run, the other factor (.7)k becomes extremely small. The solution approaches",eigenvec_val
ﬁnite element method in Section 6.4.,pos_def_matrices
that det(M) = 0. The problem is to ﬁnd det(I +M) by any method:,determinants
minimum at the point where Ax = b. At that point Pmin = −1,pos_def_matrices
(B) un+1 −un = Aun+1 or un+1 = (I −A)−1un (backward Euler).,eigenvec_val
"b = Qx, so this again proves that lengths are preserved: ∥Qx∥2 = ∥x∥2.",orthogonality
an eighth of three-dimensional space (the positive octant). How is this cut by the two,linear_prog
"nants. They gave the same answer y = 2, coming from the same ratio of −6 to −3.",gauss elim
15. The SOR splitting matrix S is the same as for Gauss-Seidel except that the diagonal,computations
Normal matrices are exactly those that have a complete set of orthonormal,eigenvec_val
best line and the best �x? Answer: The zero line—which is the horizontal axis—and,orthogonality
(d) reﬂect every vector through the 45° line x1 = x2.,gauss elim
"form a two-dimensional subspace—the nullspace of A, In the example, N(A) is gener-",vector spaces
"trafﬁc keeps circulating, and the simplest solutions are currents around small loops.",vector spaces
(b) A+B is invertible although A and B are not invertible.,gauss elim
"idea, to minimize over a subspace of V’s instead of over all possible v(x). The function",pos_def_matrices
"Wherever you ﬁnd a system of equations, rather than a single equation, matrix theory",eigenvec_val
three equations have no solution—why not? The ﬁrst two planes meet along the line,gauss elim
Dimension of a Vector Space,vector spaces
"y1,...,ym are a basis for W. Each linear transformation T from V to W is",vector spaces
17. Find the determinants of,determinants
2. Describe in words all matrices that are similar to,eigenvec_val
"9. Solve Hx = b = (1,0,...,0) for the 10 by 10 Hilbert matrix with hij = 1/(i+ j −1),",gauss elim
"What matrix P2 projects onto the y axis to produce (0,y)? If you multiply (5,7) by",gauss elim
25. The characteristic polynomial of A =,eigenvec_val
+ dy = 0,determinants
"there is a nonzero eigenvector x. To be of any use, the nullspace of A−λI must contain",eigenvec_val
"The normalized q2 is B divided by its length, to produce a unit vector:",orthogonality
The empty and unbounded cases should be very uncommon for a genuine problem in,linear_prog
outside the system. In that case A goes back to a Markov matrix. The columns add up,eigenvec_val
"complex exponential, with amplitude one and phase angle θ:",orthogonality
"vector (4,4) on the right side).",gauss elim
", ﬁnd a choice of x that gives a smaller R(x) than the bound λ1 ≤ 2",pos_def_matrices
6 by 5 matrix whose column space is spanned by the ﬁrst three and whose row,vector spaces
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
ues of ATA. Since Householder transformations can only prepare for the eigenvalue,computations
"Compute eKt = SeΛtS−1, and verify that eKt is unitary. What is the derivative of eKt",eigenvec_val
"where that is possible: A+Ax = x for x in the row space. On the left nullspace, nothing",orthogonality
1.3 An Example of Gaussian Elimination,gauss elim
(d) Left nullspace contains,vector spaces
"(all x1,x2,x3,x4 ≥ 0).",linear_prog
zero above the pivot. This time we subtract a row from a higher row. The ﬁnal result,vector spaces
"straight line whatsoever. The columns are not unit vectors, so �C and �D have the length",orthogonality
1. an m by n matrix A.,linear_prog
"The QR factorization is like A = LU, except that the ﬁrst factor Q has orthonormal",orthogonality
"strong, the satellite should move with nearly constant velocity v: b = b0 +vt.",orthogonality
is just L−1b as in the previous chapter. Start now with Ux = c.,vector spaces
The only trace left of the slack variable w is in the fact that the new matrix A is m by,linear_prog
There is no one best way to ﬁnd the eigenvalues of a matrix. But there are certainly,computations
"The column space of P is ﬁlled with eigenvectors, and so is the nullspace. If those spaces",eigenvec_val
"6. (a) If A2 = I, what are the possible eigenvalues of A?",eigenvec_val
"18. If S = {0} is the subspace of R4 containing only the zero vector, what is S⊥? If S is",orthogonality
"That is a typical difference equation, leading to the powers of A =",eigenvec_val
"27. If aij is i+ j, show that detA = 0. (Exception when n = 1 or 2.)",determinants
0 0 0 1,gauss elim
The symmetric matrix A is positive deﬁnite if and only if,pos_def_matrices
"14. If MIT beats Harvard 35-0, Yale ties Harvard, and Princeton beats Yale 7-6, what",vector spaces
with respect to two different bases (the v’s and the V’s) are similar:,eigenvec_val
component along the line is zero. They lie in the nullspace = perpendicular plane.,orthogonality
"odd. For (n,...,1) to (1,...,n), show that n = 100 and 101 are even, n = 102 and",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
y by Ohm’s Law. The diagonal matrix C contains the ﬁve conductances ci = 1/Ri. The,vector spaces
The QR Algorithm for Computing Eigenvalues,computations
"triangular, with 1s on the diagonal. The multipliers ℓij (taken from elimination)",gauss elim
way of describing this thin subset is so simple that it is easy to overlook.,vector spaces
"There does remain one property that is shared by rotations and reﬂections, and in fact",orthogonality
"20. Do these matrices have determinant 0, 1, 2, or 3?",determinants
"ized problem is parallel to the familiar Ax = λx, and λ > 0. M is a mass matrix for the",pos_def_matrices
battery in the edge of strength bi. The rest of the drop is e = b−Ax across the resistor:,vector spaces
This false proof does suggest what is true. If the eigenvector is the same for A and,eigenvec_val
each row was divided by the pivot in D. Then L and U are treated evenly. An example,gauss elim
of special solutions form the nullspace—all solutions to Ax = 0.,vector spaces
The fundamental equations of equilibrium combine Ohm and Kirchhoff into a cen-,vector spaces
"but A = −I = negative deﬁnite. The determinant test is applied not only to A itself,",pos_def_matrices
"Notice that Ax = λx is a nonlinear equation; λ multiplies x. If we could discover λ, then",eigenvec_val
"36. If J is the 5 by 5 Jordan block with λ = 0, ﬁnd J2 and count its eigenvectors, and",eigenvec_val
8 + i sin 2π,orthogonality
0 1 0 1,determinants
"Q rotates every vector through the angle θ, and QT rotates it back through −θ. The",orthogonality
C(A) and N(A): all attainable right-hand sides b and all solutions to Ax = 0.,vector spaces
0 0 0 0 0,vector spaces
"The most popular way is to work directly with B−1, calculating it explicitly at the ﬁrst",linear_prog
"represented by a straight line in the x-y plane. The line goes through the points x = 1,",gauss elim
"In short, the dual of a minimum problem is a maximum problem. Now y ≥ 0:",linear_prog
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"ing P to I, is the number of exchanges always even or always odd? If so, its determinant",determinants
"ample 3 (following, with numbers). Here is the algebra:",linear_prog
i qi = 1. That is the normalization to unit vectors of length 1.,orthogonality
then optimality in P(θ) is as follows:,linear_prog
A = LU =,gauss elim
Every matrix has a transpose AT. This matrix has an inverse A−1.,gauss elim
We choose an example in which our intuition is as good as the formulas:,orthogonality
"solution goes from u = 2, v = 0 to u = v = 1. This is a relative error of",computations
(normal) to the worst possible (defective).,eigenvec_val
6 7 8 9,determinants
"(b) the vectors (1,−3,2), (2,1,−3), and (−3,2,1).",vector spaces
"8, what weights w1 and w2 give the reliability of your guess",orthogonality
from the v’s back to the V’s. Now the product rule gives the result we want:,eigenvec_val
"3.1 Find the length of a = (2,−2,1), and write two independent vectors that are per-",orthogonality
explain how it follows from the laws of matrix multiplication.,gauss elim
u + v + w = 6,gauss elim
12. What multiple of a1 =,orthogonality
“plane” has dimension n−1.,linear_prog
A = hilb(3) =,computations
The SVD is closely associated with the eigenvalue-eigenvector factorization QΛQT of,pos_def_matrices
want to rank all teams on a more mathematical basis.,vector spaces
� Vj fdx. We can certainly ﬁnd the minimum of 1,pos_def_matrices
matrix” or “rank of a space” or “dimension of a basis.” These phrases have no meaning.,vector spaces
∥x∥ normalizes the problem against a trivial change of scale. At the same time there is,computations
"At this point Householder’s matrix H is only of order n − 1, so it is embedded into the",computations
"largest subspace is the whole of the original space. If the original space is R3, then the",vector spaces
dimensions. We emphasize that it produces the projection p:,orthogonality
A = UΣV T = (orthogonal)(diagonal)(orthogonal).,pos_def_matrices
"ﬂow, and full duality says that the cut of smallest capacity (the minimal cut) is ﬁlled by",linear_prog
"posed to change sign. But it also has to stay the same, because the matrix stays the same.",determinants
"In one dimension, F(x) has a minimum or a maximum, or",pos_def_matrices
"If λ1 < 1, (I −A)−1 is a converging sum of nonnegative matrices:",eigenvec_val
"of eigenvalues that are wanted). You can ask LAPACK for an eigenvalue subroutine,",computations
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
4 4 4 4,determinants
square roots are in trouble. An indeﬁnite equation y2,pos_def_matrices
"the individual eigenvalues of A and B, and similarly for products. Why is this",eigenvec_val
"26. Project a1 = (1,0) onto a2 = (1,2). Then project the result back onto a1. Draw these",orthogonality
Remark. One good way to write down the forward elimination steps is to include the,gauss elim
"were, then A−a j jI would have",pos_def_matrices
"Solving for x, the least-squares solution of this model system ax = b is denoted by �x:",orthogonality
0 0 0 1,determinants
They go into the columns of S:,eigenvec_val
"the column space of A, consisting of all combinations of the columns. It is denoted by",vector spaces
"determinant of A2 divided by the determinant of A1. (By convention detA0 = 1, so that",determinants
"stituting this exponential into the differential equation, it must satisfy",eigenvec_val
Example 3. The 1s in the identity matrix I leave every matrix unchanged:,gauss elim
"On the right-hand side is the column vector b. On the left-hand side are the unknowns u,",gauss elim
vectors (they turn it into an equality); and now it has given the slackness conditions in,linear_prog
2 3 5 2,vector spaces
3) and a2 =,orthogonality
Chapter 8 Linear Programming and Game Theory,linear_prog
"46. If A+iB is a unitary matrix (A and B are real), show that Q =",eigenvec_val
"To summarize, this introduction has shown how λ and x appear naturally and auto-",eigenvec_val
0 −8 0 −4,gauss elim
is left unchanged by the QR algorithm. It is one,computations
(b) A stable matrix: all solutions to du/dt = Au approach zero.,eigenvec_val
"the original second axis, and larger than the original minor axis: λ2(A) ≤ λ2(B) ≤ λ3(A).",pos_def_matrices
"In reality, b is almost certainly not in the column space. Football scores are not that",vector spaces
May we propose two analogies? A Hermitian (or symmetric) matrix can be compared,eigenvec_val
and ﬁnd an exact solution to A�x = p.,orthogonality
−2u + 7v + 2w =,gauss elim
guaranteed to be independent. The general rule is this:,vector spaces
second derivative test F′′ > 0 into n dimensions. Here are two examples:,pos_def_matrices
"Note 1. The norm and condition number are not actually computed in practice, only",computations
there exists a polynomial of degree n − 1 interpolating these values: P(ti) = bi. The,vector spaces
. So (V⊥)⊥ is the same as,orthogonality
What is the row reduced form of a square invertible matrix? In that case R is the,vector spaces
The norm of A is the number ∥A∥ deﬁned by,computations
"Note 2. The matrix A cannot have two different inverses, Suppose BA = I and also",gauss elim
entirely different box with the same volume.,determinants
"simplicity of these piecewise polynomials. Inside every element, their slopes arc easy to",pos_def_matrices
the second basis vector is perpendicular:,vector spaces
"The solution to the ﬁrst is u = 2, v = 0. The solution to the second is u = v = 1. A",gauss elim
"det(A − λI), whose roots will be the eigenvalues. (All matrices are now square; the",eigenvec_val
"The solution is p(t) = e.06t p0. After t = 5 years, this again amounts to $1349.87. The",eigenvec_val
"When we know about inverses and transposes (the next section deﬁnes A−1 and AT),",gauss elim
A symmetric matrix need not be invertible; it could even be a matrix of zeros. But if,gauss elim
"of the columns of A. Just as in a matrix-vector multiplication, the columns of A are",gauss elim
"(b) Verify, however, that the sum of the eigenvalues of A + B equals the sum of all",eigenvec_val
Splitting of Ax = b,gauss elim
"The ﬁrst step is to recognize the graph. If team j played team k, there is an edge",vector spaces
northwest or southeast? What is the shape of BC = northwest times southeast?,gauss elim
"duality. These approaches are developed in Sections 8.1, 8.2, and 8.3. Then Section",linear_prog
29. (a) Find two familiar functions that solve the equation d2y/dt2 = −y. Which one,eigenvec_val
Problems 18–26 are about A−1 = CT/detA. Remember to transpose C.,determinants
"36. Which vectors (b1,b2,b3) are in the column space of A? Which combinations of the",vector spaces
"(ii) If b is in the column space C(A), so is any multiple cb. If some combination",vector spaces
"dent, they are a basis for the column space—whether the matrix is square or rectangular.",vector spaces
"More time could be spent on that example, but I would rather move forward to n = 3.",gauss elim
reiθ divided by Reiα has absolute value r/R and angle θ −α.,eigenvec_val
This is a key property. All singular matrices have a zero determinant.,determinants
"equations has required the deeper insight of Chapter 2. The contribution of mathematics,",vector spaces
"This is a continuous Markov process. Instead of moving every year, the particles move",eigenvec_val
1 1 3 1,determinants
in each column: The transpose is the inverse.,orthogonality
"Embedding H into Q, the result Q−1AQ is tridiagonal:",computations
. Does Ak → 0 or Bk → 0?,eigenvec_val
"3. If you form a 3 by 3 submatrix of the 6 by 6 matrix F6, keeping only the entries in",orthogonality
Chapter 2 Vector Spaces,vector spaces
1. The deﬁnition and properties of orthogonal matrices Q.,orthogonality
3.5 The Fast Fourier Transform,orthogonality
to capacity. and equality has been achieved.,linear_prog
deteAt = eλ1teλ2t ···eλnt = etrace(At).,eigenvec_val
"vertical side (0,0,x3) = (0,0,3). The hypotenuse of the bold triangle (Pythagoras again)",orthogonality
"For a proof in general. (xHAx)H is the conjugate of the 1 by 1 matrix xHAx, but we",eigenvec_val
vectors are in its nullspace? Removing the last column of A (and last row of AT),vector spaces
Notice that the λ’s must be positive—the matrix must be positive deﬁnite—or these,pos_def_matrices
and it can be terriﬁc.,computations
The steps depend on the selection order of the trees. To stay with the same tree is,linear_prog
"(c) Show that Ax = u has no solution. (If it had a solution, then",eigenvec_val
"8. For a positive deﬁnite A, the Cholesky decomposition is A = LDLT = RTR, where",computations
"2, ﬁnd det(2A), det(−A), det(A2), and det(A−1).",determinants
"4 y. Verify that c0 +c1eix +c2e2ix +c3e3ix takes the values 2, 0, −2, 0 at the points",orthogonality
"up to zero. First 1+i−1−i = 0, and then",orthogonality
= 2λ 2 −6λ +3 = 0,pos_def_matrices
(a) All diagonal matrices.,vector spaces
"A = QR. The columns of Q are orthonormal, and R is upper triangular and",orthogonality
Elimination can be completed without row exchanges (so P = I and A =,determinants
the cost. If a pound of peanut butter costs $2 and a steak is $3. then the cost of the whole,linear_prog
"to solve; that was the reason for a splitting. A simple choice of S can often succeed, and",computations
new; subtracting the equations in (1) already produces b1 + b4 = b2 + b5. There are,vector spaces
"Minima, Maxima, and Saddle Points",pos_def_matrices
the derivative of eBt is BeBt.,eigenvec_val
(c) Show that a > 0 and ac > |b|2 ensure that A is positive deﬁnite.,pos_def_matrices
nant is zero. The matrix A might not be 0 but check that A2 = 0.,eigenvec_val
"18. Decide whether the following systems are singular or nonsingular, and whether they",gauss elim
know the best y+. It is y+ = Σ+UTb so the best x+ is Vy+:,pos_def_matrices
1b)q1. Thus we have a new interpre-,orthogonality
"If A is invertible, then Ax = 0 can only have the zero solution x = 0.",gauss elim
"1. If N is normal, then so is the triangular T = U−1NU:",eigenvec_val
"2. The matrix is symmetric. Each entry aij equals its mirror image a ji, so that AT = A.",gauss elim
"Unfortunately, degeneracy is extremely common in applications—if you print the cost",linear_prog
"dimensions r, n − r, r, m − r were nonzero. But the example was not produced by a",vector spaces
"If the columns come in the order (α,...,v), that term is the product a1α ···anv times the",determinants
"one vector r = b−Ax0 in double precision, solve LUy = r, and add the correction y to",computations
dimensions it comes from the hypotenuse of a right triangle (Figure 3.1a). The square,orthogonality
"forward with elimination. The 3w can still eliminate the 4w, and we will call 3 the",gauss elim
is the ﬁfth pivot? What is the nth pivot?,gauss elim
19. Split A = UΣV T into its reverse polar decomposition QS′.,pos_def_matrices
"The powers 6k and 1k appear in that last matrix Ak, mixed in by the eigenvectors.",eigenvec_val
"In its ﬁnal rankings for 1985, the computer placed Miami (10-2) in the sev-",vector spaces
+ dy = g.,gauss elim
"18. If A has r pivot columns, then AT has r pivot columns. Give a 3 by 3 example for",vector spaces
5 identity matrix. Then C(I) is the whole of R5; the ﬁve columns of I can combine to,vector spaces
We have to say immediately that these “energies” are nothing but positive deﬁnite,pos_def_matrices
Ap(t) = (2+3t)(a0 +···+antn) = 2a0 +···+3antn+1.,vector spaces
"This condition r ≥ 0 was ﬁnally met, since the number of corners is ﬁnite. At that",linear_prog
A = λ1 +···+λn = a11 +···+ann.,eigenvec_val
when it satisﬁes x ≥ 0. Phase I of the simplex method ﬁnds one basic feasible,linear_prog
"λ = 1 and 6,",eigenvec_val
"Only a square matrix can have both r = m and r = n, and therefore only a square",vector spaces
discovery produced tremendous activity to ﬁnd the smallest possible power of n. The,gauss elim
"19. Multiplying term by term, check that (IłA)(I + A + A2 + ···) = I. This series rep-",eigenvec_val
"This rule holds in many applications, even though there are examples in which Jacobi",computations
Then LU = A.,gauss elim
"conjugate transpose. For vectors and matrices, a superscript H (or a star) combines both",eigenvec_val
"(like peanut butter). We will show that expensive foods are kept out of the optimal diet,",linear_prog
(d) If AB = B then A = I.,gauss elim
A fundamental network model is the shortest path problem—in which the edges have,linear_prog
the number of independent columns. That can be hard to decide in computations! In,pos_def_matrices
0 1 2 3 4,vector spaces
every vector x (not just the eigenvectors). Since symmetric matrices have a full set of,pos_def_matrices
"Instead, accepting v = 1, which is wrong only in the fourth place, we obtain u = 0:",gauss elim
ℓ − k − 1 exchanges move the one originally in place ℓ (and now found in place ℓ − 1),determinants
look at the result!,gauss elim
(b) For which c and d can we ﬁnd three orthonormal vectors that are combinations,eigenvec_val
12. The vector b is in the subspace spanned by the columns of A when there is a solution,vector spaces
is increased. Show by example that this can fail if A is indeﬁnite.,pos_def_matrices
"1.4a you see a vector addition, component by component:",gauss elim
"shape as A, and its entries (the singular values) are the square roots of the eigenval-",computations
"formed by the FFT. To verify equation (13), split y j into even and odd:",orthogonality
million in. It ends the same way if all 90 million were originally inside.,eigenvec_val
(c) The transformation that takes x to mx+b is linear (from R1 to R1).,vector spaces
The matrices A and B that represent the same linear transformation T,eigenvec_val
and −1 is an eigenvalue as well as +1—which,eigenvec_val
"ments y = 6 at t = 0, y = 4 at t = 1, y = 0 at t = 2. Write the three equations that",orthogonality
"axes e1 = (1,0) and e2 = (0,1) are not only perpendicular but horizontal and vertical. Q",orthogonality
"(II′) The eigenvalues are λ1 = 0, λ2 = λ3 = 3 (a zero eigenvalue).",pos_def_matrices
"nodes go with the source into the set S. The sink must lie in the remaining set T, or it",linear_prog
43. (Try this question.) Which permutation makes PA upper triangular? Which permu-,gauss elim
the roots of the polynomial det(A−λI) by using only the square roots that determine,eigenvec_val
"The one-step system uk+1 = Auk is easy to solve, It starts from u0. After one step it",eigenvec_val
w = −2. Three times column 1 equals column 2 plus twice column 3. Column 1 is in,gauss elim
subspace. A perfect experiment would give a perfect C and D:,orthogonality
7D ∥A∥ is the square root of the largest eigenvalue of ATA: ∥A∥2 = λmax(ATA).,computations
column space. Why can’t this be a basis for the row space and nullspace?,vector spaces
"factoring into primes, all known algorithms can take exponentially long. The celebrated conjecture “P ̸= NP” says",linear_prog
0 0 ∗ ∗,computations
"In the existence case, one possible solution is x = Cb, since then Ax = ACb = b. But",vector spaces
inverted in the reverse direction. Inverses come in the opposite order! The second,gauss elim
"5.18 True or false, with reason if true and counterexample if false:",eigenvec_val
Therefore we go directly to the three properties of U that correspond to the earlier Prop-,eigenvec_val
pivot = 2 →,gauss elim
"This is a brief and optional section, but it has a number of good intentions:",orthogonality
the length of x. An m by n matrix multiplies an n-dimensional vector (and produces,gauss elim
Prove that all three matrices have the same rank r.,vector spaces
that. Symmetric matrices are especially easy. “Defective matrices” lack a full set of,eigenvec_val
f in row space,vector spaces
elimination is n3/6. This is to be combined with the usual n3/3 operations that are,gauss elim
also occurs if a and c have opposite signs. Then two directions give opposite results—in,pos_def_matrices
estimate of the expense so far. I hope some readers will be vigorous enough to master,pos_def_matrices
Fk+2 = Fk+1 +Fk. This can be reduced to a one-step equation uk+1 = Auk. Every step,eigenvec_val
"repeat, if b is in the column space, then",vector spaces
28. Find the range and kernel (those are new words for the column space and nullspace),vector spaces
"and (−1,1,2). In Figure 3.9, the three points are not on a line. Least squares replaces",orthogonality
the probabilities uk would blow up.) If all other eigenvalues are strictly smaller than,eigenvec_val
we cannot expect to solve Qx = b exactly. We solve it by least squares.,orthogonality
"Thirty years ago, almost every mathematician would have guessed that a general sys-",gauss elim
vector only)? The nullspace part of any vector x in R2 is xn =,orthogonality
subtracting multiples of rows 1 and 2 (of U!):,gauss elim
The mathematical problem is to prove the following: If every set of p women does,linear_prog
"3. Prove that if a = 0, d = 0, or f = 0 (3 cases), the columns of U are dependent:",vector spaces
Example 2. Consider a simple 2 by 3 matrix of rank 2:,vector spaces
"We can add any two vectors, and we can multiply all vectors by scalars.",vector spaces
25. Elimination reduces A to U. Then A = LU:,determinants
square matrices of any size.,determinants
"If we had known, we could have multiplied A by P in the ﬁrst place. With the rows in",gauss elim
2. Determinants The solution y = 2 depends completely on those six numbers in the,gauss elim
"16. If A is the 4 by 4 matrix of ones, ﬁnd the eigenvalues and the determinant of A−I.",eigenvec_val
p = �xa = aTb,orthogonality
"two changes, A to ATA and b to ATb, give a new way to reach the least-squares equation",pos_def_matrices
23. Give a quick reason why each of these statements is true:,pos_def_matrices
(d) Find the distribution of the $4 trillion at year k.,eigenvec_val
are multiplied in UΣV T by the small σ’s that are being ignored. We can do the matrix,pos_def_matrices
The row space and the column space have the same dimension r! This is one of,vector spaces
are much simpler to explain. The explicit formula (6) and the cofactor formula (10),determinants
ﬁnishes with ABu in W. This “composition” AB is again a linear transfor-,vector spaces
"tion between the minimizer and the maximizer. In the diet problem, the minimizer has n",linear_prog
The computation for λ2 is done separately:,eigenvec_val
enters—the solution is also x = QTb:,orthogonality
The trace a+d must be negative.,eigenvec_val
(c) an upper triangular matrix: aij = 0 if i > j.,gauss elim
"12. If the primal has a unique optimal solution x∗, and then c is changed a little, explain",linear_prog
"combination of the v’s: If w1 = a11v1 + ··· + am1vm, this is the ﬁrst column of a matrix",vector spaces
"At the start, the basic variables may be mixed with the free variables. Renumbering if",linear_prog
"12. If A is a 12 by 7 incidence matrix from a connected graph, what is its rank? How",vector spaces
ﬁnd A. No other matrix has the same λ’s and x’s.,eigenvec_val
"Hessenberg matrix is symmetric, it only has three nonzero diagonals.",computations
"3.23 Find an orthonormal basis for R3 starting with the vector (1,1,1).",orthogonality
"The only line of eigenvectors goes through (1,1). After dividing by",eigenvec_val
"The approximation will use three intervals and two hat functions, with h = 1",pos_def_matrices
30. Choose the second row of A = [0 1,eigenvec_val
determinant is zero. Invertible matrices have all λ ̸= 0.,eigenvec_val
4. The left nullspace contains all multiples of y =,vector spaces
(a) the sum of a complex number and its conjugate?,eigenvec_val
"The eigenvalues are distinct, even if imaginary, and the eigenvectors are independent.",eigenvec_val
0 0 3 3,vector spaces
Proof. Suppose there are more w’s than v’s (n > m). We will arrive at a contradiction.,vector spaces
produces the area x1y2 −x2y1 of a parallelogram.,determinants
(b) project every vector onto the x2 axis.,gauss elim
Their importance has earned them a name: Transformations that obey rules (i)–(iii),vector spaces
We can quickly summarize the equations for ﬁtting by a straight line. The ﬁrst column,orthogonality
"19. If v1,...,vn are linearly independent, the space they span has dimension",vector spaces
f(x) = a0 +a1cosx+b1sinx+a2cos2x+b2sin2x+··· .,orthogonality
by the same factor c. The whole space expands or contracts (or,vector spaces
"∆A. The length of a vector is already deﬁned, and now we need the norm of a matrix.",computations
1 plus equation 2 minus equation 3 is the impossible statement 0 = 1. Thus the equations,gauss elim
and also for the 3 by 6 matrix B = [A A].,vector spaces
ask whether λ was positive before it was known to be real. Chapter 5 established that,pos_def_matrices
∂ f/∂b ∂ f/∂d,determinants
vectors qi in the columns of Q have m > n components. Then Q is an m by n matrix and,orthogonality
2. Expand those determinants in cofactors of the ﬁrst row. Find the cofactors (they,determinants
of b onto the column space of P. The error vector b − Pb is orthogonal to the space.,orthogonality
in the coming section.,vector spaces
bHY +bYP +bPH = 0.,vector spaces
4. How do we know that the ith row of an invertible matrix B is orthogonal to the jth,orthogonality
column space and row space have dimension r = 2. The nullspace and left nullspace,vector spaces
"unit circle. Verify by direct multiplication that the square of the ﬁrst is the second,",eigenvec_val
"is nonsingular, elimination puts the pivots d1,...,dn on the main diagonal. We have a",determinants
", ﬁnd A100 by diagonalizing A.",eigenvec_val
the earth or spin it or reﬂect it across the plane of the equator (forth pole transforming to,vector spaces
Kirchhoff’s law for score differences,vector spaces
Invertibility of A depends on nonzero eigenvalues.,eigenvec_val
term. Probably the constant polynomials will be the left nullspace.,vector spaces
4. The second equation keeps x2 + 3x5 = 9. Here x5 can only increase as far as 3. To,linear_prog
3. C(AT) = row space of A; dimension r.,vector spaces
0 0 2 2,vector spaces
The nullspace N(A) has dimension n − r. The “special solutions” are a,vector spaces
The second difference in A is a backward difference L times a forward difference U.,gauss elim
"possible, ∥δx∥/∥x∥ = c∥δb∥/∥b∥.",computations
This matrix U is upper triangular—all entries below the diagonal are zero.,gauss elim
The number λ is an eigenvalue of A if and only if A−λI is singular:,eigenvec_val
more important! It does the multiplication a column at a time. The product Ax is found,gauss elim
"6.1 Minima, Maxima, and Saddle Points",pos_def_matrices
�� = −λ 3 +3λ 2 −2λ = 0.,eigenvec_val
Figure 1.7: A 3 by 4 matrix A times a 4 by 2 matrix B is a 3 by 2 matrix AB.,gauss elim
"Figure 3.10, B is perpendicular to q1. It sets the direction for q2.",orthogonality
"If condition IV holds, so does condition I: We are given positive pivots, and must",pos_def_matrices
5.3 Difference Equations and Powers Ak,eigenvec_val
11. Explain why partial pivoting produces multipliers ℓij in L that satisfy |ℓij| ≤ 1. Can,gauss elim
pendicular. Compute ATA. Why is it a diagonal matrix?,orthogonality
(a+ib)(c+id) = (ac−bd)−i(bc+ad) = (a+ib)(c+id).,eigenvec_val
"halfspaces. The more constraints we impose, the smaller the feasible set.",linear_prog
"H2 = (2P−I)2 = 4P2 −4P+I = I,",vector spaces
"detB = 0. Equation (3) is the expansion of detB along its row 2, where B has exactly",determinants
xx = (3+4i)(3−4i) = 25 = |x|2,eigenvec_val
The other thing is the change of basis matrix M. For that we express V1 as a combination,eigenvec_val
"Since the result is again the identity matrix, we come to a surprising conclusion: The",orthogonality
on the main diagonal.,pos_def_matrices
"trying to compute det(A−λI), whose roots should be the eigenvalues. For a large matrix,",eigenvec_val
contact occurs at the same (slightly moved) corner.,linear_prog
λ(λ −1)(λ −2)eλt = 0.,eigenvec_val
"Consider the equation AA−1 = I. If it is taken a column at a time, that equation de-",gauss elim
"Fortunately for the engineer, this crossover occurs where the accuracy is already",computations
eigenvalues can always be decoupled.,eigenvec_val
v + w =,gauss elim
difference. Equations (2) and (4) used essentially the same steps to ﬁnd y = 2. Certainly,gauss elim
The product of the ﬁrst k pivots is the determinant of Ak. This is the same rule that,determinants
at which x∗ is found.,linear_prog
"previous section. They are called elementary matrices, and it is easy to see how they",gauss elim
"3. If a 2 by 2 symmetric matrix passes the tests a > 0, ac > b2, solve the quadratic",pos_def_matrices
but even if A is not diagonalizable (and there are terms like teλt) the result is still true:,eigenvec_val
Check eAt = I when t = 0.,eigenvec_val
1.15 What do you know about C(A) when the number of solutions to Ax = b is,vector spaces
5.4 Differential Equations and eAt,eigenvec_val
"satisﬁes this constraint, so it is forced on b! Geometrically, we shall see that the vector",vector spaces
= combination of two projections.,eigenvec_val
zero for a nonzero entry—intending to carry out a row exchange. In this case the entry,vector spaces
"ignore that column, detB j is exactly the jth component in the product CTb:",determinants
As in the static problem. The method can be summarized in three steps: (1) choose,pos_def_matrices
We start with examples of particularly good matrices.,eigenvec_val
4x + 5y = 6,gauss elim
back to trigonometry in the two-dimensional case to ﬁnd that relationship. Suppose the,orthogonality
"After this elimination step, solve the triangular system. If the right-hand side changes",gauss elim
(and invertible!) matrix; it may be diagonal or triangular.,computations
"conditioned or ill-conditioned? If there is a small change in b or in A, how large a",computations
by 5 permutation P so that the smallest power to equal I is P6. (This is a challenge,gauss elim
"When A is symmetric, a very accurate choice is the Rayleigh quotient:",computations
doubled. Do you expect these random A to be invertible?,gauss elim
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"dimensions add to 2+1+1 = 4. What space is perpendicular to all of V, W, and L?",orthogonality
"The column rule will be used over and over, and we repeat it for emphasis:",gauss elim
"60. If you multiply the 4 by 4 all-ones matrix A = ones(4,4) and the column v =",gauss elim
Cramer’s Rule was actually recommended (and elimination was thrown aside):,determinants
2 + ··· + x2,pos_def_matrices
23. Compute λmax and λmin for the 8 by 8 Hilbert matrix aij = 1/(i + j − 1). If Ax = b,computations
"This will be a pretty long list. Fortunately each rule is easy to understand, and even",determinants
"from A(0) = A to A(1) = I, but in between A(t) might be singular. The problem is",determinants
"nants and eigenvalues depend continuously on ε, they will be positive until the very last",pos_def_matrices
"(x4,x3,x1,x2). Those 24 vectors, including x itself, span a subspace S. Find speciﬁc",vector spaces
combinations ﬁlls the whole space? What about four vectors?,linear_prog
"The truth is that we knew the columns would combine to give zero, because the rows",gauss elim
then get it out again.,eigenvec_val
over compounding every day is only four cents.,eigenvec_val
initial value u(0). The coefﬁcient matrix is A:,eigenvec_val
"Both columns end with a zero, so C(A) is the x-y plane within three-dimensional space",orthogonality
"(b) If you know λ is an eigenvalue, the way to ﬁnd x is to",eigenvec_val
7.4 Iterative Methods for Ax = b,computations
3. S = combination of 1 and 2 (successive overrelaxation or SOR).,computations
2x + ay = 0,gauss elim
an interior point method. The excitement began when Karmarkar claimed that his,linear_prog
26. The solution to y′′ = 0 is a straight line y = C +Dt. Convert to a matrix equation:,eigenvec_val
rows and r pivot columns. The rank of those matrices is r. The last m − r,vector spaces
0 0 1 0,determinants
Orthogonal Bases and Gram-Schmidt,orthogonality
(ii) The transpose of A−1 is (A−1)T = (AT)−1.,gauss elim
This identity (please multiply it out) is the key to the whole chapter. It generalizes,pos_def_matrices
(b) Find a power k so that all 3 by 3 permutation matrices satisfy Pk = I.,gauss elim
"second derivatives are positive, 2x2 +4xy+y2 is not positive deﬁnite. Neither F nor f",pos_def_matrices
each block gives 1 eigenvector,eigenvec_val
1 = 1 is hidden in the ﬁrst term. In,eigenvec_val
(b) If A is also symmetric what is the transpose of A−1?,gauss elim
2.5 Graphs and Networks,vector spaces
"To describe the product Ax, we use the “sigma” symbol Σ for summation:",gauss elim
Addition obeys the commutative law x + y = y + x; there is a “zero vector” satisfying,vector spaces
1. The conjugate of a product equals the product of the conjugates:,eigenvec_val
∂P/∂x1 = 2x1 −x2 −b1 = 0,pos_def_matrices
"All we can do is to go on to the next column, where the pivot entry is 3. Subtracting twice",vector spaces
we assumed that A could be diagonalized. since otherwise it has fewer than n eigenvec-,eigenvec_val
∗ ∗ ∗ ∗,computations
"13. Draw the two pictures in two planes for the equations x−2y = 0, x+y = 6.",gauss elim
speciﬁes the best choice ωopt:,computations
terms in xTAx involving x1. The sum of squares has the pivots outside. The multipliers,pos_def_matrices
until we reach detI = 1. Now come all other matrices!,determinants
0 1 0 1,determinants
(b) How do L and D (5 by 5) give the same number of choices in LDLT?,gauss elim
ﬁnite Fourier transform is UHU = I.,eigenvec_val
"18. In Arnoldi, show that q2 is orthogonal to q1. The Arnoldi method is Gram-Schmidt",computations
Reverse that to recover [1 3 6 11] in [A b] from the ﬁnal [1 1 1 5] and [0 1 2 2],gauss elim
10. Show why the iteration xk+1 = (I −A)xk +b does not converge for A =,computations
2. Its eigenvectors can be chosen to be orthonormal.,eigenvec_val
aij = i− j,gauss elim
18. Under what conditions on their entries are A and B invertible?,gauss elim
31. Find Λ and S to diagonalize B in Problem 29. What is B10u0 for these u0?,eigenvec_val
"hit at t = 0; u(0) = (0,0) and u′(0) = (1,0).",eigenvec_val
"3. Explain why ∥ABx∥ ≤ ∥A∥∥B∥∥x∥, and deduce from equation (5) that ∥AB∥ ≤ ∥A∥∥B∥.",computations
Example 3. The following case is simple but typical. Suppose we project a point,orthogonality
nn = 22 easy,determinants
of the product ST.,vector spaces
5. Find the eigenvalues and eigenvectors of,eigenvec_val
"Gauss and not recommended by Seidel. That is a surprising bit of history, because it is",computations
"gramming! There are more variables, and an interval is replaced by a feasible set, but",linear_prog
"Elimination multiplied A on the left by L−1, but not on the right by L. So U is not",eigenvec_val
a discrete Markov matrix if and only if B = A − I is a continuous Markov matrix. The,eigenvec_val
"possible value for the determinant (based on volume)? If all entries are 1 or −1,",determinants
"T from A, or set up the iteration loop directly from the entries aij. Test it on the −1,",computations
and write the result in the form A = QR.,orthogonality
The projection of b onto the column space is the nearest point A�x:,orthogonality
The column space is the whole space (Ax = b has a solution for every b). The new ques-,vector spaces
"even invertible. Projections reduce the length of a vector, whereas orthogonal matrices",orthogonality
along the edges of the feasible set. At a typical corner there are n edges to choose from.,linear_prog
(a) The 3 by 4 matrix of all 1s.,vector spaces
35. The ﬂoor and the wall are not orthogonal subspaces because they share a nonzero,orthogonality
"at the ﬁrst eigenvector x1, and it crosses back at the second eigenvector x2.",eigenvec_val
feasible direction closest to the best direction. This is the natural but expensive step in,linear_prog
or diagonal. Show that all permutation matrices are normal.,eigenvec_val
λ1 ≤ a11 ≤ λn.,pos_def_matrices
"exist (the defective case), or we may not know it, or we may not want to use it.",eigenvec_val
from the largest σ in the diagonal matrix Σ.,computations
"column numbers α,β,...,v are all different. They are a reordering, or permutation, of",determinants
yA ≤ c and y ≥ 0:,linear_prog
"The inﬁnite series must converge to a ﬁnite sum. This leaves (1, 1",orthogonality
3 of this population is outside Cali-,eigenvec_val
42. The (complex) dimension of Cn is,eigenvec_val
0 0 0 3,vector spaces
B(ATA)−1AT. But that satisﬁes BA = I; it is a left-inverse. Which step is not justiﬁed?,vector spaces
"62. Construct a matrix whose column space contains (1,1,5) and (0,3.1) and whose",vector spaces
One ﬁnal note about the language of linear algebra. We never use the terms “basis of a,vector spaces
From A to U there are subtractions of rows. From U to A there are additions of rows.,gauss elim
"Compute both sides if C is also n by n, with every c jl = 2.",gauss elim
and λ = 2. This is a general rule which makes the two methods consistent; the growth,eigenvec_val
"projections onto a sine or cosine. Since the sines and cosines are orthogonal, the Fourier",orthogonality
of A yields an example with inﬁnitely many left-inverses:,vector spaces
"mechanics (and recently in robotics). In any deformation, it is important to separate",pos_def_matrices
izations LDU and say how U is related to L for these symmetric matrices:,gauss elim
"t = 2,z = 1",orthogonality
we would like to compute an explicit basis.,vector spaces
transformation (!) when the only thing happening is the change of basis (T is I). The in-,eigenvec_val
"12. How could you factor A into a product UL, upper triangular times lower triangular?",gauss elim
The Organization of a Simplex Step,linear_prog
"24. If you know L, U, Q, and R, is it faster to solve LUx = b or QRx = b?",computations
3.3 Projections and Least Squares,orthogonality
2. Find the roots of this polynomial. The n roots are the eigenvalues of A.,eigenvec_val
"If A = L1D1U1 and also A = L2D2U2, where the L’s are lower triangular",gauss elim
(b) Columns combine with 2 and 3,gauss elim
"diagonal? In other words, is P = (5,4,3,2,1) even or odd? The checkerboard pattern",determinants
"xS. With unit resistances between cities, the three currents are in y:",gauss elim
"to numbers on the real axis, the imaginary axis, and the unit circle. Now we want the",eigenvec_val
"(c) The vectors b with b2b3 = 0 (this is the union of two subspaces, the plane b2 = 0",vector spaces
"convention, the empty set is a basis for that space, and its dimension is zero.",vector spaces
"(c) The vectors v1 and (0,0,0) are dependent because",vector spaces
This central idea—the dimension of a subspace—is made precise in the next section.,vector spaces
inner product: xTy = x1y1 +···+xnyn,eigenvec_val
1.20 How many 5 by 5 permutation matrices are there? Are they linearly independent?,vector spaces
of the hypotenuse should still be the sum of squares of the components:,orthogonality
nential u = eλtx and ﬁnd a quadratic eigenvalue problem for λ.,eigenvec_val
"Another singular system, close to this one, has an inﬁnity of solutions. When the",gauss elim
"(e) Row space = column space, nullspace ̸= left nullspace.",vector spaces
0 −8 −2 −2 1 0,gauss elim
26. Which number c leads to zero in the second pivot position? A row exchange is,gauss elim
vectors that satisfy ATy = 0. Find three vectors y and connect them to the loops in,vector spaces
"With the rows reordered in advance, PA can be factored into LU.",gauss elim
"In other words, if the number of vectors is known to be correct, either of the two",vector spaces
6x − 4y = b2.,gauss elim
"E subtracts 2 times the ﬁrst component from the second, After this step the new and",gauss elim
"natural sensitivity of the problem, measured by c. The other is the actual error δb",computations
"For ATA, the pivots are 2, 3",orthogonality
"26. If A has column 1 + column 2 = column 3, show that A is not invertible:",gauss elim
"and proceed. What coefﬁcient of v in the third equation, in place of the present −1,",gauss elim
"dimensional plane in ten-dimensional space. It is harder to see ten of those planes,",gauss elim
Every Hermitian matrix with k different eigenvalues has a spectral decomposition into,eigenvec_val
8. Verify that the vectors in the previous exercise satisfy the complementary slackness,linear_prog
"and x2. By formula (10), the third orthogonal polynomial is",orthogonality
"44. To prove that (AB)C = A(BC), use the column vectors b1,...,bn of B. First suppose",gauss elim
"independent, and the column space degenerated into a two-dimensional plane.",vector spaces
cosθ = cosβ cosα +sinβ sinα = a1b1 +a2b2,orthogonality
to the initial vector u(0) at t = 0.,eigenvec_val
"45. If uHu = 1, show that I −2uuH is Hermitian and also unitary. The rank-1 matrix uuH",eigenvec_val
−2u + 7v + 2w =,gauss elim
The fundamental principle of structural engineering is the minimization of total energy.1,pos_def_matrices
8.2 The Simplex Method,linear_prog
because of the arrow in each edge.,vector spaces
���� = Fourier matrix,eigenvec_val
= MATLAB’s particular solution A\b,vector spaces
"25. When zero appears in a pivot position, A = LU is not possible! (We need nonzero",gauss elim
has eigenvalues eit and e−it.,eigenvec_val
(b) Is the result the same as applying BC followed by A? Parentheses are unnecessary,vector spaces
"The only operation not required by our example, but needed in general, is row ex-",vector spaces
plies itself. The sines and cosines are mutually orthogonal as in equation (18) Therefore,orthogonality
31 32 33 34,determinants
"ber of spades they hold. For each guess, the errors −1, 0, 1 might have equal probability",orthogonality
In general there are (n+m)!/n!m! possible intersections. That counts the number of,linear_prog
they are projections onto the line through x1 and the line through x2.,eigenvec_val
1 0 0 1,linear_prog
perpendicular to the vector,gauss elim
21. Describe all matrices S that diagonalize this matrix A:,eigenvec_val
"(i) If A is unitary, and U is too, then so is T = U−1AU.",eigenvec_val
orthogonal basis to make those calculations simple. A further specialization makes the,orthogonality
for solutions with the same exponential dependence on t just found in the scalar case:,eigenvec_val
3. The Fourier series of a function is an expansion into sines and cosines:,orthogonality
with 1s on the reverse diagonal. Problem 5 had n = 4.,determinants
"tion λ1 ̸= λ2, which forces the conclusion that xHy = 0. In our example,",eigenvec_val
1s on the diagonal and at most one nonzero entry below?,vector spaces
"triangular form, Its diagonal entries approach its eigenvalues, which are also the eigen-",computations
32. (a) Find a basis for the subspace S in R4 spanned by all solutions of,orthogonality
I is r by r,vector spaces
"A basis is a set of independent vectors that span a space. Geometrically, it is a set of",orthogonality
(KT = −K) of order n? The diagonal of K is zero!,gauss elim
"barrier problem, and θeX−1 will stay nonnegative. The plan is to solve equations (9a–",linear_prog
"40. The matrix that transforms (1,0) and (0,1) to (1,4) and (1,5) is M =",vector spaces
1x1 + c2λ k,eigenvec_val
and −1 in column v—to indicate which teams are in that game.,vector spaces
problem in applied mathematics! We even solve the marriage problem—to maximize,linear_prog
"be given as the solution to Ax = b or Ax = λx. Instead, the vector x will be determined",pos_def_matrices
(c) The same argument for the n − p negative λ’s and the n − q positive µ’s gives,pos_def_matrices
That is not the only orthonormal basis! We can rotate the axes without changing the,orthogonality
"In our example above, the second pivot was exactly this ratio (ad − bc)/a. It is the",determinants
large an error can this cause in x?,computations
"to a 3 by 3 system, introduce v = y′ and w = v′ as additional unknowns along with y",eigenvec_val
(b) V orthogonal to W and W orthogonal to Z makes V orthogonal to Z.,orthogonality
"41. Suppose y1(x), y2(x), y3(x) are three different functions of x. The vector space they",vector spaces
"geometry is hard to visualize. Within all vector spaces, two operations are possible:",vector spaces
"You can see the same thing in mechanics. When springs and masses are oscillating,",pos_def_matrices
W. They involve only the symmetric combination C = W TW. The inner product of x,orthogonality
Rk = Pn n−1···P32P21Ak.,computations
"statisticians, and originally from Gauss. We may know that the average error is zero.",orthogonality
mal at the same time.,orthogonality
Example 5. A is 3 by 3 for 2x2,pos_def_matrices
(a) A have a two-sided inverse: AA−1 = A−1A = I?,vector spaces
"2. Decide for or against the positive deﬁniteness of these matrices, and write out the",pos_def_matrices
To compute x1 there is a neat trick. Multiply both sides of the equation by qT,orthogonality
both Reλ > 0,eigenvec_val
already contained more than b1.,linear_prog
"This corresponds to a singular system with b = (2,5,6):",gauss elim
�� = λ1P1 +λ3P3 = (+1),eigenvec_val
36. Extend Problem 35 to a p-dimensional subspace V and a q-dimensional subspace W,orthogonality
"solution has components 2, 5, 0. Of course the row picture agrees with that (and we",gauss elim
P(v) is to be minimized over all functions v(x) that satisfy v(0) = v(1) = 0. The function,pos_def_matrices
7 8 9 b3,vector spaces
matrix is sitting in the pivot columns of R!,vector spaces
have the same solutions. The fourth column of U was also (column 3) − (column 1).,vector spaces
Find an eigcnvector with Ax = 5x. These matrices will nothe diagonalizabie because,eigenvec_val
The cosine of the angle between any nonzero vectors a and b is,orthogonality
is symmetric whenever A is. Now we show that multiplying any matrix R by RT gives a,gauss elim
Those two terms are c1λ k,eigenvec_val
"2, compute A =",pos_def_matrices
A 3 by 4 example will be a good size. We will write down all solutions to Ax = 0. We,vector spaces
The complex numbers cosθ +isinθ in the Fourier matrix are extremely special. The,orthogonality
"···. Multiply your eAt times (y(0),y′(0)) to check the straight line y(t) = y(0) +",eigenvec_val
"The real problem is to ﬁnd some quick way to compute the powers Ak, and thereby ﬁnd",eigenvec_val
"We move steadily toward a more sophisticated algorithm, which starts by making a",computations
"That matrix in parentheses, to the left of A, is evidently a left-inverse! It exists, it equals",gauss elim
other equations will be impossible. The same is true at every intermediate stage. Notice,gauss elim
Question: If the measurements b = (2,orthogonality
"third component of Ax and the (1,1) entry of A2.",gauss elim
"involves the four separate multiplications ac, bd, be, ad. Ignoring i, can you compute",gauss elim
(c) A can’t be similar to −A unless A = 0.,eigenvec_val
"proaches the true value of d2u/dx2 as h → 0, but we have to stop at a positive h.",gauss elim
the maximum cost is inﬁnite.,linear_prog
"27. The “cyclic” transformation T is deﬁned by T(v1,v2,v3) = (v2,v3,v1).",vector spaces
"sink t, it has four women on the left and four men on the right. The edges correspond",linear_prog
uk = SΛkS−1u0 = c1λ k,eigenvec_val
6. The matrices in equation (4) have norms between 100 and 101. Why?,computations
"4, x3 ≤ 6. What are the optimal x∗ and y∗ (if they exist!)?",linear_prog
(b) Then ﬁnd another solution to Ax = b.,vector spaces
2. The other terms die of orthogonality. Each,orthogonality
43. Every invertible linear transformation can have I as its matrix. For the output basis,vector spaces
column) this is the requirement (EA)x = E(Ax) mentioned earlier. It is the whole basis,gauss elim
"Proof. Suppose c1v1 + ··· + ckvk = 0. To show that c1 must be zero, take the inner",orthogonality
Breakdown at the ﬁrst step can be ﬁxed by exchanging rows—but not breakdown at,gauss elim
"F(x,y) = 7+2(x+y)2 −ysiny−x3",pos_def_matrices
so they form a vector space. It is the celebrated Hilbert space.,orthogonality
"inﬁnitely many different bases. Whenever a square matrix is invertible, its columns are",vector spaces
This matrix R is the ﬁnal result of elimination on A. MATLAB would use the command,vector spaces
"The eigenvectors, with length scaled to 1, are",eigenvec_val
Cyclic permutation: Pn = I,eigenvec_val
all |λ| < 1,eigenvec_val
"would be crazy to look for one. Suppose we are handed some radioactive material, The",orthogonality
"� V 2dx. In the 1 by 1 eigenvalue problem, is λ = A/M larger or smaller than the true",pos_def_matrices
4(360°). The other fourth roots are the powers,orthogonality
The matrix itself can be seen in equation (6). It comes from changing a differential,gauss elim
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Describe all the different kinds of surfaces that appear in the positive semideﬁnite,pos_def_matrices
That step subtracted ℓ21 =,gauss elim
derivatives of E2 = (Ax − b)T(Ax − b). That gives the same 2ATAx − 2ATb = 0. The,orthogonality
methods produce a square coefﬁcient matrix ATA. It is symmetric (its transpose is not,orthogonality
perpendicular to e and ﬁnd the projection matrix P.,orthogonality
"2xTAx−xTb at the start of the section, which led to Ax = b:",pos_def_matrices
"and B are symmetric. Of course, the zero matrix is in both subspaces.",vector spaces
submatrix A2. This is a general rule if there are no row exchanges:,determinants
"(c) What is the dual problem, and what is its solution y?",linear_prog
"than unknowns, and the q’s are no longer a basis. The projection matrix is usually",orthogonality
0 1 0 1,vector spaces
41. Test the Cayley-Hamilton Theorem on Fibonacci’s matrix A =,eigenvec_val
0 1 0 1,vector spaces
"(one more binary digit becomes correct) at every step. In this example, which is much",computations
1.7 Special Matrices and Applications,gauss elim
"2. Give one free variable the value 1, set the other free variables to 0, and solve Rx = 0",vector spaces
(I′) xTAx = (x1 −x2)2 +(x1 −x3)2 +(x2 −x3)2 ≥ 0 (zero if x1 = x2 = x3).,pos_def_matrices
times (always n λ’s):,eigenvec_val
"calculations it is the discrete Fourier transform that we compute. Fourier still lives, but",orthogonality
"Vy+ is in the row space, and ATAx+ = ATb from the SVD.",pos_def_matrices
"ΛS−1, Why is there no real matrix square",eigenvec_val
original Jacobi matrix D−1(−L−U). That connection is expressed by,computations
We can list four of the main uses of determinants:,determinants
67. Prove that no reordering of rows and reordering of columns can transpose a typical,gauss elim
2. What multiple ℓ32 of row 2 of A will elimination subtract from row 3 of A? Use the,gauss elim
Proof that max ﬂow = min cut. Suppose a ﬂow is maximal. Some nodes might still be,linear_prog
The other chapters look for nonzero determinants or nonzero eigenvalues or nonzero,gauss elim
"reverse the order, giving BTAT and B−1A−1. The proof for the inverse was easy, but this",gauss elim
ij = A ji,eigenvec_val
does not produce all polynomials in Pn+1. The right side of equation (3) has no constant,vector spaces
cross section cannot be longer than the major axis of the whole ellipsoid: λ1(B) ≥ λ1(A).,pos_def_matrices
when the same basis is used for H and P.,vector spaces
there is no second line of eigenvectors.,eigenvec_val
12. Which of these rules give a correct deﬁnition of the rank of A?,vector spaces
Product rule |A||B| = |AB|,determinants
ad −bc = 1.,determinants
"Every row splits into n coordinate directions, so this expansion has nn terms. Most of",determinants
side passed up a winning move?,vector spaces
4. Describe the four subspaces in three-dimensional space associated with,vector spaces
how each determinant in Cramer’s Rule leads to your solution x.,determinants
matrix depends on the choice of basis. Suppose the ﬁrst basis vector is on the θ-line and,vector spaces
Cij = (−1)i+j detMij.,determinants
A ﬂow of 2 can go on the path 1-2-4-6-1. A ﬂow of 3 can go along 1-3-4-6-1. An,linear_prog
5 = 12 + 22,orthogonality
"(d) Compute A and its rank if u = z = (1,0,0) and v = w = (0,0,1).",vector spaces
"degree n. They look like p = a0 +a1t +···+antn, and the dimension of the vector space",vector spaces
the rate at which ek converges to zero. We certainly need ρ < 1.,computations
1 1 2 2,vector spaces
Example 2. In our original f the coefﬁcient 2b = 4 was positive. Does this ensure a,pos_def_matrices
"The vector b = (2,5,7) is in that plane of the columns—it is column 1 plus column",gauss elim
The point is that a basis is a maximal independent set. It cannot be made larger without,vector spaces
25. You may have seen the equation for an ellipse as (x,pos_def_matrices
of inputs while xT(ATy) is the value of,gauss elim
"u + 3v + 5w = 2,",gauss elim
5. 1x = x.,vector spaces
multiplies x1 and x2 it produces the eigenvalues λ1 = 3 and λ2 = 2:,eigenvec_val
"c = 0. With no free variables (rank n), there is no nullspace except c = 0; the vectors are",vector spaces
0 0 0 0 0 1,vector spaces
1. The Shifted Algorithm.,computations
(b) the ﬁrst row of AB?,gauss elim
If A is invertible this rule also applies to its inverse (the power k = −1). The eigen-,eigenvec_val
eigenvector v = xn. Then the best x is the next eigenvector xn−1. The “minimum of the,pos_def_matrices
and deeper understanding is possible. It is exactly the same for linear programming. The,linear_prog
"of zeros is too large. Each woman must like at least one man, each two women must",linear_prog
"Find the determinant of the new matrix, by rule 3 or by direct calculation.",determinants
away from the tangency point x = y = 0.,pos_def_matrices
θ2). What is A(θ) times A(−θ)?,gauss elim
"similar matrices can appear in the differential and difference equations, by the change",eigenvec_val
"In this very unusual case, eAt can also be recognized directly from the inﬁnite series.",eigenvec_val
stay at a corner and cycle forever in the choice of basis.,linear_prog
operations. This matrix AT = AH = A∗ is called “A Hermitian”:,eigenvec_val
edges. There are nn−2 = 64 spanning trees!,vector spaces
"Years ago, this led to the belief that it was useless to escape a very small pivot by",determinants
"Remark 1. For positive deﬁnite matrices, Σ is Λ and UΣV T is identical to QΛQT. For",pos_def_matrices
represent. Those “dual unknowns” y tell how much the constrained minimum PC/min,pos_def_matrices
columns 1 + 2,gauss elim
2 came from a single,gauss elim
2A The system Ax = b is solvable if and only if the vector b can be expressed,vector spaces
"n coefﬁcients c0,...,cn−1 to choose, we only ask for equality at n points. That gives n",orthogonality
"unknowns, to the continuous diffusion described by this partial differential equation:",eigenvec_val
The physical signiﬁcance of du/dt =,eigenvec_val
eigenvalues. Construct a 2 by 2 example.,pos_def_matrices
erties 1–3 of A. Remember that U has orthonormal columns:,eigenvec_val
"egg = 0, it will cost extra to add",linear_prog
x = A−1Ax = A−1b.,gauss elim
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
9. Show that the set of nonsingular 2 by 2 matrices is not a vector space. Show also that,vector spaces
dimensional subspace of matrix space. AB − BA = 0 gives four equations for the,eigenvec_val
celebrated wave equation ∂ 2u/∂t2 = ∂ 2u/∂x2.,eigenvec_val
"by w8, the sum must be zero. When n is even the roots cancel in pairs (like 1 + i2 = 0",orthogonality
in U are combinations of the original rows in A. Therefore the row space of U contains,vector spaces
"a multiple of one row is subtracted from another, the row space, nullspace. rant and",pos_def_matrices
(b) Solve the system Ax = b starting with Lc = b:,gauss elim
Tvj = combination of the basis vectors = a1jv1 +···+anjvn.,eigenvec_val
take the inner product of both sides with sinx:,orthogonality
"(3,14). Find and solve a matrix equation for the unknowns (a,b,c).",gauss elim
"nullspace of ATA. To go in the other direction, start by supposing that ATAx = 0, and",orthogonality
p = A(ATA)−1ATAx = Ax = b.,orthogonality
You are allowed to combine permutations with the usual L and U (southwest and,gauss elim
minimization equivalent to Ax = λx. We will be doing in ﬁnite dimensions exactly,pos_def_matrices
many). We start with a diagonal example.,pos_def_matrices
the solution from the “interior” where all inequalities are strict: x ≥ 0 becomes x > 0.,linear_prog
5.3 Difference Equations and Powers Ak,eigenvec_val
"3t2 = 0p1+0p2+3p3+0p4, the last column contained 0, 0, 3. 0. The rule (6) constructs",vector spaces
We can look at that system by rows or by columns. We want to see them both.,gauss elim
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
enough. A transformation from one space to another requires a basis for each.,vector spaces
What is important is that this plane is not just a subset of R3 it is a subspace. It is,vector spaces
"those edges form a loop). Rows 1,2,4 are independent because edges 1,2,4 contain no",vector spaces
∗ ∗ ∗ ∗ ∗,computations
dimensional projections does not equal b.,orthogonality
"Solve for u1, u2, u3 and ﬁnd their error in comparison with the true solution u =",gauss elim
"all combinations of the rows, as every row space does—but here the third row contributes",vector spaces
"When A−1 fails to exist, the best substitute is the pseudoinverse A+. This inverts A",orthogonality
∂x2 = 4−6x = 4,pos_def_matrices
(Qx)T(Qy) = xTy and ∥Qx∥ = ∥x∥,eigenvec_val
"coordinate axes. A vector space is deﬁned without those axes, but every time I think of",orthogonality
"orthogonal complement, and the projection is b−Pb.)",orthogonality
back-substitution (that uses U). We can and should do it without A:,gauss elim
"stable; the ﬁssion balances the decay. Slower ﬁssion makes it stable, or subcritical, and",eigenvec_val
"yields a shearing transformation, which leaves the y-axis un-",vector spaces
examples above have effective rank 1 (when ε is very small).,pos_def_matrices
"21. Removing zero rows of U leaves A = LU, where the r columns or L span the column",pos_def_matrices
2.4 The Four Fundamental Subspaces,vector spaces
"The parallel plane x−3y−z = 12 contains the particular point (12,0,0). All points",vector spaces
we get the other. They are also hard to draw.,pos_def_matrices
"Furthermore, the product of the n eigenvalues equals the determinant of A.",eigenvec_val
2αk = coskπh. They occur in plus-minus pairs and λmax is cosπh.,computations
vergence to x = A−1b.,computations
"9. Suppose v1, v2, v3, v4 are vectors in R3.",vector spaces
"for by S). The theory of eigenvectors will lead to this formula S−1AS, and to the best",vector spaces
2u + 2v + 5w =,gauss elim
"43. All Pascal matrices have determinant 1. If I subtract 1 from the n, n entry, why does",determinants
"31. Compute the determinants S1, S2, S3 of these 1, 3, 1 tridiagonal matrices:",determinants
"The same idea of orthogonality applies to functions, The sines and cosines are or-",orthogonality
"The derivative of E2 is zero at the point �x, if",orthogonality
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
". I write it as a column vector, since",vector spaces
ponents are zero. The members of this four-dimensional subspace are six-dimensional,vector spaces
"6. From the pivots, eigenvalues, and eigenvectors of A =",pos_def_matrices
Remember that the determinant is a sum of 4! = 24 terms. Each term uses all four rows,linear_prog
"that whatever i is, it satisﬁes the equation i2 = −1. It is a pure imaginary number, and",eigenvec_val
15. What are the rank and eigenvalues when A and C in the previous exercise are n by n?,eigenvec_val
"The surface z = x2 −y2 goes down in the direction of the y axis, where the legs ﬁt (if you",pos_def_matrices
j is replaced by its modulus |x j|2:,eigenvec_val
"Symmetric and Hermitian matrices are certainly normal: If A = AH, then AAH and",eigenvec_val
"40. In equation (9) with m1 = 1 and m2 = 2, verify that the normal modes are M-",pos_def_matrices
"four-dimensional space with the usual basis—the coordinate vectors p1 = (1,0,0,0),",vector spaces
"(c) If rows 1 and 3 of A are the same, so are rows 1 and 3 of AB.",gauss elim
"rank 1. At the same time, the columns are all multiples of the same column vector; the",vector spaces
4 goes to each of Asia and Europe. For Asia and,eigenvec_val
"three matrices that have trace a+d = 9, determinant 20, and λ = 4,5.",eigenvec_val
10. Find the inverses (in any legal way) of,gauss elim
maximum growth rate is 1/λ1.,eigenvec_val
"This completes the link between volumes and determinants, but it is worth coming",determinants
Combination of columns equals b,vector spaces
AB which is the key formula in matrix computations. Ordinary numbers are the same:,gauss elim
"1.22 (a) Ax = b has a solution under what conditions on b, for the following A and b?",vector spaces
3. The rows of A span Rn.,vector spaces
"DO 10 I = 1, N",gauss elim
ments for a vector space: Linear combinations stay in the subspace.,vector spaces
"plane. This plane through (0,0,0) illustrates one of the most fundamental ideas in linear",vector spaces
m equations with m ≥ n (square systems are included). The least-square choice �x is the,pos_def_matrices
(b) Every invertible matrix can be diagonalized.,eigenvec_val
appeal of geometry; p must be the “projection of b onto the column space.” The error,orthogonality
"y1 = (1,1,1,0,0), and the vectors in the column space satisfy b1 − b2 + b3 = 0. Then",vector spaces
"tion of (1,0,0,0), (1,1,0,0), (1,1,1,0), (1,1,1,1) produces b = (3,3,3,2)? What 4",gauss elim
"through other angles, projections onto other lines, and reﬂections in other mirrors are",vector spaces
a b b b,gauss elim
yTb = 0. A good example is Kirchhoff’s Voltage Law in Section 2.5. Testing for zero,orthogonality
"immediately to n dimensions, and it is a perfect shorthand for studying maxima and",pos_def_matrices
"How do you know (without multiplying those factors) that A is invertible, symmet-",gauss elim
It is beautiful that elimination and completing the square are actually the same. Elim-,pos_def_matrices
Solve subroutine obeys equation (8): two triangular systems in n2/2 steps each. The,gauss elim
"3 inside, then it ends the same way:",eigenvec_val
tem of size n. The computing cost often determines the accuracy in the model. A,gauss elim
∥A∥2 = λmax(ATA) ≈ 2.618,computations
and zero eigenvalues as A.,pos_def_matrices
"part x and an imaginary part y, but no further inventions are necessary. Every real or",orthogonality
symmetric matrices: AT = A,eigenvec_val
Show that the three columns on the left lie in the same plane by expressing the third,gauss elim
"∥x∥/∥x∥∞ and ∥x∥1/∥x∥ are never larger than √n. Which vector (x1,...,xn) gives",computations
formula that give detA = 16−4−4−4+1.,determinants
only those vectors that have a ﬁnite length:,orthogonality
"Our example has two lines in Figure 1.1, meeting at the point (x,y) = (−1,2).",gauss elim
matrix from the pivot columns and the 2 by 4 matrix from R:,vector spaces
"Show that the vector x1 = (sinπh,sin2πh,...,sinnπh) is an eigenvector of J with",computations
"They change direction when multiplied by A, so that Ax is not a multiple of x. This",eigenvec_val
Similar matrices represent the same transformation T with respect so different,eigenvec_val
. The equations have,gauss elim
(a) The number of nonzero rows in R.,vector spaces
(e) the nullspace of A.,vector spaces
not have) a solution.,vector spaces
those terms (all but n! = n factorial) will be automatically zero. When two rows are in,determinants
(lowest cost) occurs at the point where the planes ﬁrst touch the feasible set.,linear_prog
"i j , which entries are changed now?",computations
to du/dt = Au give un. Find the complete solution up +un to,eigenvec_val
"Then, after computing cx and yb, explain how you know they are optimal.",linear_prog
"Each S−1 cancels an S, except for the ﬁrst S−1 and the last S.",eigenvec_val
Just switch signs in Rx = 0,vector spaces
vector is in the column space? If A =,orthogonality
at the same time fundamental for the applications. We shall explain as much as we,linear_prog
"(α,β,v) = (1,2,3), (2,3,1), (3,1,2), (1,3,2), (2,1,3), (3,2,1).",determinants
0 0 1 0,eigenvec_val
First a new word: The matrices A and M−1AM are “similar”. Going from one to,eigenvec_val
4. Describe the nullspace of A: Which special solutions in R4?,vector spaces
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"most one such B, and it is denoted by A−1:",gauss elim
this book and this course.,gauss elim
The matrix A2 is halfway between A and A∞. Explain why A2 = 1,eigenvec_val
Symmetric matrices appear in every subject whose laws are fair. “Each action has an,gauss elim
b3 −b4 +b5 = 0.,vector spaces
"Its determinant is the product of its pivots. The numbers 2,...,n all cancel:",determinants
1 1 2 1,determinants
"multiplies the pivot row j when it is subtracted from row i, and produces a zero in the i,",gauss elim
On those r-dimensional spaces A is invertible. On its nullspace A is zero. When A is,orthogonality
"Everything depends on step 1. Unless the functions Vj(x) are extremely simple, the",pos_def_matrices
"3 of those in Section B drop the course,",eigenvec_val
The ﬁrst step is always to ﬁnd the eigenvalues (ł1 and −3) and the eigenvectors:,eigenvec_val
The cofactor C1j is the determinant of Mij with the correct sign:,determinants
"t in every row (and the determinant is multiplied by tn). It is like the volume of a box,",determinants
"1 by 1 system 0x = b, one equation and one unknown, shows two possibilities:",vector spaces
6. Explain in terms of volumes why det3A = 3ndetA for an n by n matrix A.,determinants
"on the diagonal of C, write out the 7 by 7 system",vector spaces
"eigenvalues of A are distinct. Then there are certainly n independent eigenvectors, and",eigenvec_val
"These combinations M−1AM arise in a differential or difference equation, when a",eigenvec_val
to produce a singular matrix A0. Hint: U and V do not change:,pos_def_matrices
"Technical note: To construct the matrices, we need bases for V and W, and then for",vector spaces
for the two nullspace.,vector spaces
for exponential solutions eiωtx:,pos_def_matrices
"familiar linear equations, when we set the ﬁrst derivatives to zero. Our ﬁrst goal in",pos_def_matrices
"close to all four of its neighbors. That is the true curse of dimensionality, and parallel",computations
"The symmetric case is even better, since A1 = Q−1",computations
"(c) Find the general solution to Ax = b, when a solution exists.",vector spaces
"Thus (x,y) = (0,0) is a stationary point for both functions. The surface z = F(x,y) is",pos_def_matrices
"Europe, about 1200 A.D.",eigenvec_val
"(b) (x,y) = (2,5) and (3,7) lie on the line y = mx+c. Find m and c.",gauss elim
hard to compute. The form SeΛtS−1 gives the same answer when A can be diagonalized;,eigenvec_val
imaginary. The powers of w = eiθ stay on the unit circle:,orthogonality
"(0,0,1), solve Ax = b when b = (3,5,8). Challenge problem: What is A?",gauss elim
real. Complex numbers could have been permitted. but would have contributed nothing,eigenvec_val
(a) Find the total currents ATy out of the three cities.,gauss elim
6. (c1c2)x = c1(c2x).,vector spaces
quires only Cnlog2 7 multiplications! It depends on a simple fact: Two combinations of,gauss elim
0 0 0 1,eigenvec_val
"With steel-food-labor in the ratio 1-5-5, the economy grows as quickly as possible: The",eigenvec_val
zero in place of d and h will spoil the new zero in the corner. We have to leave one,eigenvec_val
"3, using the pivots.",orthogonality
"predicts that A2 −A−I = 0, since det(A−λI) is λ 2 −λ −1.",eigenvec_val
(d) T(M) = −M is impossible.,vector spaces
(b) E32 subtracts −7 times row 2 from row 3.,gauss elim
ferences that agree with the score differences? If the score differences are known for,vector spaces
1. The column space contains all multiples of,vector spaces
"5.27 Suppose the ﬁrst row of A is 7, 6 and its eigenvalues are i, −i. Find A.",eigenvec_val
(a) Find a nonzero solution x to Ax = 0. The matrix is 3 by 3.,gauss elim
Every two-dimensional vector is a combination of those (independent!) columns.,vector spaces
"until the third pivot. Actually it is not just the pivots, but the entire upper-left corners of",determinants
"If u0 = c1x1 + ··· + cnxn, then after k steps uk = c1λ k",eigenvec_val
3.37 Find the curve y = C + D2t which gives the best least-squares ﬁt to the measure-,orthogonality
by the particular solution xp as in Figure 2.2. Equation (4) is a good way to write the,vector spaces
of that plane with the xy-plane. Then ﬁnd a basis for all vectors perpendicular to the,vector spaces
"This matrix has a large inverse, because the axes 1, x, x2 are far from perpendicular. The",orthogonality
7. Convert y′′ = 0 to a ﬁrst-order system du/dt = Au:,eigenvec_val
"off the diagonal, to show that row j of F times column k of F−1 gives zero:",orthogonality
leaving m and n unchanged as a reminder of what happened. The problem has become:,linear_prog
3.15 If every entry in an orthogonal matrix is either 1,orthogonality
43. This matrix has a remarkable inverse. Find A−1 by elimination on [A I]. Extend to a,gauss elim
in each piece. (There is nothing unfair in assigning zero potential; if all other potentials,vector spaces
"Figure 3.2: A right triangle with 5+20 = 25. Dotted angle 100°, dashed angle 30°.",orthogonality
6. Choose three independent columns of U. Then make two other choices. Do the same,vector spaces
"The last case contains the ﬁrst two. The inverse appears when ϕ is −θ, and the",vector spaces
"be helpful. For each class of matrices, here are the special properties of the eigenvalues",eigenvec_val
provides a deﬁnite basis for the nullspace:,vector spaces
Each simplex step involves decisions followed by row operations—the entering and,linear_prog
vertibility (no zero eigenvalues). The only indication given by the eigenvalues is this:,eigenvec_val
"best we can say is that the masses will come arbitrarily close to (1,0) and also (0,1).",eigenvec_val
Ax = b has a solution,linear_prog
2(x+y) with the geometric mean √xy.,orthogonality
"the calculations must be done in that order. This is wrong. There is some freedom, and",gauss elim
Here is a typical application to polynomials P(t) of degree n − 1. The only such,vector spaces
"Which of x1, x2, x3 should enter the basis, and which of x4, x5 should leave? Compute",linear_prog
"ﬁxed proportions. But if we look at a single individual, the fractions that move become",eigenvec_val
The dimension of a space is the number of vectors in every basis.,vector spaces
ﬁrst by exact computation and second by rounding off each number to three ﬁgures.,gauss elim
orthogonal QTQ = I or QT = Q−1,eigenvec_val
"as a zero block with p+q > n), then a complete matching is impossible.",linear_prog
"2. The columns are independent, so Ax = 0 has only the solution x = 0.",vector spaces
est possible column space (one vector only) comes from the zero matrix A = 0. The,vector spaces
M−1 = A−1 +A−1uvTA−1/(1−vTA−1u).,gauss elim
1 1 1 0,vector spaces
(b) The column space of a 2 by 2 matrix is the same as its row space.,vector spaces
"18. Find the largest a, b, c for which these matrices are stable or neutrally stable:",eigenvec_val
0 0 0 0,vector spaces
"The intermediate eigenvectors x2,...,xn−1 are saddle points of the Rayleigh quotient",pos_def_matrices
"center). It is also important to recognize that matrices cannot do everything, and some",vector spaces
"Figure 8.4: The cone of nonnegative combinations of the columns: b = Ax with x ≥ 0. When b is outside the cone,",linear_prog
"13. Solve by elimination, exchanging rows when necessary:",gauss elim
"What part of A−1 have you found, with this particular b?",gauss elim
17. If A has λ1 = 2 with eigenvector x1 =,eigenvec_val
"help of a basis. But to represent T by a matrix, we do need a basis. Figure 5.5 offers two",eigenvec_val
compute the eigenvalues. Find an x so that xTA1x < 0.,pos_def_matrices
"Ax = (0.8, 0.2)",eigenvec_val
"ations on a tableau. It is really the simplex method itself, boiled down.",linear_prog
What is the relation between C and CH? Does it hold whenever C is constructed from,eigenvec_val
and then adding: (A+B)T is the same as AT +BT. But what is the transpose of a product,gauss elim
The second eigenvector is any nonzero multiple of x2:,eigenvec_val
them (theoretically) to the front. This produced [B N]. The same shift reordered the,linear_prog
The proof is a good exercise with inverse matrices in the next section.,gauss elim
jdx = coefﬁcient of,pos_def_matrices
Start from A = LDU. Then A equals L(UT)−1 times UTDU.,gauss elim
"into or out of S2. The net rate of change is dv/dt, and dw/dt is similar:",eigenvec_val
"Example 6. The vectors w1 = (1,0,0), w2 = (0,1,0), and w3 = (−2,0,0) span a plane",vector spaces
"ak and bk. From this inﬁnite sequence of sines and cosines, multiplied by ak and bk, we",orthogonality
"We claimed that B is well-conditioned, and not particularly sensitive to roundoff—except",computations
lined up with the axes of the ellipse. The way to see the ellipse properly is to rewrite,pos_def_matrices
for the equivalent in cheaper foods. To compute it we return to equation (2) of Section,linear_prog
�� = x1y1 +···+xnyn.,orthogonality
3y + qz = t.,gauss elim
"8. Find UΣV T if A has orthogonal columns w1,...,wn of lengths σ1,...,σn.",pos_def_matrices
2v + 4w =,gauss elim
matrices whose upper left determinants were all zero:,pos_def_matrices
and the eigenvalue problem is nonlinear (λ times x). The trick that succeeds is to divide,pos_def_matrices
C = y1 +···+ym,orthogonality
is σ2(ATA)−1. This is the all-important covariance matrix for the error in �x.,orthogonality
"The determinant of the left-hand side is the Wronskian. It never becomes zero, because",eigenvec_val
matrix form we have the new factorization A = QR:,orthogonality
"“sensitivity” of a problem: If A and b are slightly changed, how great is the effect on",computations
"6. What are the square and the square root of w128, the primitive 128th root of 1?",orthogonality
"The vectors are nonzero, so vT",orthogonality
"16. Decide whether or not the following vectors are linearly independent, by solving",vector spaces
is also diagonal. B has the same,eigenvec_val
My class often asks about unsymmetric positive deﬁnite matrices. I never use that,pos_def_matrices
1 2 3 5,vector spaces
"only three nonzero components. If we can show that the pivot columns—the ﬁrst, fourth,",vector spaces
"all cosines lie in the interval −1 ≤ cosθ ≤ 1, this gives another proof of equation (6):",orthogonality
through b in the direction of a2. To ﬁnd the closest points x1a1 and b + x2a2 on the,orthogonality
"someone asked about x2 − i = 0, there was an answer: The square roots of a complex",orthogonality
"The i, j entry of AB is the inner product of the ith row of A and the jth",gauss elim
(b) The nullspace of the 4 by 4 identity matrix.,vector spaces
"The old minimizer x1 = x2 = 0 is not on the line. The Lagrangian function L(x,y) =",pos_def_matrices
"1. Reduce [A b] to [U c], to reach a triangular system Ux = c.",vector spaces
satisfy Ax = 0). Every solution to Ax = b is the sum of one particular solution and a,vector spaces
First difﬁculty: If b is not in the column space there is no solution. The scores must,vector spaces
�� = c1λ k,eigenvec_val
"31. Compute P2, P3, and P100 in Problem 30. What are the eigenvalues of P?",eigenvec_val
totally arbitrary �x3 and �x4 to be zero. The minimum length solution is x+:,pos_def_matrices
"independent. If the rank is less than n, at least one free variable can be nonzero and the",vector spaces
that all the answers were yes. Matrix multiplication is deﬁned exactly so that the,vector spaces
"What happens in practice is that the (n,n) entry of Ak—the one in the lower right-hand",computations
the lines through the q’s.,orthogonality
really desirable.) That claim brought a burst of research into methods that approach,linear_prog
(b) Explain why all 120 terms are zero in the big formula for detA.,determinants
"reached, from which all edges go the wrong way: The cost has been minimized. That",linear_prog
middle. The capacities out of the source and into the sink are still 1. If the maximal ﬂow,linear_prog
"into a unitary M3, which is put into the corner of U3:",eigenvec_val
"In case A is real and symmetric, its eigenvalues are real by Property 2. Its unit",eigenvec_val
normal equation for the best �x—in which QTQ = I.,orthogonality
(a1�x−b1)a1 +···+(am�x−bm)am = 0.,orthogonality
quires us to keep all components of xk until the calculation of xk+1 is complete. A,computations
and typesetting in C and then printing and binding in D. C is the set-up cost and D,orthogonality
"shift is an example of the Gram-Schmidt process, which orthogonalizes the situation in",orthogonality
"−i. The eigenvectors are also not real. Somehow, in turning through 90°, they are",eigenvec_val
"way to measure their length, and the space contains all those functions that have a ﬁnite",orthogonality
"ac−b2 may be negative. This occurred in both examples, when b dominated a and c. It",pos_def_matrices
"adds to r +(m−r) = m. Something more than orthogonality is occurring, and I have to",orthogonality
the set of singular 2 by 2 matrices is not a vector space.,vector spaces
"(ii) If f(x,y) is positive deﬁnite, then necessarily c > 0.",pos_def_matrices
"35. (a) Suppose all columns of B are the same. Then all columns of EB are the same,",gauss elim
The eigenvalues λj are the same as for the original Ax = λMx. and the eigenvectors are,pos_def_matrices
The geometry of the simplex method is now expressed in algebra—“corners” are “basic,linear_prog
have to add to zero:,vector spaces
"unbiased matrix C = W TW is the inverse of the covariance matrix—whose i, j entry is",orthogonality
"Cauchy is also attached to this inequality |aTb| ≤ ∥a∥∥b∥, and the Russians refer to it as",orthogonality
6x + 4y =,gauss elim
Figure 3.4: The true action Ax = A(xrow +xnull) of any m by n matrix.,orthogonality
(c) Find a vector x that satisﬁes Ax = v+w. Is x unique?,eigenvec_val
"vectors meet all of the m + n conditions. In other words, x lies in the intersection of",linear_prog
"the ﬁrst pivot is a/1 = a.) Multiplying together all the individual pivots, we recover",determinants
0 1 2 3 4,vector spaces
Saddle Point ac < b2:,pos_def_matrices
"The Arnoldi-Lanczos iteration orthogonalizes the columns of KN, and the conjugate",computations
inverse power step—and to use the zeros that Householder created.,computations
Does it have a right-inverse?,vector spaces
"The nullspace solutions xn are combinations of n − r special solutions, with",vector spaces
"If time is the fourth dimension, then the plane t = 0 cuts through four-dimensional ",gauss elim
which the column numbers are different for A and AT.,vector spaces
"The columns (1,1,1) and (−3,0,3) are orthogonal. We can project y separately onto",orthogonality
"Only one of the 6s will become free (zero). At the same time, one variable will move up",linear_prog
does not have a unique basis.,vector spaces
"other foods. The reduced cost of eggs is their own price, minus the price we are paying",linear_prog
"Seidel) it yields λ 2 = λµ2. Therefore λ = 0 and λ = µ2 as in Example 2, where µ = ±1",computations
"After k steps, uk is a combination of the n “pure solutions” λ kx.",eigenvec_val
"22. Suppose v1,v2,...,v6 are six vectors in R4.",vector spaces
"(2,−2) is reversed to (−2,2). On a combination like v = (2,2)+",vector spaces
"41. Let P = (1,0,−1), Q = (1,1,1), and R = (2,2,1). Choose S so that PQRS is a",determinants
. Find the complete,vector spaces
23. Solve for the columns of A−1 =,gauss elim
"gate of 3+3i. In every case, aij = a ji.",eigenvec_val
"(c) (1,2,2), (−1,2,1), (0,8,0).",vector spaces
We hope and expect that,gauss elim
two in Rn. That will complete the fundamental theorem of linear algebra.,orthogonality
"7. Comparing the eigenvalues of ATA and AAT, prove that ∥A∥ = ∥AT∥.",computations
much good for a large matrix; the computer can probably ﬁnd the eigenvalues with more,eigenvec_val
3 9 0 1,gauss elim
2x2. The factor λ k,eigenvec_val
column j of AB = A times (column j of B),gauss elim
matrix be symmetric? Can it be invertible?,gauss elim
determined by the submatrix Ak in the upper left corner of A. The remaining rows and,determinants
The separation into Factor and Solve means that a series of b’s can be processed. The,gauss elim
"ples. It will become the basis for half of this book, simplifying a matrix so that we can",gauss elim
. These diagonal matrices B form a two-,eigenvec_val
11. Describe the subspace of R3 (is it a line or a plane or R3?) spanned by,vector spaces
Then describe all matrices that diagonalize A−1.,eigenvec_val
"The problem in linear programming is to locate that cornet For this, calculus is not",linear_prog
1. S = diagonal part of A (Jacobi’s method).,computations
3.5 The Fast Fourier Transform,orthogonality
The solution to a difference equation uk+1 = Auk is uk = Aku0.,eigenvec_val
and much more “abstract” deﬁnition of the transpose:,orthogonality
(c) If A is invertible then A−1 is invertible.,gauss elim
Our other question is very practical. How many separate arithmetical operations does,gauss elim
"This may seem like a total stalemate, but I hope you will not be fooled. The optimal",linear_prog
in the next section. And the simplex method remains tremendously valuable. like the,linear_prog
"instead of a real orthogonal Q, and with 8i and −i on the diagonal of Λ.",eigenvec_val
"8. Find all the odd permutations of the numbers {1,2,3,4}. They come from an odd",determinants
"of them must be the optimal vector (unless the minimum cost is −∞). The other three,",linear_prog
analogue (5). We multiplied through by h2 to reach n equations Au = b:,gauss elim
"41. True or false, with a good reason:",eigenvec_val
longest computation. For this A we know the λ’s are all positive:,pos_def_matrices
"(b) Move the third vertex to C = (1,−4) and justify the formula",determinants
3] is positive. The stopping test is passed. The corner,linear_prog
"to U, brings back A:",gauss elim
jection onto the space spanned by the preceding rows—leaving a perpendicular “height,determinants
(a) Column space contains,orthogonality
14. Describe the smallest subspace of the 2 by 2 matrix space M that contains,vector spaces
"(b) Write an orthonormal basis for V⊥, and ﬁnd the projection matrix P1 that projects",orthogonality
30. If Ax is in the nullspace of AT then Ax = 0. Reason: Ax is also in the,orthogonality
"changes only the zeros below the 1. This part has only n − j components, so the count",gauss elim
No pivot in column 2,vector spaces
(a) the big formula (6)?,determinants
systems have exactly the same solution x.,gauss elim
completed to full duality.,linear_prog
notice that we still have QTQ = I. So QT is still the left-inverse of Q.,orthogonality
The Finite Element Method,pos_def_matrices
vector space containing both P and L is either,vector spaces
both equal I. Two steps will work for any normal matrix:,eigenvec_val
losing independence. A basis is also a minimal spanning set. It cannot be made smaller,vector spaces
"We now introduce matrix notation to describe the original system, and matrix mul-",gauss elim
(c) All skew-symmetric matrices (AT = −A).,vector spaces
"hopeless when squared, but SΛS−1 is perfect. The square is SΛ2S−1, and the eigenvec-",eigenvec_val
"y′′, and Cooley and Tukey noticed how it could be done:",orthogonality
(d) A Markov matrix.,eigenvec_val
"Now comes the ﬁnal step, to allow nonzeros away from the diagonal of A.",pos_def_matrices
The Nullspace of A,vector spaces
"cofactors are zero, is A sure to be invertible?",determinants
c0 +ic1 +i2c2 +i3c3,orthogonality
"whole class, corresponding to all complex numbers. The matrices are called “normal”.",eigenvec_val
relation between these two methods. More complicated ﬁnite elements—polynomials of,pos_def_matrices
"If xTy > 0, their angle is less than 90°. If xTy < 0, their angle is greater than",orthogonality
"eigenvectors, so they are not diagonalizable. Certainly they have to be discussed, but we",eigenvec_val
row i of AB = (row i of A) times B.,gauss elim
Problems 22–28 use the rules to compute speciﬁc determinants.,determinants
xn. The scaling factors αk will approach λn.,computations
Differentiation and integration are inverse operations. Or at least integration followed,vector spaces
The whole point of overrelaxation is to discover this optimal ω. The product of the,computations
"could have received more ﬂow! Every edge across the cut must he ﬁlled, or extra ﬂow",linear_prog
formula is again pretty well known (it has six terms):,determinants
"rows of A. With right angles, these rows are orthogonal and AAT is diagonal:",determinants
"This formula is dimensionally correct; if we double the length of b, then both numerator",orthogonality
"that a zero can appear in a pivot position, even if the original coefﬁcient in that place",gauss elim
(b) A multiplies an n by p matrix B? Then AB is m by p.,gauss elim
Now we try to explain how these ideas can be applied.,pos_def_matrices
"As θ → 0, the only eigenvector of the nondiagonalizable matrix",eigenvec_val
x + 7y − 6z =,gauss elim
"2(1,1,1,1) and u =",pos_def_matrices
We cannot ask A−1 to bring back a whole nullspace out of the zero vector.,orthogonality
Do they span the space of all 5 by 5 matrices? No need to write them all down.,vector spaces
Figure 5.3: The slow and fast modes of oscillation.,eigenvec_val
"(B) From B to J, the job is to transpose the matrix. A permutation does that:",eigenvec_val
when the rows are divided by the pivots.,gauss elim
"(b) If A is 8 by 10 with a two-dimensional nullspace, show that Ax = b can be solved",vector spaces
P1P2 is what it is.,orthogonality
"If b belongs to the column space, the solutions of Ax = b are easy to ﬁnd. The last",vector spaces
c1 = 3 and all other ci = 0; this is a nontrivial combination that produces zero.,vector spaces
"(b) Now suppose y = (1,0,0,0), and ﬁnd c.",orthogonality
(a) Show that Pu = u. Then u is an eigenvector with λ = 1.,eigenvec_val
1.6 Inverses and Transposes,gauss elim
"stituting into the second equation, we ﬁnd v = 1. Then the ﬁrst equation gives u = 1.",gauss elim
"The example keeps all three columns e1, e2, e3, and operates on rows of length six:",gauss elim
"bers will reappear: The circling solution is u(t) = (cost,sint).",eigenvec_val
tion is called the Fast Fourier Transform.,orthogonality
0 0 0 c,determinants
"4 belong in the fourth row and column, which are deleted when column 4 is removed",vector spaces
That is a column times a row—a square matrix—divided by the number aTa.,orthogonality
vTATAv j = σ2,pos_def_matrices
"(b) F = (x2 −2x)cosy, with stationary point at x = 1, y = π.",pos_def_matrices
5 by 5 “alternating matrix” and guess its inverse:,gauss elim
"complex numbers a + ib. A nonnormal matrix without orthogonal eigenvectors belongs to none of these classes,",eigenvec_val
"easily ﬁxed by exchanging them, or the equations don’t have a unique solution.",gauss elim
"9. Suppose the permutation P takes (1,2,3,4,5) to (5,4,1,2,3).",determinants
error vector e = b−A�x must be in the nullspace of AT:,orthogonality
"(5,0,0) and (0,10,0) and (0,0,10), twice as far from the origin—which is the center",gauss elim
"minant, detA and |A|.) Properties 4–10 will be deduced from the previous ones. Every",determinants
Section 8.3 has something new in this fourth edition. The simplex method is now,linear_prog
formula or pivots should give some upper bound on the determinant.,determinants
"the components of y are 5, −2, 1. This is the same combination as in b3 −2b2 +5b1 on",vector spaces
"36. For the closest cubic b = C +Dt +Et2 +Ft3 to the same four points, write the four",orthogonality
6.3 Singular Value Decomposition,pos_def_matrices
det(K −λI) = λ 2 +1.,eigenvec_val
1.1 (a) Write down the 3 by 3 matrices with entries,gauss elim
"S is also called a preconditioner, and its choice is crucial in numerical analysis.",computations
"tions are vitally important to the theory of linear algebra, and absolutely central to the",orthogonality
Ib = b is the matrix analogue of multiplying by 1. A typical elimination step,gauss elim
The Cost of Elimination,gauss elim
|µ −λ| ≤ ∥S∥∥S−1∥∥E∥ = c(S)∥E∥.,computations
2. Use the cofactor matrix C to invert these symmetric matrices:,determinants
"As w approaches n, the matrix becomes full, and the count is roughly n3. For an exact",gauss elim
"2. Modify a11 in equation (6) from a11 = 2 to a11 = 1, and ﬁnd the LDU factors of this",gauss elim
2.4 The Four Fundamental Subspaces,vector spaces
"this section is to ﬁnd the minimum principle that is equivalent to Ax = b, and the",pos_def_matrices
(# of nodes)−(# of edges)+(# of loops) = (n)−(m)+(m−n+1) = 1.,vector spaces
"The factor eλt is common to every term, and can be removed. This cancellation is the",eigenvec_val
". In letters, L multiplies",gauss elim
inequalities. When is the feasible set empty (no x)?,linear_prog
"approximately ω − 1 = 1.07, which is a major reduction from the Gauss-Seidel value",computations
equality is achieved by the right ﬂow and the right cut.,linear_prog
"This leaves only one more change in notation, condensing two symbols into one.",eigenvec_val
2uv + 2v2 = 1 to a sum of squares by ﬁnding the,pos_def_matrices
automatically means summation. He wrote aijx j or even a j,gauss elim
computed value. One standard procedure is to factor A − αI into LU and to solve,computations
The equality constraints are now Ax = b. The n + m inequalities become just x ≥ 0.,linear_prog
distinct—then its n eigenvectors are automatically independent (see 5D below). There-,eigenvec_val
Problems 23–29 are about A = SΛS−1 and Ak = SΛkS−1,eigenvec_val
P = A(ATA)−1AT for A = [a1 a2],orthogonality
"ertheless, it is the most important property a transformation can have1. Of course most",vector spaces
4.4 Applications of Determinants,determinants
"able equations Ax = b in three unknowns x = (C,D,E). Set up the three normal",orthogonality
2x + y = 6,linear_prog
point of view it was very satisfactory; the four subspaces were computable and their,vector spaces
22. Prove that 4 is the largest determinant for a 3 by 3 matrix of 1s and −1s.,determinants
complement is a whole line.,orthogonality
"quadrant b ≥ 0. For every other b, the alternative must hold for some y:",linear_prog
6+1 = 7. Is this total of 7 achievable? What is the maximal ﬂow from left to right?,linear_prog
12. Suppose there are three major centers for Move-It-Yourself trucks. Every month half,eigenvec_val
42. Find a basis for the space of polynomials p(x) of degree ≤ 3. Find a basis for the,vector spaces
(b) Write down a 4 by 4 skew-symmetric matrix with detK not zero.,determinants
nonlinear optimization problems as well.,linear_prog
"To ﬁnd the most general solution to Rx = 0 (or, equivalently, to Ax = 0) we may assign",vector spaces
����� = λ 2 −7λ +10−|3−3i|2,eigenvec_val
Projections and Least Squares,orthogonality
0 1 · 0,orthogonality
What theorem does this prove about the fundamental subspaces?,orthogonality
(I′) xTAx ≥ 0 for all vectors x (this deﬁnes positive semideﬁnite).,pos_def_matrices
1.25 What multiple of row 2 is subtracted from row 3 in forward elimination of A?,gauss elim
"36. If A = SΛS−1, diagonalize the block matrix B =",eigenvec_val
The basic operation on a quadratic form is to change variables. A new vector y is,pos_def_matrices
"diagonal, and the problem is to ﬁnd the eigenvalues for which A − λI is singular. The",determinants
"insigniﬁcant, and the ratio F1001/F1000 must be very close to (1+",eigenvec_val
The reason is that each elementary operation leaves the row space unchanged. The rows,vector spaces
"The entry 17 is (2)(1)+(3)(5), the inner product of the ﬁrst row of A and ﬁrst column",gauss elim
Matrices and Gaussian Elimination,gauss elim
"candidates t. Therefore |λ| cannot exceed λ1, which was tmax.",eigenvec_val
25. Which subspaces are the same for these matrices of different sizes?,vector spaces
The same reasoning applied to AT produces the dual result: The left nullspace N(AT),orthogonality
1 3 0 -1,vector spaces
n − p + n − q ≤ n. (We again assume no zero eigenvalues—which are handled,pos_def_matrices
"If egg is the ﬁrst free variable, then increasing the ﬁrst component of xN to δ will increase",linear_prog
also as d1v1 +d2v2. Check numerically that M connects c to d: Mc = d.,eigenvec_val
"The transposes turn up in reverse order on the right side, just as the inverses do in the",orthogonality
"ables) are a basis for the column space. These columns are independent, and it is easy",vector spaces
∥A−1∥2 = λmin(ATA) ≈ 0.382,computations
"differential equation takes an inﬁnite number of inﬁnitesimal steps, but the two theories",eigenvec_val
Figure 6.3: The pseudoinverse A+ inverts A where it can on the column space.,pos_def_matrices
"Repeat the computation with c = (0,1,0,1,0,1,0,1).",orthogonality
"stretching from rotation, and that is exactly what QS achieves. The orthogonal matrix",pos_def_matrices
"the ideal case, the difference b in the score would exactly equal the difference xh −xv in",vector spaces
"(b) If Ax = 0 and Az = 5z, which subspaces contain these “eigenvectors” x and z?",orthogonality
vectors in the nullspace of A−λI (which we call the eigenspace) will satisfy Ax = λx.,eigenvec_val
from equation (10) below.,computations
"is zero in the (2,1) position.",computations
"orthogonal axes (by Gram-Schmidt). We look for combinations of 1, x, and x2 that are",orthogonality
"or questionnaires, and it contains too many errors to be found in the subspace S. When",orthogonality
C−1y+Ax = b above ATy = f:,vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
"example, probably the determinant test is the easiest:",pos_def_matrices
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"4.9 If P1 is an even permutation matrix and P2 is odd, deduce from P1 + P2 = P1(PT",determinants
"j = 0 (omission from the diet). That is the key to y∗Ax∗ = cx∗, which we",linear_prog
"If A is symmetric positive deﬁnite, then P(x) = 1",pos_def_matrices
"(b) v = (3,1).",vector spaces
∥Ax∥/∥x∥ may be found at a vector x that is not one of the eigenvectors. This maximum,computations
"dimensions, and an ellipsoid in n dimensions.",pos_def_matrices
A = QΛQT = (Q,pos_def_matrices
(a) What is the rank of A?,vector spaces
"the lower triangular part of A. On the right-hand side, T is strictly upper triangular.",computations
1.2 The Geometry of Linear Equations,gauss elim
"U to u.) If A = I and b = (1,0,0), which multiple of V = (1,1,1) gives the smallest",pos_def_matrices
21. Suppose detA = 1 and you know all the cofactors. How can you ﬁnd A?,determinants
30. (Calculus question) Show that the partial derivatives of ln(detA) give A−1:,determinants
2(x2 +2bxy+9y2)−y for b in this range.,pos_def_matrices
(d) All rows of BA are the same as row 1 of A.,gauss elim
22. Write down the 3 by 3 matrices that produce these elimination steps:,gauss elim
The whole point is to see the analogy with projections. The component of the vector,orthogonality
imagine the geometry (and gets it right),gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"tion). What is the rank, and a particular solution?",vector spaces
difference should be odd.,determinants
"There are other “dual” theorems, of which we mention only one. We can start with a",vector spaces
"Here c is cosθ, s is sinθ, and c2 + s2 = 1 in the denominator. This matrix P was dis-",orthogonality
the shortest distance and the longest distance:,pos_def_matrices
16. If A = SΛS−1 then A3 = (,eigenvec_val
(a) A is invertible.,eigenvec_val
rates of the solutions stay ﬁxed when the equations change form. It seems to us that,eigenvec_val
The power method is reasonable only for a matrix that is large and sparse. When too,computations
"the same plane as a, b. The third vectors c and q3 were not involved until step 3.",orthogonality
"correct replacement for the condition ∂ 2F/∂x2 > 0? With only one variable, the sign of",pos_def_matrices
of the (rare) counterexamples to convergence (so we shift).,computations
we get at the minimum of Q? What are these equations called in the theory of least,pos_def_matrices
"Writing C for R−1, and multiplying through by (RT)−1 = CT, this becomes a standard",pos_def_matrices
weight w1 approaches zero? The measurement b1 is totally unreliable.,orthogonality
0 0 1 0,eigenvec_val
and the number of operations required to compute it is of order n3.,computations
the Gauss-Jordan computation of A−1.,determinants
"return ﬂow x61 (dotted line), we maximize the total ﬂow into the sink.",linear_prog
Notice that the product of lower triangular matrices is again lower triangular.,gauss elim
41. Find the weighted least-squares solution �xW to Ax = b:,orthogonality
"negative steak or peanut butter.) This is the feasible set, and me p1001cm is to minimize",linear_prog
"This test b1 + b2 + b3 = 0 makes b orthogonal to y = (1,1,1) in the left nullspace. By",orthogonality
39. Explain why all these statements are false:,vector spaces
both cases we cannot continue without a decent notation (matrix notation) and a decent,gauss elim
Our second goal is to ﬁnd a minimization problem equivalent to Ax = λx. That is not so,pos_def_matrices
Or the bank can switch to a differential equation—the limit of the difference equation,eigenvec_val
A−λI has λ 2 (and no higher power of λ) in its determinant.,eigenvec_val
"This is an enormous number, and F1001 will be even bigger. The fractions are becoming",eigenvec_val
"(b) Construct a 3 by 3 system that needs a row exchange to keep going, but breaks",gauss elim
vector of length ∥y∥ connecting x to x + y. The third side of the triangle goes,orthogonality
"row space, and the Gram-Schmidt process is not just a proof that every subspace has an",computations
Chapter 2 Vector Spaces,vector spaces
the rank and why? The columns of A are linearly,vector spaces
"bor is in limited supply and ought to be minimized. And, of course, the economy is not",eigenvec_val
The fundamental theorem of linear algebra was in Figure 3.4. Every p in the column,pos_def_matrices
mesh points. This agrees with the exact solution u(x) = x−x2 = 1,pos_def_matrices
"leaving variables have to be chosen, and they have to be made to come and go. One",linear_prog
|ai j| = ri.,computations
"Every diagonal entry aii must be positive. As we know from the examples, however,",pos_def_matrices
The right-hand side has an extra −2xiyi from each (xi −yi)2:,orthogonality
2 times the ﬁrst component of b was subtracted from the second component. The same,gauss elim
The goal of this section is to explain and use four ideas:,vector spaces
"(a) the vectors (1,3,2), (2,1,3), and (3.2,1).",vector spaces
is not diagonalizable because the rank of A − 3I is,eigenvec_val
row i of UH times column j of U is 1,eigenvec_val
Draw a graph representing each equation as a straight line in the x-y plane; the lines,gauss elim
"straint x+2y ≥ 4 is converted into w ≥ 0, which matches perfectly the other inequality",linear_prog
"Split x = (3,3,3) into a row space component xr and a nullspace component xn.",orthogonality
I think it is time to stop and call the list complete. It only remains to ﬁnd a deﬁnite,determinants
"dicular to the line). In that eigenvector basis, the matrix is diagonal:",eigenvec_val
"4. The right-hand side must satisfy b3 + b5 = b4, for elimination to arrive at 0 = 0. To",vector spaces
Chapter 2 Vector Spaces,vector spaces
"13. If the sum of the “vectors” f(x) and g(x) in F is deﬁned to be f(g(x)), then the “zero",vector spaces
"eigenvalues and eigenvectors are complex, as they are for the rotation Q.",eigenvec_val
genuine application. It did not show how fundamental those subspaces really are.,vector spaces
projection with λ = 0 and 0.,eigenvec_val
A and B multiply the columns in B and C. Then the key property is this:,gauss elim
"the basis might change without actually moving from the corner. In theory, we could",linear_prog
"(a) (1,1,1) is perpendicular to (1,1,−2), so the planes x+y+z = 0 and x+y−2z =",orthogonality
1.4 Matrix Notation and Matrix Multiplication,gauss elim
"2.1). This is the thin set of attainable b. If b lies off the plane, then it is not a combination",vector spaces
tation for formula (4): Every vector b is the sum of its one-dimensional projections onto,orthogonality
14. Show that x−y is orthogonal to x+y if and only if ∥x∥ = ∥y∥.,orthogonality
that is closer to b than any other point in the column space.,orthogonality
That is the eigenvalue equation. In matrix form it is Ax = λx. You can see it again if we,eigenvec_val
(a) How do you know that the rows are linearly dependent?,determinants
The Rayleigh quotient in equation (11) is never below λ1 and never above λn (the largest,pos_def_matrices
"[−1 2 2]. The multiple of that row that satisﬁes the equation is x+ = (−2,4,4). There",pos_def_matrices
"into LDLT, and connect the entries in D and L to 3, 2, 4 in f.",pos_def_matrices
"4. If a, d, f in Problem 3 are all nonzero, show that the only solution to Ux = 0 is x = 0.",vector spaces
"in the direction of the last eigenvector xn: if b = xn, then x = A−1b = b/λn.",computations
"determinant is zero? Are the vectors (1,0,−1), (2,1,0), (1,1,1) independent?",determinants
is no possibility of returning to anything more expensive. Eventually a special corner is,linear_prog
of the input t. We look for a straight line b = C +Dt. For example:,orthogonality
not exist; in the second case x is not unique. Probably both difﬁculties are present.,vector spaces
"11. For the matrix P = I − AT(AAT)−1A, show that if x is in the nullspace of A, then",linear_prog
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"every case we have Jordan’s similarity A = MJM−1, so now we need the powers of J:",eigenvec_val
"V = P3 into W = P4, so we need a basis for W. The natural choice is y1 = 1, y2 = t,",vector spaces
Please look again at this complete solution to Rx = 0 and Ax = 0. The special solution,vector spaces
Figure 3.8: Projection onto the column space of a 3 by 2 matrix.,orthogonality
"ation (a division by the last pivot). The second to last unknown requires two operations,",gauss elim
Find a vector in both column spaces C(A) and C(B):,orthogonality
"17. If the right side b is the last column of A, solve the 3 by 3 system Ax = b. Explain",determinants
0 1 0 0,determinants
Chapter 2 Vector Spaces,vector spaces
When b2 = 2b1 there are inﬁnitely many solutions. A particular solution to,vector spaces
A�x. Why is p = b?,orthogonality
"forward elimination, The diagonal entries of U are the pivots.",gauss elim
"contains aij = 0 if the ith woman and jth man are not compatible, and aij = 1 if they are",linear_prog
3. A reﬂection matrix transforms every vector into its image on,vector spaces
"in double precision, but a better and faster way is iterative reﬁnement: Compute only",computations
1.2 The Geometry of Linear Equations,gauss elim
Figure 3.12: Flow graph for the Fast Fourier Transform with n = 4.,orthogonality
"by working with the coordinates of the point. We can write the vector in a column, or",gauss elim
"(c) If S and T are subspaces of R5, their intersection S ∩ T (vectors in both sub-",vector spaces
Projection: P = P2 = PT,eigenvec_val
"(b) SubstitutingU−1AU for T, deduce the famous Cayley-Hamilton theorem: Every",eigenvec_val
"Figure 2.6: A directed graph (5 edges, 4 nodes, 2 loops) and its incidence matrix A.",vector spaces
computer cannot solve it exactly. It has to be approximated by a discrete problem—the,gauss elim
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"1.19 What subspace of 3 by 3 matrices is spanned by the elementary matrices Eij, with",vector spaces
"simple. The right basis led to B, which was diagonal.",eigenvec_val
21 to factor A into LU where L =,gauss elim
"2b2 +5b1 = 0. Even though there are more unknowns than equations, there may be no",vector spaces
"54. If you multiply a northwest matrix A and a southeast matrix B, what type of matri-",gauss elim
32. Find the dimensions of these vector spaces:,vector spaces
also to transposes. In Chapter 1 the transpose was constructed by ﬂipping over a matrix,orthogonality
"of order 10−9. With n = 100 unknowns and c = 5000, the error is ampliﬁed at most to",computations
"these terms together as a11C11, where the coefﬁcient of a11 is a smaller determinant—",determinants
. Multiply E31 by R31.,gauss elim
"This law applies equally to AT, which has m columns. AT is just as good a matrix as A.",vector spaces
"The second cofactor C12 is a23a31 −a21a33, which is detM12 times −1. This same tech-",determinants
eigenvectors. All eigenvalues satisfy |λ| = 1.,eigenvec_val
just adds the vectors (do it graphically).,gauss elim
"mal. Similarly, any x that achieves the cost cx = yb must be an optimal x∗.",linear_prog
0 1 2 0 0 1,gauss elim
factor tI +(1−t)R has positive diagonal. That ends the proof.,pos_def_matrices
Remark 2. The diagonalizing matrix S is not unique. An eigenvector x can be multiplied,eigenvec_val
"nodes are each connected to an eleventh node in the center, then 11−20+10 is still 1.",vector spaces
40. Construct any 2 by 3 matrix of rank 1. Copy Figure 2.5 and put one vector in each,vector spaces
"every three turns, and the willow is 13:5. The champion seems to be a sunﬂower whose",eigenvec_val
The left nullspace is the orthogonal complement of the column space in Rm.,orthogonality
matrices A1 and A2:,pos_def_matrices
"Example 3. If K is rotation through 90°, then K2 is rotation through 180° (which means",eigenvec_val
Example 2. x =,eigenvec_val
"x = b1 may be obtained from a more accurate scale—or, in a statistical problem, from a",orthogonality
"is subtracted from row 2. We can change the parallelogram to a rectangle, where it is",determinants
"The three inequalities give three slack variables, with new equations like w = x −z and",linear_prog
column space. Those spaces have dimensions (n−r)+r = n. So why doesn’t every,eigenvec_val
"Canceling eiωt, and writing λ for ω2, this is an eigenvalue problem:",pos_def_matrices
A2x = Aλx = λAx = λ 2x.,eigenvec_val
"If football were perfectly consistent, we could assign a “potential” x j to every team.",vector spaces
"diagonal, you see the invertible submatrix holding the r nonzeros.",orthogonality
. Find detATA and detA.,determinants
We show later that ATA is also positive deﬁnite (all pivots and eigenvalues are positive).,orthogonality
moment. At ε = 0 they must still be nonnegative.,pos_def_matrices
"The main point is this: If elimination can be completed with the help of row exchanges,",gauss elim
"tracting multiples of column 2 and then of column 1, we reach a matrix that is certainly",gauss elim
"potentials? Certainly Ax = b is unsolvable if Harvard beats Yale, Yale beats Princeton,",vector spaces
u(t) = SeΛtS−1u0 = c1egl1tx1 +···+cneglntxn.,eigenvec_val
6.5 The Finite Element Method,pos_def_matrices
31. For the semideﬁnite matrices,pos_def_matrices
"42. The cosine is less than 1, because b is not parallel to a.",orthogonality
The duality theorem settles the competition between the grocer and the druggist. The,linear_prog
"loose, and the key rule makes economic sense:",linear_prog
matrix satisﬁes its own characteristic equation. For 3 by 3 this is (A−λ1I)(A−,eigenvec_val
"a matrix B contains only a single column x, the matrix-matrix product AB should be",gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
"3.21 Try to ﬁt a line b =C+Dt through the points b = 0, t = 2, and b = 6, t = 2, and show",orthogonality
(c) The number of columns minus the number of free columns.,vector spaces
17. (Suggested by Moler and Van Loan) Compute b−Ay and b−Az when,computations
line (a constant function y = C) is their average,orthogonality
10. Find two independent vectors on the plane x+2y−3z−t = 0 in R4. Then ﬁnd three,vector spaces
The Fast Fourier Transform,orthogonality
(b) Show that the nullspace you just computed is orthogonal to C(AH) and not to,eigenvec_val
"15. If the columns of A are linearly independent (A is m by n), then the rank is",vector spaces
length: ∥x∥2 = |x1|2 +···+|xn|2,eigenvec_val
u(t) = eAtu(0) = SeΛtS−1u(0).,eigenvec_val
25. Suppose the columns of a 5 by 5 matrix A are a basis for R5.,vector spaces
"vectors go into the columns of A. In that case we need A−1, which takes work. In the",orthogonality
"entry becomes the needed pivot, and elimination can get going again:",gauss elim
discarding small pivots is not the answer. Consider the following:,pos_def_matrices
"15. On the space of 2 by 2 matrices, let T be the transformation that transposes every",eigenvec_val
[A][B]. The rule for multiplying matrices in Chapter 1 was totally determined by this,vector spaces
origin can be shifted by ¯t. Instead of y = C+Dt we work with y = c+d(t − ¯t). The best,orthogonality
"equation in Ux = c is 0 = 0. To the free variables v and y, we may assign any values,",vector spaces
(a) QTAQ is a diagonal matrix.,pos_def_matrices
"concentrations, superdecay is impossible and the limiting rate must be e−t. The solution",eigenvec_val
ﬁnd its Jordan form (two blocks).,eigenvec_val
eigenvalue λ = π2?,pos_def_matrices
tainly unit vectors and certainly orthogonal—because the 1 appears in a different place,orthogonality
Prove this by using the original equation for the λ’s (multiply it by λ k). Then any,eigenvec_val
(a) a rank one matrix,determinants
1. Factor the following matrices into SΛS−1:,eigenvec_val
15. v+w and v−w are combinations of v and w. Write v and w as combinations of v+w,vector spaces
"Suppose we use ω rather than λ, and write these special solutions as u = eiωtx. Sub-",eigenvec_val
(trace)2 −4(det) = (a+d)2 −4(ad −b2) = (a−d)2 +4b2 ≥ 0.,eigenvec_val
"24. For the permutation of Example 6, write out the circulant matrix C = c0I + c1P +",eigenvec_val
from A (making ATCA invertible). The 4 by 4 matrix would have all rows and columns,vector spaces
The only combination of the v’s producing zero has all ci = 0: independence!,orthogonality
"principal stays ﬁnite, even when it is compounded every instant—and the improvement",eigenvec_val
C−1y + Ax =,vector spaces
orthogonal to a1? Factor,orthogonality
2 3 4 1,vector spaces
(ii) The price is y∗,linear_prog
The Four Fundamental Subspaces,vector spaces
amounts to the rows of AT weighted by the ﬁrst row of BT. That is exactly the ﬁrst row,gauss elim
Step 2 computes the coefﬁcients Aij =,pos_def_matrices
"up the 3 by 3 transition matrix A, and ﬁnd the steady state u∞ corresponding to the",eigenvec_val
"(b) In terms of u, v, w, describe the nullspace, left nullspace, row space and column",eigenvec_val
"complex conjugate pair λn−1 = λ n. There are several ways to get around this limitation,",computations
The reader will recognize this as a disguised form of 2C: Every m by n system Ax = 0,vector spaces
"where? When does Ax go clockwise, instead of counterclockwise with x?",eigenvec_val
2x − y = 1,gauss elim
detA3 = detA = 4.,pos_def_matrices
"three Gershgorin circles for J. Show that all the radii satisfy ri < 1, and that the",computations
matrix B invertible? How would you ﬁnd B−1 from A−1?,gauss elim
improved approximation xk+1 from the previous xk. We can stop when we want to.,computations
"to (−6,0), what is the new solution?",gauss elim
"ify that P1 +P2 +P3 = I. The basis a1, a2, a3 is orthogonal!",orthogonality
"If we perturb the matrix A instead of the right-hand side b, then",computations
(b) Choose signs so that Av1 = σ1u1 and verify the SVD:,pos_def_matrices
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"sometimes called the connectivity matrix, or the topology matrix. When the graph has m",vector spaces
The pivot a11 = 1 is nonzero. The usual elementary operations will produce zeros in the,vector spaces
Multiplying a+ib times c+id uses the rule that i2 = −1:,eigenvec_val
We now ask a different question. Suppose we insist that the diet contain some small,linear_prog
"to it in Chapter 5. The goal is to make the matrix diagonal, as achieved for P and H. To",vector spaces
= LDU = L,determinants
diagonal U−1AU is impossible. But “Schur’s lemma” in 5R is very useful—at least to,eigenvec_val
"complex matrices, Σ remains real but U and V become unitary (the complex version of",pos_def_matrices
"Therefore the rows of A − I add up to the zero row, they are linearly dependent, and",eigenvec_val
2 is sent to the Americas.,eigenvec_val
The matrix eAt is never singular. One proof is to look at its eigenvalues; if λ is an,eigenvec_val
"way out of any basis whatsoever. This construction, which converts a skewed set of axes",orthogonality
extra properties are true?,vector spaces
"The converse is also true: If A is invertible, it has n pivots. In an extreme case that",gauss elim
A−1. Expecting A−1 to get smaller as A gets bigger is often wrong.,computations
"vector” like pb. This Gram-Schmidt process produces orthogonal rows, with volume =",determinants
0x = b has no solution unless b = 0. The column space of the 1 by 1 zero,vector spaces
"9. Show that max|λ| is not a true norm, by ﬁnding 2 by 2 counterexamples to λmax(A+",computations
8. Suppose the only solution to Ax = 0 (m equations in n unknowns) is x = 0. What is,vector spaces
detQn blows up. How do you know this can’t happen to Qn?,determinants
0 0 1 2 3,vector spaces
"intersect at the solution. Also, add one more line—the graph of the new second",gauss elim
A and D (and also A−1):,gauss elim
"what would be the stopping test on r, and what rules would choose the column of N",linear_prog
are especially important. They satisfy aTb = 0—they are perpendicular to a and their,orthogonality
by ﬁnding the constraints on b that turn the third equation into 0 = 0 (after elimina-,vector spaces
"of A contains 1s, and the second column contains the times ti. Therefore ATA contains",orthogonality
2u + 2v + 5w =,gauss elim
"47. If A+iB is a Hermitian matrix (A and B are real), show that",eigenvec_val
algorithm 1. To take the lengths in order is algorithm 2. To sweep through all the trees,linear_prog
detA = ai1Ci1 +ai2Ci2 +···+ainCin.,determinants
"that A must have independent columns.) In the reverse order A = S′Q, the matrix S′ is",pos_def_matrices
"to be nonzero, without changing the order of the equations. That is the best case, with",gauss elim
"we might have m equations and n unknowns. Then A is rectangular, with m rows and n",gauss elim
"part. The output is Av = (2,2)+(−2,2) = (0,4)",vector spaces
(c) If R is the reduced form rref(A) then RT is rref(AT).,vector spaces
u − v − w =,gauss elim
Because of the orthogonality xT,pos_def_matrices
Chapter 8 Linear Programming and Game Theory,linear_prog
ber of square roots (the painful part of Gram-Schmidt). The result is a set of orthonormal,orthogonality
algebra. The ﬁrst half gave the dimensions of the four subspaces. including the fact that,orthogonality
13. Find the reduced row echelon forms R and the rank of these matrices:,vector spaces
A. Find a solution to ATy = 0 and describe all other vectors in the left nullspace of,vector spaces
"To be “feasible,” the vector x must satisfy x ≥ 0 and Ax ≥ b. The optimal vector x∗ is the",linear_prog
between the primal and dual problems. The simplex method applies equally well to a,linear_prog
"λ1 = 1, then the ﬁrst term in the formula will be dominant. The other λ k",eigenvec_val
"like (ATA)−1, and this coupling is given by the ill-conditioned Hilbert matrix. On the",orthogonality
nation steps; L and U are the right matrices to solve Ax = b. In fact A could be thrown,gauss elim
1.6 Inverses and Transposes,gauss elim
This section is summarized by a table of parallels between real and complex.,eigenvec_val
3.18 If the orthonormal vectors q1 = (2,orthogonality
1 3 1 2,vector spaces
1.8 How are the rows of EA related to the rows of A in the following cases?,gauss elim
a “right-handed” system into a “left-handed” system. So we were wrong if we suggested,orthogonality
"The simplex method notices no difference between x and w, so we simplify:",linear_prog
�� has Ib = b,gauss elim
The diagonal entries must be real; they are unchanged by conjugation. Each off-diagonal,eigenvec_val
also produce sparse matrices A. You could think of ﬁnite elements as a systematic way,pos_def_matrices
29. Second assumption behind least squares: The m errors ei are independent with,orthogonality
Here is a curious pair of alternatives. It is impossible for a subspace S and its or-,linear_prog
for every b if and only if the columns span Rm. Then A has a right-inverse C,vector spaces
"For a determinant of order n, this splitting gives n smaller determinants (minors) of order",determinants
A simple least-squares problem is the estimate �x of a patient’s weight from two obser-,orthogonality
"3.22 What point on the plane x+y−z = 0 is closest to b = (2,1,0)?",orthogonality
Then U has independent columns.,vector spaces
"More than just vectors, subspaces can also be perpendicular. We will discover, so",orthogonality
1x1 + c2λ k,eigenvec_val
for partial differential equations that overrelaxation (and other ideas) will be important.,computations
vector v in V is orthogonal to every vector w in W: vTw = 0 for all v and w.,orthogonality
roots of the pivots give D =,pos_def_matrices
val dx is stretched to (dx/du)du—when u replaces x in a single integral—so the volume,determinants
notice that every Gauss-Jordan step is a multiplication on the left by an elementary,gauss elim
"below it is also zero. If A were square, this would signal that the matrix was singular.",vector spaces
"roots of the square roots, 1 and −1, i and −i. The number i will satisfy i4 = 1 because",orthogonality
minimizing ∥Ax−b∥ is the same as minimizing ∥Σy−UTb∥. Now Σ is diagonal and we,pos_def_matrices
"Saddle points at (0,0)",pos_def_matrices
"pretty good. Working in single precision, a typical computer might make roundoff errors",computations
"necessary to orthogonalize A by Gram-Schmidt, instead of computing with ATA.",computations
"of A are exchanged, so are the same two rows of AB, and the sign of d changes",determinants
is ﬁxed: A0 = 0. They must be represented by matrices. Using the natural basis,vector spaces
"row and column, from the grounded node, outside the 3 by 3 matrix:",vector spaces
invertible case Σ is deﬁnite and so is S.,pos_def_matrices
41. Give two reasons why the matrix exponential eAt is never singular:,eigenvec_val
Properties of the Determinant,determinants
"For the concept of a vector space, we start immediately with the most important",vector spaces
"a Markov matrix, except now λn may not equal 1. The scaling factor λ k",computations
times the triangular system,gauss elim
"by CA−1 and subtract from the second row, to ﬁnd the “Schur complement” S:",gauss elim
The ratio c = λmax/λmin is the condition number of a positive deﬁnite matrix,computations
"determinant all remain the same. For eigenvalues, the basic operation was a similarity",pos_def_matrices
"sequences, the number of exchanges is odd. The assertion is that an even number of",determinants
"9. (a) If y = (1,1,1,1), show that c = (1,0,0,0) satisﬁes F4c = y.",orthogonality
"gested, and everything depends on the size and the properties of A (and on the number",computations
The next three problems are about applications of (Ax)Ty = xT(ATy).,gauss elim
It is surprising that ∂r/∂x = ∂x/∂r. The product JJ−1 = I gives the chain rule,determinants
eAt times eAT equals,eigenvec_val
the sum of the two column vectors we started with. But the nullspace of B contains the,vector spaces
"For f = (x+y)2, the valley runs along the line x+y = 0.",pos_def_matrices
"For a proper deﬁnition of the condition number, we look back at equation (3). We",computations
8. (c1 +c2)x = c1x+c2x.,vector spaces
2. Spanning a subspace.,vector spaces
Linear Programming and Game Theory,linear_prog
"the (3,2) entry of P32P21Ak. After n−1 rotations, we have R0:",computations
"long as M is positive deﬁnite, the generalized eigenvalue problem Ax = −λMx behaves",pos_def_matrices
"pseudoinverse) has b13 = b23 = 0. This is a “uniqueness case,” when the rank is r = n.",vector spaces
A = LU: The n by n case,gauss elim
"entry lower down in the same column, then a row exchange is carried out. The nonzero",gauss elim
"bound (nonnegativity constraint) moves upwards. We know that r ≥ 0, and economics",linear_prog
"2, and x = 3",gauss elim
p = �xa = aTb,orthogonality
"satisﬁes the usual rule eΛ(t+T) = eΛteΛT, because",eigenvec_val
"Since no feasible y can make yb larger than cx, our y that achieves this value is opti-",linear_prog
6. Write the 6 by 4 incidence matrix A for the second graph in the ﬁgure. The vector,vector spaces
A projection matrix takes the whole space onto a lower-,vector spaces
"The nonzero rows of any echelon matrix U must be independent. Furthermore, if we",vector spaces
Projection Matrix of Rank 1,orthogonality
y = 1 and x = 1,gauss elim
"Example 4. When the measurement times average to zero, ﬁtting a straight line leads to",orthogonality
diagonal of Λ (real),eigenvec_val
(Ax)Ty = xTATy = xT(ATy).,orthogonality
goes to U =,gauss elim
the data. These matrix equations and the corresponding differential equations are in our,vector spaces
"1 1], and write B as the sum of a symmetric matrix and a skew-symmetric",gauss elim
"We don’t know each ai j, but we know the shape of A (it is m by n). The second vector",vector spaces
eAt = SeΛtS−1 =,eigenvec_val
general solution is a combination of these two normal modes. Our particular solution is,eigenvec_val
"2 + 0 < 4. Similarly, the diet",linear_prog
"That is correct but not beautiful. By substituting cost ±isint for eit and e−it, real num-",eigenvec_val
"vectors that are only half as long. The ﬁrst step is to divide c itself, by separating its",orthogonality
", choose b and c so that both of the feasible",linear_prog
"number of equations. We have n equations in n unknowns, starting with n = 2:",gauss elim
are r nonzero λ’s and r perfect squares in yTΛy = λ1y2,pos_def_matrices
"linear system. The matrix in our example had three rows and four columns, but the third",vector spaces
two equations will be zero. Then,pos_def_matrices
An orthogonal matrix is a square matrix with orthonormal columns.2 Then,orthogonality
"For the original example Ax = b = (b1,b2,b3), apply to both sides the operations that",vector spaces
and the other to ﬁnd the new entries along the row. There are n−1 rows underneath the,gauss elim
"3.20 True or false: If the vectors x and y are orthogonal, and P is a projection, then Px",orthogonality
"Since we started with A and ended with U, the reader is certain to ask: Do we have",vector spaces
"a systematic procedure for ﬁnding all the eigenvalues. In fact, the QR method is now",computations
2.1 Vector Spaces and Subspaces,vector spaces
(c) The 3 by 4 matrix with aij = (−1)j.,vector spaces
"equations, recognizing that you could solve them without a course in linear algebra.",gauss elim
"(b) Construct a matrix whose left nullspace contains y = (1,5).",vector spaces
y1 = .9y0 +.2z0,eigenvec_val
"With this formula, we can solve wn = 1. It becomes einθ = 1, so that nθ must carry",orthogonality
What happens if all right-hand sides are zero? Is there any nonzero choice of right-,gauss elim
28. A and B are symmetric across the diagonal (because 4 = 4). Find their triple factor-,gauss elim
the three solutions. L and U can be updated instead of recomputed.,linear_prog
(ATA)−1�TAT = A(ATA)−1AT = P.,orthogonality
"The determinant is just the product of the diagonal entries. It is zero if λ = 1, λ = 3",eigenvec_val
"This is the ﬁnite difference matrix with the 1, −2, 1 pattern. The right side Au approaches",eigenvec_val
17. Use 8I to show that there is no solution x ≥ 0 (the alternative holds):,linear_prog
2 they are orthonormal.,eigenvec_val
eliminate one of them in each of three pairs to derive three equations in three,determinants
nonzero.) Thus xTRTRx > 0 and RTR is positive deﬁnite.,pos_def_matrices
the unconstrained λn−1 and λn. The toughest constraint makes x perpendicular to the top,pos_def_matrices
4u + 4v + 8w =,gauss elim
The case b ̸= 0 is quite different from b = 0. The row operations on A must act also,vector spaces
rn = rn−1 −αnApn−1,computations
"and y is yTCx. For an orthogonal matrix W = Q, when this combination is C = QTQ = I,",orthogonality
Twice the ﬁrst row of A has been subtracted from the second row. Matrix multiplication,gauss elim
orthogonal to every vector in the other subspace. Subspaces of R3 can have dimension,orthogonality
"The transformation T(x) = Ax has no freedom left, after it has decided what to do with",vector spaces
Example 1. Nonsingular (cured by exchanging equations 2 and 3),gauss elim
0 1 0 0,orthogonality
"A that transforms each vj into u j to give Av1 = u1,...,Avn = un.",pos_def_matrices
Chapter 2 Vector Spaces,vector spaces
that these matrices are similar—they all belong to the same family.,eigenvec_val
"be positive, not zero. But S might be the x-axis and S⊥ the y-axis, in which case they",linear_prog
7. Which of the following are subspaces of R∞?,vector spaces
= −2x + 2y,eigenvec_val
1 1 0 2,determinants
group containing only positive deﬁnite matrices.,pos_def_matrices
"Markov: mij > 0, ∑n",eigenvec_val
"of A. The key question is the invertibility of ATA, and fortunately",orthogonality
1. Simplify its determinant to J = ρ2sinφ. Then dV = ρ2sinφ dρ dφ dθ.,determinants
"RRT is also symmetric, but it is different from RTR. In my experience, most scientiﬁc",gauss elim
"The edgenode incidence matrix is 5 by 4, with a row for every edge. If the edge goes",vector spaces
"Remark 2. We may have given the impression in describing each elimination step, that",gauss elim
Proof. Multiplication by the orthogonal matrix UT leaves lengths unchanged:,pos_def_matrices
will go from U to I (multiplying by U−1). That takes L−1 to U−1L−1 which is A−1.,gauss elim
We can summarize the main point. The way to simplify that matrix A—in fact to diag-,eigenvec_val
subspace with p(1) = 0.,vector spaces
There stands the Fourier matrix F with entries Fjk = w jk. It is natural to number the,orthogonality
column space of A. Instead of Ax = b. we solve ATA�x = ATb. But if A has dependent,pos_def_matrices
Markov. Find the eigenvalues of A and B.,eigenvec_val
"of An is n+1, from the previous determinants n and n−1:",determinants
step length xn−1 to xn,computations
original system. The ﬁrst component of Ax comes from “multiplying” the ﬁrst row of A,gauss elim
eigenvalues are squared. This continues to hold for any power of A:,eigenvec_val
Chapter 8 Linear Programming and Game Theory,linear_prog
The second approach to Ax = b is “dual” to the ﬁrst. We are concerned not only with,vector spaces
"with eigenvalues λ1 = 1 and λ2 = 3, apply the power",computations
(B has the same zeros as A.),determinants
2u + v + w = 5 (sloping plane),gauss elim
pendicular! The coordinate axes that the imagination constructs are practically always,orthogonality
"(c) T(v) = 90° rotation = (−v2,v1).",vector spaces
"For elimination and eigenvalues, matrices become simpler by elementary operations",pos_def_matrices
postmultiplication. The proof of this law is too boring for words.,gauss elim
vectors in R3 onto V⊥.,orthogonality
"that case the error matrix S−1T is zero, its eigenvalues and spectral radius are zero, and",computations
"exponent ﬁnally fell (at IBM) below 2.376. Fortunately for elimination, the constant C",gauss elim
of A−1 and A−1b. This formula will not change the way we compute; even the deter-,determinants
SOR (with the best ω):,computations
"to be confused with a positive deﬁnite matrix, which is symmetric and has all its eigen-",eigenvec_val
The next step is now easy. The only negative coefﬁcient −1 in the cost makes x4 the,linear_prog
"c1x1 + c2x2 = 0. Multiplying by A, we ﬁnd c1λ1x1 + c2λ2x2 = 0. Subtracting λ2 times",eigenvec_val
"number of steps, but it does not end up with the eigenvalues themselves. This produces",computations
"that does always happen. The spanning tree problem is exceptional, because it can be",linear_prog
"what is the triangular system after forward elimination, and what is the solution?",gauss elim
"For B, the pivot .0001 would be compared with the possible pivot I below it. A row",gauss elim
7 = 0. It is orthogonal,orthogonality
"Minimize cx, subject to x ≥ 0 and Ax = b.",linear_prog
a horizontal line instead of a parabola. The “pseudoinverse” assigns the deﬁnite value,orthogonality
0 0 a b,gauss elim
"For every n, the matrix connecting y to c can be inverted. It represents n equations,",orthogonality
"not be in the plane of q1 and q2, which is the plane of a and b. However, it may have a",orthogonality
F−1 must be simple. The multiplications by F and F−1 must be fast.,orthogonality
"again. Its i, j entry (and j, i entry) is the inner product of column i of A with column j",orthogonality
is actually too big when there is no control on the size of components v j. A much better,orthogonality
The ﬁnal result of this chapter will be an elimination algorithm that is about as efﬁ-,gauss elim
18. Find A−1 from the cofactor formula CT/detA. Use symmetry in part (b):,determinants
equation (5) should be shifted immediately by αk (which changes Qk and Rk):,computations
Solving Ax = 0 and Ax = b,vector spaces
"never singular, none of these eigenvalues can touch zero (not to mention cross over it!).",pos_def_matrices
"12. In three dimensions, λ1y2",pos_def_matrices
"this projection, to solve the weighted normal equations:",linear_prog
"23. (For professors only) If you know all 16 cofactors of a 4 by 4 invertible matrix A,",determinants
3 6 7 13,vector spaces
"by inspection, and a systematic procedure is necessary.",vector spaces
size of eλt depends only on the real part of λ. It is only the real parts of the eigenvalues,eigenvec_val
"In Chapter 2, b was anywhere in the column space. Now we allow only nonnegative",linear_prog
rows to obtain the largest possible pivot is called partial pivoting.,gauss elim
1. the length ∥x∥ of a vector;,orthogonality
"3n(n−1)(n+1). This is a whole number, since n−1, n, and n+1 are consecutive",gauss elim
neering and every problem of optimization. The mathematical problem is to move the,pos_def_matrices
"(a) If A has independent columns, its left-inverse (ATA)−1AT is A+.",pos_def_matrices
Chapter 7 Computations with Matrices,computations
"Example 1. The problem in Figure 8.3 has constraints x+2y ≥ 6, 2x+y ≥ 6, and cost",linear_prog
or it may be serious. This is decided by looking below the zero. If there is a nonzero,gauss elim
"(a) w1 = (1,1,0), w2 = (2,2,1), w3 = (0,0,2), b = (3,4,5)?",vector spaces
"reﬂection equals the length of the original, as it did after rotation—but here the θ-",vector spaces
"Start with Ux = λ1x and Uy = λ2y, and take inner products by Property 1′:",eigenvec_val
"of axes for an ellipse. Those axes are perpendicular, and they point along the eigen-",eigenvec_val
17. Which of the following matrices are guaranteed to equal (A+B)2?,gauss elim
AB or an inverse A−1? Those are the essential formulas of this section:,gauss elim
called a transition matrix.,eigenvec_val
"the left nullspace by sending it to zero, and it knocks out the nullspace by choosing xr as",pos_def_matrices
19. What are the special solutions to Rx = 0 and RTy = 0 for these R?,vector spaces
This says that Avj is an eigenvector of AAT! We just moved parentheses to (AAT)(Avj).,pos_def_matrices
two adjacent diagonals. Outside this band all entries are aij = 0. These zeros will,gauss elim
space of AB is contained in (possibly equal to) the column space of A. Give an,vector spaces
cients; the matrix A is independent of time.,eigenvec_val
component of the new xk+1 as soon as it is computed; xk+1 takes the place of xk a com-,computations
must get there too (stability). The real value of Lyapunov’s method is for a nonlinear,eigenvec_val
"Equation (2) asked for multipliers u, v, w that produce the right side b. Those numbers",gauss elim
Zero in the pivot position,gauss elim
cx + dy = 0.,determinants
0 0 0 9,vector spaces
"is different from k, W is different from 1. It is one of the other roots on the unit circle.",orthogonality
(b) How many entries can be chosen independently in a skew-symmetric matrix,gauss elim
coefﬁcient” in A−1 is a ratio of determinants.,determinants
37. The average of the four times is �t = 1,orthogonality
4x + 6y =,gauss elim
"Express each column that is not in the basis as a combination of the basic columns,",vector spaces
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
The diagonal entries of any symmetric matrix are between λ1 and λn. We drew Figure,pos_def_matrices
(d) All convergent sequences: the x j have a limit as j → ∞.,vector spaces
Problem Maximize the proﬁt 200x+300y+500z subject to,linear_prog
(b) The two vectors v1 and v2 will be dependent if,vector spaces
tion 2 will be subtracted from equation 3.,gauss elim
(d) T(v) = projection =,vector spaces
1 0 0 0,eigenvec_val
You can easily check that this multiplication leaves Q0 with the same three zeros as A0.,computations
as required by rule 2. A linear combination in the ﬁrst row of A gives the same,determinants
3. The space of functions f(x). Here we admit all functions f that are deﬁned on,vector spaces
21. Compute the exact inverse of the Hilbert matrix A by elimination. Then compute,computations
"consists of all x in Rn; its range is all possible vectors Ax, which is the column space.",vector spaces
"4 and columns 1, 2, 3 of A is entirely zero. The general rule for an n by n matrix is that",linear_prog
"7. By choosing the correct vector b in the Schwarz inequality, prove that",orthogonality
Example 4. Suppose E subtracts twice the ﬁrst equation from the second. Suppose F,gauss elim
Chapter 2 Vector Spaces,vector spaces
what is the limit? Does A have to be a Markov matrix?,eigenvec_val
"5. Factor A into LU, and write down the upper triangular system Ux = c which appears",gauss elim
the eigenvalues (as in rotations) may be complex.,eigenvec_val
"The real part 1−θ 2/2+··· is cosθ. The imaginary part θ −θ 3/6+··· is the sine, The",orthogonality
vectors. Two matrices are similar if and only if they share the same Jordan,eigenvec_val
HTH = (I −2uuT)(I −2uuT) = I −4uuT +4uuTuuT = I.,computations
to the nullspace (in Rn). The column space is orthogonal to the left nullspace,orthogonality
"Newton’s method takes a step ∆x, ∆y, ∆s from the current x, y, s. (Those solve equa-",linear_prog
"The product of ∥Qx∥/∥x∥ and ∥Px∥/∥x∥—momentum and position errors, when the",eigenvec_val
"There is no freedom in the ﬁnal L, D, and U. That is our main point:",gauss elim
xTAx = xTλx = λ∥x∥2.,pos_def_matrices
9. If A = RTR prove the generalized Schwarz inequality |xTAy|2 ≤ (xTAx)(yTAy).,pos_def_matrices
"and angles that are not right angles. We want to connect inner products to angles, and",orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"corner. At succeeding corners, the pivoting step is simple. When column k of the identity",linear_prog
"2. Project b = (0,3,0) onto each of the orthonormal vectors a1 = (2",orthogonality
and we shall describe three of them:,computations
e = b − p,orthogonality
"1. Minimize x1 +x2 −x3, subject to",linear_prog
−u + 2v −,gauss elim
0 0 0 1 2,vector spaces
three of those properties are fundamental:,gauss elim
M on the right and its inverse on the left. The invertible eigenvector matrix S may fail to,eigenvec_val
The iterative method in equation (1) is convergent if and only if every,computations
"What are a and b in the inverse of 6 ∗ eye(5) − ones(5,5)?",gauss elim
"3.5 Find the cosine of the angle between the vectors (3,4) and (4,3),",orthogonality
16. The solution to du/dt = Au =,eigenvec_val
"You might notice that the columns of A − λ1I give x2, and the columns of A − λ2I are",eigenvec_val
then uk = Aku0 = MJkM−1u0,eigenvec_val
"In that new tableau at Q, r = [1",linear_prog
"gular matrix U. Those steps factor A into L times U, where L is lower triangular.",gauss elim
a = 0 =⇒ no second pivot,gauss elim
(a) Find the matrix that gives,eigenvec_val
"Since trace = 0 and det = 1, we have purely imaginary eigenvalues:",eigenvec_val
"for the laws of matrix multiplication. And if C has several columns, we have only to",gauss elim
connecting all six nodes has,vector spaces
"computed, and we mention two good ones: sum and product.",eigenvec_val
0 0 0 9,gauss elim
What shape is that curve?,vector spaces
"If xk is the kth column of S and yk is the kth row of S−1, then λk changes by",computations
7. Find the determinants of:,determinants
1. Find the determinant and all nine cofactors Cij of this triangular matrix:,determinants
"2, 2, and 2+",computations
"these axes are rotated, the result is a new orthonormal basis: a new system of mutually",orthogonality
Jordan: J = M−1AM,eigenvec_val
"The even-numbered components (c0,c2) are transformed separately from (c1,c3), Figure",orthogonality
suppose one mass is held at equilibrium. Then the lowest frequency is increased but not,pos_def_matrices
get from U back to A? How can we undo the steps of Gaussian elimination?,gauss elim
"matrices. Since the dimension exceeds n, how is this difference explained?",eigenvec_val
11. (a) Do A and A−1 have the same condition number c?,computations
0 1 1 1,determinants
"nodes 1 and 4 (and edges from a node to itself are forbidden). This graph is directed,",vector spaces
0 0 b 0,determinants
new point of view. Elimination becomes more than just a way to ﬁnd a basis for the,computations
"18. To decide whether b is in the sub space spanned by w1,...,wn, let the vectors w be",vector spaces
"omy. The table for 1958 contained 83 industries in the United States, with a “trans-",eigenvec_val
A third possibility is R = Q,pos_def_matrices
familiar sine and cosine:,eigenvec_val
"j equals 1/h in the small interval to the left of x j, and −1/h in the interval to the right.",pos_def_matrices
"22. For the same A, compute b = Ax for x = (1,1,1) and x = (0,6,−3.6). A small change",computations
"with the oldest and most fundamental problems of the subject, Ax = b and Ax = λx, but",computations
"we can be sure λ and x stay real. More than that, the eigenvectors are perpendicular:",eigenvec_val
∗ ∗ ∗ ∗,computations
exists—and then to understand which matrices don’t have inverses.,gauss elim
"earlier, to create a zero above it as well as below it. This is not smart. At that time",gauss elim
"39. Suppose S only contains (1,5,1) and (2,2,2) (not a subspace). Then S⊥ is the",orthogonality
"If those equations Ax = b could be solved, there would be no errors. They can’t be solved",orthogonality
"The diagonal (but rectangular) matrix Σ has eigenvalues from ATA, not from A! Those",pos_def_matrices
cient as possible. It is essentially the algorithm that is in constant use in a tremendous,gauss elim
"of i (and also to −i!). That was declared to be a solution, and the case was closed. If",orthogonality
change the matrix A to a similar matrix B.,eigenvec_val
This is surprisingly important: call it a challenge question. You could use numbers,gauss elim
"From those conditions, both eigenvalues are positive. Their product λ1λ2 is determinant",pos_def_matrices
". Find the optimal x and y, and verify",linear_prog
"The complete solution to Ax = (0,6,−6) is (this xp) + (all xn).",vector spaces
0 0 2 0,gauss elim
"conjugate, since b = 0. The conjugate is denoted by a bar or a star: (a+ib)∗ = a+ib =",eigenvec_val
is easy to see for the very unsymmetric matrices,computations
", there is zero in the pivot position.",gauss elim
0 0 3 3,vector spaces
� 2V3dx. Solve Ay = f for the ﬁnite element,pos_def_matrices
transpose: AT = A. The matrix is necessarily square. Each entry on one side of the,gauss elim
places. Back-substitution with the right v = .9999 would leave u = 1:,gauss elim
"Up to this point, Ax = b either has a solution or not. If b is not in the column space C(A),",orthogonality
pure exponential solutions to du/dt = Au. Notice e−t and e2t:,eigenvec_val
"Again there is a double inﬁnity of solutions: v and y are free, u and w are not:",vector spaces
Suppose General Motors makes a proﬁt of $200 on each,linear_prog
"count, the lower right-hand corner has no room for bandwidth w. The precise number of",gauss elim
1.2 The Geometry of Linear Equations,gauss elim
Problems 1–9 are about elimination on 2 by 2 systems.,gauss elim
with |b| < 1 can have detA < 0.,pos_def_matrices
ax2 +2bxy+cy2 = a,pos_def_matrices
0 1 0 1,eigenvec_val
"27. (Recommended) This problem projects b = (b1,...,bm) onto the line through a =",orthogonality
"A, matrices P with positive eigenvalues, matrices D with determinant 1. Invent a",pos_def_matrices
"solution is x = (2,−1). We expect this shortest vector x to be perpendicular to the line,",pos_def_matrices
To summarize: A good elimination code saves L and U and P. Those matrices carry,gauss elim
"oscillations, corresponding to the two eigenvectors. In the ﬁrst mode x1 = (1,1), the",eigenvec_val
λ1 along the eigenvector with the smallest eigenvalue.,pos_def_matrices
0 0 2 0,vector spaces
each subdiagonal element in turn by a “plane rotation” Pij. The ﬁrst is P21:,computations
subset is the ﬁrst quadrant of the x-y plane; the coordinates satisfy x ≥ 0 and y ≥ 0. It,vector spaces
"nullspace contains (1,0,1) and (0,0,1).",vector spaces
"But this square can be zero, and the second term must then be positive. That term",pos_def_matrices
H1 = I −2 vvT,computations
"on its importance, and its proper place in the theory of linear algebra, but also to choose",determinants
Example 2. The eigenvalues of a projection matrix are 1 or 0!,eigenvec_val
40. Find a basis for the space of functions that satisfy,vector spaces
"of synthetic protein is p = $1.50. The maximum revenue is 4p = $6, and the shopper",linear_prog
variable. The zero vector,vector spaces
B = SΛ2S−1. Prove that AB = BA.,eigenvec_val
"(1) we want y∗Ax∗ = y∗b at the optimum. Feasibility requires Ax∗ ≥ b, and we look for",linear_prog
6. Reduce [U c] to [R d]: Special solutions from R and xp from d.,vector spaces
(b) Column space has basis,vector spaces
"Ax ≥ b are translated into w1 ≥ 0,...,wm ≥ 0, with one slack variable for every row of",linear_prog
are looking along that line for the point p closest to b. The key is in the geometry: The,orthogonality
"description, A = LU or A = QR, of what they do.",computations
"36, that is the",orthogonality
1One which I never really believed—but a bridge did crash this way in 1831.,eigenvec_val
has B2 = 0. Find eBt from a (short) inﬁnite series. Check that,eigenvec_val
Virtually every step in this chapter has involved the combination S−1AS. The eigenvec-,eigenvec_val
"steps of the shifted algorithm, the matrix Ak looks like this:",computations
4. Apply elimination to produce the factors L and U for,gauss elim
tions as Ax = b. We multiply A by “elimination matrices” to reach an upper trian-,gauss elim
59. The MATLAB commands A = eye(3) and v = [3:5]’ produce the 3 by 3 identity,gauss elim
the SVD. We come back to this in the second application.,pos_def_matrices
"We emphasize that those projections do not reconstruct b. In the square case m = n,",orthogonality
(a) A+B is not invertible although A and B are invertible.,gauss elim
dimensional space. Column 2 is three times column 1. The fourth column equals the,vector spaces
tries harder to make (x−b1)2 small:,orthogonality
of C make it periodic (a circulant matrix):,orthogonality
(a) A is triangular. (b) A is symmetric. (c) A is tridiagonal. (d) All entries are whole,gauss elim
derivatives in xTAx take the graph up or down (or saddle). If the stationary point is at x0,pos_def_matrices
"31. (MATLAB) The Hilbert matrix hilb(n) has i, j entry equal to 1/(i + j − 1). Print",determinants
3) ≈ 1.07. The two equal eigenvalues are,computations
det(A−λI) = λ 2 −λ −1,eigenvec_val
11. The eigenvalues of A equal the eigenvalues of AT. This is because det(A − λI),eigenvec_val
15. How many multiplications and how many additions are used to compute PA? (A,computations
"(c) If Ax = Ay, then x = y.",vector spaces
"x on a line 2x1 −x2 = 5. We are looking for the closest point to (0,0) on this line. The",pos_def_matrices
2x + 3y + 2z = 5.,gauss elim
when the particular problem has special properties—as almost every problem has. Sec-,computations
"different ways, and relate the projection p to inner products and angles. Projection onto a",orthogonality
This chapter begins the “second half” of linear algebra. The ﬁrst half was about Ax =,eigenvec_val
(V′) A = RTR with dependent columns in R:,pos_def_matrices
the square root of c(A). Elimination without row exchanges cannot hurt a positive,computations
(b) Elimination keeps column 1 + column 2 = column 3. Explain why there is no,gauss elim
"29. (Recommended) Find orthogonal vectors A, B, C by Gram-Schmidt from a, b, c:",orthogonality
Example 4. U =,eigenvec_val
ATA = RTQTQR = RTR.,orthogonality
"30. Which classes of matrices does P belong to: orthogonal, invertible, Hermitian, uni-",eigenvec_val
"understood proof, because cosines are so familiar. Either proof is all right in Rn, but",orthogonality
5.3 Difference Equations and Powers Ak,eigenvec_val
"2, this is the ﬁrst",eigenvec_val
"A = LU is so crucial, and so beautiful, that Problem 8 at the end of this section",gauss elim
19. A nonlinear transformation is invertible if T(x) = b has exactly one solution for,vector spaces
"paraboloid). To assure a minimum of P(x), not a maximum or a saddle point, A must be",pos_def_matrices
16. This problem shows in two ways that detA = 0 (the x’s are any numbers):,determinants
and H1x is zero below the pivot. The next step is to multiply on the right by an H(1),computations
(which only allows x when Cx = d) exceeds the unconstrained Pmin (allowing all x):,pos_def_matrices
One formula for A+ depends on the singular value decomposition—for which we ﬁrst,orthogonality
4u + 6v + 8w =,gauss elim
4. (a) Find the determinant when a vector x replaces column j of the identity (consider,determinants
not the original vector b—which is all we can expect when there are more equations,orthogonality
and v3 = w1 +w2 are independent. (Write c1v1 +c2v2 +c3v3 = 0 in terms of the w’s.,vector spaces
3x + 6y = 18.,gauss elim
Chapter 2 Vector Spaces,vector spaces
reach the next corner when a component of xB drops to zero.,linear_prog
z1 = .1y0 +.8z0,eigenvec_val
"EA is constructed exactly so that these equations agree, and we don’t need parentheses:",gauss elim
[1 −1 0] equals [2 −1 0] minus [1 0 0]:,determinants
of the old basis vectors: Vj = ∑mijvi. That matrix M is really representing the identity,eigenvec_val
system is quickly solved. That is exactly what a good elimination code will do:,gauss elim
in which detA is always zero or never zero. Then show from the cofactor expansion,determinants
nullspace Axn = 0,vector spaces
eigenvalue λ = 1.,eigenvec_val
"If we can choose y∗ = cBB−1 in the dual, we certainly have y∗b = cx∗. The minimum",linear_prog
proach is “b must be orthogonal to every vector that is orthogonal to the columns.” That,orthogonality
pivots. This last test is the one we meet through Gaussian elimination. We want to show,gauss elim
skew-symmetric KT = −K,eigenvec_val
"Then ﬁnd the projection of b = (4,3,1,0) onto the column space of",orthogonality
every symmetric matrix has real eigenvalues. Now we will ﬁnd a test that can be applied,pos_def_matrices
Do a 3 by 3 example. How many pivots are produced by elimination?,gauss elim
"57. If A = AT needs a row exchange, then it also needs a column exchange to stay sym-",gauss elim
because on the left-hand side of that equality Ax is sure to be positive.,eigenvec_val
vector x produces a “pure solution” with powers of λ:,eigenvec_val
2 4 0 7,vector spaces
"For the converse, we have to deduce from P2 = P and PT = P that Pb is the projection",orthogonality
"agrees with 1 + 0 as it should. So does the determinant, which is 0 · 1 = 0. A singular",eigenvec_val
11. (a) Describe a subspace of M that contains A =,vector spaces
be greater than the sum of the ﬁrst two:,orthogonality
1. The Gram-Schmidt factorization A = QR. Remember that R is to be upper trian-,computations
with! What are the pivots?,determinants
"case, and very important. And ﬁnally we have a job for Schmidt: To orthogonalize the",orthogonality
"rule (6), those numbers go into the columns of the matrix (we use c and s for cosθ",vector spaces
These two special solutions give the complete solution. They can be multiplied by any,eigenvec_val
0 0 0 1,determinants
Symmetric matrices have perpendicular eigenvectors (see Section 5.5).,orthogonality
"How could we ﬁnd the 1000th Fibonacci number, without starting at F0 = 0 and",eigenvec_val
factorization of AT? Note that A and AT (square matrices with no row exchanges),gauss elim
is adjusted to ﬁt the initial conditions.,eigenvec_val
plus the original equals twice the projection. It also conﬁrms that H2 = I:,vector spaces
"non singular, and it is solved by forward elimination and back-substitution. But if a zero",gauss elim
the theory. (The rest of this chapter is devoted more to theory than to applications. The,eigenvec_val
2x1 +3x2 = 1,determinants
"such a W, then ∥Wu∥ will decrease steadily to zero, and after a few ups and downs u",eigenvec_val
"An iterative method is easy to invent, by splitting the matrix A. If A = S − T, then",computations
The “sign” of P tells whether the number of row exchanges is even (sign = +1) or odd,gauss elim
"D, the square roots of the pivots",pos_def_matrices
Not only a row but also a column exchange may be needed.,gauss elim
"i , then the",orthogonality
= c1eλ1tx1 +···+cneλntxn = combination of eλtx.,eigenvec_val
4. For each x there is a unique vector −x such that x+(−x) = 0.,vector spaces
∥x∥2 = xTx = xT(QP−PQ)x ≤ 2∥Qx∥∥Px∥.,eigenvec_val
"7 at the measurement times −1, 1, 2. Those points",orthogonality
"Press, 1942) or Peter Stevens’s beautiful Patterns in Nature (Little, Brown, 1974). Hundreds of other properties",eigenvec_val
into a new vector Ax. This happens at every point x of the n-dimensional space Rn.,vector spaces
"If A is real, all complex eigenvalues come in conjugate pairs: Ax = λx and Ax = λx.",eigenvec_val
as SΛS−1. Multiply SeΛtS−1 to ﬁnd the matrix exponential eAt.,eigenvec_val
"46. Find ATA if the columns of A are unit vectors, all mutually perpendicular.",orthogonality
Cosines and Projections onto Lines,orthogonality
point is that we are dealing with a square matrix; the number n of coefﬁcients in P(t) =,vector spaces
The convergence factors |λ1 − αk|/|λ2 − αk| are themselves converging to zero. Then,computations
The Fourier matrix has remarkable properties.,orthogonality
"basis—each free variable is given the value 1, while the other free variables",vector spaces
50. Verify that (AB)T equals BTAT but those are different from ATBT:,gauss elim
Every vector orthogonal to the nullspace is in the row space: C(AT) = (N(A))⊥.,orthogonality
"describes a plane in three dimensions. The ﬁrst plane is 2u+v+w = 5, and it is sketched",gauss elim
is the 3 in Σ). The two zero eigenvalues of AAT leave some freedom for the eigenvectors,pos_def_matrices
"worst possible, since each entry is at most doubled when |ℓij| ≤ 1.",gauss elim
completely described. It only remains to catch up on the eigenvectors—that is a single,computations
The optimal vector occurs at a corner of the feasible set. This is guaranteed by the,linear_prog
x2 = 39.) The ratio 9,linear_prog
The same idea of replacing summation by integration produces the inner product of,orthogonality
"Q is a rotation, and possibly a reﬂection. The material feels no strain. The symmetric",pos_def_matrices
number of exchanges and lead to detP = −1.,determinants
10. If the primal problem is constrained by equations instead of inequalities—Minimize,linear_prog
"You recognize p as the (nonnegative) eigenvector of the Markov matrix A, with λ = 1.",eigenvec_val
"In these examples (and in almost all examples), linearity is not difﬁcult to verify. It",vector spaces
zero if two columns are equal. When b = Ax = x1a1+x2a2+x3a3 goes into column,determinants
must occur along its boundary! The “simplex method” will go from one corner of the,linear_prog
"The minimum of R is λ1, at the point where y1 = 1 and y2 = ··· = yn = 0:",pos_def_matrices
u(t) = (a1cosω1t +b1sinω1t)x1 +···+(ancosωnt +bnsinωnt)xn.,eigenvec_val
"(b) What matrix transforms (2,5) to (1,0) and (1,3) to (0,1)?",vector spaces
1. Compute ATA and its eigenvalues σ2,pos_def_matrices
"13. Illustrate the action of AT by a picture corresponding to Figure 3.4, sending C(A)",orthogonality
m by n matrix. Suppose now that n > m. There are too many columns to be independents,vector spaces
"Suppose .0001 is accepted as the ﬁrst pivot. Then 10,000 times the ﬁrst row is subtracted",gauss elim
"linear. What is its 2 by 2 matrix (using x = 1, y = 0 and x = 0, y = 1 as basis for V,",vector spaces
"an explanation of its convergence. If |λ1| > |λ2| > ··· > |λn|, these eigenvalues will",computations
"(2,y3) lie on a straight line?",gauss elim
21. The adjacency matrix of a graph has Mij = 1 if nodes i and j are connected by an,vector spaces
cut is only 3.,linear_prog
40. Suppose L is a one-dimensional subspace (a line) in R3. Its orthogonal complement,orthogonality
the grocer’s price equals the druggist’s price. We recognize an optimal food diet and,linear_prog
"can be approximated by stopping ∆u/∆x at a ﬁnite stepsize, and not permitting h (or ∆x)",gauss elim
"14. Which of A1, A2, A3, A4 has two positive eigenvalues? Test a > 0 and ac > b2, don’t",pos_def_matrices
+ 2w = 0,gauss elim
"37. When a+b = c+d, show that (1,1) is an eigenvector and ﬁnd both eigenvalues:",eigenvec_val
"If yb is arbitrarily large, a feasible x would contradict yb ≤ cx. Similarly, if cx can go",linear_prog
engineering or economics would lead us far aﬁeld. But there is one natural and important,gauss elim
"can be done: A+y = 0. Thus A+ inverts A where it is invertible, and has the same rank r.",orthogonality
a a a a,gauss elim
The general case is the same. We “solve” ax = b by minimizing,orthogonality
"10. For n = 2, write y0 from the ﬁrst line of equation (13) and y1 from the second line.",orthogonality
"idea is simple, the applications can be complicated. For problems on this scale, the one",pos_def_matrices
Networks and Discrete Applied Mathematics,vector spaces
into the pivots makes it impossible to ﬁgure out how a change in one entry would affect,determinants
13. For the skew-symmetric equation,eigenvec_val
4. (Review) Another quadratic that certainly has its minimum at Ax = b is,pos_def_matrices
Tests for Positive Deﬁniteness,pos_def_matrices
"Otherwise, if we could make A diagonal and see its eigenvalues, we would be ﬁnding",eigenvec_val
of the ﬁrst two rows. Find a third,gauss elim
"2. Verify that the length of the projection in Figure 3.7 is ∥p∥ = ∥b∥cosθ, using formula",orthogonality
"cations, in chemistry, in biology, and elsewhere, but the most important law of physics",eigenvec_val
4. Suppose each “Gibonacci” number Gk+2 is the average of the two previous numbers,eigenvec_val
"3. Apply Gram-Schmidt to replace x by x − (1,x)/(1,1). That is x − 1",orthogonality
2 −1 5 −1 5,vector spaces
"inequality |vTw| ≤ ∥v∥∥w∥. The cosine, even in Hilbert space, is never larger than 1.",orthogonality
"ac − b2 > 0, so the eigenvalues are either both positive or both negative. They must be",pos_def_matrices
"and T = −U. To accelerate the convergence, we move to",computations
"5. Write the complete solutions x = xp +xn to these systems, as in equation (4):",vector spaces
product of the matrices corresponds to the product of the transformations.,vector spaces
"2, which means that the error is cut in half",computations
hope it will be useful to the reader. We particularly want to ﬁnd out about symmetric,eigenvec_val
"transform, done twice, requires only half as much work as a direct 4 by 4 transform. If",orthogonality
31. Suppose A is a symmetric matrix (AT = A).,orthogonality
"form is J2 with two blocks, As for J3 = zero matrix, it is in a family by itself; the only",eigenvec_val
Verify that the error b− p is perpendicular to the columns of A.,orthogonality
"with unit diagonal, and an m by n echelon matrix U, such that PA = LU.",vector spaces
Any matrix leads immediately to a linear transformation. The more interesting question,vector spaces
The square root of this equation says that the determinant equals the volume. The sign,determinants
nodes. The ﬁve components of Ax give the differences in potential across the ﬁve edges.,vector spaces
(b) By cofactors ﬁnd the relation between Cn and Cn−1 and Cn−2. Find C10.,determinants
3.12 Find all 3 by 3 orthogonal matrices whose entries are zeros and ones.,orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
who found a series of inequalities on the entries aij. I do not think this approach is,eigenvec_val
A = SΛS−1 =,eigenvec_val
difﬁculties are minimized by the fact that all entries of F and F−1 tare powers of a single,orthogonality
points along the ﬁrst eigenvector. So R(x) is a minimum at x1.,pos_def_matrices
"mathematics, by giving a direct deﬁnition of the exponential of a matrix. The other",eigenvec_val
"24. What are the dimensions of the four subspaces for A, B, and C, if I is the 3 by 3",vector spaces
4. Carry out the same steps as in the previous problem to ﬁnd the complete solution of,vector spaces
"36. Starting from a 3 by 3 matrix A with pivots 2, 7, 6, add a fourth row and column to",gauss elim
that AAH = AHA. These are exactly the normal matrices.,eigenvec_val
"f3 = −f1, f2 = 0 as in sin0, sinπ/2, sinπ, sin3π/2. This is copied by c and it leads",orthogonality
"27. Every permutation matrix leaves x = (1,1,...,1) unchanged. Then λ = 1. Find two",eigenvec_val
"14. Decide whether the following matrices are positive deﬁnite, negative deﬁnite, semidef-",pos_def_matrices
The Diet Problem and Its Dual,linear_prog
", then det(A−λI) is (λ −a)(λ −d). Check the Cayley-Hamilton state-",eigenvec_val
"matrices and Hermitian matrices: Where are their eigenvalues, and what is special",eigenvec_val
"the x1-x2 plane, and much easier to throw!",pos_def_matrices
"was one of the ﬁrst to be patented—something we then believed impossible, and not",linear_prog
"f(x,y) is positive on the x and y axes. But this function is negative on the line x = y,",pos_def_matrices
a natural way on the complex plane (Figure 5.4).,eigenvec_val
"22. Find the inverses (directly or from the 2 by 2 formula) of A, B, C:",gauss elim
2 = 1 describes a hyperbola,pos_def_matrices
(a) T 2 = identity transformation.,vector spaces
(c) The space of all 4 by 4 matrices.,vector spaces
"(a) 0 or 1, depending on b.",vector spaces
within R3. C(U) is not the same as the column space C(A) before elimination—but the,vector spaces
"matrix with rank 0), One basic theme of mathematics is, given something complicated,",vector spaces
0 0 0 9,vector spaces
"ATA. By equation (12), another formula for the norm is ∥A∥ = σmax. The orthogonal",computations
(b) the projection matrix P onto V.,orthogonality
"be done in 7. That lowered the exponent from log28, which is 3, to log27 ≈ 2.8. This",gauss elim
"Multiplying by Ak, the second component is Lk = λ k",eigenvec_val
form of a logarithm that blows up when any variable x or any slack variable w = Ax−b,linear_prog
"does not. It is Newton’s law F = ma, and the acceleration a is a second derivative. In-",eigenvec_val
"For which differences b1,...,b5 can we solve Ax = b? To ﬁnd a",vector spaces
(d) all vectors with positive components.,vector spaces
"(2, 1) = column 1",gauss elim
1b. Similarly the second coefﬁcient is x2 = qT,orthogonality
invertible submatrix of A.,vector spaces
"The simplest box is a little cube dV = dxdydz, as in",determinants
"variants, and the best of them seems to be the QR algorithm. (The shifted inverse power",computations
Which symmetric matrices have the property that xTAx > 0 for all nonzero vectors x?,pos_def_matrices
"This section follows through on four major applications: inverse of A, solving Ax = b,",determinants
2v + 4w =,gauss elim
"Note. The novelty is that condition III′ applies to all the principal submatrices, not only",pos_def_matrices
There should be no confusion between the diagonal entries and the eigenvalues. For,eigenvec_val
"The term vv′ is zero at both limits, because v is. Now",pos_def_matrices
"equals the sum of the eigenvalues, and the determinant equals their product.",eigenvec_val
"almost as easy to visualize, They are still linear transformations, provided that the origin",vector spaces
"makes us suspect that wn lies at the angle nθ, and we are right.",orthogonality
0 0 0 1,eigenvec_val
"17. Find the 4 by 3 matrix A that represents a right shift: (x1,x2,x3) is transformed to",vector spaces
(b) The nullspaces of A and M−1AM have the same (vectors)(basis)(dimension).,eigenvec_val
∥x+y∥∞ ≤ ∥x∥∞ +∥y∥∞,computations
Px = x. The nullspace stays unchanged under this projection.,linear_prog
roundoff errors in Gaussian elimination.,computations
orthogonal unit vectors. In R2 we have cos2 θ +sin2θ = 1:,orthogonality
We will assume that A allows the symmetric factorization A = LDLT (without row ex-,pos_def_matrices
"desperate, it is always possible just to solve ATy = 0.",vector spaces
"changes). In practice, more row exchanges may be equally necessary—or the computed",gauss elim
"F4 to F2—or rather to two copies of F2, which go into a matrix F∗",orthogonality
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
error b−A�xW are again perpendicular.,orthogonality
Figure 2.3: The entries of a 5 by 8 echelon matrix U and its reduced form R.,vector spaces
Find a matrix square root of A from R = S,eigenvec_val
These are the same three equations in two unknowns. Now the problem is: Find numbers,vector spaces
"are, and the trucks in Chicago are split equally between Boston and Los Angeles Set",eigenvec_val
arbitrary values to the free variables. Suppose we call these values simply v and y. The,vector spaces
used to solve large systems of equations. From the examples in a textbook (n = 3 is,gauss elim
28. (a) When do the eigenvectors for λ = 0 span the nullspace N(A)?,eigenvec_val
possible tree. The full name is spanning tree because the tree “spans” all nodes of the,vector spaces
incidence matrix A is shown next to the graph (and you could recover the graph if you,vector spaces
"1.16 In the previous exercise, how is r related to m and n in each example?",vector spaces
the solution at the end).,gauss elim
"A and Q are m by n when the n vectors are in m-dimensional space, and there has to be",orthogonality
number w. That number has wn = 1.,orthogonality
"19. Which numbers a, b, c lead to row exchanges? Which make the matrix singular?",gauss elim
"6.1 Minima, Maxima, and Saddle Points",pos_def_matrices
"The speed of the FFT, in the standard form presented here, depends on working with",orthogonality
"exchanges may be needed, but the columns of A−1 are determined.",gauss elim
"3, and then produce six females on the way out:",eigenvec_val
These sparse factors L and U completely change the usual operation count. Elimina-,gauss elim
"The ﬁrst two planes meet along a line. The third plane contains that line, because",gauss elim
Figure 1.2: Row picture (two lines) and column picture (combine columns).,gauss elim
47. Construct a 3 by 3 matrix A with no zero entries whose columns are mutually per-,orthogonality
8. Suppose A = uvT is a column times a row (a rank-1 matrix).,eigenvec_val
its coefﬁcients are positive. Write f as a difference of two squares.,pos_def_matrices
"entry in row i, column j of AT is the (j,i) entry of A:",orthogonality
combination of the V’s = b1 jV1 + ··· + bnjVn. But also each V must be a combination,eigenvec_val
"Verify that ST = S−1. (The columns are the eigenvectors of the tridiagonal −1, 2,",eigenvec_val
fastest way is just to multiply the unsolvable equation Ax = b by AT. All these equivalent,orthogonality
(b) What subspace does the matrix P = 0 project onto?,orthogonality
A complete matching (if it is possible) is a set of four is in the matrix. They would,linear_prog
"zero, it is easy to use a plane rotation as illustrated in equation (7), found near the end",computations
"In geometry or mechanics, this is the principal axis theorem. It gives the right choice",eigenvec_val
∂y = 4(x+y)−ycosy−siny = 0,pos_def_matrices
"35. What conditions on b1, b2, b3, b4 make each system solvable? Solve for x:",vector spaces
40. Substitute A = SΛS−1 into the product (A − λ1I)(A − λ2I)···(A − λnI) and explain,eigenvec_val
"18. In the vector space P3 of all p(x) = a0 + a1x + a2x2 + a3x3, let S be the subset of",vector spaces
0 0 4 8,vector spaces
"vector v onto the plane of the ﬁrst two, you just add (aTv)a + (bTv)b. To project onto",orthogonality
5.2 Diagonalization of a Matrix,eigenvec_val
ask your patience about that one further point: the dimensions.,orthogonality
3.3 Projections and Least Squares,orthogonality
"17. For both graphs drawn below, verify Euler’s formula:",vector spaces
"had the off-diagonal entries ∑ti, and shifting the time by ¯t made these entries zero. This",orthogonality
b Ab A2b ···,computations
"45° line y = x, and a point like (2,2) is unchanged. A point like",vector spaces
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"changed. Sketch its effect on the x-axis, by indicating what happens to (1,0) and",vector spaces
Find u(t) and show that it blows up instead of decaying as t → ∞. (Diffusion is,eigenvec_val
λ = 0 is a double eigenvalue—its algebraic multiplicity is 2. But the geometric multi-,eigenvec_val
of the columns of Q = I:,orthogonality
18. Place the smallest number of zeros in a 4 by 4 matrix that will guarantee detA = 0.,determinants
24. Find a basis for the plane x−2y+3z = 0 in R3. Then ﬁnd a basis for the intersection,vector spaces
(d) the inverse matrix U−1.,determinants
tive law to remove parentheses. Notice how B sits next to B−1:,gauss elim
"4 and 0. The error is divided by 4 every time, so a sin-",computations
"a limiting state u∞ = (y∞,z∞):",eigenvec_val
T(v) for every vector in Rn.,vector spaces
0 1 2 3,vector spaces
ized eigenvalue problem. The minimum value Λ1 will be the smallest eigenvalue of,pos_def_matrices
"but there is only one line of eigenvectors (x1,0).",eigenvec_val
eliminates x from the second equation. and it leaves one equation for y:,gauss elim
"22. Every matrix Z can be split into a Hermitian and a skew-Hermitian part, Z = A+K,",eigenvec_val
"Up to now, we accepted b1 and b2 as equally reliable. We looked for the value �x that",orthogonality
∂t = ∂ 2u,eigenvec_val
"is the solution that we want, and not all the entries in the inverse.",gauss elim
"than just orthonormal. When A multiplies a column vj of V, it produces σ j times a",pos_def_matrices
symmetric matrices) two parts of this book that were previously separate: pivots and,pos_def_matrices
The second step needs a new idea. since to continue in the same direction is useless.,linear_prog
"There is another requirement on matrix multiplication. We know how to multiply Ax,",gauss elim
"Inside the sums, wk(j+1)",orthogonality
"like x + 2y + z = 4, and above it is the halfspace x + 2y + z ≥ 4. In n dimensions, the",linear_prog
"3.26 Find an orthonormal basis for the plane x − y + z = 0, and ﬁnd the matrix P that",orthogonality
Figure 8.2 is bounded by the coordinate axes: x ≥ 0 admits all points to the right of,linear_prog
(AB)32 = a31b12 +a32b22 +a33b32 +a34b42.,gauss elim
"A−1vk, which means that we solve the linear system Avk+1 = vk (and save the factors",computations
"direct Cx uses n2 separate multiplications. Knowing E and F, the second way uses",orthogonality
geometry but less clear from the matrix. One approach is through the relationship of,vector spaces
"axis above the origin, both eigenvalues are purely imaginary (because the trace is Zero).",eigenvec_val
We can describe all combinations of the two columns geometrically: Ax = b can be,vector spaces
"any A and b, one and only one of the following systems has a solution:",orthogonality
Note the beautiful connections between the two ﬁgures. The problem is the same but,orthogonality
"38. If AB = I and BC = I, use the associative law to prove A = C.",gauss elim
and R2 = A2 in this case?,vector spaces
λ = +i and −i.,eigenvec_val
2x + by = 16,gauss elim
"symmetric matrix A, the product xTAx is a pure quadratic form f(x1,...,xn):",pos_def_matrices
"29. The powers Ak approach zero if all |λi| < 1, and they blow up if any |λi| > 1. Peter",eigenvec_val
i x j = 0,eigenvec_val
QTQ = I and QT = Q−1.,eigenvec_val
That failure of diagonalization was not a result of λ = 0. It came from λ1 = λ2:,eigenvec_val
row 3 of A = ℓ31(row 1 of U)+ℓ32(row 2 of U)+1(row 3 of U).,gauss elim
A set of n vectors in Rm must be linearly dependent if n > m.,vector spaces
"problem—to minimize a distance. By setting the derivative of E2 to zero, calculus con-",orthogonality
The optimal vectors x∗ and y∗ satisfy complementary slackness:,linear_prog
"The determinant is now settled, but that fact is not at all obvious. Therefore we grad-",determinants
6. Find E2 and E8 and E−1 if,gauss elim
"u = eλtx; the eigenvalue gives the rate of growth or decay, and the eigenvector x develops",eigenvec_val
Then several approaches lead to u(t). Probably the best is to match the general solution,eigenvec_val
"(b) If Ax = b, show that AM is the matrix B j in equation (4), with b in column j.",determinants
"11. Show by induction that, without shifts, (Q0Q1···Qk)(Rk ···R1R0) is exactly the QR",computations
(a) Expand in cofactors along the ﬁrst row to show that Dn = Dn−1 −Dn−2.,determinants
"3.4 What is the projection p of b = (1,2,2) onto a = (2,−2,1)?",orthogonality
"19. If all the cofactors are zero, how do you know that A has no inverse? If none of the",determinants
What value should replace the last zero on the right side to allow the equations to,gauss elim
Chapter 2 Vector Spaces,vector spaces
31. True or false (give an example in either case):,orthogonality
Example 2. Integration from 0 to t is also linear (it takes Pn to Pn+1):,vector spaces
"If all coefﬁcients of x5 had been negative, this would be an unbounded case: we can",linear_prog
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"Figure 2.8: The framework for equilibrium: sources b and f, three steps to ATCA.",vector spaces
The Breakdown of Elimination,gauss elim
"cients in this combination, and what is the ﬁrst row of AB, if",gauss elim
"matrix itself is perturbed: If Ax = b and (A+δA)(x+δx) = b, then by subtraction",computations
"14. What are the eigenvalues λ and frequencies ω, and the general solution, of the fol-",eigenvec_val
"(1,0,1) mentioned in the text.",gauss elim
(c) An orthogonal matrix.,eigenvec_val
come almost accidentally from the statement that ∥e∥2 = ∥b− p∥2 in Figure 3.7 cannot,orthogonality
space has dimension n−1. The spanning tree from elimination gives a basis for that row,vector spaces
20. Find the eigenvalues and the eigenvectors of these two matrices:,eigenvec_val
for some coefﬁcients ci.,vector spaces
there is a “Crout algorithm” that arranges the calculations in a slightly different way.,gauss elim
−1 −3 3 0,vector spaces
"that if Gaussian elimination is applied in a stupid way, the matrix becomes completely",computations
"This has a natural interpretation for an ellipsoid, when it is cut by a plane through the",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"On the boundaries of the second quadrant, the equation is neutrally stable. On the hori-",eigenvec_val
tor deﬁned below. When they are all positive we stop.) The ratios decided the leaving,linear_prog
xTAx = (LTx)TD(LTx) = 2,pos_def_matrices
3. (Recommended) Describe the intersection of the three planes u+v+w+z = 6 and,gauss elim
p = projection of b,orthogonality
share the same pivots.,gauss elim
basis. Why is A2 = I?,vector spaces
"What are the determinants of B, C, AB, ATA, and CT?",determinants
more λ’s for these permutations:,eigenvec_val
Stability of uk+1 = Auk,eigenvec_val
We verify in a moment that this gives the correct y. (You may prefer the ﬂow graph,orthogonality
"ΛQT, the symmetric positive deﬁnite square root of A.",pos_def_matrices
42. Prove that AB has the same eigenvalues as BA.,eigenvec_val
"tiply the 8 error vectors b − Ax = (±1,±1,±1) by (ATA)−1AT to show that the 8",orthogonality
eigenvalue λ1 = cosπh = cosπ/(n+1).,computations
u = u(0) at t = 0.,eigenvec_val
"come back from U to A. In the forward direction, GFEA was U. In the backward",gauss elim
"Ax ≥ b. In the previous section we had two variables, and one constraint x+2y ≥ 4. The",linear_prog
"6F CTAC has the same number of positive eigenvalues, negative eigenvalues,",pos_def_matrices
u + v +,gauss elim
out at every node:,vector spaces
23. Show that the columns of the 4 by 4 Fourier matrix F in Example 5 are eigenvectors,eigenvec_val
1.4 Solve by elimination and back-substitution:,gauss elim
"If there are n equations in n unknowns, we have a square n by n matrix. More generally,",gauss elim
C D] the result is MT =,gauss elim
to deal with more than three variables. This consideration invites us to explore,determinants
that reaches zero ﬁrst becomes the leaving variable—it changes from basic to free. We,linear_prog
This is also a law of linear algebra. Ax = b can be solved when b satisﬁes the same linear,vector spaces
reversible. The nullspace of A is the same as the nullspace of U and R. Only r of,vector spaces
matrix Q is orthogonal (QTQ = I) because eigenvectors of a symmetric matrix can be,pos_def_matrices
Orthogonal Vectors and Subspaces,orthogonality
"To start, each row can be broken down into vectors in the coordinate directions:",determinants
"Suppose elimination reduces Ax = b to Ux = c and Rx = d, with r pivot",vector spaces
matrix. The matrix depends on the choice of basis! If we change the basis by M we,eigenvec_val
Chapter 8 Linear Programming and Game Theory,linear_prog
(b) Compute P1 +P2 and P1P2 and explain.,orthogonality
3. Multiply the matrix L = E−1F−1G−1 in equation (6) by GFE in equation (3):,gauss elim
"ﬁrst, this rearranges the constraints in equation (6) into",linear_prog
of eigenvectors—which are needed for S. That needs to be emphasized:,eigenvec_val
"magnitude, and |λ1| ≤ ··· ≤ |λn−1| < |λn|. Then as long as the initial guess u0 contained",computations
the coefﬁcient �x. All we need is the geometrical fact that the line from b to the closest,orthogonality
"combinations, and the b’s no longer ﬁll out a subspace. Instead, they ﬁll a cone-shaped",linear_prog
"(orthogonal unit vectors), we will ﬁnd",orthogonality
13. Why doesn’t the SVD for A+I just use Σ+I?,pos_def_matrices
Inverse of AT = Transpose of A−1,gauss elim
∂P/∂x2 = −x1 +2x2 −b2 = 0,pos_def_matrices
"The eigenvalues all equal one, but the proper condition number is not λmax/λmin = 1.",computations
orthonormal basis from these vectors (mutually orthogonal unit vectors).,orthogonality
(d) the sum of two numbers on the unit circle?,eigenvec_val
there is nothing more complicated than a triple eigenvalue.,eigenvec_val
KCL: The currents yi (and fi) into each node add to zero.,vector spaces
"The variable xegg will jump from basic to free. To follow it properly, we would have",linear_prog
which also comes in the next section.,gauss elim
multiplied into each column of B.,gauss elim
These formulas give two different approaches to the same solution uk = SΛkS−1u0.,eigenvec_val
standing member of this family of similar matrices (the capo). The Jordan form will,eigenvec_val
the points. What are p and e?,orthogonality
The Fast Fourier Transform,orthogonality
Example 2. (to change a13 = a31 to zero),computations
"and R2, such that R1 +R2 is the row echelon form of A1 +A2. Is it true that R1 = A1",vector spaces
46. Show that the product ST of two reﬂections is a rotation. Multiply these reﬂection,vector spaces
"derivatives at α, β:",pos_def_matrices
69. Compare tic; inv(A); toc for A = rand(500) and A = rand(1000). The n3 count,gauss elim
"22. Without elimination, ﬁnd dimensions and bases for the four subspaces for",vector spaces
"If condition I holds, so does condition III: The determinant of A is the product of",pos_def_matrices
Example 3. Project onto the “θ-direction” in the x-y plane. The line goes through,orthogonality
"At the end of the simplex method, when the right basic variables are known, the",linear_prog
(d) exactly one solution for every b.,vector spaces
of the box come from the rows of A (Figure 4.1). The columns of A would give an,determinants
"matrix, you can see which “host” spaces contain the four subspaces by looking at the",vector spaces
0 0 2 4,vector spaces
0 0 2 1,gauss elim
on the right to be any two orthogonal matrices U and V T—not necessarily transposes of,pos_def_matrices
"nant = ± (product of the pivots), it follows that regardless of the order of elimination,",determinants
"The line through a is the same, and that’s all the projection matrix cares about. If a has",orthogonality
"The steady state is the eigenvector of A corresponding to λ = 1. Multiplication by A,",eigenvec_val
extra ﬂow on the right goes backward to cancel an existing marriage. This extra ﬂow,linear_prog
an iterative method to solve Ax = b. Gaussian elimination will reach the solution x in,computations
1O Even a well-conditioned matrix like B can be ruined by a poor algorithm.,gauss elim
There are plenty of useful functions other than,orthogonality
(b) Deduce that the a’s and b’s are zero (proving linear independence). From that,pos_def_matrices
We give an example with two foods and two vitamins. Note how AT appears when,linear_prog
x came faster by the back-substitution in equation (3) than the ratio in (5). For larger,gauss elim
"20. By back-substitution or by computing eigenvectors, solve",eigenvec_val
"(1+.06/12)pk. After 5 years, or 60 months, you have $11 more:",eigenvec_val
The inverse of an n by n matrix is another n by n matrix. The inverse of A is written A−1,gauss elim
0 0 0 0,vector spaces
The Proof of Duality,linear_prog
we needed negative eigenvalues so that eλt would decay. The new and highly important,pos_def_matrices
"ﬁrst column. As it is, SΛ is correct. Therefore,",eigenvec_val
B−λI = M−1AM −λI = M−1(A−λI)M,eigenvec_val
The exponential of a diagonal matrix Λ is easy; eΛt just has the n numbers eλt on,eigenvec_val
"of λ = ω − 1. Since our goal is to make λmax as small as possible, that extremal pair",computations
For least squares that is all we need. The normal equations came from multiplying,orthogonality
"found y2 = 0, and that vitamin is a free good. You can see how the duality has become",linear_prog
12. Find the eigenvalues and eigenvectors of,eigenvec_val
"9. For the same two bases, express the vector (3,9) as a combination c1V1 + c2V2 and",eigenvec_val
"x0 = (1,1,...,1). Since the cost is cx, the best cost-reducing direction is toward −c.",linear_prog
multiplied by the entries in B.,gauss elim
P⊥. Construct a matrix that has P as its nullspace.,orthogonality
"AT is n by m. The ﬁnal effect is to ﬂip the matrix across its main diagonal, and the entry",gauss elim
"1. If B is similar to A and C is similar to B, show that C is similar to A. (Let B = M−1AM",eigenvec_val
"the elimination steps, which subtract a multiple of one equation from another and reach",gauss elim
Figure 2.11: Reﬂection through the θ-line: the geometry and the matrix.,vector spaces
"diagonal entries, and eigenvalues are completely different, And for a 2 by 2 matrix, the",eigenvec_val
LU misses the symmetry but LDLT captures it perfectly.,gauss elim
A to B by elementary row operations. So B equals an,vector spaces
"This is valid for a wide class of ﬁnite difference matrices, and if we take ω = 1 (Gauss-",computations
0 5 4 3,vector spaces
do lie on a line. Therefore the vector p = (5,orthogonality
"This is simpler than it looks, because L, U, LT, and UT are triangular with unit diagonal.",determinants
"7. If that second graph represents six games between four teams, and the score dif-",vector spaces
"46. The matrix P that multiplies (x,y,z) to give (z,x,y) is also a rotation matrix. Find P",gauss elim
"The coding of the information is simple. To transform a space to itself, one basis is",vector spaces
"52. If the three solutions in Question 51 are x1 = (1,1,1) and x2 = (0,1,1) and x3 =",gauss elim
"subset. Rule (ii) is violated, since if the scalar is −1 and the vector is [1 1], the multiple",vector spaces
xh −xv. To determine the potentials we can arbitrarily assign zero potential to Harvard.,vector spaces
The key equation was Ax = λx. Most vectors x will not satisfy such an equation.,eigenvec_val
"minimum cost is x∗ +2y∗, which equals 4 for all these optimal vectors. On our feasible",linear_prog
there is a y with yA ≥ 0 and,linear_prog
0 2 1 0,gauss elim
"You can substitute GFEA for U, to see how the inverses knock out the original steps.",gauss elim
"42. If the symmetric matrices A and M are indeﬁnite, Ax = λMx might not have real",pos_def_matrices
37. Find a basis for each of these subspaces of 3 by 3 matrices:,vector spaces
20. Forward elimination changes,gauss elim
"agonal matrix, with zeros everywhere except along the main diagonal and the one",computations
"5.20 If KH = −K (skew-Hermitian), the eigenvalues are imaginary and the eigenvectors",eigenvec_val
Therefore Ax is a combination of the columns of A. The coefﬁcients are the,gauss elim
"Matrices are added to each other, or multiplied by numerical constants, exactly as",gauss elim
34. The matrix B =,eigenvec_val
"also look to the graph. The ﬁrst three rows are dependent (row 1 + row 3 = row 2, and",vector spaces
"eigenvector. Our goal is to ﬁnd the eigenvalues and eigenvectors, λ’s and x’s, and to use",eigenvec_val
"(a) The plane of vectors (b1,b2,b3) with ﬁrst component b1 = 0.",vector spaces
a b c d,vector spaces
"feasible set to the next until it ﬁnds the corner with lowest cost. In contrast, “interior",linear_prog
the conjugate transpose. The condition will become UHU = I. The new letter U reﬂects,eigenvec_val
controls δA—so the burden of the roundoff error is carried by the condition number c.,computations
"Any function F(x1,...,xn) is approached in the same way. At a stationary point",pos_def_matrices
3.34 With weighting matrix W =,orthogonality
"by A leaves the direction of x unchanged, then so does the second.",eigenvec_val
"nalize A = QΛQT. We straightened the picture by rotating the axes. Algebraically, the",pos_def_matrices
3.2 Cosines and Projections onto Lines,orthogonality
17. (Silly problem) Describe all 2 by 3 matrices A1 and A2 with row echelon forms R1,vector spaces
sure to be zero if a21 = 0?,determinants
the integral of 1/x2 is inﬁnite.,orthogonality
"at the point x = b, y = −a. The surface z = f(x,y) degenerates from a bowl into a valley.",pos_def_matrices
"to see that they span the space. In fact, the column space of U is just the x-y plane",vector spaces
says that computing time (measured by tic; toc) should multiply by 8 when n is,gauss elim
"The diagonalization in Section 5.2 will be applied to difference equations, Fibonacci",eigenvec_val
Now we deﬁne what it means for a set of vectors to span a space. The column space of,vector spaces
u(0) = u(1) = 0.,pos_def_matrices
"3. Row exchange: Add row 1 of A to row 2, then subtract row 2 from row 1. Then",determinants
It is not clear that these equations have a solution. The third equation is very much,vector spaces
From equation (5) we can ﬁnally read off the answer to our original question: The,determinants
A j = a j −(qT,orthogonality
"after a million steps, roundoff error could be signiﬁcant. (Some problems are sen-",gauss elim
This 2 by 2 matrix A has only one eigenvector and cannot be diagonalized. Compute,eigenvec_val
", the unshifted QR algorithm produces only the",computations
"(b) Show that eA+B = eAeB is not true for matrices, from the example",eigenvec_val
"43. Choose the number q so that (if possible) the ranks are (a) 1, (b) 2, (c) 3:",vector spaces
"(1,...,1). We solve m equations ax = b in 1 unknown (by least squares).",orthogonality
Multiply by t in row 1,determinants
same. The volume and determinant are unchanged if we subtract from each row its pro-,determinants
inner product: xHy = x1y1 +···+xnyn,eigenvec_val
(a) Describe the three types of subspaces of R2.,vector spaces
condition” at each end of the interval:,gauss elim
Look at all the terms a1αa2β ···anv involving a11. The ﬁrst column is α = 1. This,determinants
The ﬁrst approach concentrates on the separate equations (the rows). That is the,gauss elim
"symmetric case even cubic convergence, to the smallest eigenvalue. After three or four",computations
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"reverse the signs of a, b, and c. This actually leaves ac > b2 unchanged: The quadratic",pos_def_matrices
"additional ﬂow of 1 can take the lowest path 1-3-5-6-1. The total is 6, and no more is",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
(EA times x) equals (E times Ax). We just write EAx.,gauss elim
That shows how a matrix A with three diagonals has factors L and U with two diagonals.,gauss elim
a reason why A is symmetric positive deﬁnite.,pos_def_matrices
Example 3. The eigenvalues are on the main diagonal when A is triangular:,eigenvec_val
"lar, the steps are downward as usual. If A is upper triangular, the last column is cleared",determinants
"1.24 Starting with a ﬁrst plane u+2v−w = 6, ﬁnd the equation for",gauss elim
21 22 23 24,determinants
numerical method can avoid this sensitivity to small perturbations. The ill-conditioning,gauss elim
Note. Problems 7 and 8 are perhaps the most typical and most important results,pos_def_matrices
"ﬂected onto its negative, to produce this second column. The matrix H is still 2P−I",vector spaces
"14. Compute the determinants of A, B, C. Are their columns independent?",determinants
Rule 3 would say that there is a further term −ℓ,determinants
Property 3′ Eigenvectors corresponding to different eigenvalues are orthonor-,eigenvec_val
"In all other cases, CD = −DC is only possible with D = zero matrix.",determinants
"in row i, column j of AT comes from row j, column i of A:",gauss elim
"mal, the “cross-product matrix” ATA becomes QTQ = I. The hard part of least squares",orthogonality
Does the product of Qθ and Qϕ equal Qθ+ϕ (rotation through θ then ϕ)?,vector spaces
ω are all real: Pure diffusion is converted into pure oscillation. The factors eiωt produce,eigenvec_val
faster than matrix-matrix multiplication. A crucial construction starts with a vec-,computations
values of B have absolute value less than 1 if the real eigenvalues of A lie between,computations
"equation? Which are the free variables? The special solutions are (3,1,0) and",vector spaces
That last paragraph describes all inner products: They come from invertible matrices,orthogonality
12. Find the rank of A and write the matrix as A = uvT:,vector spaces
"This count is remarkably low. Since matrix multiplication already takes n3 steps, it",gauss elim
�� = PU =,gauss elim
Also in the right-hand ﬁgure is one of the central ideas of linear algebra. It uses both,gauss elim
steady state for both is the eigenvector for λmax. It is multiplied by 1k = 1 in difference,eigenvec_val
"third minus the ﬁrst. These dependent columns, the second and fourth, are exactly the",vector spaces
c31 and c32 to be zero:,vector spaces
dimension of C(A)+dimension of N(A) = number of columns.,vector spaces
"Starting from a different u(0) = (a,b), the solution u(t) ends up as",eigenvec_val
"34. If A and B have nonzeros in the positions marked by x, which zeros are still zero in",gauss elim
"For AT, the basic law of network theory is Kirchhoff’s Current Law. The total ﬂow",vector spaces
its eigenvalue matrix Λ (diagonal). We see this for projections and rotations.,eigenvec_val
sists of one block. We now check that. The determinants all equal 1. The traces (the,eigenvec_val
Those are standard formulas for the sums of the ﬁrst n numbers and the ﬁrst n squares.,gauss elim
"cosine: aTb = ∥a∥∥b∥cosθ. In fact, this proves the cosine formula in n dimensions,",orthogonality
we cannot have n > m.,vector spaces
"pound of peanut butter gives a unit of protein, and each steak gives two units. At least",linear_prog
matrix. Find the eigenvalues and “eigenmatrices” for AT = λA.,eigenvec_val
"We close with three remarks, I hope your intuition says that they are correct.",pos_def_matrices
"13. (a) A skew-symmetric matrix satisﬁes KT = −K, as in",determinants
modest improvement A1 = 1,computations
"In a larger problem, forward elimination takes most of the effort. We use multiples",gauss elim
"The nullspace always contains the vector of 1s, since A looks only at the differences",vector spaces
now below it (the e above it is useless). If a is not zero then another row exchange P23 is,gauss elim
reader. It shows a more “abstract” method of reasoning. I wish I knew which proof is,orthogonality
"columns are orthogonal, so the sum of these two separate pieces is the best ﬁt by any",orthogonality
even-numbered components from its odd-numbered components:,orthogonality
"63. Ax gives the amounts of steel, rubber, and labor to produce x in Problem 62. Find A.",gauss elim
"this constraint on vitamin prices, the druggist can sell the required amount bi of each",linear_prog
Similar matrix: B = M−1AM,eigenvec_val
Here is a little trick. The special solutions are especially easy from R. The numbers 3,vector spaces
1 0 2 3,vector spaces
"more serious, and larger by at least an order of magnitude. When the computer rounds",computations
. Multiply this L,gauss elim
Figure 6.2: The ellipse xTAx = 5u2 +8uv+5v2 = 1 and its principal axes.,pos_def_matrices
"This equation describes the unshifted QR algorithm, and almost always Ak approaches a",computations
"solutions, as in equation (4).",vector spaces
(ATA)−1AT and on the right by A(ATA)−1 to show that the average of (�x−x)(�x−x)T,orthogonality
properties apply equally well to symmetric matrices. A real symmetric matrix is cer-,eigenvec_val
"Minimize the cost cx, subject to x ≥ 0 and Ax ≥ b.",linear_prog
"18. Multiply matrices to ﬁnd ATA, and guess how its entries come from the graph:",vector spaces
n exponentials eλtx—which is the best solution of all:,eigenvec_val
"with zero determinant is singular, so there must be nonzero vectors x in its nullspace. In",eigenvec_val
"governed by computational necessity, rather than by elegance, and I don’t know whether",computations
is invertible. Start from B(I −AB) = (1−BA)B.,gauss elim
0 0 0 1,linear_prog
compute the dimensions of those subspaces and a convenient set of vectors to generate,vector spaces
Then (Ax)Ty is the,gauss elim
"Then, elimination multiplies the ﬁrst row by −1, to give a unit pivot, and uses the second",linear_prog
"change to cylindrical coordinates by x = rcosθ, y = rsinθ, z = z. Just as a small inter-",determinants
The cost function cx brings to the problem a family of parallel planes. One plane,linear_prog
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
8. Change Ax = b to x = (I −A)x+b. What are S and T for this splitting? What matrix,computations
"The important orthogonal subspaces don’t come by accident, and they come two at",orthogonality
eigenvectors as A. Its eigenvalues are,eigenvec_val
14. (a) Construct a 3 by 3 system that needs two row exchanges to reach a triangular,gauss elim
"(d) In elimination on A, what multiple of the third row is subtracted to knock out",vector spaces
"N(AT)⊥C(A). This is the same as the ﬁrst half of the theorem, with A replaced by",orthogonality
is multiplied on the right by P−1,computations
"the primal.) The slack variable is s = c − yA, with s ≥ 0. What are the Kuhn-Tucker",linear_prog
1 −1 1 0 0,vector spaces
perpendicular to L. Then (L⊥)⊥ is a,orthogonality
"The column above that negative entry is B−1u = (3,2); its ratios with the last column",linear_prog
"Comparing the left to the right, λ 1λ2 = 1 or xHy = 0. But Property 2′ is λ 1λ1 = 1, so we",eigenvec_val
"and λ = 0, λ = 1",computations
3(column 2) are orthogonal.,orthogonality
T(v) = v1 +v2 +v3.,vector spaces
[D+ωL]xk+1 = [(1−ω)D−ωU]xk +ωb.,computations
5.4 Differential Equations and eAt,eigenvec_val
rows. The b column tells which combinations you have taken of the rows:,vector spaces
"the third equation −2u+7v+2w = 9, and it crosses the line at u = 1, v = 1, w = 2. That ",gauss elim
"If this is du/dt = Au, what is the 2 by 2 matrix A? Find its eigenvalues and eigen-",eigenvec_val
"Conversely, any symmetric matrix with P2 = P represents a projection.",orthogonality
C = C∗ −(qT,orthogonality
"that circle of ideas, we have to show that distinct eigenvalues present no problem.",eigenvec_val
The special solution for this variable is the vector x =,vector spaces
"When you multiply the highlighted equation by A, you get Axcomplete = b+0.",vector spaces
"In the language of linear programming, we are ﬁnding the optimal corner ﬁrst. The",linear_prog
"3.13 What multiple of a1 should be subtracted from a2, to make the result orthogonal to",orthogonality
"polynomial time.1 His algorithm stayed inside the feasible set, and captured x∗ in a series",linear_prog
Problems 45–50 are about perpendicular columns and rows.,orthogonality
method really comes into its own.,pos_def_matrices
same combination of these special solutions.,eigenvec_val
1.32 (a) Find a basis for the space of all vectors in R6 with x1 +x2 = x3 +x4 = x5 +x6.,vector spaces
"The coordinate vectors e1,...,en coming from the identity matrix span Rn. Every",vector spaces
xTy = 0 in the real symmetric case and xHy = 0 in the complex Hermitian case.,eigenvec_val
"3. If we shift to A − 7I, what are the eigenvalues and eigenvectors and how are they",eigenvec_val
three elimination matrices at once:,gauss elim
4. The left nullspace of A is the nullspace of AT. It contains all vectors y such that,vector spaces
last m−r rows of the invertible matrix L−1P must be a basis of y’s in the left nullspace—,vector spaces
"34. Under what condition on b1, b2, b3 is the following system solvable? Include b as a",vector spaces
34. The transformation T that transposes every matrix is deﬁnitely linear. Which of these,vector spaces
r2 = a2 + b2,eigenvec_val
"(3) If all diagonal entries of A are zero, then A is singular.",gauss elim
"c0 −c2, c1 +c3, c1 −c3? You are ﬁnding the Fast Fourier Transform!",orthogonality
It is this result that leads to a “nonconstructive proof” of the duality theorem.,linear_prog
43. Write the 3 by 3 identity matrix as a combination of the other ﬁve permutation matri-,vector spaces
19. Find a third column so that U is unitary. How much freedom in column 3?,eigenvec_val
the derivative of every polynomial.,vector spaces
closest to a on the line through b.,orthogonality
be able to predict the number of operations.,gauss elim
"33. (a) If x is in the nullspace of A, show that M−1x is in the nullspace of M−1AM.",eigenvec_val
in linear algebra. For that reason we can only accept a ﬁnite amount of information about,gauss elim
add all the rows: only one row is allowed to change. Both sides give the answer ad +,determinants
Λk as k → ∞? What is the limit of SΛkS−1? In the columns of this limiting matrix,eigenvec_val
The ﬂows xij enter node j from earlier nodes i. The ﬂows x jk leave node j to later,linear_prog
The inverse matrix is built from the powers of w−1 = 1/w = w:,orthogonality
"because of this invention, which probably took him ten minutes. Ironically, it is the most",gauss elim
detUH. Describe all 2 by 2 matrices that are unitary.,eigenvec_val
"for Rn. Some things in linear algebra are unique, but not this. A vector space has",vector spaces
"For most matrices, there is no doubt that the eigenvalue problem is computationally",eigenvec_val
"Linear algebra moves that picture into ten dimensions, where the intuition has to",gauss elim
Example 1. A =,vector spaces
"+ ones(4,4) times w = zeros(4,1) + 2 ∗ ones(4,1), what is B ∗ w?",gauss elim
"to all linear equations have this form, x = xp +xn:",vector spaces
(i) The projection matrix is back to P =,vector spaces
"In the opposite direction, suppose AB = BA. Starting from Ax = λx, we have",eigenvec_val
in Figure 4.3. The parallelogram has unit base and unit height; its area is also 1.,determinants
"Now we know the dimensions of the four spaces. We can summarize them in a table,",vector spaces
Powers and Products: Ak and AB,eigenvec_val
"(0,−1,1,0) at the corner P. Compute r and decide which column u should enter the",linear_prog
rectangular system with no solution for most b.,orthogonality
"10. Suppose A has eigenvalues 1, 2, 4. What is the trace of A2? What is the determinant",eigenvec_val
"We go to a more stable measure of rank. The ﬁrst step is to use ATA or AAT, which",pos_def_matrices
the 45° line. Their matrices were especially simple:,vector spaces
Let me repeat the proof. Any feasible vectors x and y satisfy weak duality:,linear_prog
41. Redraw Figure 2.5 for a 3 by 2 matrix of rank r = 2. Which subspace is Z (zero,vector spaces
"(d) the entry in row 1, column 1 of CDE?",gauss elim
The overall projection matrix is,orthogonality
"orthonormal eigenvectors, any x is a combination c1x1 +···+cnxn. Then",pos_def_matrices
"We begin with a change in the right-hand side, from b to b +δb. This error might",computations
"to match the new deﬁnition of length, and we conjugate the ﬁrst vector in the inner",eigenvec_val
"3 6] has no inverse by trying to solve for the column (x,y):",gauss elim
"certainly not all four. It is true that columns 1 and 4 are also independent, but if that",vector spaces
Example 1. The nonnegative combinations of the columns of A = I ﬁll the positive,linear_prog
"12. Compute y = F8c by the three steps of the Fast Fourier Transform if c = (1,0,1,0,1,0,1,0).",orthogonality
This gives a complete analogy with difference equations and SΛS−1u0. In both cases,eigenvec_val
The differential equation du/dt = Au is,eigenvec_val
"detA = 6. The pivots are obviously converging to 1, as n gets large. Such matrices make",gauss elim
"allowed along each edge. But we do have a choice of n different edges, and Phase II",linear_prog
of A. The rows of A are in the,orthogonality
(# of nodes) − (# of edges) + (# of loops) = 1.,vector spaces
c1x1 +c2x2 = d.,pos_def_matrices
"In the example, the steady state has v = w.",eigenvec_val
It can easily happen that a feasible set is bounded or even empty. If we switch our,linear_prog
"40. Which pairs are similar? Choose a, b, c, d to prove that the other pairs aren’t:",eigenvec_val
"difference. When we apply E and then G, the second row is altered before it affects the",gauss elim
29. (Very optional) Normally the multiplication of two complex numbers,gauss elim
One more point: The permutation P23P13 will do both row exchanges at once:,gauss elim
"The transpose of A = LDU gives AT = UTDTLT. Since A = AT, we now have two",gauss elim
only vector with length zero—the only vector orthogonal to itself—is the zero vector.,orthogonality
"Factoring out the aij, there is a term for every one of the six permutations:",determinants
"26. Solve det(Q−λI) = 0 by the quadratic formula, to reach λ = cosθ ±isinθ:",eigenvec_val
7.3 Computation of Eigenvalues,computations
"the interior points x1 = h,x2 = 2h,...,xn = nh, just as for ﬁnite differences. Then Vj is",pos_def_matrices
What are all the possible 4 by 4 determinants of I +Peven?,determinants
those that correspond to columns with pivots. The ﬁrst and third columns contain the,vector spaces
c1(λ1 −λ2)x1 = 0.,eigenvec_val
quadratic functions. And the derivative of a quadratic is linear. We get back to the,pos_def_matrices
in Problem 9 solves u′′ = u? This transformation from initial values to solution is,vector spaces
when the length involves W. The weighted length of x equals the ordinary length of Wx.,orthogonality
"produce cb. In other words, A(cx) = cb.",vector spaces
1 0 0 0,eigenvec_val
"In our example, we shift A by λI to make it singular:",eigenvec_val
terms of y′ and y′′.,orthogonality
"In any speciﬁc example like Fibonacci’s, the ﬁrst step is to ﬁnd the eigenvalues:",eigenvec_val
associated with the positive eigenvector of A−1. In this example the expansion factor is,eigenvec_val
on orthogonal matrices at every step of the computation of λ.,computations
vectors are always linearly dependent in R3.,vector spaces
"y2 = 1 and sketch around it the points (2x,y) that result from multiplication by A.",vector spaces
I +Λt + (Λt)2,eigenvec_val
"10. If Ax = b always has at least one solution, show that the only solution to ATy = 0 is",vector spaces
"Every quadratic form f = ax2+2bxy+cy2 has a stationary point at the origin, where",pos_def_matrices
"(c) If A is invertible and B is singular, then A+B is invertible.",determinants
Ak is again in Hessenberg or tridiagonal form:,computations
1+W +W 2 +···+W n−1 = 0.,orthogonality
"Example 2. For a 3 by 3 matrix, this way of collecting terms gives",determinants
n−r = 1 and m−r = 1. Figure 2.5 shows that two pairs of lines are perpendicular. That,vector spaces
dt (eAt) = AeAt.,eigenvec_val
into the same family. How many families? How many matrices (total 16) in each,eigenvec_val
"(i) no more than $20,000 can be invested in junk bonds, and",linear_prog
1. Compute the row vector λ = cBB−1 and the reduced costs r = cN −λN.,linear_prog
contain only the vectors x = 0 and y = 0. The matrix is invertible.,vector spaces
"labor? We must start with larger amounts p1, p2, p3, because some part is consumed",eigenvec_val
64. Here is a new factorization of A into triangular times symmetric:,gauss elim
Difference equations uk+1 = Auk move forward in a ﬁnite number of ﬁnite steps. A,eigenvec_val
feasible solutions.” The vector r and the ratio α are decisive. Their calculation is the,linear_prog
"a full set of eigenvectors, the projections add up to the identity. And since the eigenspace",eigenvec_val
1. Find the eigenvalues and eigenvectors of the matrix A =,eigenvec_val
How do all these individual errors contribute to the ﬁnal error in Ax = b?,gauss elim
"The ﬁrst step is to deﬁne linear independence. Given a set of vectors v1,...,vk, we",vector spaces
Neumann asked for the maximum rate t at which the economy can expand and still stay,eigenvec_val
1. x+y = y+x.,vector spaces
(i) Solving Ax = b is equivalent to minimizing P(x) = 1,pos_def_matrices
"You can compute A+, or ﬁnd the general solution to ATA�x = ATb and choose the",pos_def_matrices
ces! Then show that those ﬁve matrices are linearly independent. (Assume a combi,vector spaces
"vector, and vice versa. That was the idea of Descartes, who turned geometry into algebra",gauss elim
6. The determinant of A is not zero.,vector spaces
everything cancels. The current law asks us to add the currents into each node by the,vector spaces
"with matrices, and now we see how they represent linear transformations.",vector spaces
the relation between primal and dual. Here is the fundamental result:,linear_prog
"the art shows it differently. In Figure 3.9b, b is not a combination of the columns (1,1,1)",orthogonality
"equations are inconsistent, and Ax = b has no solution.",orthogonality
Each sum on the right has m = 1,orthogonality
Example 1. Consider all vectors in R2 whose components are positive or zero. This,vector spaces
"To ﬁnd q1, make the ﬁrst vector into a unit vector: q1 = a/",orthogonality
"(i) If ax2 +2bxy+cy2 is positive deﬁnite, then necessarily a > 0.",pos_def_matrices
"vectors x so that the dimension of S is: (a) 0, (b) 1, (c) 3, (d) 4.",vector spaces
"3, the masses move oppositely but with equal speeds. The",eigenvec_val
"segment, the concentration remains uniform (zero in the inﬁnite segments). The process",eigenvec_val
∥A∥2 = max ∥Ax∥2,computations
"You may carry the right-hand side as a ﬁfth column (and omit writing u, v, w, z until",gauss elim
"We need equality, and there is only one way in which y∗b can equal y∗(Ax∗). Any time",linear_prog
is orthogonal. It must be a unit vector that is orthogonal to the other columns; how,orthogonality
"A = LU as before? There is no reason why not, since the elimination steps have not",vector spaces
"Example 5. A is diagonal, with dependent rows and dependent columns:",pos_def_matrices
"There are four or ﬁve different ways to answer this question, and we hope to ﬁnd all of",pos_def_matrices
"ampliﬁes by no more than ∥A−1∥. Then ∥δx∥ < ∥A−1∥∥δA∥∥x+δx∥, which is",computations
"2. We vary the load on a structure, and measure the movement it produces. In this",orthogonality
"ﬁrst one, so the ﬁrst stage of elimination needs n(n − 1) = n2 − n operations. (Another",gauss elim
"(c) a third plane that meets the ﬁrst and second in the point (4,1,0).",gauss elim
(ii) Solving Ax = λ1x is equivalent to minimizing R(x) = xTAx/xTx.,pos_def_matrices
plane as t increases from 0 to 2π.,eigenvec_val
complex numbers will appear as eigenvalues (even of real matrices). Here we need only,orthogonality
This matrix Ak+1 is similar to Ak (always the same eigenvalues):,computations
The SVD extends this “polar factorization” to matrices of any size:,pos_def_matrices
process (factoring into QR) takes O(n3) operations for a full matrix A. For a Hessenberg,computations
perpendicular to L⊥. In,orthogonality
you ﬁnd ﬁve growths for every two turns around the stem. The pear tree has eight for,eigenvec_val
"and �x is an exact solution, or Q is rectangular and we need least squares.",orthogonality
"of eigenvectors from Ax = λx, we have eigenfunctions from d2u/dx2 = λu. Those are",eigenvec_val
Which permutation matrices are required?,gauss elim
"is to recover a matrix from its projections. in the 2 by 2 case, can you recover the",orthogonality
2u + 3v −,vector spaces
"whose pivot columns are 2, 4, 5.",vector spaces
is well deﬁned by rule 2 as either +1 or −1.,determinants
Remark 5. Here is the reason that Avj = σ ju j. Start with ATAvj = σ2,pos_def_matrices
"equations in 400,000 unknowns.",orthogonality
40. Suppose column 5 of U has no pivot. Then x5 is a,vector spaces
"understand it. Together with the mechanics of the algorithm, we want to explain four",gauss elim
"α, the second row is nonzero in column β, and ﬁnally the nth row in column v. The",determinants
(c) Every matrix is in the range of T.,vector spaces
"26. If the eigenvalues of A are 1 and 0, write everything you know about the matrices A",eigenvec_val
12. The functions f(x) = x2 and g(x) = 5x are “vectors” in the vector space F of all real,vector spaces
"(c) the eigenvalues of BTB, and",eigenvec_val
and your basis for W)?,vector spaces
Hessenberg times triangular is Hessenberg. So is triangular times Hessenberg:,computations
then the nullspace has a larger dimension than,vector spaces
and write two different diagonalizing matrices S.,eigenvec_val
"So much for a single equation. We shall take a direct approach to systems, and look",eigenvec_val
Nullspace ⊥ Row space,orthogonality
"38. For A4 in Problem 6, ﬁve of the 4! = 24 terms in the big formula (6) are nonzero.",determinants
"makes 3 marriages, which is maximal. The minimal cut is crossed by 3 edges.",linear_prog
Which eigenvectors correspond to nonzero eigenvalues?,eigenvec_val
"When Ax = b is inconsistent, its least-squares solution minimizes ∥Ax −",orthogonality
"30. If you exchange the ﬁrst two rows of a matrix A, which of the four subspaces stay",vector spaces
"orthogonal. If Q1 is rotation through θ, and Q2 is rotation through φ, what is Q1Q2?",orthogonality
2 by 2 inverse,gauss elim
"9. Verify the inverse in equation (5), and show that BE has Bv = u in its kth column.",linear_prog
"8. Solve the same system with y = (2,0,−2,0) by knowing F−1",orthogonality
5.2 Diagonalization of a Matrix,eigenvec_val
ax + by =,gauss elim
5.4 Differential Equations and eAt,eigenvec_val
The 4 by 4 discrete Fourier transform uses w = i (and notice i4 = 1). The success of,orthogonality
2.5 Graphs and Networks,vector spaces
Diagonalizing Symmetric and Hermitian Matrices,eigenvec_val
"27. With positive pivots in D, the factorization A = LDLT becomes L",pos_def_matrices
"we can handle n = 2 and n = 3 in an organized way, you will see the pattern.",determinants
know Ax for each vector in the entire space. Suppose the basis consists of the n vectors,vector spaces
matrix equation Ax = b is equivalent to the three simultaneous equations in equation (1).,gauss elim
"w1 = (1,0), w2 = (0,1). (It is not the identity matrix!)",eigenvec_val
"8. Project the vector b = (1,2) onto two vectors that are not orthogonal, a1 = (1,0)",orthogonality
18. What is the minimum-length least-squares solution x+ = A+b to the following?,pos_def_matrices
"come from a matrix, and the product of those matrices must be F4:",orthogonality
r columns containing pivots are a basis for the column space of U. We will pick those,vector spaces
"fraction like 2/3, the masses would eventually return to u(0) = (1,0) and begin again.",eigenvec_val
Figure 2.9: Transformations of the plane by four matrices.,vector spaces
"35. With block multiplication, A = LU has Ak = LkUk in the upper left corner:",determinants
w8 = cos π,orthogonality
1. There are no real eigenvectors. Ax stays behind or ahead of x. This means the,eigenvec_val
"That cut will have capacity below n, so no middle edges cross it. Suppose p nodes on",linear_prog
"16. Orthogonal matrices have norm ∥Q∥ = 1. If A = QR, show that ∥A∥ ≤ ∥R∥ and also",computations
"(a) If A = AH is Hermitian, then all λi are real.",eigenvec_val
"inner product is always positive, because it is the integral of x2.) Therefore the closest",orthogonality
"whose coordinates are 5, −2, 9. Every point in three-dimensional space is matched to a",gauss elim
"elimination with partial pivoting. Some further reﬁnements, such as watching to see",gauss elim
"5.23 If Ax = λ1x and ATy = λ2y (all real), show that xTy = 0.",eigenvec_val
that line while x circles around. One eigenvector x is along the line. Another eigenvector,eigenvec_val
Hockney; certain special matrices will fall apart when they are dropped the right way.,computations
"edge, from node 3 to node 4.",vector spaces
from equation (5) that ∥A+B∥ ≤ ∥A∥+∥B∥?,computations
Now it is the last column of B that is completely arbitrary. The best left-inverse (also the,vector spaces
Then the trick is to split this last matrix into a quite different product SΛ:,eigenvec_val
3. Find two inner products and a matrix product:,gauss elim
v = 0. Another possibility is the second column: u = 0 and v = 1. A third is the right-,vector spaces
ment that (A−aI)(A−dI) = zero matrix.,eigenvec_val
Ax = b has a nonnegative solution,linear_prog
"The third column is zero in B, so it is zero in AB. B consists of three columns side by",gauss elim
have the rank-1 form uvT above the diagonal.),determinants
"and a2 = (1,1). Show that, unlike the orthogonal case, the sum of the two one-",orthogonality
"As the discrete masses and springs merge into a solid rod, the “second differences” given",eigenvec_val
"of those in Boston and in Los Angeles go to Chicago, the other half stay here they",eigenvec_val
Example 2 (Gauss-Seidel). Here S−1T has smaller eigenvalues:,computations
10. Construct a homogeneous equation in three unknowns whose solutions are the linear,orthogonality
"binations cx + dy—the “vectors” are x and y, but they may actually be polynomials or",vector spaces
3. a row vector c (cost vector) with n components.,linear_prog
"Using this terminology, the nullspace is the orthogonal complement of the row space:",orthogonality
3. Where is the largest error?,pos_def_matrices
The Fourier Matrix and Its Inverse,orthogonality
last 1 were changed to 0 they would be dependent. It is the columns with pivots that are,vector spaces
We come now to the most sensational event in the recent history of linear programming.,linear_prog
Forward elimination produces U on the left and c on the right:,vector spaces
This is the whole point of an “associative law” like 2 × (3 × 4) = (2 × 3) × 4. The law,gauss elim
column of A. The key is that A has a row for every v and a column for every w. A is a,vector spaces
Other transformations Ax can increase the length of x; stretching and shearing are in,vector spaces
formula (AB)−1 = B−1A−1. We mention again that these two formulas meet to give the,orthogonality
"the “hat function” that equals 1 at the node x j, and zero at all the other nodes (Figure",pos_def_matrices
Probably the reader has already met complex numbers; a review is easy to give. The,eigenvec_val
the Cauchy-Schwarz-Buniakowsky inequality! Mathematical historians seem to agree,orthogonality
"23. Suppose the 3 by 3 matrix A is invertible. Write bases for the four subspaces for A,",vector spaces
"5Fourth derivatives are also possible, in the bending of beams, but nature seems to resist going higher than four.",eigenvec_val
A = UΣV T = (UV T)(VΣV T).,pos_def_matrices
"(i) If we add any vectors x and y in the subspace, x+y is in the subspace.",vector spaces
The fundamental theorem of linear algebra connects the dimensions of the subspaces:,vector spaces
3 under the hat:,pos_def_matrices
A corner is degenerate if more than the usual n com-,linear_prog
"F4 contains the powers of w4 = i, the fourth root of 1. F∗",orthogonality
The eigenvalues of A appear along the diagonal of this similar matrix T.,eigenvec_val
a computer very happy.,gauss elim
"ex = 1+x+x2/2!+x3/3!+···. If we replace x by At and 1 by I, this sum is an n by n",eigenvec_val
1 0 1 0,determinants
A−2I has a negative pivot,pos_def_matrices
"The cost coefﬁcients 7, −1, −3 at the ﬁrst corner and 1, 8, −1 at the",linear_prog
shaded region in Figure 8.2. This feasible set is the intersection of the three halfspaces,linear_prog
"a broken line, we have the approximate solution U(x).",pos_def_matrices
the exercises. Each example has a matrix to represent it—which is the main point of this,vector spaces
determinant of a permutation matrix P. The determinant of the whole matrix is the sum,determinants
"This has all solutions to Ax = 0, plus the new xp = (−2,0,1,0). That xp is a particular",vector spaces
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
applied mathematics. Often that requires more insight than the solution of the problem.,vector spaces
", describe the cone of nonnegative combinations of the columns. If b lies",linear_prog
AAT!) and it is the fundamental matrix of this chapter.,orthogonality
"can be shifted from one place to another, but it cannot be removed. The true solution is",gauss elim
0 0 0 0,vector spaces
much larger (n = 1000 is a very moderate size in scientiﬁc computing). The truth is that,gauss elim
"column of B−1, if i ̸= j?",orthogonality
"problem, and verify that the two values are the same. If the ﬁrst component of b is",linear_prog
39. Estimate the time difference for each new right-hand side b when n = 800. Create A,gauss elim
a diagonal matrix D. Then we have room to move. The rescaling from x to X = D−1x,linear_prog
1·1+w jw−k +w2jw−2k +···+w(n−1)jw−(n−1)k = 0,orthogonality
The product GFE is the true order of elimination. It is the matrix that takes the original,gauss elim
"In the language of linear algebra, n−1 is the rank of the incidence matrix A. The row",vector spaces
A Formula for the Norm,computations
", write A as RTR in three",pos_def_matrices
"component,” with x = xr +xn. When multiplied by A, this is Ax = Axr +Axn:",orthogonality
1. The column space of A The column space is sometimes called the range. This,vector spaces
0 1dt = t,vector spaces
"the pivot variables and free variables. If there are r pivots, there are r pivot variables",vector spaces
0 0 ∗ ∗,computations
careful organization of all the rotations gives 2,computations
(a) ﬁnd their positions in the complex plane.,eigenvec_val
"4. A is normal, AAH = AHA: then U can be chosen so that U−1AU = Λ.",eigenvec_val
so the tetrahedron is regular—what is the cosine of the angle between the rays going,orthogonality
"Note. With a different cost function, the intersection might not be just a single point. If",linear_prog
higher degree. deﬁned on triangles or quadrilaterals for partial differential equations—,pos_def_matrices
1.2 For the matrices,gauss elim
"network, what are the potentials at the nodes and currents on the edges?",vector spaces
"A newer approach uses the ordinary methods of numerical linear algebra, regarding",linear_prog
9). The best U(x) is 2,pos_def_matrices
have n zero components. The ith component of xN went from zero to α. The kth com-,linear_prog
···+cnxn. What is Ax? What is Bx?,eigenvec_val
This section introduces a class of rectangular matrices with two advantages. They,vector spaces
"Example 1. Positive pivots 2, 3",pos_def_matrices
Example 1. Using the Gauss-Jordan Method to Find A−1,gauss elim
5.25 (a) Find a nonzero matrix N such that N3 = 0.,eigenvec_val
(c) The other eigenvalues satisfy ∥λi∥ ≤ 1.,eigenvec_val
"35. By direct multiplication, ﬁnd J2 and J3 when",eigenvec_val
Orthogonal: QT = Q−1,eigenvec_val
marriages and one divorce (backward ﬂow).,linear_prog
"One good choice for Q is to apply Gram-Schmidt to the columns of C. Then C = QR,",pos_def_matrices
3. The cost reaches its minimum (or maximum) on the feasible set: the good case.,linear_prog
"4.11 Explain why the point (x,y) is on the line through (2,8) and (4,7) if",determinants
"multiplication ATy. If there are no external sources of current, Kirchhoff’s Current Law",vector spaces
"magic, was the starting point for 20 years of enormous activity in numerical analysis.",computations
"order to have many positive candidates t. For the largest value tmax (which is attained),",eigenvec_val
"inverses, which are also permutation matrices. The inverses satisfy PP−1 = I and are",gauss elim
"30. The parallelogram with sides (2,1) and (2,3) has the same area as the parallelogram",determinants
"(b) Find the determinant if aij = smaller of ni and n j, where n1 = 2, n2 = 6, n3 = 8,",determinants
"is used almost “as is” in commercial interior-point software, and for a large class of",linear_prog
tral problem of applied mathematics. These equations appear everywhere:,vector spaces
and the cube of the ﬁrst is the third.,eigenvec_val
"once, and P2 = P:",vector spaces
This left us with the 2 by 2 determinant. Altogether row 1 has produced 2C11 −C12:,determinants
6.3 Singular Value Decomposition,pos_def_matrices
3)2dx and f3 =,pos_def_matrices
a + ib = reiθ,eigenvec_val
has 0 and 6 in the ﬁrst column for x1 and in the second column for x2:,determinants
"= rand(800) and b = rand(800,1) and B = rand(800,9). Compare the times from",gauss elim
"Suppose we do a series of experiments, and expect the output b to be a linear function",orthogonality
Partial credit if you ﬁnd this determinant when a = b = c = d = 1. Sudden death if,determinants
"is so great that the material becomes plastic, a linear relation b = C +Dt is normal",orthogonality
30. (Recommended) Find Λ and S to diagonalize A in Problem 29. What is the limit of,eigenvec_val
want to ﬁnd the distance from a point b to the line in the direction of the vector a. We,orthogonality
"det(A−λI) = (λ1 −λ)(λ2 −λ)···(λn −λ),",eigenvec_val
T(T(T(v)))? What is T 100(v)?,vector spaces
"convenient. The derivatives of those four basis vectors are 0, 1, 2t, 3t2:",vector spaces
2. The pivot row was very short.,gauss elim
have two roots—but those roots are not real.,eigenvec_val
"Two of them are (1,2,3,4) with no exchanges and (4,3,2,1) with two exchanges.",gauss elim
back one more time to the simplest case. We know that,determinants
"“checkerboard matrix,” with aij = 0 when i+ j is even, aij = 1 when i+ j is odd?",vector spaces
∂x∂y = ∂ 2 f,pos_def_matrices
Problems 18–20 are about vector norms other than the usual ∥x∥ = √x·x.,computations
b that make Ax = b solvable.,vector spaces
would not become 0 = 0.),vector spaces
turned up when we decomposed xTAx into a sum of squares:,pos_def_matrices
are on the diagonal of C. Find C (lower triangular) for,pos_def_matrices
1 2 0 3,vector spaces
are not convenient for A+B. Much better to prove xT(A+B)x > 0.,pos_def_matrices
1.6 Inverses and Transposes,gauss elim
"N(AT), It is the nullspace of the transpose, or the left nullspace of A. ATy = 0 means",vector spaces
"automatically goes to x = A−1b, where the total energy P(x) is a minimum.",pos_def_matrices
"earlier example was EA, because of the elementary matrix E. Later we have PA, or LU,",gauss elim
43. Describe all 1 by 1 matrices that are Hermitian and also unitary. Do the same for 2,eigenvec_val
The plane is three-dimensional with no 4D volume.,gauss elim
"did. That is a fact of mathematics, not of computation—and it remains true in dimension",gauss elim
"Then if visiting team v played home team h, the one with higher potential would win. In",vector spaces
"matrix is 1. To compute the norm, square both sides to reach the symmetric ATA:",computations
"dimensional space,” containing only the point at the origin. Rules (i) and (ii) are satisﬁed,",vector spaces
"On the right-hand side, every integral is zero except one—the one in which sinx multi-",orthogonality
A has positive pivots,pos_def_matrices
"rows of A can be recovered from U. It is true that A and U have different rows, but the",vector spaces
. Applying E32 before E21 gives E21E32b =,gauss elim
c2(c2 +s2) cs(c2 +s2),vector spaces
"1. The row space component also solves ATA�xr = ATb, because Axn = 0.",pos_def_matrices
"are orthogonal, two projections produce zero: PjPi = 0.",eigenvec_val
QT = Q−1 =,orthogonality
"corner. Since the two corners are neighbors, m − 1 basic variables will remain basic.",linear_prog
52. The command N = null(A) will produce a basis for the nullspace of A. Then the,orthogonality
22. Write the most general matrix that has eigenvectors,eigenvec_val
its eigenvalues and eigenvectors.,eigenvec_val
"Starting from x = (0,0,6,12), should x1 or x2 be increased from its current value of",linear_prog
products Fc and F−1y require only 1,orthogonality
Ax = λMx = λRTRx,pos_def_matrices
"Comparing the last matrix with A, the corner LkDkUk coincides with Ak. Then:",determinants
and has orthogonal eigenvectors.,eigenvec_val
"As long as A can be diagonalized, there will be n pure exponential solutions to the",eigenvec_val
This number W is still a root of unity: W n = wnjw−nk is equal to 1 j1−k = 1. Since j,orthogonality
Example 1. The eigenvalues of A are approximately λ1 = 10−4/2 and λ2 = 2:,computations
(c) S is invertible.,eigenvec_val
subject to any other constraint xTv = 0 is not above λ2:,pos_def_matrices
Suppose the largest eigenvalue λn is all by itself; there is no other eigenvalue of the same,computations
I can’t resist putting this orthonormal basis into a square matrix Q. The vector equa-,orthogonality
"Proof. Every matrix, say 4 by 4, has at least one eigenvalue λ1. In the worst case, it",eigenvec_val
"8. If λ1 and µ1 are the smallest eigenvalues of A and B, show that the smallest eigen-",pos_def_matrices
"(c) If A is real, then A+iI is invertible.",eigenvec_val
Algebra. The problem in Chapter 2 was to ﬁnd b in the column space of A. After elim-,linear_prog
over all values of k from 1 to n:,gauss elim
the rule holds for each diagonal entry.,eigenvec_val
Whole line of solutions,gauss elim
2.4 The Four Fundamental Subspaces,vector spaces
Chapter 2 Vector Spaces,vector spaces
Most sets of four vectors in R4 are independent. Those e’s might be the safest.,vector spaces
"Columns 1, 3, 6 of U are a basis for the column space of A—true or false?",vector spaces
"1. Show that v1, v2, v3 are independent but v1, v2, v3, v4 are dependent:",vector spaces
"After this elimination step, write down the upper triangular system and circle the two",gauss elim
diagonal matrix ΣTΣ has the same σ2,pos_def_matrices
"numbers c1 and c2, and they can be added together. When u1 and u2 satisfy the linear",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
3.3 Projections and Least Squares,orthogonality
The dual of this problem is the original minimum problem. There is complete symmetry,linear_prog
(b) what subspace is spanned by the permutation matrices?,vector spaces
"product is zero, and we have a conﬁrmation of a wonderful fact:",eigenvec_val
w2 = (cosθ +isinθ)2 = cos2θ −sin2 θ +2isinθ cosθ.,orthogonality
3. The shifted inverse power method is best of all. Replace A by A−αI. Each eigen-,computations
rule 9 for the determinant of a product:,determinants
"2[row 1] + 3[row 2] = [17 1 0]. Exactly as in elimination, where all this started, each",gauss elim
"25. Choosing the largest available pivot in each column (partial pivoting), factor each A",computations
"Note. I remember the day when a letter came to MIT from a prisoner in New York,",orthogonality
dt2 = Au =,eigenvec_val
0 −8 −2 −12,gauss elim
"in doubt, because its left-hand side is zero. The equations are inconsistent unless b3 −",vector spaces
"around the x, y, and z axes. If P32P31P21A = I, the three robot turns are in A =",computations
"The step ∆x is a multiple of the projection −Pc. The longer the step, the more the",linear_prog
Linear combinations involve two operations—adding vectors and multiplying by scalars.,determinants
"(a) Is this system stable, neutrally stable, or unstable?",eigenvec_val
3.1 Orthogonal Vectors and Subspaces,orthogonality
2 of the spectral theorem:,eigenvec_val
2xTx is reached at that solution point:,pos_def_matrices
eigenvectors and can be diagonalized.,eigenvec_val
"In Figure 8.7, the edge lengths would come in the order 1, 2, 7, 4, 3, 6. The last step",linear_prog
"the sum of |x j|2, and it is also the sum of the energies in the separate frequencies. The",eigenvec_val
"because the eigenvalues 1 and 0 are distinct. The diagonal A was actually Λ, the out-",eigenvec_val
vectors for all four subspaces. Those subspaces play a central role in network theory.,vector spaces
"calculations. If LU is Hertz, then QR is Avis.",orthogonality
The key is to recognize xTAx as xTRTRx = (Rx)T(Rx). This squared length ∥Rx∥2 is,pos_def_matrices
"The unknowns u, v, w, y go into two groups. One group contains the pivot variables,",vector spaces
because the points are not on a line. Therefore they are solved by least squares:,orthogonality
"38. If S is the subspace of R3 containing only the zero vector, what is S⊥? If S is spanned",orthogonality
"row 1 = (1, 0)",determinants
"such matrices, and no others, the triangular T = U−1NU is the diagonal Λ.",eigenvec_val
"32. With b = 0,8,8,20 at t = 0,1,3,4, set up and solve the normal equations ATA�x = ATb.",orthogonality
"computed time and time again, just waiting for their turn to enter the basis. It makes",linear_prog
and is outside the whole analogy.,eigenvec_val
duced a plane. The system Ax = b gives an intersection of planes. Least squares gives,pos_def_matrices
1. Every symmetric matrix (and Hermitian matrix) has real eigenvalues.,eigenvec_val
2. Lengths and Inner Products. Suppose f(x) = sinx on the interval 0 ≤ x ≤ 2π. This,orthogonality
"trix, if they come from different eigenvalues, are orthogonal to one another.",eigenvec_val
has yA ≤ c instead of Ax ≥ b.,linear_prog
What happens to the column picture when the system is singular? it has to go wrong;,gauss elim
That last equation expresses an essential property of orthogonal matrices. When they,eigenvec_val
will ﬁnd the conditions for b to lie in the column space (so that Ax = b is solvable). The,vector spaces
8.2 The Simplex Method,linear_prog
"1.24 If e1,e2,e3 are in the column space of a 3 by 5 matrix, does it have a left-inverse?",vector spaces
Q−1AQ is a matrix that is ready to reveal its eigenvalues—the QR algorithm is ready to,computations
"26. If A is 2 by 3 and C is 3 by 2, show from its rank that CA ̸= I. Give an example in",vector spaces
the most important class of all. A symmetric matrix is a matrix that equals its own,gauss elim
"(d) If A is invertible and B is singular, then AB is singular.",determinants
"5. Add a single inequality constraint to x ≥ 0, y ≥ 0 such that the feasible set contains",linear_prog
M = I −UV,gauss elim
ax + 3y = −3,gauss elim
Problems 15–24 are about the eigenvalue and eigenvector matrices.,eigenvec_val
(b) This is the row xTA =,gauss elim
"question, and his books are now classics.",gauss elim
"This sum is xTy = ∑xiyi = yTx, the row vector xT times the column vector y:",orthogonality
2x + 3y +,gauss elim
"integrate the left side by parts to reach Ay = f, proving that Galerkin gives the same",pos_def_matrices
"If r = |λn−1|/|λn| is close to 1, then convergence is very slow. In many applications",computations
"9. Compute ATA and ATCA, where the 6 by 6 diagonal matrix C has entries c1,...,c6.",vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
"B(I) = B(I) + A(I,J) * X(J)",gauss elim
"some terrible ways which should never be tried, and also some ideas that do deserve a",computations
"The vectors q1,...,qn are orthonormal if",orthogonality
This distance r is the absolute value |a+ib| =,eigenvec_val
relationship between aTb and cosθ:,orthogonality
Problems 3–5 ask for the SVD of matrices of rank 2.,pos_def_matrices
"on the right-hand side (on b). We begin with letters (b1,b2,b3) to ﬁnd the solvability",vector spaces
1. What multiple ℓ of equation 1 should be subtracted from equation 2?,gauss elim
"3.32 In the parallelogram with corners at 0, v, w, and v + w, show that the sum of the",orthogonality
"complex conjugate pair—both have |λ| = ω −1, which is now increasing with ω.",computations
"Use row operations to produce zeros, or use cofactors of row 1.",determinants
40. If W =,orthogonality
"n choices for the ﬁrst column α, n − 1 remaining choices for β, and ﬁnally only one",determinants
"are not independent. Any two of these vectors, say v1 and v2, have both properties—they",vector spaces
(b) The off-diagonals −1 or 0 tell which pairs of nodes are,vector spaces
"Problems 18–20 introduce the idea of an eigenvector and eigenvalue, when a matrix",orthogonality
"to (4,44), what is the new solution?",gauss elim
"(b) What is the only nonzero eigenvalue of P, and what is the corresponding eigen-",eigenvec_val
output b will be the reading on a Geiger counter at various times t. We may know that,orthogonality
"10. If q1 and q2 are the outputs from Gram-Schmidt, what were the possible input vectors",orthogonality
Find the special solutions to Ax = 0 and Bx = 0. Find all solutions.,vector spaces
"spanning tree problem is being solved like back-substitution, with no false steps. This",linear_prog
eigenvalues of a rectangular matrix make no more sense than its determinant.) The,eigenvec_val
Dividing this by detA gives x j. Each component of x is a ratio of two determinants. That,determinants
(b) Prove that the solutions of dX/dt = AX −XA keep the same eigenvalues for all,eigenvec_val
0 0 ∗ ∗ ∗,computations
1 0 1 0,eigenvec_val
If a diagonal entry is zero then elimination will produce a zero row. By rule 5 these,determinants
"16. (a) If A = LDU, with 1s on the diagonals of L and U, what is the corresponding",gauss elim
That is the “expected value” of the error in b—although the error is not really expected,orthogonality
allowed). Elimination solves Ax = b without explicitly ﬁnding A−1.,gauss elim
"b = (a21, a22)",determinants
Example 6. This application is the important one:,pos_def_matrices
"3. The determinant gives a formula for each pivot. Theoretically, we could predict",determinants
37. The text solved du/dt = Ju for a 3 by 3 Jordan block J. Add a fourth equation,eigenvec_val
1 3 3 2,vector spaces
has rank 1. What are the pivots?,vector spaces
"every k, the submatrix Ak is going through a Gaussian elimination of its own.",determinants
"Multiplying by δA ampliﬁes a vector by no more than ∥δA∥, and multiplying by A−1",computations
(b) Compare the main diagonals and then compare the off-diagonals.,gauss elim
"2, found above. The whole factorization is",orthogonality
Chapter 2 Vector Spaces,vector spaces
59. Suppose column 1 + column 3 + column 5 = 0 in a 4 by 5 matrix with four pivots.,vector spaces
n = ∥x∥2. The,orthogonality
37. Polar coordinates satisfy x = rcosθ and y = rsinθ. Polar area J dr dθ includes J:,determinants
38. What happens to the weighted average �xW = (w2,orthogonality
"span the space. (Example: The columns span the column space.) Second, we may be",vector spaces
"11. Compute y = F4c by the three steps of the Fast Fourier Transform if c = (1,0,1,0).",orthogonality
39. Exchange rows and continue with Gauss-Jordan to ﬁnd A−1:,gauss elim
Intertwining of the Eigenvalues,pos_def_matrices
part way through elimination and then stop.,computations
Multiply the three matrices UΣV T to recover A.,pos_def_matrices
operations that have already taken place.,gauss elim
1 1 0 0,vector spaces
"the whole process at once, and see what matrix takes U back to A.",gauss elim
Now we come to the most important question: How do we multiply two matrices? There,gauss elim
We have a right triangle when that sum of cross-product terms xiyi is zero:,orthogonality
"3n3). With each operation we must expect a roundoff error. Normally,",gauss elim
"ple: linear programming is about inequalities, but it is unquestionably a part of linear",linear_prog
The direct approach was “b must be a combination of the columns.” The indirect ap-,orthogonality
14 = 12 + 22 + 32,orthogonality
A. The column space contains all linear combinations of the columns of A. It is a,vector spaces
Remember that the eigenvalue λ = 0 is repeated n−r times.,eigenvec_val
1 2 0 1,vector spaces
"column of U. That comes directly from AV = UΣ, looked at a column at a time.",pos_def_matrices
1. Write out the LDU = LDLT factors of A in equation (6) when n = 4. Find the deter-,gauss elim
w2 is also a combination of the v’s. The coefﬁcients in that combination ﬁll the second,vector spaces
"(b) the determinant of BTB,",eigenvec_val
has λ = −1 and λ = −3.,eigenvec_val
5 because the eigenvalues did. If we choose a matrix with λ1 = 1 and λ2 = 6. we can,eigenvec_val
to ﬁnd its second component. This is the kth Fi-,eigenvec_val
"Remark 1. In spite of this brilliant success in computing A−1, I don’t recommend it, I",gauss elim
2x2 +4x3 = 0. Find a 3 by 3 matrix with that same nullspace.,vector spaces
satisﬁes the rule of linearity:,vector spaces
"is done by repeating Gram-Schmidt, so it is known as the QR method.",computations
is still Fibonacci’s matrix. Add its eigenvectors x1 +x2:,eigenvec_val
singular (fewer than 3 pivots) and explain why that must be the case.,gauss elim
"forming a+b before multiplying, without any penalty.)",gauss elim
6. Use the Gauss-Jordan method to invert,gauss elim
"walls, and it is certainly not orthogonal to itself.",orthogonality
"Given n vectors ai with m components, what are the shapes of A, Q, and R?",orthogonality
"12. Suppose the only eigenvectors of A are multiples of x = (1,0,0). True or false:",eigenvec_val
"the energy P. The trial solution is still u = y1V1 + y2V2 + ··· + ynVn, and the y’s are",pos_def_matrices
vector only)? The nullspace part of any vector x in R2 is xn =,vector spaces
Actually that example is too perfect. It has the nullspace orthogonal to the column,orthogonality
Part of u(0) increases exponentially while the nullspace part stays ﬁxed.,eigenvec_val
"21. Compute the projection matrices aaT/aTa onto the lines through a1 = (−1,2,2) and",orthogonality
2. Find the dimension and construct a basis for the four subspaces associated with each,vector spaces
0+x = x; and there is a vector “−x” satisfying −x+x = 0. Eight properties (including,vector spaces
1 and λ k,eigenvec_val
"difﬁculty is not to solve the equations, but to set them up. For an irregular region we",pos_def_matrices
stay absolutely in parallel. It is the same analogy between the discrete and the continuous,eigenvec_val
The closest point p is just b itself—which is obvious.,orthogonality
"columns of DAT, which can be expensive (although it makes the rest of the calculation",linear_prog
28. Every m by n matrix of rank r reduces to (m by r) times (r by n):,vector spaces
"leaving a combination of x1,...,xk−1, which produces zero. By repeating the same steps",eigenvec_val
. This matrix is constructed as always:,vector spaces
"(2,0) and (−1,0)—and how the whole axis is transformed.",vector spaces
"gular. We no longer have to accept an extra nonzero diagonal below the main one,",computations
28. The matrix A =,eigenvec_val
"The mistake lies in assuming that A and B share the same eigenvector x. In general, they",eigenvec_val
"exchange)? If columns 1 and 2 are the same, which pivot is missing?",gauss elim
The essential thing is to know which properties of the matrix stay unchanged. When,pos_def_matrices
"column as a combination of the ﬁrst two. What are all the solutions (u,v,w) if b is",gauss elim
The columns of A are independent exactly when N(A) = {zero vector}.,vector spaces
tions. Find one that needs four exchanges to reach the identity matrix.,determinants
(I) xTkx > 0 for all nonzero real vectors x.,pos_def_matrices
"If we give the value 1 to the free variable c3, then back-substitution in Uc = 0 gives",vector spaces
"what to buy. In the dual, y∗ ﬁxes the natural prices (shadow prices) at which the economy",linear_prog
u · i = cos θ,orthogonality
"2. Those are at 45° angles with the u-v axes, and they are",pos_def_matrices
"are N(A) and C(A) as before, and then N(AH) and C(AH).",eigenvec_val
"ﬁrst woman and fourth man, because the matrix has a14 = 0.",linear_prog
"Figure 8.6: Two marriages on the left, three (maximum) on the right. The third is created by adding two new",linear_prog
"columns. The second factor is called R, because the nonzeros are to the right of the di-",orthogonality
"41. How many exchanges will permute (5,4,3,2,1) back to (1,2,3,4,5)? How many",gauss elim
Find a particular solution xp and all nullspace solutions xn.,vector spaces
Cofactors of row 1,determinants
length comes from two applications of the Pythagorean formula. The two-dimensional,orthogonality
"3.6 Where is the projection of b = (1,1,1) onto the plane spanned by (1,0,0) and",orthogonality
"The signs of the eigenvalues are often crucial. For stability in differential equations,",pos_def_matrices
"26. A Middle-Aged man was stretched on a rack to lengths L = 5, 6, and 7 feet under",orthogonality
"maximize the interest, with two constraints:",linear_prog
b = Ce−λt +De−µt.,orthogonality
5.5 Does there exist a matrix A such that the entire family A + cI is invertible for all,eigenvec_val
rectangular. Then the volume is the product of the edge lengths: volume = ℓ1ℓ2···ℓn.,determinants
"(ω −1)+(ω −1) = 2−2ω + µ2ω2,",computations
2. The last equation shows the solvability condition b3 +b2 −5b1 = 0. Then 0 = 0.,vector spaces
"(a) All sequences like (1,0,1,0,...) that include inﬁnitely many zeros.",vector spaces
"the feasible set. That intersection occurs at B, where x∗ = 0 and y∗ = 2; the minimum",linear_prog
18. Evaluate detA by reducing the matrix to triangular form (rules 5 and 7).,determinants
"λ and x. It is an algebra problem, and differential equations can be forgotten! The",eigenvec_val
The critical question is: Why do we get zeros off the diagonal? If we combine the,determinants
"the left nullspace, which has dimension m − r = 3 − 2 = 1. It is the last row of L−1P,",vector spaces
0 0 0 1,determinants
(d) A−I can’t be similar to A+I.,eigenvec_val
"We mention here the effect on the matrix of a change of basis, while the linear trans-",vector spaces
8v + 3w =,gauss elim
handle the trivial problem of marriage.,linear_prog
"eigenvectors of ATA, AAT in V, U",eigenvec_val
"errors, so that blunders can be avoided.",computations
"isin2θ. The square of w is still on the unit circle, but at the double angle 2θ. That",orthogonality
A changes to ATA,pos_def_matrices
"we never go as far as ω = 2. The product of the eigenvalues would be too large, and",computations
12. The product of two lower triangular matrices is again lower triangular (all its entries,gauss elim
To ﬁnd a vector p such that,eigenvec_val
v + w =,gauss elim
"But linear algebra is ready to do more, because the second derivatives ﬁt into a symmetric",pos_def_matrices
multiply them you stay inside the group; they have inverses in the group; the identity,gauss elim
22. Reduce A to U and ﬁnd detA = product of the pivots:,determinants
to −A? But show that v(t),eigenvec_val
"approaches, the eigenvectors stay perpendicular. This can fail if A ̸= AT:",eigenvec_val
1. The pivots are the ﬁrst nonzero entries in their rows.,vector spaces
"equation detAB = (detA)(detB) is easily veriﬁed. By rule 8, it becomes 0 = 0.",determinants
Each of the following tests is a necessary and sufﬁcient condition for the,pos_def_matrices
7. Construct the matrix with rank 1 that has Av = 12u for v = 1,pos_def_matrices
Example 6. Application to difference and differential equations (powers and expo-,eigenvec_val
Problems 10–19 study elimination on 3 by 3 systems (and possible failure).,gauss elim
A basis for V is a sequence of vectors having two properties at once:,vector spaces
"We regret to say that for the matrix B, direct Gaussian elimination is a poor algorithm.",gauss elim
0 3 3 3,vector spaces
"starting conditions can lead to “superdecay” at the rate e−3t, In fact, those conditions",eigenvec_val
I will be in trouble if that example from relativity goes any further. The point is that ,gauss elim
Problems 31–35 may be harder. The input space V contains all 2 by 2 matrices,vector spaces
"b(1, 1, 2) = point of intersection",gauss elim
integers. Give a 2 by 2 example.,determinants
"ﬁrst is a subset of Rn, marked out by x ≥ 0 and Ax ≥ b. The second is a subset of Rm,",linear_prog
"Remark 3. In the Gauss-Jordan calculation we went all the way forward to U, before",gauss elim
"exact arithmetic, counting the pivots is correct. Real arithmetic can be misleading—but",pos_def_matrices
A “cut” splits the nodes into two groups S and T (source in S and sink in T). Its capacity,linear_prog
ﬂow is possible. If you label each node in S by the previous node that ﬂow could come,linear_prog
(b) An invertible matrix has no free variables.,vector spaces
Figure 3.9: Straight-line approximation matches the projection p of b.,orthogonality
"we think of z as a 1 by 1 matrix, r corresponds to a positive deﬁnite matrix and eiθ",pos_def_matrices
"At a stationary point, grad F = (∂F/∂x1,...,∂F/∂xn) is a vector of zeros. The second",pos_def_matrices
nant? What is A?,eigenvec_val
"agreement is at x = 0, where c0 +···+cn−1 = y0. The remaining points bring powers of",orthogonality
"2.3 Linear Independence, Basis, and Dimension",vector spaces
. Write the eigenvalues and eigen-,pos_def_matrices
"These are two of the six ± signs. For n = 2, we only have (1,2) and (2,1):",determinants
"from Ax = λx, and multiply again by A:",eigenvec_val
But the major axis of the cross section is at least as long as the second axis of the original,pos_def_matrices
The factorization A = LU is so important that we must say more. It used to be missing,gauss elim
+ w = 4,gauss elim
Find a basis for the subspace that has y(0) = 0.,vector spaces
"3. Reﬂection Figure 2.11 shows the reﬂection of (1,0) in the θ-line. The length of the",vector spaces
of Rn. What inequality on p+q guarantees that V intersects W in a nonzero vector?,orthogonality
into the pivot. In this example the matrix would become upper triangular:,gauss elim
reason for assuming the same exponent λ for both unknowns; it leaves,eigenvec_val
numbers that have x−1 = x?,eigenvec_val
"we can work with the nonsingular A+ εI and A−εI, and at the end let ε → 0.)",pos_def_matrices
"1.18 If A is an n by n matrix such that A2 = A and rankA = n, prove that A = I.",vector spaces
The length squared is the inner product of x with itself: xTx = x2,orthogonality
verify R2 = A for,pos_def_matrices
35. Diagonalize A (real λ’s) and K (imaginary λ’s) to reach UΛUH:,eigenvec_val
"If w1 > w2, more importance is attached to b1. The minimizing process (derivative = 0)",orthogonality
"14. Show that every number is an eigenvalue for T f(x) = d f/dx, but the transformation",eigenvec_val
"This is an orthogonal matrix, so by Property 3′ it must have orthogonal eigenvectors.",eigenvec_val
The lower right-hand corner remains symmetric:,gauss elim
"Since the ﬁrst three spaces were C(A), N(A), and C(AT), the fourth space must be",vector spaces
4.3 Formulas for the Determinant,determinants
I want to split xTAx into xTLDLTx:,pos_def_matrices
"36. Without multiplying matrices, ﬁnd bases for the row and column spaces of A:",vector spaces
dt = Au =,eigenvec_val
spaces) is a subspace of R5. Check the requirements on x+y and cx.,vector spaces
1 0 0 0,linear_prog
9. Decide the stability of u′ = Au for the following matrices:,eigenvec_val
"i b, as in equation (4).",orthogonality
Figure 1.3: The row picture: three intersecting planes from three linear equations.,gauss elim
A is invertible—between row space and column space. The pseudoinverse knocks out,pos_def_matrices
5.2 Diagonalization of a Matrix,eigenvec_val
has λ > 0 but 1,pos_def_matrices
Matrix Notation and Matrix Multiplication,gauss elim
"To ﬁnd q3, subtract from c its components along q1 and q2:",orthogonality
Example 3. Polar decomposition:,pos_def_matrices
"and Princeton beats Harvard. More than that, the score differences in that loop of games",vector spaces
because it managed to avoid matrix notation almost completely (by the skillful device of,linear_prog
"The answer is: With a full set of n pivots, there is only one solution. The system is",gauss elim
"of alternating direction, in which the splitting separated the tridiagonal matrix in the x",computations
Figure 3.6: The cosine of the angle θ = β −α using inner products.,orthogonality
0 0 1 0,vector spaces
0 cN − cBB−1N −cBB−1b,linear_prog
"Therefore the matrix AD takes the place of A, and the vector cTD takes the place of cT.",linear_prog
"the previous equation, the vector x2 disappears:",eigenvec_val
"(c) If G0 = 0 and G1 = 1, show that the Gibonacci numbers approach 2",eigenvec_val
"We will try to follow this outline, which opens up a range of new applications for",orthogonality
Proof. Suppose Ax = λx. The trick is to multiply by xH: xHAx = λxHx. The left-hand,eigenvec_val
"(ii) For reﬂections, that same basis gives H =",vector spaces
"The closest polynomial of degree ten is now computable, without disaster, by projecting",orthogonality
Every component of the corresponding eigenvector x1 is also positive.,eigenvec_val
= linear combination equals b,gauss elim
"22. For which right-hand sides (ﬁnd a condition on b1, b2, b3) are these systems solvable?",vector spaces
We are actually continuing a discussion that began in Chapter 1 with,computations
Every linear dependence Ax = 0 among the columns of A is matched by a dependence,vector spaces
is to give a physical interpretation of the equation and its solution. It is the kind of,eigenvec_val
"3n3 multiplications and additions, the",computations
"to V. Their product AB starts with a vector u in U, goes to Bu in V, and",vector spaces
"1. Prove that every third Fibonacci number in 0,1,1.2,3,... is even.",eigenvec_val
“natural” condition on u′ need not be imposed on the trial functions V. With h = 1,pos_def_matrices
"product of Ax with y equals the inner product of x with ATy. Formally, this",orthogonality
"Diagonalization can fail only if there are repeated eigenvalues. Even then, it does not",eigenvec_val
"try, by actually going through the elimination process.",gauss elim
(b) reﬂect every vector through the x-y plane?,vector spaces
"d1d2···dk−1, we can isolate each pivot dk as a ratio of determinants:",determinants
c d] times [ d −b,gauss elim
"x +2y ≥ 4, x ≥ 0, and y ≥ 0. A feasible set is composed of the solutions to a family of",linear_prog
"A solution is basic when n of its m+n components are zero, and it is feasible",linear_prog
"7. If we wanted to maximize instead of minimize the cost (with Ax = b and x ≥ 0),",linear_prog
−I) and K−1 is rotation through −90°:,eigenvec_val
"18. If A = QR, ﬁnd a simple formula for the projection matrix P onto the column space",orthogonality
ATA�x = ATb for the closest polynomial of degree ten.,orthogonality
"trices have all nonzeros close to the main diagonal, and Ax = b is easy to solve. In linear",linear_prog
0 1 4 0,vector spaces
much better to choose the x that minimizes an average error E in the m equations.,orthogonality
"The same result comes from diagonalization, by squaring S−1AS = Λ:",eigenvec_val
holder transformation with v = x−y gives Hx = y and Hy = x.,computations
that term survives when we multiply by qT,orthogonality
(b) Find a matrix with that subspace as its nullspace.,vector spaces
(Ux)H(Uy) = xHy and ∥Ux∥ = ∥x∥,eigenvec_val
We solve this Markov difference equation using uk = SΛkS−1u0. Then we show that,eigenvec_val
is no shortage of eigenvectors in that case.,eigenvec_val
(a) Give a basis for the nullspace and a basis for the column space.,eigenvec_val
The least-squares method selects p as the best choice to replace b. There can be no,orthogonality
0 0 1 −1 1,vector spaces
detA = ±detLdetDdetU = ±(product of the pivots).,determinants
3.3 Projections and Least Squares,orthogonality
(b) f = 2(x2,pos_def_matrices
cation are all right. We give three examples:,vector spaces
1x1 + ··· + cnλ k,eigenvec_val
just F/n. In a moment we will look at the complex number w = e2πi/n (which equals i,orthogonality
"The most important example is the standard basis. For the x-y plane, the best-known",orthogonality
equations ATA�x = ATb (solution not required). You are now ﬁtting a parabola to four,orthogonality
(b) What triangular systems will give the solution to ATy = b?,gauss elim
"by the “triangle inequality.” This |z| is a nonnegative vector, so |λ| is one of the possible",eigenvec_val
All Ak are similar,computations
ﬁrst column of the identity: Ax1 = e1. Similarly Ax2 = e2 and Ax3 = e3 the e’s are the,gauss elim
is an eigenvalue of B = I −A. The real eigen-,computations
"F1 = 1, and working all the way out to F1000? The goal is to solve the difference equation",eigenvec_val
���� −→ 1−2t −3t2.,vector spaces
that x+ = A+b satisﬁes the normal equation as it should?,pos_def_matrices
this “nearly singular” matrix is as well-conditioned as possible.,computations
of applying the elementary operations GFE to the identity matrix.) Now the second half,gauss elim
This is PF = FΛ or P = FΛF−1.,orthogonality
"But the dimension of its column space is also r, so",vector spaces
7. Give 3 by 3 examples (not just the zero matrix) of,gauss elim
"∥A∥ is the same as λmax, and ∥A−1∥ is the same as 1/λmin. The correct replacement for",computations
(2) If A2 +A = I then A−1 = A+I.,gauss elim
0 1 0 0,determinants
27. Suppose A and B have the same reduced-row echelon form R. Explain how to change,vector spaces
individual vectors. That honor belongs to A−1 if it exists—and it only exists if r = m = n.,orthogonality
(c) the columns of a 3 by 5 echelon matrix with 2 pivots.,vector spaces
"nodes k. The balance in equation (1) can be written as Ax = 0, where A is a node-edge",linear_prog
"between stable with real eigenvalues, stable with complex eigenvalues, and unstable?",eigenvec_val
Applications of positive deﬁnite matrices are developed in my earlier book Intro-,pos_def_matrices
"In practice, the important question is the choice of C. The best answer comes from",orthogonality
Explain why A100 is close to A∞:,eigenvec_val
nullspace. If f is in the row space and x is in the nullspace then f Tx = 0.,vector spaces
54. When you transpose a block matrix M = [ A B,gauss elim
Here is the fundamental formula of this section: SeΛtS−1u(0) solves the differential,eigenvec_val
inverse and the system Ax = b has one solution. In exceptional cases the method,gauss elim
"Nullspace: dimension 1, contains x = (1,...,1).",vector spaces
"To establish the formula for (A−1)T, start from AA−1 = I and A−1A = I and take trans-",gauss elim
eventually it runs down. Unstable ﬁssion is a bomb.,eigenvec_val
projection of b onto the plane of a1 and a2?,orthogonality
space and will automatically be satisﬁed in every subspace. Notice in particular that the,vector spaces
"8. If A is symmetric positive deﬁnite and C is nonsingular, prove that B = CTAC is also",pos_def_matrices
"not only in the solution of Au = f and Ax = λx, but in its formulation.",pos_def_matrices
"of A to a22 = 1, it is singular. Consider two very close right-hand sides b:",gauss elim
4. The rows are linearly independent.,vector spaces
remaining n − 1 equations form an edge that comes out of the corner. This edge,linear_prog
were sent to Bellevue Hospital with a warning that the quality and contents of,vector spaces
"it is Mu′′ +Au = 0, with a mass matrix M. The eigenvalue problem arises when we look",pos_def_matrices
"Of course every matrix has a nullspace. It was ridiculous to suggest otherwise, but you",eigenvec_val
"order n is genuine. The better we approximate −u′′ = f, by increasing the number of",computations
9. (a) How is the determinant of AH related to the determinant of A?,eigenvec_val
26. The columns of AB are combinations of the columns of A. This means: The column,vector spaces
"7. Suppose that λ is an eigenvalue of A, and x is its eigenvector: Ax = λx.",eigenvec_val
Complex Hermitian: AT = A,eigenvec_val
"to 1, x, and x2 − 1",orthogonality
"Example 1. Project b = (1,2,3) onto the line through a = (1,1,1) to get �x and p:",orthogonality
"43. From this general solution to du/dt = Au, ﬁnd the matrix A:",eigenvec_val
2. Its smallest value is certainly Pmin = 0.,pos_def_matrices
"multiplication above had x1 = 2, x2 = 5, x3 = 0.) Normally x is written as a column",gauss elim
part; every coefﬁcient ai − bi must be zero. Therefore ai = bi. There is one and only,vector spaces
(F) un+1 −un = Aun or un+1 = (I +A)un (this is Euler’s method).,eigenvec_val
"(a) For A = one block, ﬁnd Mi = permutation so that M−1",eigenvec_val
Example 3. (with all pivots and multipliers equal to 1),gauss elim
1 0 0 0,determinants
"columns, so v and y are free variables.",vector spaces
"the power of the computer—in constructing a discrete approximation, solving it, and",pos_def_matrices
55. Explain why the inner product of x and y equals the inner product of Px and Py.,gauss elim
a case of existence but not uniqueness. The matrix A has no left-inverse because the last,vector spaces
"Show that A cannot be invertible. The third row of A−1, multiplying A, should give",gauss elim
tem of order n could not be solved with much fewer than n3/3 multiplications. (There,gauss elim
"side, and A multiplies each column separately. Every column of AB is a combination",gauss elim
"plane moved up to meet the others, and there is a line of solutions. Problem 1.5c is still",gauss elim
"We may start from a single equation of higher order, like y′′′ −3y′′ +2y′ = 0. To convert",eigenvec_val
0 0 0 1,determinants
f2 = x2 −y2,pos_def_matrices
"matrices; they are matrices with only one column. As with vectors, two matrices can be",gauss elim
Example 1. The projection A =,eigenvec_val
"(a) If λj is positive, infer that every S j contains a vector x with R(x) > 0.",pos_def_matrices
"the origin, as any subspace must.) We want to be able, for any system Ax = b, to ﬁnd",vector spaces
Only the solutions to a homogeneous equation (b = 0) form a subspace. The nullspace,vector spaces
32. Apply any three tests to each of the matrices,pos_def_matrices
Chapter 2 Vector Spaces,vector spaces
26. Draw the tilted ellipse x2 +xy+y2 = 1 and ﬁnd the half-lengths of its axes from the,pos_def_matrices
"erties are used, the result is always the same—the deﬁning properties are self-consistent.",determinants
3. A is arbitrary: The unitary U can be chosen so that U−1AU = T is triangular.,eigenvec_val
Problems 29–33 are about powers of matrices.,eigenvec_val
"column 1, it takes n operations for every zero we achieve—one to ﬁnd the multiple ℓ,",gauss elim
"33. Find the pieces xr and xn, and draw Figure 3.4 properly, if",orthogonality
f(x); they are projected onto the sines and cosines; that produces the Fourier coefﬁcients,orthogonality
(a) subtract 2 times the ﬁrst equation from the second,gauss elim
"axis is one-third as long, since we need (1",pos_def_matrices
and so on. Then the total for back-substitution is 1+2+···+n.,gauss elim
33. For C =,pos_def_matrices
"Before going back to the application (the differential equation), we emphasize the",eigenvec_val
b. That column picture in Figure 1.6b corresponds to the row picture in Figure 1.5c.,gauss elim
What is the nullspace matrix N containing the special solutions?,vector spaces
Possibly Ax is ahead of x. Possibly Ax is behind x. Sometimes Ax is parallel to x. At that,eigenvec_val
Singular case: Column picture,gauss elim
24. Find the fourth Legendre polynomial. It is a cubic x3+ax2+bx+c that is orthogonal,orthogonality
"(iii) If the vectors x and y go to x′ and y′, then their sum x+y must go to x′ +y′—since",vector spaces
5. A diagonal matrix like Λ =,eigenvec_val
"choose that vector, x+ = xr, as the best solution to Ax = b.",pos_def_matrices
"their lengths, M is a",vector spaces
2. Minimize E2 =,orthogonality
"We can invert each step of elimination, by using E−1 and F−1 and G−1. I think it’s",gauss elim
"Thus x and Bx are both eigenvectors of A, sharing the same λ (or else Bx = 0). If we",eigenvec_val
37. Diagonalize this unitary matrix V to reach V = UΛUH. Again all |λ| = 1:,eigenvec_val
"If x and y are feasible in the primal and dual problems, then yb ≤ cx.",linear_prog
"The other goal is to illustrate, by this same application, the special properties that co-",gauss elim
points x on the interval from 0 to 2π:,orthogonality
"(b) B2 = 0, although B ̸= 0.",gauss elim
symmetric matrix tridiagonal and ends by making it virtually diagonal. That second step,computations
Figure 6.7: Hat functions and their linear combinations.,pos_def_matrices
"that a, b, c were not independent in the ﬁrst place) What is left is the component C we",orthogonality
"tional, and normally LU is different from LDU (also written LDV).",gauss elim
"each vector (x,y) in the plane to the nearest point (x,0) on the hor-",vector spaces
"For this optional topic, the key is to ﬁnd a constrained minimum or maximum. The",pos_def_matrices
This combination of properties is absolutely fundamental to linear algebra. It means,vector spaces
"value is now −ω2, so the frequency ω is connected to the decay rate λ by the law",eigenvec_val
The sum of the n eigenvalues equals the sum of the n diagonal entries:,eigenvec_val
"three given vectors v1,v2,v3? When will that matrix be invertible?",vector spaces
"To see once more that both the row and column spaces of U have dimension r, con-",vector spaces
"pass. The top of the pass is a minimum as you look along the range of mountains, but it",pos_def_matrices
0 0 1 2,vector spaces
(c) Any four of those vectors (are)(are not)(might be) a basis for R4.,vector spaces
"in linear programming, with two sources of protein—say steak and peanut butter. Each",linear_prog
There is another remarkable thing about this space: It is found under a great many,orthogonality
ellipse corresponds to the smallest eigenvalue of A.,pos_def_matrices
"matrix is rather exceptional, because its column space contains only the zero vector. By",vector spaces
"In calculus, everybody knows the condition for a maximum or a minimum: The ﬁrst",linear_prog
game involves random strategies).,linear_prog
λ1(A) ≤ λ1(B) ≤ λ2(A) ≤ λ2(B) ≤ ··· ≤ λn−1(B) ≤ λn(A).,pos_def_matrices
0 0 0 0,vector spaces
will change but stay positive. The choice of edge (see Example 2 below) decides which,linear_prog
"at (0,0,0), (1,1,0), (1,0,1), and (0,1,1)—note that all six edges have length",orthogonality
U must be the eigenvector matrix for AAT. The eigenvalue matrix in the middle is ΣΣT—,pos_def_matrices
Chapter 7 Computations with Matrices,computations
"U3 will take care of the third column. For a 5 by 5 matrix, the Hessenberg form is",computations
"This can only be appreciated by an example. Suppose A is of order 21, which is very",computations
ucts and inverses stay in the group.” Which of these sets are groups? Positive deﬁnite,pos_def_matrices
2x1 −4x2 +x3 +x4,linear_prog
"If there is any justice, orthonormal columns should make the problem simple. It",orthogonality
written down immediately. If the computer stores each multiplier ℓij—the number that,gauss elim
"(b) The intersection of a plane through (0,0,0) with a line through (0,0,0) is prob-",vector spaces
"It also preserves inner products and angles, since (Qx)T(Qy) = xTQTQy = xTy.",orthogonality
(b) A is diagonalizable.,eigenvec_val
umn of the old B leaves the basis (xk becomes 0) and the new column u enters.,linear_prog
The initial displacement u(0) is easy to keep separate: t = 0 means that sinωt = 0 and,eigenvec_val
"36. If E adds row 1 to row 2 and F adds row 2 to row 1, does EF equal FE?",gauss elim
"y′(0) = 4. Check that your (y,y′) satisﬁes y′′ = 0.",eigenvec_val
uncoupled because Λ = S−1AS is diagonal. The eigenvectors evolve independently. This,eigenvec_val
This process is called back-substitution.,gauss elim
8.1 would look almost the same in three dimensions. The boundary becomes a plane,linear_prog
y1 −y3 −y4 = 0,vector spaces
on the right-hand half must have carried I into A−1. Therefore we have computed the,gauss elim
"a basis to convert geometric constructions into algebraic calculations, and we need an",orthogonality
space doesn’t get larger by including b?,vector spaces
orthogonality: xHy = 0,eigenvec_val
"which are the whole object of orthogonalization, are in the ﬁrst factor Q.",orthogonality
and no more can get across! Weak duality says that every cut gives a bound to the total,linear_prog
spectral theorem says that every symmetric matrix is a combination of n projection,eigenvec_val
"2. Since the ﬁrst ratio is smaller, the ﬁrst unknown w (and the ﬁrst column of",linear_prog
by the bold lines in Figure 1.2b. The unknowns are the numbers x and y that multiply,gauss elim
"16. Show that f(x,y) = x2 + 4xy + 3y2 does not have a minimum at (0,0) even though",pos_def_matrices
"leads to a “formal solution”: to Cramer’s rule for x = A−1b, and to the polynomial",eigenvec_val
"determinant can actually be used if n = 2 or 3. For large n, computing λ is more difﬁcult",eigenvec_val
"duces zero, multiply by A, subtract λk times the original combination, and xk disappears—",eigenvec_val
1. The vectors are linearly independent (not too many vectors).,vector spaces
1.6 Inverses and Transposes,gauss elim
"1. For these matrices, ﬁnd the only nonzero term in the big formula (6):",determinants
4.4 Applications of Determinants,determinants
−1 −3 3 4,vector spaces
turns up on both sides of the weighted normal equations:,orthogonality
2 came from one division. The new pivot 3,gauss elim
28. True or false (give a good reason)?,vector spaces
rook can take any other rook.,linear_prog
"(b) the three vectors (0,1,1) and (1,1,0) and (0,0,0).",vector spaces
coefﬁcient must be positive:,pos_def_matrices
39. Do A and CTAC always satisfy the law of inertia when C is not square?,pos_def_matrices
1.7 Special Matrices and Applications,gauss elim
"Suppose ﬁrst that k = 2, and that some combination of x1 and x2 produces zero:",eigenvec_val
The reason is this: Ax = 0 exactly when Ux = 0. The two systems are equivalent and,vector spaces
matrix contains only b = 0.,vector spaces
"pared with the “theoretical” description—ﬁnd A−1, and multiply A−1b—our description",gauss elim
"S is an eigenvector, and so is every vector in S⊥. What are the eigenvai (Note the",eigenvec_val
Example 2. One from each quadrant: only #2 is stable:,eigenvec_val
equation is exactly the content of yTA = 0.,orthogonality
the third row [0 0 1 0] of A−1A = I. Why is this impossible?,gauss elim
"Another constraint is still to be heard from. It is the “conservation law,” that the ﬂow",linear_prog
"The problem is to ﬁnd the unknown values of u, v, and w, and we shall apply Gaussian",gauss elim
v + w =,gauss elim
3w = 6 and 4w = 8—then this singular case has an inﬁnity of solutions. We know that,gauss elim
nullspace N(AT)—which is orthogonal to the column space.,orthogonality
Remark 6. The higher-degree terms in F have no effect on the question of a local min-,pos_def_matrices
deeper aspects in this chapter. They are:,gauss elim
Example 4. Reverse polar decomposition:,pos_def_matrices
"corresponds to an orthogonal matrix. More exactly, since eiθ is complex and satisﬁes",pos_def_matrices
is not similar to,eigenvec_val
"the SVD—this has become much more efﬁcient, but it is expensive for a big matrix.",pos_def_matrices
tioned: c(Q) = 1. The change δλ in the eigenvalues is no greater than the change δA.,computations
"The special thing is that the entries below the diagonal are the multipliers ℓ = 2, −1,",gauss elim
"The other pair of orthogonal subspaces comes from ATy = 0, or yTA = 0:",orthogonality
What are the two pure exponential solutions?,eigenvec_val
sum of squares of the two errors.,orthogonality
"4. If a 3 by 3 upper triangular matrix has diagonal entries 1, 2, 7, how do you know it",eigenvec_val
"ﬁts the data b. In other words, the vector b probably will not be a combination of the",orthogonality
That term in L is chosen exactly so that ∂L/∂y = 0 brings back Cx = d. When we set,pos_def_matrices
"produced the exact answer in a ﬁnite time. (Or equivalently, Cramer’s rule gave an exact",eigenvec_val
AB times C equals A times BC. If C happens to be just a vector (a matrix with only one,gauss elim
is the relation of A2 to A?,eigenvec_val
"triangular L. Instead it is the product Q = H1H2···Hn−1, which can be stored in this",computations
0 1 0 0 0,vector spaces
the “real part” of Z is half of Z +ZH. Find a similar formula for the “imaginary part”,eigenvec_val
found a new way to accomplish exactly the same thing. A Householder transformation,computations
x = xp +xn,vector spaces
"(ii) This second proof is less elegant. For a diagonal matrix, detDB = (detD)(detB)",determinants
v + w =,gauss elim
from elimination; that PA can have zeros on the diagonal.),determinants
The complex conjugate of a + ib is the number a − ib. The sign of the imaginary,eigenvec_val
u and v that multiply the ﬁrst and second columns to produce b. The system is solvable,vector spaces
"ated by the special vectors (−3,1,0,0) and (1,0,−1,1). The combinations of these two",vector spaces
A 1% change in b has produced a hundredfold change in x; the ampliﬁcation factor is,computations
(b) Find 3 equations in 3 unknowns for the best least-squares solution.,orthogonality
J = M−1AM =,eigenvec_val
r′ = |λn−p|/|λn|. We will obtain approximations to p different eigenvalues and their,computations
eλt = eateibt = eat(cosbt +isinbt),eigenvec_val
"skips the edge of length 5, which closes a loop. The total length is 23—but is it minimal?",linear_prog
pivot entries are all nonzero whenever the numbers detAk are all nonzero:,determinants
"of detA will indicate whether the edges form a “right-handed” set of coordinates, as in",determinants
Problems 34–44 are about orthogonal subspaces.,orthogonality
a b 0 0,gauss elim
3x − 2y =,gauss elim
"any R by a matrix Q with orthonormal columns, then (QR)T(QR) = RTQTQR = RTIR =",pos_def_matrices
which order of columns (if any) gives all 1s? Explain why that permutation is even,determinants
show that A has no square root. Change the diagonal entries of A to 4 and ﬁnd a,eigenvec_val
A rectangular matrix brings new possibilities—U may not have a full set of pivots. This,vector spaces
"sensitive that this roundoff error completely changes the solution, then almost certainly",computations
"customer has no economic reason to prefer vitamins over food, even though the druggist",linear_prog
"deﬁnition in the 2 by 2 case,",determinants
"Important: The multiplication AB can also be done a row at a time. In Example 1, the",gauss elim
triangular matrices are diagonal.,eigenvec_val
"sible. Therefore its Phase I is already set, and the simplex method can proceed",linear_prog
times equation 1 and ℓ32 =,gauss elim
27. U comes from A by subtracting row 1 from row 3:,vector spaces
Figure 4.1: The box formed from the rows of A: volume = |determinant|.,determinants
The second point is as follows.,gauss elim
"The dual problem starts from the same A, b, and c, and reverses everything. In the",linear_prog
15. Invert the three factors in equation (14) to ﬁnd a fast factorization of F−1.,orthogonality
on the θ-line are projected to themselves! Projecting twice is the same as projecting,vector spaces
is consistent with the row operations of elimination. We can write the result either as,gauss elim
Each Jordan block Ji is a triangular matrix that has only a single eigenvalue λi,eigenvec_val
S−1T has |λ|max =,computations
elementary operations and their invariants for xTAx?,pos_def_matrices
32. Suppose A =,vector spaces
"tion IV implies condition I, and the proof is complete.",pos_def_matrices
A product AB of invertible matrices is inverted by B−1A−1:,gauss elim
(ad− bc)/a (af −ec)/a,determinants
"q r ], write out R2 and check that it is positive deﬁnite unless R is singular.",pos_def_matrices
"E(Ax) = Eb, applying E to both sides of our equation, or as (EA)x = Eb. The matrix",gauss elim
such that AC = Im (m by m). This is possible only if m ≤ n.,vector spaces
1. C(A) = column space of A; dimension r.,vector spaces
17. All entries in the factorization of F6 involve powers of w = sixth root of 1:,orthogonality
"Therefore the number of eigenvalues to the right of zero, and the number to the left, is",pos_def_matrices
possible. How do you prove that the maximal ﬂow is 6 and not 7?,linear_prog
"equations Ax = b. Solve them by elimination, This cubic now goes exactly through",orthogonality
Of course any multiples x/α and y/β are equally good as eigenvectors. MATLAB,eigenvec_val
"If Q has orthonormal columns, the least-squares problem becomes easy:",orthogonality
Note 3. The singular values of A in the SVD are the square roots of the eigenvalues of,computations
"by the 1, −2, 1 matrix A turn into second derivatives. This limit is described by the",eigenvec_val
"2. Squaring w produces (1+2i+i2)/2, which is i—because",orthogonality
with λ = 3 and 1:,pos_def_matrices
"∂x2 (α,β)+xy ∂ 2F",pos_def_matrices
"We need one more matrix, and fortunately it is much simpler than the inverse. The",gauss elim
(AT)ij = A ji.,gauss elim
"(x, y) = (2, 3)",gauss elim
3 will be constant?,eigenvec_val
−u j+1 +2u j −u j−1 = h2 f(jh),gauss elim
"and eventually to a very small F, the usual n2 steps are reduced to 1",orthogonality
1. There was only one nonzero entry below the pivot.,gauss elim
0 0 1 1,vector spaces
and side walls that do not meet at a right angle. The line along the corner is in both,orthogonality
"If λ1 = 1, (I −A)−1 fails to exist.",eigenvec_val
"1.11 (a) Find the rank of A, and give a basis for its nullspace.",vector spaces
"68. A square northwest matrix B is zero in the southeast corner, below the antidiagonal",gauss elim
so does the opposite vector −x. The important step is to go from the identity matrix to a,pos_def_matrices
38. Invert these matrices A by the Gauss-Jordan method starting with [A I]:,gauss elim
3u + v + 4w = 6.,gauss elim
λ1 and the closet x = xn/√λn both give xTAx = xTλx = 1. These are the major,pos_def_matrices
"too small to be typical, the convergence is fast.",computations
Chapter 7 Computations with Matrices,computations
bases for the row space of U.,vector spaces
"19. In Problem 17 the columns are (1,1,2) and (1,2,3) and (1,1,2). This is a “singular",gauss elim
+x3 +6x4 +2x5 = 8,linear_prog
see how Lc = b is solved forward (c1 comes before c2). This is precisely what happens,gauss elim
"pivot variables. Setting the n free variables to zero, the m equations Ax = b determine",linear_prog
We have not yet shown that there is a matrix A+ that always gives x+—but there is.,pos_def_matrices
Note 5. A 2 by 2 matrix is invertible if and only if ad −bc is not zero:,gauss elim
"22. If (a,b) is a multiple of (c,d) with abcd ̸= 0, show that (a,c) is a multiple of (b,d).",gauss elim
We want to prove that c∗x = y∗b. It may seem obvious that the druggist can raise,linear_prog
35. True or false?,vector spaces
and G were not disturbed. They went straight into the product. It is the wrong order for,gauss elim
"For b = (2,5,7) this was possible; for b = (2,5,6) it was not. The reason is that those",gauss elim
interval 0 ≤ x ≤ 1?,orthogonality
row 3 of U,gauss elim
"third plane is not parallel to the other planes, but it is parallel to their line of intersection.",gauss elim
"instead of 0, F(x) and all derivatives are computed at x0. Then x changes to x − x0 on",pos_def_matrices
eiθe−iθ = (cosθ +isinθ)(cosθ −isinθ) = cos2θ +sin2θ = 1.,orthogonality
length ℓ = |a|,determinants
"have 0 = 0. If A is nonsingular, then it allows the factorization PA = LDU, and we apply",determinants
"(−3,1,0,0) has free variables v = 1, y = 0. The other special solution (1,0,−1,1) has",vector spaces
"Now we face the second difﬁculty. To make �x as short as possible, we choose the",pos_def_matrices
"9. (a) If P = PTP, show that P is a projection matrix.",orthogonality
"Now we go in the other direction. If all λi > 0, we have to prove xTAx > 0 for",pos_def_matrices
E2 = (2x−b1)2 +(3x−b2)2 +(4x−b3)2.,orthogonality
"23. In Problem 22, applying E21 and then E32 to the column b = (1,0,0) gives E32E21b =",gauss elim
= (ℓ31 Row 1 +ℓ32 Row 2+1 Row 3) of,gauss elim
"(a) Explain why Ax = (1,0,0) cannot have a solution.",gauss elim
v = 0 and y = 1. All solutions are linear combinations of these two. The best way to ﬁnd,vector spaces
tion. And it leads to a factorization A = QR that is nearly as famous as A = LU.,orthogonality
+ 4v + 2w = −2,gauss elim
In fact we look at one special tridiagonal matrix.,gauss elim
"In three-dimensional space, x = (x1,x2,x3) is the diagonal of a box (Figure 3.1b). Its",orthogonality
the eigenvalues e0 and e1?,orthogonality
Chapter 8 Linear Programming and Game Theory,linear_prog
u + 2v + 3w = 0,gauss elim
1 1 0 1,determinants
What is the matrix for T when the input and output bases are the v’s?,vector spaces
We call S the “eigenvector matrix” and Λ the “eigenvalue matrix”—using a capital,eigenvec_val
"add to zero, Around the upper loop in Figure 2.6, the differences satisfy (x2−x1)+(x3−",vector spaces
one free variable equal to 1. The pivot variables in that special solution can be,vector spaces
"When the initial u0 is an eigenvector x, this is the solution: uk = λ kx. In general u0",eigenvec_val
The factor S =VΣV T is symmetric and semideﬁnite (because Σ is). The factor Q =UV T,pos_def_matrices
Find bases for the two column spaces. Find bases for the two row spaces. Find bases,vector spaces
16. Write one signiﬁcant fact about the eigenvalues of each of the following.,eigenvec_val
3. Ordinarily the eigenvalue test is the,pos_def_matrices
has a minimum when the pure quadratic xTAx is positive deﬁnite. These second-order,pos_def_matrices
Each simplex step exchanges a column of N for a column of B. Those columns are,linear_prog
for some y with,linear_prog
"the whole space Rm. Together with its perpendicular space, it gives one of our two",vector spaces
The solution is x =,gauss elim
"druggist’s price, since y1 + 3y2 ≤ 4 is a strict inequality 1",linear_prog
This case is by far the most common and most important. Independence is not so,orthogonality
A formula for p is easy when the subspace is a line. We will project b onto a in several,orthogonality
"into QR, recognizing that the ﬁrst column is already a unit vector.",orthogonality
a − ib = a + ib = re−iθ,eigenvec_val
(Note again that the eigenvalues can be complex numbers.) The stability tests are,eigenvec_val
"incidence matrix (the transpose of Section 2.5). A has a row for every node and a +1,",linear_prog
"the computational complexity of this algorithm. Regardless of the dimensions m and n,",linear_prog
3u + 5v + 7w = 1.,gauss elim
It was Khachian’s method that showed that linear programming could be solved in,linear_prog
Problems 19–37 are about the requirements for a basis.,vector spaces
The proof is to compute Hx and reach −σz:,computations
"The ﬁrst step is to ﬁnd the length of a vector. It is denoted by ∥x∥, and in two",orthogonality
"8. If A is singular, then detA = 0. If A is invertible, then detA ̸= 0.",determinants
"7. If B is positive deﬁnite, show from the Rayleigh quotient that the smallest eigenvalue",pos_def_matrices
"Find also a matrix A with this echelon form U, but a different column space.",vector spaces
same as before. No corner is ever revisited! The simplex method must end at the optimal,linear_prog
and produces the zero row in U—and we can often see it without computing L−1. When,vector spaces
this makes all the odd powers of x orthogonal to all the even powers:,orthogonality
ratios equal to √n?,computations
"12. If A is singular, then all splittings A = S − T must fail. From Ax = 0, show that",computations
Chapter 1 Matrices and Gaussian Elimination,gauss elim
What happens to the picture of b projected to A�x? The projection A�xW is still the,orthogonality
out ﬁrst—using multiples of ann. Either way we reach the diagonal matrix D:,determinants
49. Diagonalize this matrix by constructing its eigenvalue matrix Λ and its eigenvector,eigenvec_val
"that every vector in the space is a combination of the basis vectors, because they span.",vector spaces
"line is the same! As in the example, we ﬁnd",orthogonality
Chapter 2 Vector Spaces,vector spaces
"(c) All vectors that are perpendicular to (1,1,0,0) and (1,0,1,1).",vector spaces
0 0 0 0,eigenvec_val
But for DB it is already conﬁrmed that rule 9 is correct.,determinants
"19. Suppose P is a plane through (0,0,0) and L is a line through (0,0,0). The smallest",vector spaces
"With rectangular matrices, the key is almost always to consider ATA and AAT.",pos_def_matrices
number aTa and �x is aTb/aTa. We return to the earlier formula.,orthogonality
"Markov matrix, for differential equations, has its column sums equal to λmax = 0. A is",eigenvec_val
itself. Then these two equations combine with the original one to give u′ = Au:,eigenvec_val
"equals |x| times |y|, and the absolute value |1/x| equals 1 divided by |x|.",eigenvec_val
How can we decide whether two vectors x and y are perpendicular? What is the test,orthogonality
"Chevrolet, $300 on each Buick, and $500 on each Cadillac. These get 20, 17, and 14",linear_prog
the cost by r1δ. The real cost of egg is r1. This is the change in diet cost as the zero lower,linear_prog
"[1 2]+ [−2 − 1] gives [−1 1], which is not in either quadrant. The smallest subspace",vector spaces
trices in their eigenvalue problems. But sometimes Ax = λx is replaced by Ax = λMx.,pos_def_matrices
in columns 2 and 3 of U. We kept that matrix orthogonal.,pos_def_matrices
21. Let P be the plane in R2 with equation x + 2y − z = 0. Find a vector perpendicular,orthogonality
"(b) If the right-hand side is changed from (0,0,0) to (a,b,0), what are all solutions?",vector spaces
20. Is (AB)+ = B+A+ always true for pseudoinverses? I believe not.,pos_def_matrices
W −1 = 0.,eigenvec_val
"clearest when all angles are right angles—the edges are perpendicular, and the box is",determinants
List the three row operations: Subtract,gauss elim
"Discrete Fourier Transform Ux = (1,1,...,1)/√n also has length 1.",eigenvec_val
Orthogonality of a and e,orthogonality
"n, and each eigenvector of A is still an",eigenvec_val
0 1 1 1,determinants
solution for any new right-hand side b can be found in only n2 operations. That is,gauss elim
so the projection matrix is,orthogonality
"3.35 To solve a rectangular system Ax = b, we replace A−1 (which doesn’t exist) by",orthogonality
year.” They survive the ﬁrst year with probability 1,eigenvec_val
from adding the other three equations.,vector spaces
"That has the 1s on the diagonals of L and U, and the pivots 1 and −2 in D.",gauss elim
Triangular factorization A = LU with no exchanges of rows. L is lower,gauss elim
(i) Ax = b.,orthogonality
"If we stay with determinants (which we don’t plan to do), there will be a similar",gauss elim
"uses inﬁnitely many sines and cosines (or exponentials). In the discrete case, with only",orthogonality
an orthogonal Q is the product of a rotation and a reﬂection.,orthogonality
"AB has columns Ab1,...,Abn, and Bc has one column c1b1 +···+cnbn.",gauss elim
The Matrix and the Subspaces,orthogonality
on storage—the Gauss-Seidel method is better.,computations
The rectangular matrix will be R and the least-squares problem will be Rx = b. It has,pos_def_matrices
"per ton, $3 per pound, and $3000 per month, what are the values of one truck and",gauss elim
(a) f = 2(x2,pos_def_matrices
1 0 1 0,determinants
"invertible matrices D, permutation matrices P. Invent two more matrix groups.",gauss elim
will be orthogonal to the whole plane V.,orthogonality
u + 3v + 3w =,gauss elim
"If there is an exact solution, the minimum error is E = 0. In the more likely case that b",orthogonality
"NAQN are often very close to those of A, even for N ≪ n. The Lanczos iteration is",computations
only moves as far as,pos_def_matrices
"need row exchanges. In other words, PA allows the standard factorization into L times",gauss elim
"decay for large times t. Our argument depended on having n pure exponential solutions,",eigenvec_val
Chapter 7 Computations with Matrices,computations
"Our goal is to describe methods that start from any initial guess x0, and produce an",computations
and RRT = [5].,gauss elim
"42. (a) Suppose you guess your professor’s age, making errors e = −2,−1,5 with prob-",orthogonality
number of independent columns didn’t change.,vector spaces
First we have to introduce matrices and vectors and the rules for multiplication.,gauss elim
"f(a,b,c,d) = ln(ad −bc)",determinants
"35. Starting from u(0), the solution at time T is eATu(0). Go an additional time t to reach",eigenvec_val
"Now we face a matrix problem: Minimize yTAy/yTMy. With M = I, this leads to the",pos_def_matrices
"27. If A is any 8 by 8 invertible matrix, then its column space is",vector spaces
"tion 7.4 will concentrate on the property of sparseness, when most of the entries in A",computations
the fact that heavy liquids sink to the bottom is a consequence of minimizing their po-,pos_def_matrices
and we are right.,pos_def_matrices
"giving ac−b2 > 0, but also to the 1 by 1 submatrix a in the upper left-hand corner.",pos_def_matrices
1 4 1 0,gauss elim
(b) For which matrices A is that region a square?,vector spaces
0 0 0 5,vector spaces
AS = SΛ =,eigenvec_val
"multiplications to produce F times c (which we want to do often). By contrast, a fast",orthogonality
these Rayleigh quotient shifts give cubic convergence of αk to λ1.2,computations
begin again at Q.,linear_prog
have to come in different columns. Suppose the ﬁrst row has a nonzero term in column,determinants
(c) the product of pivots formula (including the elimination steps)?,determinants
"Then we apply the property of linearity, ﬁrst in row 1 and then in row 2:",determinants
What matters now is what happens inside the space—which means inside n-dimensional,vector spaces
22. Explain why AA+ and A+A are projection matrices (and therefore symmetric). What,pos_def_matrices
third variable will enter the basis. The current corner P and its cost +6 are not optimal.,linear_prog
one way to write v as a combination of the basis vectors.,vector spaces
A = MBM−1 =,eigenvec_val
"A graph consists of a set of vertices or nodes, and a set of edges that connect them.",vector spaces
"orthonormal basis. Instead, we really need these algorithms. And we need a convenient",computations
Any m by n matrix A can be factored,pos_def_matrices
The solution for y will be a combination of e4t and et.,eigenvec_val
We return to the fact that A is tridiagonal. What effect does this have on elimination?,gauss elim
"1 is zero, that vitamin is a free good and the small change has no effect. The diet",linear_prog
"transformations are not linear—for example, to square the polynomial (Ap = p2), or to",vector spaces
has dependent rows then it has dependent columns.,gauss elim
is almost the same as R6. (The six components are arranged in a rectangle instead,vector spaces
"1The usual rule of thumb, experimentally veriﬁed, is that the computer can lose logc decimal places to the",computations
"3. If x = 2+i and y = 1+3i, ﬁnd x, xx, 1/x, and x/y. Check that the absolute value |xy|",eigenvec_val
"interval 0 ≤ x ≤ 1,",orthogonality
(b) A and AT have the same left nullspace.,vector spaces
"without any special accident, should produce linear independence (not in a plane). Four",vector spaces
"For any symmetric matrix A, the signs of the pivots agree with the signs",pos_def_matrices
Example 3. Throw away the last row and column of any symmetric matrix:,pos_def_matrices
vectors �x−x also average to zero. The estimate �x is unbiased.,orthogonality
it replaces one factor of 1024 by 5. In general it replaces n2 multiplications by 1,orthogonality
"think of them placed side by side, and apply the same rule several times. Parentheses",gauss elim
"touch node 1. The next diagonal entry is 1+1 or c1 +c2, from the edges touching node",vector spaces
"where i = j, we have qT",orthogonality
"A zero in the pivot location raises two possibilities: The trouble may be easy to ﬁx,",gauss elim
"could easily drop the positivity assumption, and use absolute values |λ|. But to go",computations
"Columns 2 and 4 can be ignored. Then we immediately have u = −2 and w = 1, exactly",vector spaces
"vertical distances that are squared, summed, and minimized.",orthogonality
(b) The unsymmetric matrices in M (with AT ̸= A) form a subspace.,vector spaces
"In the 3 by 3 case, why is det(−K) = (−1)3detK? On the other hand detKT =",determinants
"A, B, C and a, b, c are bases for the vectors perpendicular to d = (1,1,1,1).",orthogonality
3. True or false?,determinants
"19. Suppose that CD = −DC, and ﬁnd the ﬂaw in the following argument: Taking de-",determinants
show that those functions don’t!,pos_def_matrices
eigenvalue of S−1T satisﬁes |λ| < 1. Its rate of convergence depends on the,computations
"44. If B has the columns of A in reverse order, solve (A−B)x = 0 to show that A−B is",gauss elim
"14. True or false, with reason if true and counterexample if false:",determinants
Example 1. This A has only one column: rank r = 1. Then Σ has only σ1 = 3:,pos_def_matrices
representing A and B.,vector spaces
space? It is the identity matrix.,orthogonality
"The upper triangular U will be the transpose of the lower triangular L, and A =",gauss elim
Problems 21–30 are about column spaces C(A) and the equation Ax = b.,vector spaces
Then back-substitution in the ﬁrst equation produces y. Nothing mysterious—substitute,vector spaces
"a = (cosθ,sinθ) and the matrix is symmetric with P2 = P:",orthogonality
0 1 0 0,eigenvec_val
"In some texts the condition Reλ < 0 is called asymptotic stability, because it guarantees",eigenvec_val
than the ﬁrst one. It goes to the heart of linear algebra.,vector spaces
8. Show that the determinant equals the product of the eigenvalues by imagining that,eigenvec_val
"∂ 2F/∂xi∂x j. This automatically equals a ji = ∂ 2F/∂x j∂xi, so A is symmetric. Then F",pos_def_matrices
For safety ﬁnd A = LU and solve Ax = b as usual. Circle c when you see it.,gauss elim
"3.9 Construct the projection matrix P onto the space spanned by (1,1,1) and (0,1,3).",orthogonality
Transformations Represented by Matrices,vector spaces
Chapter 7 Computations with Matrices,computations
"2. When A0 is tridiagonal or Hessenberg, each QR step is very fast. The Gram-Schmidt",computations
"When the elimination is down to k equations, only k2 − k operations are needed to",gauss elim
AB = (column 1)(row 1)+···+(column n)(row n) = sum of simple matrices.,gauss elim
"(a) A2 = −I, A having only real entries.",gauss elim
0 0 0 0,eigenvec_val
"method has its place at the very end, in ﬁnding the eigenvector.) The ﬁrst step is to pro-",computations
"Here is the main theorem on positive deﬁniteness, and a reasonably detailed proof:",pos_def_matrices
The new right side c was derived from the original vector b by the same steps that,gauss elim
"4. Find a and b for the complex numbers a + ib at the angles θ = 30°,60°,90° on the",eigenvec_val
(b) If A = [1 0,gauss elim
a projection P1 of rank 2 (onto the plane of eigenvectors). Then A is,eigenvec_val
n = e2πim/n = eπi = −1.,orthogonality
"from zero to become basic. The other m−1 basic components (in this case, the other 6)",linear_prog
"We return to linear algebra, and make the conversion from real to complex. By deﬁnition,",eigenvec_val
"for n = 4,8,12,... and odd for n = 2,6,10,...",determinants
"Let me summarize this section, before working a new example. Elimination reveals",vector spaces
nullspace of A contains only the zero vector c1 = c2 = c3 = 0.,vector spaces
"(a) the rank of B,",eigenvec_val
"(f) All geometric progressions (x1,kx1,k2x1,...) allowing all k and x1.",vector spaces
20. Find a basis for each of these subspaces of R4:,vector spaces
"at w(0) = 0, and at t = 0 we let go. Their motion u(t) becomes an average of two pure",eigenvec_val
"was not zero. Roughly speaking, we do not know whether a zero will appear until we",gauss elim
4. Check that the expected error E(e) is zero and ﬁnd the variance,orthogonality
by its eigenvectors and eigenvalues.,eigenvec_val
"terminants gives (detC)(detD) = −(detD)(detC), so either detC = 0 or detD = 0.",determinants
reverse step is F−1 and the last is E−1:,gauss elim
"has m = n = 2, and rank r = 1.",vector spaces
"direction of a), that component has to be subtracted:",orthogonality
how would you ﬁnd A?,determinants
"a triangular matrix they are the same—but that is exceptional. Normally the pivots,",eigenvec_val
ually use these rules to ﬁnd the determinant of any matrix.,determinants
"improvement is easy: Divide each vector by its length, to make it a unit vector. That",orthogonality
It is certainly true that the null space is perpendicular to the row space—but it is not,orthogonality
= UΣV T =,pos_def_matrices
Hx + x = 2Px,vector spaces
"4. If an m by n matrix A multiplies an n-dimensional vector x, how many separate",gauss elim
(b) Explain for permutations why P−1 is always the same as PT. Show that the 1s,gauss elim
From A to U,gauss elim
λ in the polynomial p(λ) = det(A−λI). The Cayley-Hamilton Theorem says that,eigenvec_val
(b) What is the exact row reduced echelon form R of A?,vector spaces
"all at once, as a combination of the three columns of A:",gauss elim
C1 j = (−1)1+j detM1 j.,determinants
"There cannot be n pivots, since there are not enough rows to hold them. The rank will",vector spaces
"the four equations Ax = b (unsolvable). Change the measurements to p = 1,5,13,17",orthogonality
3. Polar decomposition Every nonzero complex number z is a positive number r times,pos_def_matrices
equation that can’t be solved if x+y+z = 0 and x−2y−z = 1.,gauss elim
"must be even, and give an example.",eigenvec_val
∗ ∗ ∗ ∗,computations
"No solution, as in Figure 1.5b",gauss elim
Figure 6.6: The farthest x = x1/,pos_def_matrices
"that the normal equations break down. Sketch all the optimal lines, minimizing the",orthogonality
"are solved if the curve goes through the three points, and ﬁnd the best C and D.",orthogonality
x + 4y −,determinants
"from node j to node k, then that row has −1 in column j and +1 in column k. The",vector spaces
and unfortunately there is no interval on which even 1 and x2 are perpendicular. (Their,orthogonality
requires as many operations to compute A2 as it does to compute A−1! That fact seems,gauss elim
and n−r free variables. That important number r will be given a name—it is the rank,vector spaces
To ﬁnd the combination of the columns producing zero we solve Ac = 0:,vector spaces
at least n−m free variables. There will be even more free variables if some rows of R,vector spaces
are orthonormal. That is the good case. We turn now from the best possible matrices,eigenvec_val
"turn in his grave. For a 5 by 5 matrix, det(A−λI) involves λ 5. Galois and Abel proved",eigenvec_val
45. Find the eigenvalues and eigenvectors for both of these Markov matrices A and A∞.,eigenvec_val
"40. There are 12 “even” permutations of (1,2,3,4), with an even number of exchanges.",gauss elim
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
"(b) Assuming λ ̸= 0, show that x is also an eigenvector of A−1—and ﬁnd the eigen-",eigenvec_val
"Ap1 is its ﬁrst column, which is zero. Ap2 is the second column, which is p1. Ap3 is",vector spaces
"Proceeding carefully, we cheek that x and y also solve the second equation. This",gauss elim
"DO 10 J = 1, N",gauss elim
(empty 3 by 1).,vector spaces
an increase in n will actually produce a poorer answer.,computations
"If the size is doubled, and few of the coefﬁcients are zero, the cost is multiplied by 8.",gauss elim
into du/dt = Au gives λeλtx = Aeλtx. The cancellation of eλt produces,eigenvec_val
y1 +3x2 ≤ 4.,linear_prog
Chapter 1 Matrices and Gaussian Elimination,gauss elim
", row space contains",vector spaces
∗ ∗ ∗ ∗ ∗,computations
The feasible sets for the primal and dual problems look completely different. The,linear_prog
3 −x1x2 −x1x3 −x2x3).,pos_def_matrices
"Regardless of ω, the matrix on the left is lower triangular and the one on the right is",computations
"We had better say at once that the coordinate vectors e1,...,en are not the only basis",vector spaces
"4.5 If the entries of A and A−1 are all integers, how do you know that both determinants",determinants
46. Block multiplication separates matrices into blocks (submatrices). If their shapes,gauss elim
has a solution and AT� 1,orthogonality
(Find its real and imaginary part.) Which power of w6 is equal to 1/w6? What is,orthogonality
"(1,1,1) in the nullspace.",pos_def_matrices
"t = 0,z = 0.",orthogonality
Stable powers: An → 0,eigenvec_val
2x2. Both eigenvectors x1 and x2 have,eigenvec_val
"You recognize S, the matrix of eigenvectors. The constants c = S−1u(0) are the same as",eigenvec_val
45. Suppose an n by n matrix is invertible: AA−1 = I. Then the ﬁrst column of A−1 is,orthogonality
0 −8 −2 −2 1 0,gauss elim
close to the upper limit on the patience of the author and reader) too might not see much,gauss elim
Figure 2.7: Part of the graph for college football.,vector spaces
This quadratic equation gives ωopt = 4(2−,computations
has condition number about,computations
"How to compute C and D? If there is no experimental error, then two measurements",orthogonality
the left and r nodes on the right are in the set S with the source. The capacity across that,linear_prog
"42. (MATLAB) The −1, 2, −1 matrices have determinant n + 1. Compute (n + 1)A−1",determinants
of LU splitting into LDU is,gauss elim
the rate of convergence (usually deﬁned as −logρ) is inﬁnite. But Ax1 = b may be hard,computations
"crucial, and the debate is still going on. But Karmarkar’s ideas were so natural, and ﬁt",linear_prog
columns of A are a basis for its column space. The second column is three times,vector spaces
(C−D−4)2 +(C−5)2 +(C+D−9)2? What is the projection of b onto the column,orthogonality
(d) Exchanging the rows of a 2 by 2 matrix reverses the signs of its eigenvalues.,eigenvec_val
"different disguises. Its “vectors” can turn into functions, which is the second point.",orthogonality
"decide which matrices actually have these inverses. Roughly speaking, an inverse exists",vector spaces
"(c) λ +1 is an eigenvalue of A+I, as in Problem 20.",eigenvec_val
"bridge, it would begin to oscillate. (Just as a child’s swing does; you soon notice the",eigenvec_val
"(and several other sets of three marriages), but there is no way to reach four. The minimal",linear_prog
"produces u1 = Au0. Then u2 is Au1, which is A2u0. Every step brings a multiplication",eigenvec_val
"10. If P is an odd permutation, explain why P2 is even but P−1 is odd.",determinants
"Note that the values t = −1,1,2 are not required to be equally spaced. The ﬁrst step is",orthogonality
"Often v is normalized to become a unit vector u = v/∥v∥, and then H becomes I −2uuT.",computations
2 1 4 6,gauss elim
"the eigenvalues. And if condition I holds, we already know that these eigenvalues are",pos_def_matrices
"Instead of solving QRx = b, which can’t be done, we solve R�x = QTb which is just",orthogonality
(a) The equation Ax = 0 has only the solution x = 0 because,vector spaces
"It is quicker, and in the end simpler, to see what calculations are really necessary.",linear_prog
Remark 2. There is also a minimax principle for λn−j:,pos_def_matrices
The minus signs enter because cofactors always include (−1)i+j.,determinants
An example is the motion of two unequal masses in a line of springs:,pos_def_matrices
and verify that minimum equals maximum.,linear_prog
3 = 1 represents an ellipsoid when all λi > 0.,pos_def_matrices
"bi < (Ax∗)i, the factor y∗",linear_prog
"These quadratic forms are indeﬁnite, because they can take either sign. So we have",pos_def_matrices
"This decays for a < 0, it is constant for a = 0, and it explodes for a > 0. The imaginary",eigenvec_val
"be similar to J1. The matrix B has the additional eigenvector (0,1,0), and its Jordan",eigenvec_val
"What is remarkable is that the same condition number appears in equation (9), when the",computations
"doubt of the importance of this application. In economics and statistics, least squares",orthogonality
"way to organize the step is to ﬁt A, b, c into a large matrix, or tableau:",linear_prog
3. There is a unique “zero vector” such that x+0 = x for all x.,vector spaces
sums down the main diagonal) are 2. The eigenvalues satisfy 1·1 = 1 and 1+1 = 2. For,eigenvec_val
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
(d) e−A is symmetric positive deﬁnite.,pos_def_matrices
"tangent to the horizontal plane z = 7, and the surface z = f(x,y) is tangent to the plane",pos_def_matrices
cx = [−1 −1] is in the third quadrant instead of the ﬁrst.,vector spaces
property is a consequence of the ﬁrst three. We emphasize that the rules apply to,determinants
Chapter 7 Computations with Matrices,computations
The multiplication x = A−1b is just CTb divided bydetA.,determinants
"equation du/dt = Au, so does their sum u1 +u2:",eigenvec_val
ek = c1λ k,computations
eigenvalues in the following case?,eigenvec_val
0 0 0 1,eigenvec_val
"For an n by n matrix, this sum is taken over all n! permutations (α,...,v) of the numbers",determinants
as A. Construct a chain of nonsingular matrices C(t) linking C to an orthogonal,pos_def_matrices
Proof. Suppose A > 0. The key idea is to look at all numbers t such that Ax ≥ tx for,eigenvec_val
8.2 The Simplex Method,linear_prog
− 8v − 2w = −12,gauss elim
"direction and half in back-substitution. With n right-hand sides e1,...,en this makes n3.",gauss elim
"4. If P is a projection matrix, show from the inﬁnite series that",eigenvec_val
"that multiplication by A should be easy—if the matrix is large, it had better be sparse—",computations
has the eigenvalue λ = 1 (twice).,eigenvec_val
That last equation can be veriﬁed at a glance. Therefore S−1AS = Λ.,eigenvec_val
"21. When equation 1 is added to equation 2, which of these are changed: the planes in",gauss elim
already proved that volume = determinant.,determinants
7.3 Computation of Eigenvalues,computations
Problems 36–40 are about changing the basis,vector spaces
"vector (x,y,z,t) to produce b. The solutions ﬁll a plane in four-dimensional space.",gauss elim
"Three equations are still manageable, and they have much more variety:",gauss elim
Check that the projection A�xW is still perpendicular (in the W-inner product!) to the,orthogonality
"positive deﬁnite. In the Rayleigh quotient, xTx becomes xTMx:",pos_def_matrices
"In our 3 by 4 example, the zero row was row 3 − 2(row 2) + 5(row 1). Therefore",vector spaces
the right order to put the multipliers directly into L:,vector spaces
0 0 0 0,vector spaces
the interval 0 ≤ x ≤ π and 0 on the remaining interval π < x < 2π:,orthogonality
circle ∥x∥ = 1,computations
"(the x-y plane) in R3. The ﬁrst two vectors also span this plane, whereas w1 and w3 span",vector spaces
Again we cannot leave the problem without drawing a parallel to the continuous case.,eigenvec_val
is easy to ﬁnd for the example given above; it is as small as possible:,vector spaces
� ViVjdx for n hat functions with h =,pos_def_matrices
"4 by 4 determinants, but it contains 4! = 24 terms (not just eight). You cannot even",determinants
Sketch and solve a straight-line ﬁt that leads to the minimization of the quadratic,orthogonality
(c) Find all functions that satisfy dy,vector spaces
the determinant of A.,pos_def_matrices
0 d − b2,gauss elim
11. Suppose P is the projection matrix onto the subspace S and Q is the projection onto,orthogonality
the tableau) is pushed out of the basis. We move along the feasible set from corner P to,linear_prog
formations. Remember the key idea: Every linear transformation is represented by a,eigenvec_val
"5. Again in Example 3, change the cost to x+3y. Verify that the simplex method takes",linear_prog
"matrices is T = Λ? Symmetric, skew-symmetric, and orthogonal T’s are all diagonal!",eigenvec_val
Find those ﬁve terms to show that D4 = −1.,determinants
(a) Matrix A times columns of B.,gauss elim
"i xi = 1,",pos_def_matrices
"1. What is the dual of the following problem: Minimize x1 + x2, subject to x1 ≥ 0,",linear_prog
"it would be too much, But the formula is already discovered—it is (8), and the only point",determinants
"1. In a tableau, as above.",linear_prog
46. Use inv(S) to invert MATLAB’s 4 by 4 symmetric matrix S = pascal(4). Create,gauss elim
eigenvalues and would be positive deﬁnite. But,pos_def_matrices
"3 3] has no inverse by solving Ax = 0, and by failing to solve",gauss elim
algebraically why H2 = I.,orthogonality
have a property that is the most important and most characteristic of all:,orthogonality
detAn = 2(n)−(n−1) = n+1.,determinants
splits into Lc = b and Ux = c.,gauss elim
is the projection onto what line in Cn?,eigenvec_val
I realize that so far in this book we have given no reason to care about N(AT). It is,vector spaces
"occurs when Ax = b. If A is just a scalar, that is easy to do:",pos_def_matrices
This same argument extends to any number of eigenvectors: If some combination pro-,eigenvec_val
Find a 3 by 3 system with these solutions exactly when b1 +b2 = b3.,vector spaces
reaches the identity matrix we have found A−1.,gauss elim
"To preserve the zeros and stop, we have to settle for less than a triangular form.",computations
"scaling but also on the order n; if A = I/10, then the determinant of A is 10−n. In fact,",computations
We summarize these three different ways to look at matrix multiplication.,gauss elim
14. Suppose the eigenvector matrix S has ST = S−1. Show that A = SΛS−1 is symmetric,eigenvec_val
(current) = (conductance)(voltage drop).,vector spaces
"The notation is simpler if the long vector (x,w) is renamed x and [A −I] is renamed A.",linear_prog
"3. Suppose −u′′ = 2, with the boundary condition u(1) = 0 changed to u′(1) = 0. This",pos_def_matrices
"only way to have c1x1+c2x2 = 0 is to have c1 = c2 = 0, so these vectors are independent.",vector spaces
zk+1 = .2yk +.7zk,eigenvec_val
"(x,x) x = x2 −",orthogonality
"state phenomenon—the temperature distribution in a rod, for example, with ends ﬁxed",gauss elim
formation stays the same. The matrix A (or Q or P or H) is altered to S−1AS. Thus a,vector spaces
"As a simple but still very typical continuous problem, our choice falls on the differential",gauss elim
The best example of an eigenvalue problem has u(x) = sinπx and λ1 = π2:,pos_def_matrices
The rank is r = 1.,orthogonality
"(a) the two vectors (1,1,−1) and (−1,−1,1).",vector spaces
"37. (a) What matrix M transforms (1,0) and (0,1) to (r,t) and (s,u)?",vector spaces
Find and solve equations for the c’s.),vector spaces
S−1T has |λ|max = cos,computations
"To ﬁnd the c’s we have to invert F. In the 4 by 4 case, F−1 was built from 1/i = −i.",orthogonality
are the same. This is the “change of basis formula” Mc = d.,eigenvec_val
"f, they must contain the same answer. The two functions behave in exactly the same",pos_def_matrices
cost more than the protein in peanut butter ($2 a unit) or the protein in steak ($3 for two,linear_prog
"In that singular case ﬁnd a nonzero solution x, y, z.",gauss elim
. Find a nonreal basis for Cn.,eigenvec_val
|λ −aii| ≤ ∑,computations
"49. True or false: If we know T(v) for n different nonzero vectors in R2, then we know",vector spaces
Your answer represents T(v) = v with input basis of v’s and output basis of w’s.,vector spaces
x + 2y ≥ 4,linear_prog
has consumed a lot of the reader’s time (and patience). I wish there were an easier way,gauss elim
simpler system (equivalent to the old) is just E(Ax) = Eb. It is simpler because of the,gauss elim
matrix can be chosen with orthonormal columns when A = AH.,eigenvec_val
The ﬁrst idea is to start from a point inside the feasible set—we will suppose it is,linear_prog
"the eigenvalues of A and B, and the eigenvalues of A+B are not the sums of the",eigenvec_val
There are two matrices rather than one.,pos_def_matrices
their factors L and U?,gauss elim
"miles per gallon, respectively, and Congress insists that the average car must get 18. The",linear_prog
This shows that a left-inverse B (multiplying from the left) and a right-inverse C (multi-,gauss elim
"that contain pivots (in this case the ﬁrst and third, which correspond to the basic vari-",vector spaces
A = LDLT = (L,pos_def_matrices
"Find four conditions on a, b, c, d to get A = LU with four pivots.",gauss elim
There were two main ideas in the preceding section on minimum principles:,pos_def_matrices
3. All solutions of ATA�x = ATb have the same xr. That vector is x+.,pos_def_matrices
"When the eigenvalues are real, those tests guarantee them to be negative. Their product",eigenvec_val
"determinant does not change, except for a sign reversal when rows are exchanged.",determinants
with xMIT = 1 and all other x j = 0. We have to ground not only Harvard but one team,vector spaces
(c) Illustrate both formulas when A = [2 1,gauss elim
18. Suppose A = SΛS−1. What is the eigenvalue matrix for A + 2I? What is the eigen-,eigenvec_val
1.31 When does the rank-1 matrix A = uvT have A2 = 0?,vector spaces
"would have to be the zero matrix, But if Λ = S−1AS = 0, then we premultiply by S and",eigenvec_val
do you need to know that i2 = −1?,orthogonality
"(i) Suppose b and b′ lie in the column space, so that Ax = b for some x and Ax′ = b′",vector spaces
This section is about linear programming with n unknowns x ≥ 0 and m constraints,linear_prog
"2, it must just move the ﬁrst term to the nearest integer:",eigenvec_val
"c = i, show that iA is skew-Hermitian. The 3 by 3 Hermitian matrices are a subspace,",eigenvec_val
"(a) Suppose the ﬁrst three pivots of A are 2, 3, −1. What are the determinants of L1,",determinants
"The solution will rotate around the axis w = (a,b,c), because Au is the “cross prod-",eigenvec_val
"other points P(y) is larger than P(x), so the minimum occurs at x.",pos_def_matrices
"2xH(A+AH)x = (Reλ)xHx > 0, so that Reλ > 0.",pos_def_matrices
The matrix A =,eigenvec_val
variables; otherwise A−1 would be much easier to ﬁnd than it actually is.,determinants
"columns, this �x will not be unique. We have to choose a particular solution of ATA�x =",pos_def_matrices
(a) a diagonal matrix: aij = 0 if i ̸= j.,gauss elim
33. Change 3 to 2 in the upper left corner of the matrices in Problem 32. Why does,determinants
"of a column.) Any choice of m and n would give, as a similar example, the vector",vector spaces
"6Later we compare “skew-Hermitian” matrices with pure imaginary numbers, and “normal” matrices with all",eigenvec_val
"mum, the ﬁrst derivatives must vanish at x = y = 0:",pos_def_matrices
"Choose the rotation angle θ to produce zero in the (3,1) entry of M−1AM.",eigenvec_val
matrix M = P23E21 does both steps at once?,gauss elim
"This matrix cannot have an inverse, no matter what the x’s are. One proof is to use",gauss elim
"This also has the merit of being symmetric about x. To repeat, the right-hand side ap-",gauss elim
"idea is to keep the familiar deﬁnition of length, using a sum of squares, and to include",orthogonality
(a) How many members belong to the groups of 4 by 4 and n by n permutation,gauss elim
c0 +i3c1 +i6c2 +i9c3,orthogonality
"ﬁrst step subtracted 2 times the ﬁrst equation from the second. On the right-hand side,",gauss elim
Those examples could be lifted into three dimensions. There are matrices to stretch,vector spaces
"surements, what are the coefﬁcient matrix A, the unknown vector x, and the data",orthogonality
"zero, except for the column sequence (1,2,...,n). This term gives detI = 1. Property 2",determinants
Show that it is contradictory for (i) and (ii) both to have solutions.,orthogonality
Figure 1.1: The example has one solution. Singular cases have none or too many.,gauss elim
Our goal is to derive these formulas directly from the deﬁning properties 1–3 of detA. If,determinants
1 3 0 −1,vector spaces
"1. The mth root is the square of the nth root, if m is half of n:",orthogonality
"12. (a) Minimize the cost cTx = 5x1 +4x2 +8x3 on the plane x1 +x2 +x3 = 3, by testing",linear_prog
"What property do you expect for the eigenvectors, and is it true?",eigenvec_val
"really from Ak − αkI). We may use Householder again, but it is simpler to annihilate",computations
appears when Ax2 = 0. Zero is an eigenvalue of a singular matrix.,eigenvec_val
The Gauss-Jordan process is really a giant sequence of matrix multiplications:,gauss elim
A note for the future: You can see the determinant −16 appearing in the denominators,gauss elim
wave function is x—is at least 1,eigenvec_val
"nal. The nonzero entries of U have a “staircase pattern,” or echelon form. For the 5 by",vector spaces
"Orthogonal, invertible, projection, permutation, Hermitian, rank-1, diagonalizable,",eigenvec_val
32. Construct any 3 by 3 Markov matrix M: positive entries down each column add to,eigenvec_val
trial candidates V = y1V1 +···+ynVn:,pos_def_matrices
"to the true solution u(x), those steps will be useless. To combine both computability",pos_def_matrices
the orthogonal complement of V. It is denoted by V⊥ = “V perp.”,orthogonality
"y2/y1, to derive again the orthogonality condition xTy = 0.",orthogonality
"positive entries (also called sigma) will be σ1,...,σr. They are the singular values of A.",pos_def_matrices
which lie at right angles and go their own way (toward zero).,vector spaces
problems and not on others. The underlying idea was analyzed and improved. Newer,linear_prog
and safer choice by exchanging the rows of B. When “partial pivoting” is built into the,computations
(a) F = −1+4(ex −x)−5xsiny+6y2 at the point x = y = 0.,pos_def_matrices
command B = null(N’) will produce a basis for the,orthogonality
The normal way to compute y is by elimination. Gram-Schmidt will orthogonalize the,linear_prog
"Ax = b. If Ax0 = b and Ax1 = b, then ∆x = x1 −x0 has to satisfy A∆x = 0. The step ∆x",linear_prog
parentheses in E−1F−1G−1 were not necessary because of the associative law.,gauss elim
Which two matrices have the same column,vector spaces
"13. Compute the determinants of A, B, C from six terms. Independent rows?",determinants
3. Find the 5 by 5 matrix A0 (h = 1,gauss elim
the same as the question of stability: xk → x exactly when ek → 0.,computations
"Example 5. (A = LU, with zeros in the empty spaces)",gauss elim
"with unit diagonal, the U’s are upper triangular with unit diagonal, and the",gauss elim
column space is the n-dimensional space Pn−1; the right-hand side of equation (2) is,vector spaces
We emphasize that the word “graph” does not refer to the graph of a function (like a,vector spaces
There is no way to number the N2 mesh points in a square so that each point stays,computations
less than the n3/3 on the left. The total for forward and back is,gauss elim
"14. Choose x = (x1,x2,x3,x4) in R4. It has 24 rearrangements like (x2,x1,x3,x4) and",vector spaces
. The linear transformation T is deﬁned by,vector spaces
∗ ∗ 0 0,computations
"looks for independent rows and independent columns, Chapter 3 inverts AAT or ATA.",gauss elim
columns. The column space of all attainable vectors b is closed under addition.,vector spaces
inﬁnitely many solutions (the whole line L). Find three solutions.,gauss elim
the inner product is not new or different. Rotating the space leaves the inner product,orthogonality
minant as the product of the pivots in D.,gauss elim
"repeated eigenvalues, a symmetric matrix still has a complete set of orthonormal eigen-",eigenvec_val
begin—but we digress for a moment to mention two other applications of these same,computations
much freedom does this leave? Verify that the rows automatically become orthonor-,orthogonality
orthogonal). We take complex conjugates in UHU = I and V HV = I and A = UΣV H.,pos_def_matrices
positive pivots matches the number of positive eigenvalues of A.,pos_def_matrices
as a combination of the columns of A. Then b is in the column space.,vector spaces
(Note the number aTa in the middle of the matrix aaTaaT!),orthogonality
(detA)(detA−1) = detAA−1 = detI = 1.,determinants
"Our particular solution xp, (one choice out of many) has free variables v = y = 0.",vector spaces
1.5 Triangular Factors and Row Exchanges,gauss elim
2(−L −U) = I − 1,computations
"now done with complex numbers c. The vectors v1,...,vk are linearly dependent if",eigenvec_val
"length of the projection is c = cosθ. Notice that the point of projection is not (c,s), as",vector spaces
Suppose A has a full set of n pivots. AA−1 = I gives n separate systems Axi = ei,gauss elim
"10. If you throw away two rows and columns of A, what inequalities do you expect",pos_def_matrices
this case there is only the third one) so as to eliminate v. We add the second equation to,gauss elim
cx subject to Ax = b and x ≥ 0—then the requirement y ≥ 0 is left out of the dual:,linear_prog
"basis is, but not how to ﬁnd one. Now, starting from an explicit description of a subspace,",vector spaces
"10. Decide on the stability or instability of dv/dt = w, dw/dt = v. Is there a solution",eigenvec_val
5.10 Find the general solution to du/dt = Au if,eigenvec_val
"the starting values 1, 1, 1 for n = 1,2,3. What are the pivots?",determinants
"∥Ux∥ = ∥x∥ by Property 1′, and always ∥λx∥ = |λ|∥x∥. Therefore |λ| = 1.",eigenvec_val
leads again to the conclusion that it must be diagonal.,eigenvec_val
"At time zero, when the exponentials are e0 = 1, u(0) determines c1 and c2:",eigenvec_val
inﬁnitely many solutions? Find the solution that has z = 1.,gauss elim
The model is called “closed” when everything produced is also consumed. Nothing goes,eigenvec_val
"y1,...,yn go into a vector y. Then P(V) = 1",pos_def_matrices
1 1 1 1,eigenvec_val
2ω(1−ω) 1−ω + 1,computations
(AB)ij = (row i of A) times (column j of B),gauss elim
tion space containing multiples of cosnx or sinnx. It is completely parallel to the vector,orthogonality
u + 3w =,gauss elim
1 2 3 5,vector spaces
"3.29 For any A, b, x, and y, show that",orthogonality
"of decay), but we do not know how much of each is in our hands. If these two unknown",orthogonality
30. Show that A and B are similar by ﬁnding M so that B = M−1AM:,eigenvec_val
(d) For which A is the new area still 1?,vector spaces
The point of intersection lies on both lines. It is the only solution to both equations.,gauss elim
"have dimension r and n −r, then λ = 1 is repeated r times and λ = 0 is repeated n −r",eigenvec_val
32. Find experimentally the average size (absolute value) of the ﬁrst and second and third,gauss elim
0 (Q0R0)Q0 = A1. So the process continues with,computations
"straints on x, y, w. The variables w that “take up the slack” are now included in the vector",linear_prog
2 is insigniﬁcant compared to λ k,eigenvec_val
Example 2. (which needs a row exchange),gauss elim
ℓij are inside! You can see the numbers −1,pos_def_matrices
whenever A2 is invertible.),gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
(c) A diagonal matrix with positive diagonal entries is positive deﬁnite.,pos_def_matrices
S−1Tx = x. So this matrix B = S−1T has λ = 1 and fails.,computations
determined by y ≥ 0 and AT and c. The whole theory of linear programming hinges on,linear_prog
containing the ﬁrst quadrant is the whole space R2.,vector spaces
8. For the hat functions V1 and V2 centered at x = h = 1,pos_def_matrices
"The nine coefﬁcients fall into three rows and three columns, producing a 3 by 3 matrix:",gauss elim
This matrix goes into the middle of the weighted normal equations.,orthogonality
case when one or more of the eigenvalues is zero.,pos_def_matrices
"Elimination can solve Ax = b, but the four fundamental subspaces showed that a different",linear_prog
"The rows are all multiples of the same vector vT, and the columns are all multiples of u.",vector spaces
"17. If A = L1D1U1 and A = L2D2U2, prove that L1 = L2, D1 = D2, and U1 = U2. If A is",gauss elim
Lengths and Transposes in the Complex Case,eigenvec_val
The eigenvalues of K are purely imaginary instead of purely real; we multiply i. The,eigenvec_val
24. Show that [1 2,gauss elim
"71. Show that L−1 has entries j/i for i ≤ j (the −1, 2, −1 matrix has this L):",gauss elim
"normal order (row 1, ... , row n − 1, row n)? Find detP for the n by n permutation",determinants
5.4 Differential Equations and eAt,eigenvec_val
"right. If we multiply a vector in the plane by 3, or −3, or any other scalar, we get a",vector spaces
There is one more situation in which the calculations are easy. The eigenvalue of A2 are,eigenvec_val
algorithm and looks more directly at the geometry. I think the key ideas will be just as,linear_prog
2 in the second component of uk:,eigenvec_val
across. If they happen to march at a frequency equal to one of the eigenvalues of the,eigenvec_val
"of B−1u. (If B−1u < 0, the minimal cost is −∞.) When the smallest ratio",linear_prog
"but matrix notation is better. (Einstein used “tensor notation,” in which a repeated index",gauss elim
b3 + b2 − 5b1,vector spaces
The ℓi are the lengths of the rows (the edges). and the zeros off the diagonal come,determinants
mizes the squared error E2 = ∥Ax−b∥2. This is a quadratic and it ﬁts our framework! I,pos_def_matrices
"basis vector x j, and writing T(x j) as a combination of the y’s:",vector spaces
This minimum is taken only over positive components of B−1u. The kth col-,linear_prog
square brackets (with no commas) when a column vector is printed vertically. What,gauss elim
"the angle −2π/n, where w was at the angle +2π/n:",orthogonality
yTb = 0: Vectors in the column space and left nullspace are perpendicular! That is soon,vector spaces
"A. Because A = AT, those eigenvectors and axes are orthogonal. The major axis of the",pos_def_matrices
Notice that ATCA is symmetric. It has positive pivots and it comes from the basic,vector spaces
ATA = VΣTΣV T.,pos_def_matrices
6. Let P be the plane in 3-space with equation x +2y+z = 6. What is the equation of,vector spaces
5.2 Diagonalization of a Matrix,eigenvec_val
"it has positive coefﬁcients. Write f as a difference of squares and ﬁnd a point (x,y)",pos_def_matrices
"The remedy is equally clear. Exchange the two equations, moving the entry 3 up",gauss elim
cations by F and F−1 can be done in an extremely fast and ingenious way. Instead of,orthogonality
"��, but that term is zero by rule 4. The",determinants
"0. The nullspace contains the line through x. And if there are additional free variables,",vector spaces
order. Give examples with P1P2 ̸= P2P1 and P3P4 = P4P3.,gauss elim
1 L2D2 = D1U1U−1,gauss elim
. Show that the identity matrix I is not in the range of T. Find a,vector spaces
5. Starting with the 2 by 2 matrix A =,linear_prog
natural resistance to roundoff error is no longer compromised.,computations
requirement—it must match the product of linear transformations.,vector spaces
we will show that equality holds: Ax = tmaxx.,eigenvec_val
"years. For most of us it brought an aura of mystery to linear programming, chieﬂy",linear_prog
. AHA is an invertible Hermitian matrix when the,eigenvec_val
axes? What is the projection matrix P onto that vector?,orthogonality
"This matrix has no inverse, because the transformation has no inverse. Points on the",vector spaces
"With these rules established, we can introduce a special class of matrices, probably",gauss elim
"7. Find three 2 by 2 matrices, other than A = I and A = −I, that are their own inverses:",gauss elim
4.2 Properties of the Determinant,determinants
2Linear convergence means that every step multiplies the error by a ﬁxed factor r < 1. Quadratic convergence,computations
6.3 Singular Value Decomposition,pos_def_matrices
"Gauss-Seidel (S = −1, 2, 0 matrix):",computations
ination. In fact we now have A = L + D +U.) The Jacobi method has S = D on the,computations
"tion on each column needs only two operations, as above, and there are n columns. In",gauss elim
"Note 3. If A is invertible, the one and only solution to Ax = b is x = A−1b:",gauss elim
1 0 0 0,determinants
"In other words, ∥A∥ bounds the “amplifying power” of the matrix:",computations
"λ1 has already been computed by another algorithm (such as QR), then α is this",computations
(They were our introduction to eigenvalues at the start of the chapter.) In this 2 by 2,eigenvec_val
the product of the pivots remains the same apart from sign.,determinants
1. Rotation Figure 2.10 shows rotation through an angle θ. It also shows the effect on,vector spaces
6. Give an example to show that the eigenvalues can be changed when a multiple of one,eigenvec_val
reversed order” on the left side of the graph. Even numbers come before odd (numbers,orthogonality
"between the two halfspaces is the line x+2y = 4, where the inequality is “tight.” Figure",linear_prog
"14. In the list below, which classes of matrices contain A and which contain B?",eigenvec_val
be the same as the form SeΛtS−1u(0) used for computation. To prove directly that those,eigenvec_val
is broken if multiplying f(x) by c gives the function f(cx)?,vector spaces
E−1F−1G−1U = A is LU = A.,gauss elim
(III′) No principal submatrices have negative determinants.,pos_def_matrices
The underlying theory is easier to explain if M is split into RTR. (M is assumed to be,pos_def_matrices
a j = (qT,orthogonality
diagonal. The eigenvalues appear on the diagonal because J is triangular. And distinct,eigenvec_val
The row space and column space are lines—the easiest case.,vector spaces
x−y+z = 4. Find the point with z = 0 and a third point halfway between.,gauss elim
"9. Write the 3 by 3 transition matrix for a chemistry course that is taught in two sections,",eigenvec_val
"tion (8) are similar: J1 ̸= M−1J2M, J1 ̸= M−1J3M, and J2 ̸= M−1J3M.",eigenvec_val
"Fibonacci’s rule Fk+2 = Fk+1 +Fk must produce whole numbers, Somehow that formula",eigenvec_val
inequality in mathematics. A special case is the fact that arithmetic means 1,orthogonality
"16. In most applications the second-order equation looks like Mu′′+Ku = 0, with a mass",eigenvec_val
Problems 13–23 use the big formula with n! terms: |A| = ∑±a1αa2β ···anv.,determinants
"Any linearly independent set in V can be extended to a basis, by adding",vector spaces
1. (a) Write the four equations for ﬁtting y = C +Dt to the data,orthogonality
5. Subtracting a multiple of one row from another row leaves the same determinant.,determinants
"(2,−2) = (4,0), the matrix leaves one part and reverses the other",vector spaces
6. Phase I ﬁnds a basic feasible solution to Ax = b (a corner). After changing signs,linear_prog
Proof. Condition I deﬁnes a positive deﬁnite matrix. Our ﬁrst step shows that each,pos_def_matrices
"(b) Find e = b−a�x, the variance ∥e∥2, and the standard deviation ∥e∥.",orthogonality
", then B = an arbitrary matrix with λ = 0 and 0.",eigenvec_val
"The ﬁrst step toward the SVD is exactly as in QR above: x is the ﬁrst column of A,",computations
The best illustration of this approach came in the Fundamental Theorem of Linear,linear_prog
of the columns (don’t do it!)?,eigenvec_val
which is m by m with σ2,pos_def_matrices
"deﬁnite matrix and its xTAx, we ﬁnally get a ﬁgure that is curved. It is an ellipse in two",pos_def_matrices
"bandwidth” is w = 1 for a diagonal matrix, w = 2 for a tridiagonal matrix, and w = n",gauss elim
Our aim is to compute x∗. We could do it (in principle) by ﬁnding all the corners,linear_prog
eigenvalues stay the same.,eigenvec_val
"and some serious teams, and also a college that is not famous for big time football.",vector spaces
"a combination of the orthonormal q1 and q2, and we know what combination it is:",orthogonality
"there will be trouble with 10,000 unknowns, or with a 1, −4, 6, −4, 1 approximation to",computations
"37. Prove that every y in N(AT) is perpendicular to every Ax in the column space, using",orthogonality
"cut is n− p from the source to the remaining women, and r from these men to the sink.",linear_prog
only when the rank is as large as possible.,vector spaces
(a) The skew-symmetric matrices in M (with AT = −A) form a subspace.,vector spaces
Suppose one of the n intersecting planes is removed. The points that satisfy the,linear_prog
matrices to ﬁnd the rotation angle:,vector spaces
higher dimensional subspace is by far the most important case; it corresponds to a least-,orthogonality
"11. From their trace and determinant, at what time t do the following matrices change",eigenvec_val
"and deserves a book of its own. Leaves grow in a spiral pattern, and on the apple or oak",eigenvec_val
"15. Suppose A commutes with every 2 by 2 matrix (AB = BA), and in particular",gauss elim
as geometry gives the normal equations. Find the solution �x and the projection p =,orthogonality
"from sinθ to −sin3θ, which is cubic convergence.",computations
"only so much about a matrix. Still, it is amazing how much this number can do.",determinants
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
bly it gets straighter as the edges become inﬁnitesimal.),determinants
"zero, and xr −x′",orthogonality
of A−1. The determinant is the product of the pivots (2)(−8)(1). It enters at the end,gauss elim
"matrix. For every missing eigenvector, the Jordan form will have a 1 just above its main",eigenvec_val
Chapter 2 Vector Spaces,vector spaces
"With our 3 by 3 example, we are able to write out all the equations in full. We can list",gauss elim
"off-diagonal entries, without changing the determinant (by rule 5). If A is lower triangu-",determinants
"functions, and the dimension is somehow a larger inﬁnity than for R∞.",vector spaces
5. Diagonalize the Fibonacci matrix by completing S−1:,eigenvec_val
1. Any vector xn in the nullspace can be added to a particular solution xp. The solutions,vector spaces
1 2 0 3,vector spaces
"The row a, b times the column C11, C12 produces ad −bc. This is the cofactor expan-",determinants
1.13 Solve Ax = b by solving the triangular systems Lc = b and Ux = c:,gauss elim
We come to the orthogonality of two subspaces. Every vector in one subspace must be,orthogonality
Column picture: Combination of columns,gauss elim
4. What multiple ℓ of equation 1 should be subtracted from equation 2?,gauss elim
of pivot variables plus the number of free variables must match the total number of,vector spaces
. They are the same line if,orthogonality
The rows are also linearly dependent; row 3 is two times row 2 minus ﬁve times row,vector spaces
doesn’t sound like an improvement (to put it mildly). But if only one or two vectors,orthogonality
the most important theorems in linear algebra. It is often abbreviated as “row rank =,vector spaces
"perpendicular to every other), then those vectors are linearly independent.",orthogonality
"about 1960, after one important improvement—to make A tridiagonal ﬁrst. Then the",pos_def_matrices
"9. If the vectors q1, q2, q3 are orthonormal, what combination of q1 and q2 is closest to",orthogonality
the resistor. The multiplication Ax gave the potential difference between the nodes.,vector spaces
Find the eigenvalues λ = eiθ of B and C to show that B4 = I and C3 = −I.,eigenvec_val
"In other words, solve F4c = y.",orthogonality
invertible and �x is not determined. Any vector in the nullspace could be added to �x. We,pos_def_matrices
the ﬁnal matrix U has r nonzero rows. This deﬁnition could be given to a computer. But,vector spaces
has designed pipeline systems that are millions of dollars cheaper than the intuitive (not,linear_prog
"This equation ﬁnds x+, and it also displays the matrix that produces x+ from b. That",pos_def_matrices
"We note that S2 is ATA, which is symmetric positive deﬁnite when A is invertible. S",pos_def_matrices
4x + dy + z =,gauss elim
(c) Rows of A times columns of B.,gauss elim
eigenvalue will be positive:,pos_def_matrices
"to introduce “parametric” programming. But if the changes are small, the corner that",linear_prog
27. What are L and D for this matrix A? What is U in A = LU and what is the new U in,gauss elim
"Again we can study the rows or the columns, and we start with the rows. Each equation",gauss elim
", and the left nullspace is",vector spaces
"9. Square the matrix P = aaT/aTa, which projects onto a line, and show that P2 = P.",orthogonality
34. This matrix is singular with rank 1. Find three λ’s and three eigenvectors:,eigenvec_val
Chapter 1 concentrated on square invertible matrices. There was one solution to Ax = b,vector spaces
settle for an approximate x that can be obtained more quickly—and it is no use to go,computations
5.30 What is the limit as k → ∞ (the Markov steady state) of,eigenvec_val
elsewhere. Then matrix multiplication executes the row operation.,gauss elim
(c) Dimension of nullspace = 1+ dimension of left nullspace.,vector spaces
�x = aTb/aTa. There is a reason behind that apparently trivial change. Projection onto,orthogonality
as t → ∞?,eigenvec_val
one shifted step with α = uT,computations
�1 0 1 0 1,vector spaces
"same constant c, and the differences will not change—conﬁrming that x = (c,c,c,c) is in",vector spaces
"2) to the vertices? (The bond angle itself is about 109.5°, an",orthogonality
"Figure 2.1: The column space C(A), a plane in three-dimensional space.",vector spaces
1 0 0 1,determinants
the discrete transform would have an important place. Now there is more. The multipli-,orthogonality
"35. For the closest parabola b = C +Dt +Et2 to the same four points, write the unsolv-",orthogonality
we try to combine them to produce b. Stay with equation (3):,gauss elim
Is detA equal to 1+1 or 1−1 or −1−1? What is detB?,determinants
T(x j) = Ax j = a1 jy1 +a2 jy2 +···+amjym.,vector spaces
"The error is reduced by 25% at every step, and a single SOR step is the equivalent of",computations
skew-Hermitian KH = −K,eigenvec_val
λ1 = −1 :,eigenvec_val
4 6 8 2,vector spaces
"2, and the second with probability",eigenvec_val
Of course everything goes to the column space—the matrix cannot do anything else. I,orthogonality
35. Suppose T(M) =,vector spaces
four units are required in the diet. Therefore a diet containing x pounds of peanut butter,linear_prog
"prices such that p = Ap, and does the system take us there?",eigenvec_val
"Check that the error vector (10−3�x,5−4�x) is perpendicular to the column (3,4).",orthogonality
(d) An m by n matrix has no more than m pivot variables.,vector spaces
Problems 33–41 are about the matrix exponential eAt.,eigenvec_val
"back down to place k. Since (ℓ − k) + (ℓ − k − 1) is odd, the proof is complete. The",determinants
(c) Find a matrix with that subspace as its column space.,vector spaces
"dimensions we need three vectors, along the x-y-z axes or in three other (linearly in-",vector spaces
take the inner product with x to show that Ax = 0:,orthogonality
47. Apply elimination with the extra column to reach Rx = 0 and Rx = d:,vector spaces
"less than 90°, so yA ≥ 0. This is the alternative we are looking for. This theorem of the",linear_prog
and xTAx = 5u2 + 8uv + 5v2 = 1. That ellipse is centered at,pos_def_matrices
4. A Formula for the Pivots.,determinants
for an even or odd number of row exchanges.,determinants
y is certain to occur at the end of the interval. That is exactly the situation in linear pro-,linear_prog
"row 2 = (0, 1)",determinants
"For the linear transformation of transposing, ﬁnd its matrix A with respect to this",vector spaces
3. (For Linear Algebra) Every matrix with the same zeros as A is singular.,linear_prog
them. The previous section began with some hints about the signs of eigenvalues. but,pos_def_matrices
"nant is applied to ﬁnd A−1. Then we compute x = A−1b by Cramer’s rule. And ﬁnally, in",determinants
9. Explain how UΣV T expresses A as a sum of r rank-1 matrices in equation (3):,pos_def_matrices
This problem and its matrix have the two essential properties of a Markov process:,eigenvec_val
"permanent place. We begin by describing one very rough and ready approach, the power",computations
"Thus H = HT = H−1. Householder’s plan was to produce zeros with these matrices, and",computations
Remark on scaling The projection matrix aaT/aTa is the same if a is doubled:,orthogonality
engineer tries to keep the natural frequencies of his bridge or rocket away from those of,eigenvec_val
"has +ℓ in that position. Thus E−1E = I, which is equation (4).",gauss elim
"each column, and the best coefﬁcients �C and �D can be found separately:",orthogonality
1 1 1 1,eigenvec_val
"tion Ax = b. The trend now is back to direct methods, based on an idea of Golub and",computations
4. Write down the 3 by 3 ﬁnite-difference matrix equation (h = 1,gauss elim
frequently used of all the ideas that bear his name.) The method starts by subtracting,gauss elim
and S is symmetric positive semideﬁnite. If A is invertible then S is positive,pos_def_matrices
"and 3000 miles from the three producers, respectively; and 2,200,000 barrels are",linear_prog
"directly to A, without computing its eigenvalues, which will guarantee that all those",pos_def_matrices
value θ1 of A+B is at least as large as λ1 + µ1. (Try the corresponding eigenvector,pos_def_matrices
factorization of Ak+1. This identity connects QR to the power method and leads to,computations
decomposition A = QS:,pos_def_matrices
"construct a consumption matrix—in which aij, gives the amount of product j that is",eigenvec_val
24. Solve the system and ﬁnd the pivots when,gauss elim
also invertible—and ﬁnd a formula for C−1.,gauss elim
"Requirement (i) holds: If Ax = 0 and Ax′ = 0, then A(x+x′) = 0. Requirement (ii) also",vector spaces
3k +1 3k −1,eigenvec_val
lengths instead of capacities. We want the shortest path from source to sink. If the edges,linear_prog
"Example 1. The operation of differentiation, A = d/dt, is linear:",vector spaces
(b) inﬁnitely many solutions for every b.,vector spaces
13. Write the dual of the following problem: Maximize x1+x2+x3 subject to 2x1+x2 ≤,linear_prog
(IV′) No pivots are negative.,pos_def_matrices
"computer science than to calculus—and it is easy to explain. This section is optional, but",vector spaces
"16. Express the Gram-Schmidt orthogonalization of a1, a2 as A = QR:",orthogonality
"A: Not only is the largest eigenvalue λ1 positive, but so is the eigenvector x1. Then",eigenvec_val
equals det(AT −λI). That is true because,eigenvec_val
"equation on all edges at once, Ohm’s Law is y = Ce.",vector spaces
"The equation to consider is xTAx = 1. If A is the identity matrix, this simpliﬁes to",pos_def_matrices
"B(I) = B(I) + A(I,J) * X(J)",gauss elim
Chapter 2 Vector Spaces,vector spaces
Show that its eigenvalues are real exactly when a2 +b2 ≥ c2.,eigenvec_val
"variety of applications. And at the same time, understanding it in terms of matrices—the",gauss elim
by a minimum principle.,pos_def_matrices
linear inequalities like Ax ≥ b (the intersection of m halfspaces). When we also require,linear_prog
hand side) to have only one solution? Could Ax = B have no solution?,vector spaces
"(a) If columns 1 and 3 of B are the same, so are columns 1 and 3 of AB.",gauss elim
Splitting Rn into orthogonal parts will split every vector into x = v+w. The vector v,orthogonality
"dence matrix, and a basis for each subspace.",vector spaces
"is not an eigenvector. But if u0 is a combination of eigenvectors, the solution uk is the",eigenvec_val
"c3 = 0. Then the next equation gives c2 = 0, and substituting into the ﬁrst equation forces",vector spaces
"The n by n case is more difﬁcult. A test for Reλi < 0 came from Routh and Hurwitz,",eigenvec_val
Minimize cx subject to Ax = b and x ≥ 0.,linear_prog
Quick proof that eAt is invertible: Just recognize e−At as its inverse.,eigenvec_val
= L times U.,gauss elim
initial values 8 and 5. The problem is to ﬁnd v(t) and w(t) for later times t > 0.,eigenvec_val
∗ ∗ ∗ ∗,computations
1/i = −i and 1/(−i) = i. Then K4 is a complete rotation through 360°:,eigenvec_val
"up from x = 0). Equivalently, the tests decide whether the matrix A is positive deﬁ-",pos_def_matrices
P = A(ATA)−1AT = AA−1(AT)−1AT = I.,orthogonality
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"section. But there is also the question of choosing a basis, and we emphasize that the",vector spaces
length ∥x∥). (Find a 2 by 2 counterexample.),vector spaces
"22. How could you quickly compute these four components of Fc starting from c0 +c2,",orthogonality
questionable point in the big formula. Independent of the particular row exchanges link-,determinants
The projection of b onto the line through a lies at p = a(aTb/aTa). That is our formula,orthogonality
"There is a famous way in which to write the answer (x1,...,xn):",determinants
change by a permutation matrix P. Since we keep going to the next column when no,vector spaces
"by the production itself. The amount consumed is Ap, and it leaves a net production of",eigenvec_val
58. The equation x − 3y − z = 0 determines a plane in R3. What is the matrix A in this,vector spaces
unitary matrix U. What property of P makes these eigenvectors orthogonal?,eigenvec_val
(b) Show that detA = 0 if ad = bc (so C is singular).,determinants
The solution (the ﬁrst eigenvector) is any nonzero multiple of x1:,eigenvec_val
"unknowns: The right-hand side (−4,−3,−1,0) is in the",orthogonality
"example turns all vectors through 90°, transforming every point",vector spaces
"From the ATA = VΣTΣV T, the V matrix must be the eigenvector matrix for ATA. The",pos_def_matrices
7t has heights 5,orthogonality
15. Show that the length of Ax equals the length of ATx if AAT = ATA.,orthogonality
is called diagonally dominant because every |aii| > ri. Show that zero cannot lie in,computations
"= 0” gives a differential equation. In every problem, we are free to solve the linear",pos_def_matrices
20. Compute the three upper left determinants to establish positive deﬁniteness. Verify,pos_def_matrices
"Again this transforms Pn to Pn+1, with no nullspace except p = 0.",vector spaces
"Then CTAC must have one positive and one negative eigenvalue, like A.",pos_def_matrices
lar matrices of any shape. Then C(A) can be somewhere between the zero space and,vector spaces
Example 7. The column space of A is exactly the space that is spanned by its columns.,vector spaces
nents of x.) Its solution is x = Q−1b. But since Q−1 = QT—this is where orthonormality,orthogonality
eigenvalues are distinct or not—has a complete set of orthonormal eigenvectors. We,eigenvec_val
"Circle the pivots. Solve by back-substitution for z, y, x.",gauss elim
"those in the upper left-hand corner. Otherwise, we could not distinguish between two",pos_def_matrices
components are positive. Then x belongs to the feasible set.,linear_prog
six dimensions it is probably easiest to choose the six numbers.,gauss elim
produces a single number. This number is called the inner product of the two vectors.,gauss elim
"The ﬁrst formula recognized that Ak is identical with SΛkS−1, and we could stop there.",eigenvec_val
the determinant become zero? (Use rule 3 or a cofactor.),determinants
we discover an important fact: P−1 is always the same as PT.,gauss elim
We emphasize again that M−1AM does not arise in solving Ax = b. There the basic,eigenvec_val
"That is the difference equation. It turns up in a most fantastic variety of applications,",eigenvec_val
ellipsoid: λ1(B) ≤ λ2(A). Similarly the minor axis of the cross section is smaller than,pos_def_matrices
"pivots 2/1, 3/2, 4/3, 5/4, 6/5 are all positive. Their product is the determinant of A:",gauss elim
Hermitian matrices: AH = A,eigenvec_val
But it is remarkably close. The underlying theory is explained in the author’s book An,pos_def_matrices
and only a few hundred unknowns. The equations xh − xv = bi go into a linear system,vector spaces
displaying the results—than any other technique in scientiﬁc computation2. If the basic,pos_def_matrices
32. There are sixteen 2 by 2 matrices whose entries are 0s and 1s. Similar matrices go,eigenvec_val
row rank = column rank. Now we know that those subspaces are perpendicular. More,orthogonality
"second corner decided the entering variables. (These numbers go into r, the crucial vec-",linear_prog
Leontief’s inspiration was to ﬁnd a model that uses genuine data from the real econ-,eigenvec_val
(d) (AB)2 = A2B2.,gauss elim
solution to Ax = b. The last two terms with v and y yield more solutions (because they,vector spaces
"of the inequality constraints are tight, meaning that equality holds. Other constraints are",linear_prog
"This is a ratio of potential to kinetic energy, and they are in balance at the eigenvector.",pos_def_matrices
This is the whole key to differential equations du/dt = Au: Look for pure exponential,eigenvec_val
asking if Euler’s formula (2) was true. It is really astonishing that three of the key,orthogonality
inverse iteration is an automatic choice.,computations
"At t = 0 we get e0 = I. The inﬁnite series eAt gives the answer for all t, but a series can be",eigenvec_val
for n = 4).,orthogonality
1x + 2y =,gauss elim
The Factorization A = QR,orthogonality
"u = Mv, so they ought to have something in common, and they do: Similar matrices",eigenvec_val
"the reasoning just completed, A1 is also Hessenberg. So A1 must be tridiagonal. The",computations
(b) Construct a matrix that has column 1 + 2(column 3) = 0. Check that A is,gauss elim
25. Find the cofactor matrix C and compare ACT with A−1:,determinants
(b) subtract −1 times the ﬁrst equation from the third.,gauss elim
"deﬁnite matrix, they are two completely different sets of positive numbers, In our 3 by 3",pos_def_matrices
The answers are certainly yes. This is exactly the problem of the least-squares solu-,orthogonality
"26. If aij is i times j, show that detA = 0. (Exception when A = [1].)",determinants
by U cannot destroy the scaling.,pos_def_matrices
Hint: Subtracting the last row from each of the others leaves,determinants
"41. A=2∗eye(n)−diag(ones(n−1, 1),1)−diag(ones(n−1, 1),−1) is the −1, 2, −1",determinants
"optimal λ and µ. In the exercises, we stay with linear least squares.",orthogonality
"In other words V⊥⊥ = V. The dimensions of V and W are right, and the whole space",orthogonality
dx −2y = 0.,vector spaces
b in column space,orthogonality
. The column picture is in,gauss elim
"This is again AT(b − A�x) = 0 and ATA�x = ATb, The calculus way is to take partial",orthogonality
"constraint into a function L(x,y). This was the brilliant insight of Lagrange:",pos_def_matrices
"With equality constraints, the simplex method can begin. A corner is now a point where",linear_prog
"equal their transpose, but to matrices that equal their conjugate transpose. These are",eigenvec_val
"1.25 Suppose T is the linear transformation on R3 that takes each point (u,v,w) to (u+",vector spaces
"of A. For differential equations, the solution u(t) = eAtu(0) depends on the exponential",eigenvec_val
"error would be ampliﬁed by more than 1013. On the other hand, we cannot just give",orthogonality
"stage, x consists of the last n − 2 entries in the second column (three bold stars). Then",computations
�x = (ATA)−1ATb =,orthogonality
solution in the same form as equation (4).,vector spaces
"Figure 3.9a, and they are the components of the dashed vector in Figure 3.9b. This error",orthogonality
"63. Construct a matrix whose column space contains (1,1,0) and (0,1,1) and whose",vector spaces
"aT(b− �a) = 0,",orthogonality
"more general M, we go the other way and restrict M to be unitary. M−1AM can achieve",eigenvec_val
It is no longer possible to work only with real vectors and real matrices In the ﬁrst half of,eigenvec_val
Show that the 25 by 25 matrix is singular by noticing a simple nonzero solution B.,eigenvec_val
are 0. Then Ax = 0 or Ux = 0 or Rx = 0 gives the pivot variables by back-,vector spaces
be of order 10−5—which is still more accurate than any ordinary measurements. But,computations
Chapter 2 Vector Spaces,vector spaces
"39. For the 4 by 4 tridiagonal matrix (entries −1, 2, −1), ﬁnd the ﬁve terms in the big",determinants
"Then BE is the correct basis matrix for the next stop, E−1B−1 is its inverse, and E−1",linear_prog
"only nlog2n+n multiplications. How many of those come from E, how many from",orthogonality
"According to our theory, the shortest solution should be in the row space of A =",pos_def_matrices
"the second row by its pivot 3, so that all pivots are 1. Then use the pivot row to produce",vector spaces
in xHy = 1+i2 = 0.) After division by,eigenvec_val
Choose the c’s to match the starting vector u0:,eigenvec_val
0 p(x)dx = 0. Verify that S is a subspace and ﬁnd a basis.,vector spaces
eigenvalues of A are real—as we now prove.,eigenvec_val
right-hand sides f. Construct an example of A and f.,vector spaces
"(1, 2, 0) has length",orthogonality
pivots. This is easy to correct. Divide out of U a diagonal pivot matrix D:,gauss elim
solution to Ax = b?,linear_prog
space of A and the r rows of U span the row space. Then A+ has the explicit formula,pos_def_matrices
"3. Compute the ratios of B−1b to B−1u, admitting only positive components",linear_prog
"seem self-evident (at least, not to the author).",vector spaces
"meet at right angles. Those four subspaces are perpendicular in pairs, two in Rm and",orthogonality
"8. From the cubics P3 to the fourth-degree polynomials P4, what matrix represents",vector spaces
"to the zero column. The nullspace contains x = (1,1,1,1), since Ax = 0. The equation",vector spaces
7x3 −x4 −(9−x2 −x3) = x2 +8x3 −x4 −9,linear_prog
The column picture conﬁrms that x = 2 and y = 3.,gauss elim
"1.26 (a) What vector x will make Ax = column 1 of A + 2(column 3), for a 3 by 3 matrix",gauss elim
r. Exactly one vector in the row space is carried to b.,orthogonality
"This matrix projects any vector b onto the column space of A.1 In other words, p = Pb",orthogonality
(c) Interpret the zero error in terms of the original system of four equations in two,orthogonality
has 6 compatible pairs.,linear_prog
1−W n = (1−W)(1+W +W 2 +···+W n−1).,orthogonality
(b) The only positive deﬁnite projection matrix is P = I.,pos_def_matrices
. Conclusion: ATA has the same nullspace as A.,orthogonality
onto a line. This problem arises from Ax = b when A is an m by n matrix. Instead,orthogonality
"2. For this n by n matrix, describe the Jacobi matrix J = D−1(−L−U):",computations
"(f, f) = ∥ f∥2. The Schwarz inequality is still satisﬁed: |(f,g)| ≤ ∥ f∥∥g∥. Of course,",orthogonality
(a) The vectors for which x1 = 2x4.,vector spaces
The remarkable matrix multiplication (2) is correct.,determinants
The way to understand this subject is by example. We begin with two extremely humble,gauss elim
"It is the steady state p∞, and it is approached from any starting point p0. By repeating a",eigenvec_val
polynomial can be passed through any bi at distinct points ti. Later we shall actually ﬁnd,vector spaces
equation—then stability can be proved without knowing a formula for u(t).,eigenvec_val
is their sum 2x (which is < 0) and the determinant is (x + iy)(x − iy) = x2 + y2 > 0.,eigenvec_val
still counts as nonsingular; it is only the algorithm that needs repair. In other cases a,gauss elim
When the simplex method reshufﬂes the long matrix and vector to put the basic variables,linear_prog
This is the pseudoinverse—a way of choosing the best C in Section 6.3. The transpose,vector spaces
b changes to ATb,pos_def_matrices
what is the column space of P and what is its rank?,orthogonality
"13. Suppose A is a symmetric 3 by 3 matrix with eigenvalues 0, 1, 2.",eigenvec_val
is a “vector” in the space M of all 2 by 2 matrices. Write the,vector spaces
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Exchange rows 3 and 4 of the second matrix and recompute the pivots and determi-,determinants
"The determinant in (b) is zero; along what line is f(x,y) = 0?",pos_def_matrices
"Now we recognize the matrix L that takes U back to A. It is called L, because it is",gauss elim
any components in which equality fails. This corresponds to a vitamin that is oversup-,linear_prog
"6. For each n, how many exchanges will put (row n, row n − 1,..., row 1) into the",determinants
"can be assigned any value, leading to the following conclusion:",vector spaces
"3.27 Let A = [3 1 1], and let V be the nullspace of A.",orthogonality
factored form (keep only the v’s) and never computed explicitly. That completes,computations
needed and A = LU is not possible. Which c produces zero in the third pivot position?,gauss elim
"produces uk = Aku0, although the matrix Ak will never appear. The essential thing is",computations
"TT H and T HT, and show that if they are equal, then T must be diagonal. All normal",eigenvec_val
A similar remark applies to A−1; the multiplication A−1b would still take n2 steps. It,gauss elim
The equations ATA�x = ATb are known in statistics as the normal equations.,orthogonality
it is numerically impossible to concentrate all that information into the polynomial and,eigenvec_val
of BTAT. The other rows of (AB)T and BTAT also agree.,gauss elim
derivatives are zero. But this is completely changed by constraints. The simplest exam-,linear_prog
"with older texts in abstract linear algebra, the underlying theory has not been radically",computations
"It will be n by m, because it takes b and p in Rm back to x+ in Rn. We look at one more",pos_def_matrices
Matrix Norm and Condition Number,computations
"Einstein, we keep the Σ.)",gauss elim
Property 2′ Every eigenvalue of U has absolute value |λ| = 1.,eigenvec_val
"book—pivots, determinants, and eigenvalues.",pos_def_matrices
"At the same time, we have y∗A ≤ c. All strict inequalities (expensive foods) corre-",linear_prog
If A is invertible,orthogonality
matrix A if you know the sum along each row and down each column?,orthogonality
and A = LU.,gauss elim
"It is the identity matrix on the columns of Q (P leaves them alone), But QQT is the zero",orthogonality
0 0 3 3,vector spaces
2 +y(c1x1 +c2x2 −d) has n+ℓ = 2+1 partial derivatives:,pos_def_matrices
The product of a 4 by 1 matrix and a 1 by 3 matrix is a 4 by 3 matrix. This product has,vector spaces
Remark 1. The LU form is “unsymmetric” on the diagonal: L has 1s where U has the,gauss elim
"region. For n columns in Rm, the cone becomes an open-ended pyramid. Figure 8.4 has",linear_prog
"v1,...,vn, we can put the solutions eAtv into a matrix:",eigenvec_val
equations themselves may be solvable or unsolvable. If the last two equations are 3w = 6,gauss elim
"(a) By multiplying A times u, show that u is an eigenvector. What is λ?",eigenvec_val
"This chapter is about Ax = b, when A is not necessarily square. For Qx = b we now",orthogonality
and compare the solutions.,gauss elim
when the columns span Rm is 1 or ∞.,vector spaces
"30. If the 9 by 12 system Ax = b is solvable for every b, then C(A) =",vector spaces
"How do we ﬁnd u(t)? If there were only one unknown instead of two, that question",eigenvec_val
So take R =,pos_def_matrices
are all inequalities). These methods hope to move more directly to x∗ (and also ﬁnd y∗).,linear_prog
"form a triangle. Every pair of planes intersects in a line, and those lines are parallel. The",gauss elim
"diagonalized, uk will be a combination of pure solutions:",eigenvec_val
r on the diagonal.,pos_def_matrices
"tives with respect to C and D, after dividing by 2, bring back the normal equations",orthogonality
"Similarly, feasibility gives yAx ≤ cx. We get equality only when the second slackness",linear_prog
x − 2y − z =,gauss elim
These two matrices do commute and the product does both steps at once:,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
matrix S. Show that the A’s form a subspace (cA and A1 + A2 have this same S).,eigenvec_val
"uniqueness of x, for m equations in n unknowns.",gauss elim
The simplest and most widely used ﬁnite element is piecewise linear. Place nodes at,pos_def_matrices
"foods (peanut butter and steak, in Section 8.1). They enter the diet in the (nonnegative)",linear_prog
quadratics we are accustomed to. The matrix entries Aij are,pos_def_matrices
The eigenvalues of S−1T are 1,computations
1.16 For which values of k does,gauss elim
= shortest particular solution xp,vector spaces
(b) Find the eigenvalues and eigenvectors of A.,eigenvec_val
"For complex eigenvalues, b and c have opposite signs and are sufﬁciently large.",eigenvec_val
"as well as the transpose, for UH.",pos_def_matrices
", nullspace has basis",vector spaces
"15. If P is the projection matrix onto a line in the x-y plane, draw a ﬁgure to describe",orthogonality
"2, and ∥C∥2 = 4",orthogonality
"x = (1, 0)",eigenvec_val
"as A. When A has eigenvalues λ1 and λ2, A2 has eigenvalues",eigenvec_val
row space and nullspace have dimensions that add to r + (n − r) = n. The other pair,orthogonality
"sines and cosines, and they are not always orthogonal. The simplest are the powers of x,",orthogonality
touches zero. The number θ is a small parameter to be chosen:,linear_prog
"easy. The function to minimize cannot be a quadratic, or its derivative would be linear—",pos_def_matrices
problem—the nonsingular case—where there is one solution and it has to be found.,gauss elim
x + 2y = 0,linear_prog
"vector δb is a combination of the corresponding unit eigenvectors x1,...,xn. The worst",computations
Chapter 7 Computations with Matrices,computations
its diagonal. Show that this matrix fails to have xTAx > 0:,pos_def_matrices
"The total population is still y0 +z0, but in the limit 2",eigenvec_val
of charge requires ﬂow in = ﬂow out. The beautiful thing is that AT is exactly the right,vector spaces
zero that was created below the ﬁrst pivot. It is equivalent because we can recover the,gauss elim
"echelon matrix U or R, with r pivots and r nonzero rows: The nonzero rows are a basis,",vector spaces
8. For which three numbers k does elimination break down? Which is ﬁxed by a row,gauss elim
"value is shifted by α, and the convergence factor for the inverse method will change",computations
"5. Choose b = (0,6,−6), which has b3 + b2 − 5b1 = 0. Elimination takes Ax = b to",vector spaces
"The 3 by 3 example has λ1 = .9, and output exceeds input. Production can go on.",eigenvec_val
(c) Describe a subspace of M that contains no nonzero diagonal matrices.,vector spaces
Chapter 1 Matrices and Gaussian Elimination,gauss elim
Figure 2.10: Rotation through θ (left). Projection onto the θ-line (right).,vector spaces
always in that space. The sum of nullity (= 1) and rank (= n) is the dimension of the,vector spaces
it is the one that network programming solves.,linear_prog
cs(c2 +s2) s2(c2 +s2),vector spaces
Start with A and b;,gauss elim
"(d) 1, regardless of b.",vector spaces
though it was constructed to solve the primal). Recall that the m inequalities Ax ≥ b,linear_prog
"been found). We update the matrices L and P the same way. At the start, P = I and sign",gauss elim
"y1 ≥ 0, y2 ≥ 0",linear_prog
Problems 20–22 move up to 4 by 4 and n by n.,gauss elim
The Schwarz inequality |aTb| ≤ ∥a∥∥b∥ is 6 ≤,orthogonality
(b) Show also that the column space of A2 is contained in the column space of A.,vector spaces
(Look at b3 −2b2 +b1 on the right-hand side.) Which vectors are in the nullspace of,vector spaces
"determine, so we complete the matrix in any way that leaves it unitary, and call it U1.",eigenvec_val
29. Ak = SΛkS−1 approaches the zero matrix as k → ∞ if and only if every λ has absolute,eigenvec_val
"Figure 3.11: The eight solutions to z8 = 1 are 1,w,w2,...,w7 with w = (1+i)/",orthogonality
AT = −A gives a conservative system. No energy is lost in damping or diffusion:,eigenvec_val
equations on the right-hand side: the derivative of v+w is zero.,eigenvec_val
u = u(0) at t = 0.,eigenvec_val
"To ﬁnd Ak, and the distribution after k years, change SΛS−1 to SΛkS−1:",eigenvec_val
is not positive when,pos_def_matrices
"row j from row i. This Ei j includes −ℓ in row i, column j.",gauss elim
3. Describe the column space of A: Which plane in R3?,vector spaces
is exactly the distance from b to the point Ax in the column space. Searching for the,orthogonality
if V is the subspace spanned by,vector spaces
"61. Invent a 3 by 3 magic matrix M with entries 1,2,...,9. All rows and columns and",gauss elim
orthonormal.” Now we propose to ﬁnd a way to make them orthonormal.,orthogonality
8A The corners of the feasible set are the basic feasible solutions of Ax = b.,linear_prog
. Find a matrix with T(M) ̸= 0. Describe all ma-,vector spaces
x + y = 5.,gauss elim
2.1 Vector Spaces and Subspaces,vector spaces
"can happen without linear algebra, but linear algebra turns it into matrix multiplication.",eigenvec_val
"In reality, those nonlinear equations are approximately solved by Newton’s method",linear_prog
F1000 = nearest integer to 1,eigenvec_val
"1. For the equations x + y = 4, 2x − 2y = 4, draw the row picture (two intersecting",gauss elim
"In the uniqueness case, if there is a solution to Ax = b, it has to be x = BAx = Bb. But",vector spaces
eAt = I +At + (At)2,eigenvec_val
Therefore this rule can be split into two parts:,determinants
u(t) = eλ1tx1 = e−t,eigenvec_val
clear out the column below the pivot—by the same reasoning that applied to the ﬁrst,gauss elim
gular. It is the square matrix ATA that is invertible.,orthogonality
Nullspace of A: Is there a combination of the columns that gives Ax = 0? Normally,vector spaces
1. Construct a subset of the x-y plane R2 that is,vector spaces
. When you rotate it,determinants
"5. (a) If x = reiθ what are x2, x−1, and x in polar coordinates? Where are the complex",eigenvec_val
Example 5. These three columns in R2 cannot be independent:,vector spaces
pivot columns in the column space. There are n − r special solutions in the nullspace.,vector spaces
"Example 5. If we close off the inﬁnite segments, nothing can escape:",eigenvec_val
"with λ = 0? From the dimensions of those subspaces, A has a full set of independent",eigenvec_val
"about their eigenvectors? For practical purposes, those are the most important questions",eigenvec_val
"In practice, we also consider a row exchange when the original pivot is near zero—",gauss elim
"(since combinations of solutions are still solutions). Find two independent solutions,",vector spaces
"7, �D = 4",orthogonality
ﬁnal factors L and U—is an essential foundation for the theory. I hope you will enjoy,gauss elim
Chapter 2 Vector Spaces,vector spaces
"3. Find x, y, and z by Cramer’s Rule in equation (4):",determinants
"26. If every column of A is a multiple of (1,1,1), then Ax is always a multiple of (1,1,1).",gauss elim
2. There is only one line of eigenvectors (unusual). The moving directions Ax and x,eigenvec_val
"more freedom in S. For the trivial example A = I, any invertible S will do: S−1IS is is",eigenvec_val
3. D (or D−1) to divide all rows by their pivots.,gauss elim
The third column of this exponential comes directly from solving du/dt = Jiu:,eigenvec_val
"all column vectors with complex components, and it has new deﬁnitions of length and",eigenvec_val
. Show by an example that the eigen-,eigenvec_val
"Under what conditions on the numbers a, b, c, d are the columns linearly indepen-",vector spaces
square appears when ϕ is +θ. All three questions were decided by trigonometric,vector spaces
"7. On the space P3 of cubic polynomials, what matrix represents d2/dt2? Construct",vector spaces
c1v1 +c2v2 +c3v3 +c4v4 = 0:,vector spaces
appear in P at the same places where the a’s appeared in A.,determinants
A series of rotations in the right planes will produce the required zeros. Householder,computations
unless it is square.,orthogonality
equation which arises after elimination.,gauss elim
orthogonality: xTy = 0,eigenvec_val
(a) Write the 3 by 3 matrix D such that,eigenvec_val
"The usual factorization B = LU (or PB = LU, with row exchanges for stability) leads to",linear_prog
"related to x by some nonsingular matrix, x =Cy. The quadratic form becomes yTCTACy.",pos_def_matrices
"it is the plane generated by columns 1 and 3. The other columns lie in that plane,",vector spaces
and the associative law (AB)C = A(BC) holds for linear transformations. This is,vector spaces
"an optional remark on permutations, we show that whatever the order in which the prop-",determinants
Example 2. Singular (incurable),gauss elim
"In many cases this problem can be cured, and elimination can proceed. Such a system",gauss elim
�� = 12 +22 +(−3)2 = 14.,orthogonality
"vector in the same plane. If we add two vectors in the plane, their sum stays in the",vector spaces
21. Find three different bases for the column space of U above. Then ﬁnd two different,vector spaces
5.15 Find the eigenvalues and eigenvectors of,eigenvec_val
Column Vectors and Linear Combinations,gauss elim
"values of A0. If there was already some processing to obtain a tridiagonal form, then A0",computations
(a) is the set of rank 1 matrices a subspace?,vector spaces
"At the start, $2 trillion are in the Americas and $2 trillion in Europe. Each year 1",eigenvec_val
"Any matrix is the sum of r matrices of rank 1. If only 20 terms are kept, we send 20",pos_def_matrices
"We know that if A has a left-inverse (BA = I) and a right-inverse (AC = I), then the two",vector spaces
The left-hand side is ax2 + 2bxy + cy2. The right-hand side is a(x + b,pos_def_matrices
dt (a0 +a1t +···+antn) = a1 +···+nantn−1.,vector spaces
1.7 Special Matrices and Applications,gauss elim
"will mean that I −P has no inverse, and has determinant zero.)",gauss elim
x = 0. Then ATA is positive deﬁnite.,pos_def_matrices
(i) Each entry of AB is the product of a row and a column:,gauss elim
"(d) If those vectors are the columns of A, then Ax = b (has) (does not have) (might",vector spaces
Note that A =,eigenvec_val
"is a partial clue from Gaussian elimination: We know the original coefﬁcient matrix A,",gauss elim
32. Describe the four subspaces of R3 associated with,vector spaces
Figure 3.3: Orthogonal complements in R3: a plane and a line (not two lines).,orthogonality
"n , and every eigenvector of A is also an eigenvector of A2. We start",eigenvec_val
"b = Dt through the origin. Solve D = 1 and 2D = 7 by least squares, and sketch the",orthogonality
consists of all vectors that satisfy Ax = 0.),vector spaces
"same system Ax = b, but add the constraint x ≥ 0. When does there exist a nonnegative",linear_prog
"We solved linear equations in Chapter 1, as the ﬁrst step in linear algebra. To set up the",vector spaces
the identity and the integral of the derivative of tn is tn.,vector spaces
A = LU to use.,gauss elim
"Physically, two masses are connected to each other and to stationary wails by three",eigenvec_val
", the row space is",vector spaces
"σ’s are signiﬁcant and others are extremely small. If we keep 20 and throw away 980,",pos_def_matrices
illustration) in the upper left-hand corner. Its subdiagonal elements will be somewhat,computations
"nonnegative, meaning that u1 ≥ tu0 ≥ 0.",eigenvec_val
The free variables have values 1 and 0. When the free columns moved to the right-,vector spaces
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"5. Put the diagonal matrix C with entries c1, c2, c3 in the middle and compute ATCA.",vector spaces
Those are the four equations in system (6). At x = 2π the series returns y0 = 2 and,orthogonality
Spanning Trees and Independent Rows,vector spaces
"43. If A is 6 by 4 and B is 4 by 6, AB and BA have different sizes. Nevertheless,",eigenvec_val
"18. (a) The intersection of two planes through (0,0,0) is probably a",vector spaces
but not B =,vector spaces
11. The ﬁrst row of AB is a linear combination of all the rows of B. What are the coefﬁ-,gauss elim
Certainly if Ax = 0 then ATAx = 0. Vectors x in the nullspace of A are also in the,orthogonality
"For the moment, ignore the right-hand sides of the equations, and count only the",gauss elim
"The right-hand sides are changed only by ∥δb∥ = .0001 = 10−4. At the same time, the",computations
Chapter 1 Matrices and Gaussian Elimination,gauss elim
the same eight points. Since zero is the only number that is unchanged when multiplied,orthogonality
"to P. What matrix has the plane P as its nullspace, and what matrix has P as its row",orthogonality
"(c) Under what condition on a and b does uk approach a ﬁnite limit as k → ∞, and",eigenvec_val
eP ≈ I +1.718P.,eigenvec_val
gives b = 0. So there is a whole line of solutions—as we know from the row picture.,gauss elim
"Let me review the meaning of each entry in this tableau, and also call attention to Ex-",linear_prog
conditions for x and y to be the optimal x∗ and y∗? Along with the constraints we require,linear_prog
0 JM0 = JT.,eigenvec_val
"horizontal line, whereas �Dt is the best ﬁt by a straight line through the origin. The",orthogonality
"3T The Gram-Schmidt process starts with independent vectors a1,...,an and",orthogonality
0 1 1 0,vector spaces
2 0 4 9,vector spaces
Figure 8.5: A 6-node network with edge capacities: the maximal ﬂow problem.,linear_prog
operations on the left. These operations are of two kinds. We divide by the pivot to,gauss elim
is to identify the cofactors C1 j that multiply a1 j.,determinants
"Do these conditions a > 0 and c > 0 guarantee that f(x,y) is always positive? The",pos_def_matrices
"If we divide by ω, these two matrices are the S and T in the splitting AS−T; the iteration",computations
vector matrix? Check that A+2I = (,eigenvec_val
"We go back to the 3 by 3 matrix A that gave the consumption of steel, food, and labor.",eigenvec_val
3.33 (a) Find an orthonormal basis for the column space of A.,orthogonality
"5.4 In the previous problem, what will be the eigenvalues and eigenvectors of A2? What",eigenvec_val
"For the differentiation matrix, column 1 came from the ﬁrst basis vector p1 = 1. Its",vector spaces
e = b − p,orthogonality
"of rotation from v = (2,3,−5) to Pv = (−5,2,3)?",gauss elim
"Stability is governed by those factors eglit. If they all approach zero, then u(t) approaches",eigenvec_val
2. to extend the ideas of length and inner product from vectors v to functions f(x):,orthogonality
"the diagonal, respectively. (This splitting has nothing to do with the A = LDU of elim-",computations
but other orders are possible. We could have used the second pivot when we were there,gauss elim
what are the eigenvalues of the 6 by 6 matrix A =,eigenvec_val
"But the subspace always has an orthonormal basis, and it can be constructed in a simple",orthogonality
3 2 4 1,vector spaces
"After squaring both sides, and expanding (x + y)T(x + y), reduce this to the",orthogonality
ﬁnite element approximation with the true u = x−x2.,pos_def_matrices
include the signs (−1)i+j) and the determinants of A and B.,determinants
"4.4 Solve 3u+2v = 7, 4u+3v = 11 by Cramer’s rule.",determinants
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
and 1 in the free columns of R have opposite sign in the special solutions (the nullspace,vector spaces
", then B =",eigenvec_val
"the coordinate vectors (1,0,0), (0,1,0), (0,0,1).",vector spaces
"At the end of the season, the polls rank college football teams. The ranking is mostly an",vector spaces
"arises constantly in engineering analysis. If A and M are positive deﬁnite, this general-",pos_def_matrices
"Solving Ax = b, Ux = c, and Rx = d",vector spaces
and the professor’s guess?,orthogonality
loops. Now it has a linear algebra proof for any connected graph:,vector spaces
by n) are eigenvectors of ATA. The r singular values on the diagonal of Σ (m,pos_def_matrices
The rows have n components and the columns have m. For a simple matrix like,vector spaces
u + v + w = 1.,gauss elim
we have no idea whether R(x) is above or below them. That makes the intermediate,pos_def_matrices
"(0,x1,x2,x3). Find also the left shift matrix B from R4 back to R3, transforming",vector spaces
18. (Recommended) It is impossible for a system of linear equations to have exactly two,gauss elim
"time a pivot row is used,",gauss elim
The polynomials constructed in this way are called the Legendre polynomials and they,orthogonality
"and n by n matrices. We preferred to deﬁne the determinant by its properties, which",determinants
"4 at ω = 1. In this example, the right choice of ω has again doubled the rate of",computations
21. Explain why the pivot rows and pivot columns of A (not R) always give an r by r,vector spaces
Chapter 8 Linear Programming and Game Theory,linear_prog
"v3 = x2 − (1,x2)",orthogonality
"Figure 6.5 shows what problem the linear algebra has solved, if the constraint keeps",pos_def_matrices
28. The n by n determinant Cn has 1s above and below the main diagonal:,determinants
"of the eigenvalues. If µ is an eigenvalue of A + E, then its distance from one of the",computations
"25. Suppose a33 = 7 and the third pivot is 5. If you change a33 to 11, the third pivot is",gauss elim
"deﬁnite color. We can code the colors, and send back 1,000,000 numbers. It is better to",pos_def_matrices
"Paradoxically, the way to understand the good case is to study the bad one. Therefore",gauss elim
"completely helpless. Far from it, because “Lagrange multipliers” will bring back zero",linear_prog
the determinant of A; it is not zero.,vector spaces
1 0 0 0,vector spaces
"Property 1 If A = AH, then for all complex vectors x, the number xHAx is real.",eigenvec_val
"every step (as in elimination), or we can work with a whole column at once. For a single",computations
"2.3 Linear Independence, Basis, and Dimension",vector spaces
uk → c1x1 = u∞ = steady state.,eigenvec_val
"1. Construct a system with more unknowns than equations, but no solution. Change the",vector spaces
"from one time step to the next, leaves u∞ unchanged.",eigenvec_val
"required seven units of the second vitamin, but actually supplied 5x1 +3x2 = 15. So we",linear_prog
toward it. Dantzig chose an edge that leads to a new corner with a lower cost. There,linear_prog
That completes the ﬁrst idea—the projection that gives the steepest feasible descent.,linear_prog
"They have to be solved! One possibility is to determine x from part of the system, and",orthogonality
"42. Suppose S is spanned by the vectors (1,2,2,3) and (1,3,3,2). Find two vectors that",orthogonality
"16. Find the 4 by 4 cyclic permutation matrix: (x1,x2,x3,x4) is transformed to Ax =",vector spaces
"Its nth power is e2πi, which equals 1. For n = 8, this root is (1+i)/",orthogonality
"In the continuous case, the Fourier series can reproduce f(x) over a whole interval. It",orthogonality
into PA = LU:,computations
"37. Why can’t a 1 by 3 system have xp = (2,4,0) and xn = any multiple of (1,1,1)?",vector spaces
"11. Prove that if you keep multiplying A by the same permutation matrix P, the ﬁrst row",determinants
1 3 0 −1,vector spaces
which was assumed in Sections 5.3 and 5.4.,eigenvec_val
diagonal equals its “mirror image” on the other side: aij = a ji. Two simple examples are,gauss elim
0 0 0 0 0 0,vector spaces
"Proof. If the same S diagonalizes both A = SΛ1S−1 and B = SΛ2S−1, we can multiply",eigenvec_val
has eigenvalues 0 and 1,eigenvec_val
1.5 Triangular Factors and Row Exchanges,gauss elim
Ax = b if b = [1 2 7]T?,orthogonality
"Until those optimal vectors are actually produced, the duality theorem is not complete.",linear_prog
"L and U!). Now we converge to the smallest eigenvalue λ1 and its eigenvector x1,",computations
produces a stretching in the x-direction. Draw the circle x2 +,vector spaces
"As a side remark, notice the degenerate case a = 0. All multiples of a are zero, and",orthogonality
"7. Show that x = (1,1,1,0) and y = (1,1,0,1) are feasible in the primal and dual, with",linear_prog
In this example the dual is easier to solve than the primal; it has only one unknown,linear_prog
continue on the assumption that a corner has been found.,linear_prog
�� has b in column j.,determinants
3.3 Projections and Least Squares,orthogonality
columns lie in the same plane.,gauss elim
Remark 4. Eigenvectors of AAT and ATA must go into the columns of U and V:,pos_def_matrices
"The rows are multiples of (1,3). The nullspace contains x = (−3,1), which is orthogonal",orthogonality
that there can be no algebraic formula for the roots of a ﬁfth-degree polynomial.,eigenvec_val
is the projection. Figure 3.9b is in three dimensions (or m dimensions if there are m,orthogonality
fundamental subspaces do they project onto?,pos_def_matrices
This leads hack to a key property of matrix multiplication. Suppose the shapes of,gauss elim
Every product Ax can be found using whole columns as in equation (5).,gauss elim
"onto the line through a, and Pb is the point p = �xa. Rank-1 projections correspond",orthogonality
13. Find the best straight-line ﬁt (least squares) to the measurements,orthogonality
"The ﬁrst question is: Can we produce y1 units of steel, y2 units of food, and y3 units of",eigenvec_val
0 1 4 0,vector spaces
admit that A−1 solves Ax = b in one step. Two triangular steps are better:,gauss elim
the columns of A and try to solve Ax = b. What is the result for,vector spaces
Look for a combination of the columns that makes zero:,vector spaces
"28. If Az = 0, then AHAz = 0. If AHAz = 0, multiply by zH to prove that Az = 0. The",eigenvec_val
. Normally 4 col-,gauss elim
"A and M are being simultaneously diagonalized. If S has the x j in its columns, then",pos_def_matrices
"n. If the n planes have no point in common, or inﬁnitely many points, then the n",gauss elim
"Ay > tmaxy, and tmax could have been larger. This contradiction forces the equality Ax =",eigenvec_val
vector b? You need not compute �x.,orthogonality
"Both proofs assume that A and B are nonsingular; otherwise AB is singular, and the",determinants
1.33 Suppose the matrices in PA = LU are,vector spaces
we are projecting onto. When we project again nothing is changed. The vector Pb is,orthogonality
", ﬁnd the W-inner product of x = (2,3) and y = (1,1), and the W-length",orthogonality
Problems 11–18 are about the space spanned by a set of vectors. Take all linear,vector spaces
"13. Find the dimensions of (a) the column space of A, (b) the column space of U, (c) the",vector spaces
S from the pivot rows and pivot columns of each A:,vector spaces
cost = (cN −cBB−1N)xN +cBB−1b = rxN +cBB−1b.,linear_prog
"parallelogram, and compute its area. Choose T, U, V so that OPQRSTUV is a tilted",determinants
independent vectors. Why not four? This plane is the nullspace of what matrix?,vector spaces
dependent!) directions. The dimension of the space Rn is n. The column space of U,vector spaces
"For a single loop of 10 nodes and 10 edges, the Euler number is 10−10+1. If those 10",vector spaces
(d) What is the minimum if b = 3?,pos_def_matrices
"27. Complete these matrices so that detA = 25. Then trace = 10, and λ = 5 is repeated!",eigenvec_val
At the end of the ﬁrst year the numbers outside and inside are y1 and z1:,eigenvec_val
2 5 7 6,vector spaces
of x. What line of vectors is W-perpendicular to y?,orthogonality
was to look at the power series for the exponential:,orthogonality
"is not zero (Chapter 4). In MATLAB, the invertibility test is to ﬁnd n nonzero pivots.",gauss elim
"8 equals 1.) The eight points each move through 45°, but they remain",orthogonality
"28. Construct a matrix with (1,0,1) and (1,2,0) as a basis for its row space and its",vector spaces
"mechanics of the simplex method will solve a linear program, but duality is really at the",linear_prog
and C = N−1BN.) Which matrices are similar to I?,eigenvec_val
by solving Ay = b. Therefore the Rayleigh-Ritz method has three steps:,pos_def_matrices
But the second approach brings out the analogy with a differential equation: The pure,eigenvec_val
1. to introduce the most famous inﬁnite-dimensional vector space (Hilbert space);,orthogonality
tions and h = 1,pos_def_matrices
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
It is permitted that a different combination of w’s could give the same vector v. The,vector spaces
That Vandermonde matrix is n by n and full rank. Ax = b always has a solution—a,vector spaces
origin. The cross section is an ellipsoid of one lower dimension. The major axis Of this,pos_def_matrices
upper left 2 by 2 submatrix B (without row 3 and column 3)? Explain why.,gauss elim
Cost at this corner = cBB−1b.,linear_prog
the a’s are not orthogonal.,orthogonality
multiple of a. The angle is θ = 0° or θ = 180° and the cosine is 1 or −1. In this case b,orthogonality
1 0 0 0,determinants
6. A basic identity for quadratics shows y = A−1b as minimizing:,pos_def_matrices
agonal (and the letter U is already taken). The off-diagonal entries of R are the numbers,orthogonality
"x2 ≥ 0, 2x1 ≥ 4, x1 +3x2 ≥ 11? Find the solution to both this problem and its dual,",linear_prog
and conﬁrm that block multiplication succeeds.,gauss elim
"6. If all entries of A, b, and c are positive, show that both the primal and the dual are",linear_prog
"Ux = 0 among the columns of U, with exactly the same coefﬁcients. If a set of columns",vector spaces
"Working upward in the usual way, c3 must be zero because the pivot d3 ̸= 0, then c2 must",vector spaces
"Translating back to F, that means that ∂ 2F/∂x2 > 0. The graph must go up in the x",pos_def_matrices
"dw/dt = 5w+x. Follow the pattern of solutions for z, y, x to ﬁnd w.",eigenvec_val
really matters is addition of vectors and multiplication by a scalar (a number). In Figure,gauss elim
"this subtraction, we continually meet a “multiply-subtract” combination; the terms in",gauss elim
tion x1q1 +···+xnqn = b is identical to Qx = b. (The columns of Q multiply the compo-,orthogonality
"15. For four linear equations in two unknowns x and y, the row picture shows four",gauss elim
0 0 0 7,gauss elim
It must be multiplication T(x) = Ax by the matrix,vector spaces
"exactly when such coefﬁcients exist, and the vector (u,v) is the solution x.",vector spaces
4x1 +6x2 = 1.,determinants
equations and solve again—the errors should almost disappear.,computations
"lowest point, where the derivative is zero:",orthogonality
"In the 2 by 2 case, the product rule could be patiently checked:",determinants
2. Solve −u′′ = x with u(0) = u(1) = 0. Then solve approximately with two hat func-,pos_def_matrices
30. Use elimination to solve,gauss elim
subspaces contains eigenvectors with λ = 1? Which subspace contains eigenvectors,eigenvec_val
"8. If V and W are orthogonal subspaces, show that the only vector they have in common",orthogonality
"(c) The horizontal line �b = 3 is closest to b = (1,2,6), Check that p = (3,3,3) is",orthogonality
3 is a constant.,eigenvec_val
"In practice, ykExk is a realistic estimate of δλ. The idea in every good algorithm is to",computations
x2 +c2y = 0,pos_def_matrices
reduced echelon form (the block B should be r by r):,vector spaces
Orthogonal complements V = W⊥,orthogonality
necessary and sufﬁcient condition for positive deﬁniteness. The simplest technique is to,pos_def_matrices
"−1, 2, −1 in x and y",computations
Three columns in the same plane,gauss elim
(a) Add vectors along axes,gauss elim
Note. We write AB when the matrices have nothing special to do with elimination. Our,gauss elim
"to ﬁnd the optimal pair x∗, w∗. If w∗ = 0, then x∗ is the required corner in the",linear_prog
"For n = 4, use the ﬁrst line to ﬁnd y0 and y1, and the second to ﬁnd y2 and y3, all in",orthogonality
Jordan form M−1AM = J is block diagonal.,eigenvec_val
"and S⊥ are perpendicular lines in the plane, one or the other must enter the ﬁrst quadrant.",linear_prog
(a) Every positive deﬁnite matrix is invertible.,pos_def_matrices
"To reach the fully reduced row echelon form R = rref(T), subtract cB times the top",linear_prog
to a fast sine transform.,orthogonality
"42. If P1 and P2 are permutation matrices, so is P1P2. This still has the rows of I in some",gauss elim
1.17 Find the symmetric factorization A = LDLT of,gauss elim
web.mit.edu/18.06) to ﬁnd the same vectors v1 and v2 graphically.,pos_def_matrices
"1. They test for invertibility. If the determinant of A is zero, then A is singular. If",determinants
xB +B−1NxN = B−1b,linear_prog
tions (6) look for a four-term Fourier series that matches the inputs at four equally spaced,orthogonality
"2. Working a column at a time, compute the products",gauss elim
"We look at x = 1, y = 0, where ax2 + 2bxy + cy2 is equal to a. This must be positive.",pos_def_matrices
algebra; it is a subspace of the original space R3.,vector spaces
"different corners, which means an operation count of about m2n. That is comparable to",linear_prog
This singular case has no solution. Other singular cases have inﬁnitely many solu-,gauss elim
"(a) Verify that the best line goes through the center point (�t,�b) = (2,9).",orthogonality
0 0 0 x x,determinants
if uk+1 = Auk,eigenvec_val
"7) were those errors, what would be the",orthogonality
Check part 3 by carefully multiplying MM−1 to get I:,gauss elim
0 −8 −2 −12,gauss elim
(D−1···E ···P···E)A = I.,gauss elim
10. Find a 2 by 3 system Ax = b whose complete solution is,vector spaces
"24. Suppose T(v) = v, except that T(0,v2) = (0,0). Show that this transformation satis-",vector spaces
0 1 0 0,determinants
coming from the rectangular matrices of Chapter 2.,pos_def_matrices
"clearer, and more permanently understood.",orthogonality
Those coefﬁcients a and (ac − b2)/a are the pivots for a 2 by 2 matrix. For larger,pos_def_matrices
"(c) What condition on a, b, c, d will make part (b) impossible?",vector spaces
0 0 2 1,vector spaces
48. Reduce to Ux = c (Gaussian elimination) and then Rx = d:,vector spaces
pivots. The numbers 1 and 11 have no inﬂuence on those pivots.,gauss elim
question. Combine a 2 by 2 block with a 3 by 3 block.),gauss elim
"25. (Recommended) If we add an extra column b to a matrix A, then the column space",vector spaces
"matrix C, which is m by m. C reﬂects “material properties,” in contrast to the incidence",vector spaces
earlier constraint of sufﬁcient protein. The entry aij measures the ith vitamin in the jth,linear_prog
row of AB is a combination of the rows of B.,gauss elim
5. Count row exchanges to ﬁnd these determinants:,determinants
"subtracting 2 dropped it below zero. The next step looks at A−I, to see if λmin < 1. (It",pos_def_matrices
". When E32 comes ﬁrst, row",gauss elim
2. The determinant of A equals the volume of a box in n-dimensional space. The edges,determinants
but they are more complicated than pure exponentials eλtx. They involve “generalized,eigenvec_val
"5.3 and 5.4. Appendix B returns to the nondiagonalizable case, and shows how the",eigenvec_val
"dimensions can be too small. The line V spanned by (0,1,0) is orthogonal to the line",orthogonality
Chapter 1 Matrices and Gaussian Elimination,gauss elim
or even LDU. The rule for matrix multiplication stays the same.,gauss elim
"diet is 2x+3y. Fortunately, the optimal diet is two steaks: x∗ = 0 and y∗ = 2.",linear_prog
"38. (a) If Ax = b has two solutions x1 and x2, ﬁnd two solutions to Ax = 0.",vector spaces
11. (a) If A =,pos_def_matrices
29. Compute A10 and eA if A = MJM−1:,eigenvec_val
in the theory of elasticity.,orthogonality
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
39. Can tic-tac-toe be completed (5 ones and 4 zeros in A) so that rank(A) = 2 but neither,vector spaces
�� = 4(x1 −x2 +2x3)2.,pos_def_matrices
0 0 3 3,vector spaces
piece the mesh together from triangles or quadrilaterals or tetrahedra. Then we need a,pos_def_matrices
"34. If the pivots of a matrix are all greater than 1, are the eigenvalues all greater than 1?",pos_def_matrices
"shape, so they can be added, and A and D are the right size for premultiplication and",gauss elim
2vk+1 = wk +b1,computations
2. The inverse power method operates with A−1 instead of A. A single step is vk+1 =,computations
minimized E2 = (x−b1)2 +(x−b2)2:,orthogonality
This was almost the ﬁrst practical method of computing eigenvalues. It was dominant,pos_def_matrices
column vector) is a 1 by 1 matrix:,gauss elim
(a) The determinant of S−1AS equals the determinant of A.,determinants
"(b) Write A as QR, where Q has orthonormal columns and R is upper triangular.",orthogonality
20. For A =,eigenvec_val
p = (I −A)−1y.,eigenvec_val
A = LU =,vector spaces
"ATA is a 1 by 1 matrix, and the normal equation is 2�x = b1 +b2.",orthogonality
Problems 22–31 are about elimination matrices.,gauss elim
"Notice that QTQ is the n by n identity matrix, whereas QQT is an m by m projection P.",orthogonality
probabilities. With probability 1,eigenvec_val
"so perfectly into the framework of applied linear algebra, that they can be explained in a",linear_prog
Now suppose the time step is reduced to a month. The new difference equation is pk+1 =,eigenvec_val
subtractions. Much larger problems than usual can be solved.,linear_prog
the probability of moving out is 2,eigenvec_val
Problems 4–5 require Gershgorin’s “circle theorem”: Every eigenvalue of A lies in at,computations
dt = r +w.,eigenvec_val
"antidiagonal going from (1,n) to (n,1).",gauss elim
"(b) For A = any J, build M0 from blocks so that M−1",eigenvec_val
dt = Au =,eigenvec_val
"37. If C is nonsingular, show that A and CTAC have the same rank. Thus they have the",pos_def_matrices
1. The inner product of x and y is xHy. Orthogonal vectors have xHy = 0.,eigenvec_val
"cients −3 and 1 add to zero, so ATy = 0.",vector spaces
approaches to understanding Ax = b.,vector spaces
(c) subtract −1 times the second equation from the third.,gauss elim
Yesterday morning nine cartons of oranges arrived at the loading dock. They,vector spaces
"side in order for Ax = b to be consistent. Unless b3 −2b2 +5b1 = 0, the third equation",vector spaces
interior point methods (staying inside the feasible set) are a major success—mentioned,linear_prog
The beauty of network theory is that both A and AT have important roles.,vector spaces
. The nullspace of the matrix is,orthogonality
"(b) If 25 planes meet at two points, where else do they meet?",gauss elim
"“degrees of freedom” of the space, is the dimension of V.",vector spaces
theoretical interest. The newest problem is the cost with many processors in parallel.,gauss elim
b b b a,gauss elim
ABx = Aµx = µAx = µλx.,eigenvec_val
there may be no solution. The number of solutions is 0 or 1.,vector spaces
5. Best Straight Line.,orthogonality
"advance, the multiplier will change to ℓ31 = 1 in PA = LU.",gauss elim
"zero. This forces c1 = 0, and ultimately every ci = 0. Therefore eigenvectors that come",eigenvec_val
"Split b into p + q, with p in the column space and q perpendicular to that space.",orthogonality
19. Which 3 by 3 symmetric matrices A produce these functions f = xTAx? Why is the,pos_def_matrices
"that have superseded almost all of their predecessors: the QR algorithm, the family of",computations
(Spectral Theorem) Every real symmetric A can be diagonalized by an,eigenvec_val
u(t) = c1eλ1tx1 +c2eλ2tx2,eigenvec_val
nal. They are even orthonormal after division by their length √π.,orthogonality
"bers y0,...,yn−1, instead of a function f(x). The output c0,...,cn−1 has the same length",orthogonality
makes the ﬁrst term on the right as small as possible; it is the key to convergence of,pos_def_matrices
0 −8 0 −4,gauss elim
"B, then the eigenvalues multiply and AB has the eigenvalue µλ. But there is something",eigenvec_val
"(a) What are the determinants of C1, C2, C3, C4?",determinants
"Those are both true. F−1 has been known for years, and it looks just like F. In fact,",orthogonality
i x j = 0,eigenvec_val
"41. If you know xp (free variables = 0) and all special solutions for Ax = b, ﬁnd xp and",vector spaces
Chapter 1 Matrices and Gaussian Elimination,gauss elim
Chapter 2 Vector Spaces,vector spaces
known from the boundary conditions. These values would be shifted to the right-hand,gauss elim
is the projection onto the subspace V. The orthogonal component w is the projection of,orthogonality
C and D. But the least-squares principle will give optimal values �C and �D.,orthogonality
(b) Choose a particular function that satisﬁes dy,vector spaces
4(0+1+3+4) = 2. The average of the four b’s,orthogonality
"The components b j on the right side are new. Instead of just the value of f at x j,",pos_def_matrices
"(c) The subspace spanned by (1,1,1,1), (1,2,3,4), and (2,3,4,5).",vector spaces
"x = 0, and y ≥ 0 is the halfspace above y = 0.",linear_prog
"30. By locating the pivots, ﬁnd a basis for the column space of",vector spaces
invertible (Section 4.2). Rewrite xTATAx to show why it is positive except when,pos_def_matrices
2. Compute the coefﬁcients Aij and b j.,pos_def_matrices
"and a sink, we are now minimizing the cost of connecting all the nodes. There are no",linear_prog
"x = A−1b? Before starting on that question, we need a way to measure A and the change",computations
"identical, because they are looking at different hands. Say the chance that they are",orthogonality
But there is a chance that b does lie in the plane of the columns. In that case there are too,gauss elim
"by a ji. We will see this symmetry in the next section, for differential equations. Here,",gauss elim
The corner point P in Figure 8.3 is the intersection of x = 0 with 2x+y−6 = 0.,linear_prog
follows by factoring each di from its row. Reduce a general matrix A to D by,determinants
0 1 1 0,vector spaces
Solve Ac = 0,vector spaces
The ﬁrst problem was solved in Young’s 1950 thesis—a simple formula for the optimal,computations
ThenU is the transpose of L. The symmetric factorization becomes A = LDLT.,gauss elim
2. A rotation matrix turns the whole space around the origin. This,vector spaces
"t = 0,z = 3",orthogonality
c0 +c1 +c2 +c3,orthogonality
0 x x x,gauss elim
"QTQ = QTb. But QTQ is the identity matrix! Therefore �x = QTb, whether Q is square",orthogonality
"and not an ellipse. A hyperbola is a cross-section through a saddle, and an ellipse is a",pos_def_matrices
Start from Ax = λx and substitute A = MBM−1:,eigenvec_val
not invertible. Therefore the original A was not invertible. Elimination gives a complete,gauss elim
"rank was introduced as the number of pivots in the elimination process. Equivalently,",vector spaces
(a) A is not invertible.,eigenvec_val
"This cost is nonlinear (but linear programming is already nonlinear, from inequalities).",linear_prog
"Proof. It is easy to see why P2 = P. If we start with any b, then Pb lies in the subspace",orthogonality
"y + z = 2 and 2y + 2z = 4 is xp = (1,1). The nullspace of A in Figure 2.2",vector spaces
35. Use the pivots of A− 1,pos_def_matrices
"have n positive eigenvalues, conﬁrming the law of inertia.",pos_def_matrices
4 of those in Section A and 1,eigenvec_val
(c) the lower triangular matrix UT.,determinants
Every vector f in the row space has xT f = f1+···+ fn = 0—the currents from outside,vector spaces
22. Apply elimination and back-substitution to solve,gauss elim
"According to formula (2), the ratio between aTb and ∥a∥∥b∥ is exactly |cosθ|. Since",orthogonality
U. The theory of Gaussian elimination can be summarized in a few lines:,gauss elim
2. The numbers outside and inside can never become negative: The matrix has no,eigenvec_val
"44. If a subspace S is contained in a subspace V, prove that S⊥ contains V⊥.",orthogonality
"If the ﬁrst coefﬁcient is zero, in the upper left corner, the elimination of u from the",gauss elim
"positive (unless x = 0), because R has independent columns. (If x is nonzero then Rx is",pos_def_matrices
"by its pivot. On the left-hand side, this produces R, as before. On the right-hand side,",vector spaces
"b then yAx = yb ̸= 0, and this contradicts yAx = 0x = 0. In the language of subspaces,",linear_prog
5 by 5 matrix,determinants
"that govern stability: If λ = a+ib, then",eigenvec_val
"1. Image processing Suppose a satellite takes a picture, and wants to send it to Earth.",pos_def_matrices
onalize it—is to ﬁnd its eigenvectors. They go into the columns of M (or S) and M−1AM,eigenvec_val
"columns b1, b2, b3, the columns of AB should be Ab1, Ab2, Ab3!",gauss elim
"12. Starting with x+4y = 7, ﬁnd the equation for the parallel line through x = 0, y = 0.",gauss elim
6. With A =,pos_def_matrices
both λ < 0,eigenvec_val
"1.8 (a) Construct a matrix whose nullspace contains the vector x = (1,1,2).",vector spaces
satisfy the constraint Cx = d. Verify that equation (5) correctly gives PC/min = Pmin;,pos_def_matrices
T(v) = largest component of v.,vector spaces
"entries a1 j from row 1 with the cofactors C2j for row 2, why is the result zero?",determinants
"λ = 1 has a plane of eigenvectors, and we pick an orthonormal pair x1 and x2:",eigenvec_val
thogonal complement S⊥ both to contain positive vectors. Their inner product would,linear_prog
5 by 5 matrix [A b] is invertible. Show that Ax = b is solvable when [A b] is singular.,vector spaces
"There we have something important! The last matrix is multiplying u(0), so it must be",eigenvec_val
(e) The determinant of AB−BA is zero.,determinants
the equations Ax = 0 are independent. Choosing the n−r “special solutions” to Ax = 0,vector spaces
"24. Project the vector b = (1,1) onto the lines through a1 = (1,0) and a2 = (1,2). Draw",orthogonality
"left nullspace. The real action is between the row space and column space, and you see",orthogonality
a22(x2)k+1 = −a21(x1)k+1 +(−a23x3 −···−a2nxn)k +b2.,computations
"must lie in the nullspace of A. Therefore we project −c onto the nullspace, to ﬁnd the",linear_prog
"4.10 If detA > 0, show that A can be connected to I by a continuous chain of matrices",determinants
(d) S is diagonalizable.,eigenvec_val
Suppose A = QΛQT with λi > 0. Rotating y = QTx simpliﬁes xTAx = 1:,pos_def_matrices
"If d = 0, the problem is incurable and this matrix is singular. There is no hope for a",gauss elim
10. (a) Why does it take approximately n2/2 multiplication-subtraction steps to solve,gauss elim
Second Proof. The contrast with this “coordinate-free proof” should be useful to the,orthogonality
positive—proving again the law of inertia in Section 6.2.,pos_def_matrices
These optimality conditions are easy to understand in matrix terms. From equation,linear_prog
Example 4. T =,eigenvec_val
"columns are clearly orthogonal, and they are orthonormal because sin2θ + cos2θ = 1.",orthogonality
if AB = BA.,eigenvec_val
"17. Prove that every unitary matrix A is diagonalizable, in two steps:",eigenvec_val
"vector—like an n by 1 matrix. But sometimes it is printed on a line, as in x = (2,5,0).",gauss elim
32 . The three angles are Euler angles. Choose the ﬁrst θ so that,computations
"This conﬁrms that the proposed solution x = (1,1,2) does satisfy the ﬁrst equation.",gauss elim
inequality divides n-dimensional space into a halfspace in which the inequality is satis-,linear_prog
Example 1. Suppose a battery b3 and a current source f2 (and ﬁve resistors) connect,vector spaces
1.1 Find a basis for the following subspaces of R4:,vector spaces
Real equations can have complex solutions. The equation x2 +1 = 0 led to the invention,orthogonality
"commute. Here the matrices have meaning. There was a reason for EF = FE, and a",gauss elim
Then solve the system Ac = 0; the vectors are dependent if there is a solution other than,vector spaces
2. The nullspace of A Elimination simpliﬁes a system of linear equations without,vector spaces
(c) Find the limiting distribution of the $4 trillion as the world ends.,eigenvec_val
Take determinants of these matrices to prove correct rules for square blocks:,determinants
(a) T(v) = −v.,vector spaces
"A, we do not intend to go back to the power method. There are much more powerful",computations
Example 3 (SOR). For the same A =,computations
"3I All vectors a and b satisfy the Schwarz inequality, which is |cosθ| ≤ 1 in",orthogonality
"b1v1 + ··· + bkvk, then subtraction gives 0 = ∑(ai − bi)vi. Now independence plays its",vector spaces
is the zero vector: V∩W = {0}.,orthogonality
The single matrix F with n2 nonzeros is a product of approximately ℓ = log2n matrices,orthogonality
1.5 Triangular Factors and Row Exchanges,gauss elim
0 0 1 0,determinants
The eigenvalue of B is still λ. The eigenvector has changed from x to M−1x.,eigenvec_val
"45. (VISA to AVIS) This takes an odd number of exchanges (IVSA, AVSI, AVIS). Count",determinants
Projection onto a plane = sum of projections onto orthonormal q1 and q2.,orthogonality
squared lengths of the four sides equals the sum of the squared lengths of the two,orthogonality
"planes in 4 dimensions, and it solves the 4 underlying equations.",gauss elim
"calculus, is to set the partial derivatives to zero. This gives Ax = b:",pos_def_matrices
29. For which numbers c and d do these matrices have rank 2?,vector spaces
"All this time, the simplex method was doing the job—in an average time that is now",linear_prog
3. From AB = C ﬁnd a formula for A−1. Also ﬁnd A−1 from PA = LU.,gauss elim
set of values A2p0. The question is whether the prices approach equilibrium. Are there,eigenvec_val
and test the prediction for n = 4.,determinants
8. Suppose there is an epidemic in which every month half of those who are well be-,eigenvec_val
"textbook Introduction to Applied Mathematics, and the new Applied Mathematics and",vector spaces
side g that makes it solvable. Find two solutions in that singular case.,gauss elim
"leaves some permutation (β,...,v) of the remaining columns (2,...,n). We collect all",determinants
0x = 0 has inﬁnitely many solutions. The nullspace contains all x. A particular,vector spaces
5 4 0 3,determinants
"Signals are digitized, whether they come from speech or images or sonar or TV (or",orthogonality
"space and produces the three-dimensional universe we live in (or rather, the universe as ",gauss elim
"If these eigenvectors are the columns of a matrix S, then S−1AS is a diagonal",eigenvec_val
"the ﬁrst square—which is either positive semideﬁnite, when a > 0, or negative semidef-",pos_def_matrices
+ 3z = 0.,determinants
could have gone further forward to a node in T. Thus the maximal ﬂow does ﬁll this cut,linear_prog
"66. If every row of a 4 by 4 matrix contains the numbers 0, 1, 2, 3 in some order, can the",gauss elim
0 6 7 0,vector spaces
"The ﬁrst stage is complete, and U−1",computations
(d) Under what conditions on b does Ax = b have a solution?,eigenvec_val
"planes from the constraints, and what shape is the feasible set? How do its corners",linear_prog
2. The cost function is unbounded on the feasible set.,linear_prog
26. Find the LU factorization of A =,computations
"2.3 Linear Independence, Basis, and Dimension",vector spaces
arriving at the Times sports department. The irritation stems from the fact that,vector spaces
"1. The block power method works with several vectors at once, in place of uk. If we",computations
Ay = λMy. That Λ1 will be close to (and above) π2. The eigenvector y will give the,pos_def_matrices
"b = (0,1), what vector y will satisfy the alternative?",linear_prog
The matrix form also shows what happens when the columns are not orthonormal.,orthogonality
3.38 If the columns of A are orthogonal to each other what can you say about the form,orthogonality
"a nonnegative twist, since steel, food, and labor cannot come in negative amounts. Von",eigenvec_val
21. Describe the column spaces (lines or planes) of these particular matrices:,vector spaces
This tells us that (bTb)(aTa) ≥ (aTb)2—and then we take square roots:,orthogonality
. S⊥ is a subspace even if S is not.,orthogonality
"Those are the columns of the identity matrix. They form the simplest basis for Rn, and",orthogonality
1.3 An Example of Gaussian Elimination,gauss elim
"The ﬁrst two methods are iterative, and the last is direct. It does its job in a ﬁnite",computations
Chapter 2 Vector Spaces,vector spaces
There is a unitary matrix M = U such that U−1AU = T is triangular.,eigenvec_val
"beyond (I −A)−1, to decide natural prices and questions of optimization. Normally la-",eigenvec_val
equation does the solution un stay on a circle?,eigenvec_val
"(c) Suppose (x1,x2) + (y1,y2) is deﬁned to be (x1 + y2,x2 + y1). With the usual",vector spaces
is all of W and the kernel contains only v = 0. Why are these transformations not,vector spaces
Chapter 2 Vector Spaces,vector spaces
"n. In matrix terms,",orthogonality
"Remark. If A is real and its eigenvalues happen to be real, then its eigenvectors are also",eigenvec_val
"nents. Pb is in the column space C(A), and the other component (I − P)b is in the left",orthogonality
"nalizable A equals the trace of Λ, which is",eigenvec_val
"27. (a) Find the area of the parallelogram with edges v = (3,2) and w = (1,4).",determinants
only the man 4. If p women can marry only n−q men and p > n−q (which is the same,linear_prog
(c) Two subspaces that meet only in the zero vector are orthogonal.,orthogonality
4.4 Applications of Determinants,determinants
1. Eij to subtract a multiple ℓ of row j from row i,gauss elim
r in the row space gives Ax′,orthogonality
8. Which of the following descriptions are correct? The solutions x of,vector spaces
main theme of the chapter: To transform A into a diagonal or triangular matrix without,eigenvec_val
Matrix multiplication imposes those rules on the transformation. The second rule con-,vector spaces
it cannot be solved.,gauss elim
programming is to ﬁnd the point that lies in the feasible set and minimizes the cost.,linear_prog
real part is plotted on the x-axis and the imaginary part on the y-axis (Figure 3.11). Then,orthogonality
they are continually changing and improving. In numerical analysis there is a survival,computations
might have the same capacity. Certainly the total ﬂow can never be greater than the total,linear_prog
cannot happen if all ai j > 0.,eigenvec_val
12. Suppose A is a linear transformation from the x-y plane to itself. Why does A−1(x+,vector spaces
"deduce that xTAx > 0. This is what we did in the 2 by 2 case, by completing the square.",pos_def_matrices
n(1+W +W 2 +···+W n−1) = W n −1,eigenvec_val
"We now come to the key examples, the column space and the nullspace of a matrix",vector spaces
many nonzero vectors come out of Gram-Schmidt?,orthogonality
"least-squares solution �x, which minimizes E, is the same as locating the point p = A�x",orthogonality
positive because their sum is the trace a+c > 0.,pos_def_matrices
"not nonnegative. If λ1 = 1, then I −A is singular. The productive case is λ1 < 1, when",eigenvec_val
them are familiar and two are new.,vector spaces
Chapter 2 Vector Spaces,vector spaces
23. Find the eigenvalues of A and B and A+B:,eigenvec_val
"jointly with George Fix. Other books give more detailed applications, and the subject",pos_def_matrices
Including the barrier gives an approximate problem P(θ). For its Kuhn-Tucker op-,linear_prog
rithm in scientiﬁc computing. We mention one more method—Arnoldi in ARPACK—,computations
(a) Why are rows of U subtracted off and not rows of A? Answer: Because by the,gauss elim
(b) λ1 > 0 and λ2 > 0.,eigenvec_val
The unknown is x =,gauss elim
"times the constant vector (C,C,C,C,C), yields zero; A0 is singular. Analogously, if",gauss elim
How many independent vectors satisfy ATy = 0?,vector spaces
. AB has rank 1 unless,vector spaces
Rotation to kill a21,computations
1 2 3 5 0,vector spaces
(b) the upper triangular matrix,determinants
1 3 2 0,vector spaces
Suppose we want to approximate y = x5 by a straight line,orthogonality
2n terms. Since w2,orthogonality
positive deﬁnite.) Then the substitution y = Rx changes,pos_def_matrices
tion. The system will be solvable only for a very “thin” subset of all possible b’s. One,vector spaces
. The cofactor of x3 is V3 =,determinants
1 1 1 1,pos_def_matrices
Figure 5.1: A model of diffusion between four segments.,eigenvec_val
"The ﬁrst Fibonacci numbers F0 = 0 and F1 = 1 go into u0, and S−1u0 = c:",eigenvec_val
5. Multiply Ax to ﬁnd a solution vector x to the system Ax = zero vector. Can you ﬁnd,gauss elim
This will be a vector Ax and also B�x. Think 3 by 4 with the matrix [A B].,orthogonality
"highly composite numbers like 210 = 1024. Without the fast transform, it takes (1024)2",orthogonality
entirely on the other columns. No row or column can be used twice in the same term.,determinants
"(d) (1,2,2), (−1,2,1), (0,8,6).",vector spaces
(b) The intersection of two subspaces of a vector space cannot be empty.,vector spaces
"variance σ2, so the average of (b − Ax)(b − Ax)T is σ2I. Multiply on the left by",orthogonality
∂x∂y = ∂ 2F,pos_def_matrices
+ 2z = 5.,gauss elim
"usually happens in practice, if an abnormally small pivot is not avoided, is that it is very",determinants
"19. Three planes can fail to have an intersection point, when no two planes are parallel.",gauss elim
"ﬁnd that formula (Cramer’s Rule) in Chapter 4, but we want a good method to solve",gauss elim
second component 1. That leaves Fk = c1λ k,eigenvec_val
much more abstract than the main approach in this book. We preferred to begin directly,vector spaces
spaces! The ﬁrst pair is the nullspace and row space. Those are subspaces of Rn—the,orthogonality
"4.3 Starting with A, multiply its ﬁrst row by 3 to produce B, and subtract the ﬁrst row",determinants
"is the 2 by 2 identity matrix. In n dimensions the standard basis e1,...,en again consists",orthogonality
"The proof starts with Ax = λ1x, Ay = λ1y, and A = AH:",eigenvec_val
This “maximin principle” makes λ2 the maximum over all v of the minimum of R(x) with,pos_def_matrices
−2u − 8v + 3w =,gauss elim
"A Markov matrix A has all aij ≥ 0, with each column adding to 1.",eigenvec_val
Second description: The column space contains all vectors with b3+b2−5b1 = 0.,vector spaces
"At the corner, the free variables are xN = 0. There, Ax = b turns into BxB = b:",linear_prog
"Suppose we call each division, and each multiplication-subtraction, one operation. In",gauss elim
u(x) = sinnπx with λ = −n2π2. Then the solution to the heat equation is,eigenvec_val
"n trial functions V1(x),...,Vn(x). From all combinations V = y1V1(x) + ··· + ynVn(x),",pos_def_matrices
x + 2y ≥ 4,linear_prog
In theory the nonsingular case is completed. There is a full set of pivots (with row ex-,gauss elim
44. Give examples of matrices A for which the number of solutions to Ax = b is,vector spaces
share the same eigenvalues.,eigenvec_val
which there is pure compression or pure tension—with no shear.,eigenvec_val
0 0 0 0,vector spaces
"will be) deﬁned by its three most basic properties: detI = 1, the sign is reversed by a",determinants
to the true solution. The approximations had to be rough; the computers were human.,pos_def_matrices
(a) no solution for some b.,vector spaces
What are the eigenvalues of A? Find them also by substituting y = eλt into the scalar,eigenvec_val
"that give b = (2,3,5). This is only possible for b = (4,6,c) if c =",gauss elim
"The vector p = A�x is as close as possible to b. Of all straight lines b = C + Dt, we are",orthogonality
"applied forces of F = 1, 2, and 4 tons. Assuming Hooke’s law L = a+bF, ﬁnd his",orthogonality
"3. Find the special solutions to Ax = 0 (or Ux = 0 or Rx = 0). Each free variable, in",vector spaces
"the (i, j) position. Otherwise keep the identity matrix, with 1s on the diagonal and 0s",gauss elim
value problem? The condition number of the diagonalizing S measures the sensitivity,computations
of B from the second to produce C. How is detC related to detA?,determinants
0 t3dt = 1,vector spaces
"as three planes (front wall, side wall, and ﬂoor) produce a corner in three dimensions.",linear_prog
"15. If rows 1 and 2 are the same, how far can you get with elimination (allowing row",gauss elim
"No equation is written for node 4, where the current law is y4+y5+ f2 = 0. This follows",vector spaces
"x and y leads to condition (4) for n vectors x1,...,xn. The transformation does have a",vector spaces
Remark 4. The linear terms give a necessary condition: To have any chance of a mini-,pos_def_matrices
multiple of equation 1 will be subtracted from equation 3.,gauss elim
"12. (a) If A changes to 4A, what is the change in the SVD?",pos_def_matrices
"This leads directly to Property 1′, that multiplication by U has no effect on inner prod-",eigenvec_val
"(b) What does P−1 do to (1,2,3,4,5)?",determinants
"22. By setting the derivative to zero, ﬁnd the value of b1 that minimizes",orthogonality
"The problem comes down to this: For a function of two variables x and y, what is the",pos_def_matrices
25. Reverse the diffusion of people in Problem 24 to du/dt = −Au:,eigenvec_val
gives inﬁnitely many solutions. What are two of those solutions?,gauss elim
important consequences. Section 4.3 gives two more formulas for the determinant—the,determinants
constraints x j ≥ 0. This is the role of the slack variables w = Ax − b. The constraints,linear_prog
"(c) Find the least-squares solution to Ax = b, if b = (−3,7,1,0,4).",orthogonality
"1. Following the ﬁrst example in this section, ﬁnd the eigenvalues and eigenvectors,",eigenvec_val
2) at the end of the major axis. The minor,pos_def_matrices
Notice the columns of A. Column 3 gives information about node 3—it tells which,vector spaces
"(b) If a11 = a22 = a33 = a44 = 0, how many of the 24 products a1ja2ka3ℓa4m are sure",determinants
"whether a whole row or column needs to be resealed, are still possible. But essentially",gauss elim
"For novelty, we take the four subspaces in a more interesting order.",vector spaces
"the stopping condition r ≥ 0 that we know to be satisﬁed! Therefore our y∗ is feasible,",linear_prog
The strategy of complete pivoting looks also in all later columns for the largest pos-,gauss elim
"lies on the “θ-line.” The second basis vector (0,1) rotates into (−sinθ,cosθ). By",vector spaces
1 0 1 1,determinants
a single “1” in every row and column. The most common permutation matrix is P = I (it,gauss elim
22. What are the limits as k → ∞ (the steady states) of the following?,eigenvec_val
a reason if true or a counterexample if false:,eigenvec_val
"rows of U and R are zero, so there is a solution only if the last m−r entries of",vector spaces
B = [T]V to V =,eigenvec_val
to give the position of the 1 in each row.,gauss elim
1.29 Describe the linear transformations of the x-y plane that are represented with stan-,vector spaces
−2u + 7v + 2w =,gauss elim
"eigenvalues λ2,...,λn−1 harder to estimate.",pos_def_matrices
"condition—for b to lie in the column space. Then we choose b = (1,5,5) and ﬁnd all",vector spaces
is not invertible if and only if,determinants
vTw = v1w1 +v2w2 +v3w3 +··· = 0.,orthogonality
"With cofactorsC11,...,C1n in the ﬁrst column and not the ﬁrst row, they multiply a11,...,a1n",determinants
0 0 0 0,vector spaces
column j with column i. So RTR is symmetric.,gauss elim
1 2 3 4,vector spaces
The nullspace of a matrix consists of all vectors x such that Ax = 0. It is,vector spaces
"the right-hand side, leading to 0 = 0 as the ﬁnal equation. That vector y is a basis for",vector spaces
around loops is much easier than recognizing combinations of the columns.,orthogonality
"the left nullspace, so y1 and y2 are a basis (the dimension had to be m−r = 5−3 = 2).",vector spaces
the direction that is ampliﬁed most by A−1.,computations
"2. What is the dual of the following problem: Maximize y2 subject to y1 ≥ 0, y2 ≥ 0,",linear_prog
"nxn, no eigenvalue can be larger than 1. (Otherwise",eigenvec_val
"number of operations—we just use the new value instead of the old, and actually save",computations
four nodes. Node 4 is grounded and the potential x4 = 0 is ﬁxed. The ﬁrst thing is the,vector spaces
"second derivatives: Fxx, Fxy = Fyx, and Fyy. These three numbers (like 4, 4, 2) must",pos_def_matrices
and b = rsinθ. Combining these two equations moves us into polar coordinates:,eigenvec_val
Does the inverse of Qθ equal Q−θ (rotation backward through θ)? Yes.,vector spaces
"From the b column after elimination, read off m−r basis vectors in the left nullspace",vector spaces
"Creating zeros above the pivots, we reach A−1:",gauss elim
"combinations of the vectors (1,1,2) and (1,2,3). This is the reverse of the previous",orthogonality
"34. If U is unitary and Q is a real orthogonal matrix, show that U−1 is unitary and also",eigenvec_val
combine these “normal modes” to ﬁnd the solution. To say the same thing in another,eigenvec_val
16. Suppose P is the projection matrix onto the line through a.,orthogonality
33. Diagonalize B and compute SΛkS−1 to prove this formula for Bk:,eigenvec_val
8J Ax ≥ b has a solution x ≥ 0,linear_prog
0 1 0 3,vector spaces
Generalized problem Ax = λMx,pos_def_matrices
"The dimension of this nullspace N(AT) is easy to ﬁnd, For any matrix, the number",vector spaces
"ordinary elimination for Ax = b, and is the reason for the simplex method’s success. But",linear_prog
"we keep a ﬁxed number of signiﬁcant digits (say three, for an extremely weak computer).",gauss elim
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
. The rotation matrix has,determinants
"the network, across which all capacities are ﬁlled. That cut separates nodes 5 and 6 from",linear_prog
"Find the determinants of L, U, A, U−1L−1, and U−1L−1A.",determinants
"variables zero. Its pivot variables are the ﬁrst r entries of d, so Rxp = d.",vector spaces
4x1 +6x2 = 2.,determinants
"Compared with a general 5 by 5 matrix, that step displays two major simpliﬁcations:",gauss elim
"(b) A new corner at (−1,0) makes it lopsided (four sides). Find the area.",determinants
"One solution (x, y) = (−1, 2)",gauss elim
"braically so simple, sending (x,y) to (y,x), that the geometric pic-",vector spaces
"In practice, the Geiger counter is not exact. Instead, we make readings b1,...,bm at",orthogonality
2. The conjugate of a sum equals the sum of the conjugates:,eigenvec_val
"Continuing the search, we also ﬁnd that rows 1+4 equal rows 2+5. But this is nothing",vector spaces
"W spanned by (0,0,1), but V is not W⊥. The orthogonal complement of W is a two-",orthogonality
"v1cosθ⊥v2sinθ and put those coefﬁcients into column 1. Similarly V2 (or IV2, the",eigenvec_val
8.3 The Dual Problem,linear_prog
10. Suppose A is a 2 by 2 symmetric matrix with unit eigenvectors u1 and u2. If its,pos_def_matrices
1 0 0 1,determinants
we now assume to be perfectly understood!). It is a genuine example of the large linear,gauss elim
Multiplying this series by I −A leaves the identity matrix—all higher powers cancel—so,eigenvec_val
solution xB = B−1b.,linear_prog
projections and multiply the projection matrices P1P2: Is this a projection?,orthogonality
question is fundamental throughout Chapter 5. A table that organizes the key facts may,eigenvec_val
Fully reduced at P,linear_prog
You see the pattern: Every Fibonacci number is the sum of the two previous F’s:,eigenvec_val
I think an example is the best way to explain B = M−1AM. Suppose T is projection,eigenvec_val
"Algebra is about equations, and analysis is often about inequalities. The line between",linear_prog
two equations in one unknown:,orthogonality
42. If A =,eigenvec_val
second pivot. (There won’t be a third pivot.) For the present we trust all n pivot entries,gauss elim
1.4 What is the echelon form U of A?,vector spaces
into the inﬁnite series to ﬁnd eAt. First compute A2:,eigenvec_val
ω and the vector x.,eigenvec_val
"the question is how, There are still three columns on the left side of the equations, and",gauss elim
cause S−1 cancels S). The whole exponential is diagonalized by S:,eigenvec_val
1 2 3 6 9,vector spaces
vertical distances b−C −Dt to the straight line (not perpendicular distances!). It is the,orthogonality
13. Find the norms and condition numbers from the square roots of λmax(ATA) and,computations
are zero. We develop iterative rather than direct methods for solving Ax = b. An iter-,computations
34. Suppose that A = SΛS−1. Take determinants to prove that detA = λ1λ2···λn = prod-,eigenvec_val
"Example 9. These four columns span the column space of U, but they are not indepen-",vector spaces
"ﬁrst two perpendicular axes. Since (x,x2) = 0, it only has to correct the angle between 1",orthogonality
"Example 1. u′′ = 2 with u(0) = u(1) = 0, and solution u(x) = x−x2.",pos_def_matrices
"vectors q1, q2, q3, which go into the columns of an orthogonal matrix Q:",orthogonality
"The story is a long one, because these principles have been known for more than a",pos_def_matrices
"Left nullspace: dimension m−r = m−n+1, contains y’s from the loops.",vector spaces
"This is the characteristic polynomial. Its roots, where the determinant is zero, are the",eigenvec_val
"9. Suppose you do two row operations at once, going from",determinants
"To express this in matrix terms, we need the permutation matrix P that produces the",gauss elim
"J is block-diagonal, and the powers of each block can be taken separately:",eigenvec_val
2y + 2z = 4,vector spaces
both of those subspaces?,vector spaces
Solution at time k,eigenvec_val
0 ∗ ∗ ∗,computations
followed directly from these properties.,determinants
description may include repeated conditions (dependent rows). We can’t write a basis,vector spaces
"1 to produce B1, the determinant is",determinants
"so that x1 is close to, but a little inside, the boundary at which a component of x reaches",linear_prog
of eigenvectors. Now we look at all combinations M−1AM—formed with any invertible,eigenvec_val
"you from P to Q to R, and that the corner R is optimal.",linear_prog
You see that we keep coming back to the geometrical interpretation of a least-squares,orthogonality
"pivots are available, there is no need to assume that A is nonsingular. Here is PA = LU",vector spaces
−c a ]. What is the inverse of each matrix if ad ̸= bc?,gauss elim
5. Which of these matrices cannot be diagonalized?,eigenvec_val
"(1,1,1). Construct a 3 by 3 matrix whose column space is only a line.",vector spaces
"are still combinations of pure exponentials, but now there are inﬁnitely many. Instead",eigenvec_val
"v = (2,1) then T(v) = (1,2). Find S(T(v)) and T(S(v)). This shows that generally",vector spaces
"(Figure 4.2), the “volume” of a parallelogram equals the base ℓ times the height h, The",determinants
0 0 0 0,vector spaces
"�� → minimum at (0,0,0).",pos_def_matrices
(set b = Ax),pos_def_matrices
7.4 Iterative Methods for Ax = b,computations
away row 1 and column j. Its determinant is multiplied by a1 j—and by a plus or minus,determinants
"ces L with is on the diagonal, symmetric matrices S, positive matrices M, diagonal",gauss elim
Example 1. The steps from n = 4 to m = 2 are,orthogonality
39. Prove in three steps that AT is always similar to A (we know that the λ’s are the,eigenvec_val
of method 1 (and the solution is �C = 1,orthogonality
0 1 0 0,eigenvec_val
"1. For the complex numbers 3+4i and 1−i,",eigenvec_val
ﬁfty projections give the same point p as the ﬁrst projection:,orthogonality
23. Multiplying the rank 1 matrices A = uvT and B = wzT gives uzT times the number,vector spaces
established variational form of the eigenvalue problem.,pos_def_matrices
"30. A particular solution to du/dt = Au−b is up = A−1b, if A is invertible. The solutions",eigenvec_val
uct” u×w—which is perpendicular to u and w.,eigenvec_val
"pivots will be small; how many do we ignore? The second has one small pivot, but we",pos_def_matrices
test b3 +b2 −5b1 = 0. This is the equation for the plane (in the ﬁrst description of,vector spaces
6. Suppose An is the n by n tridiagonal matrix with is on the three diagonals:,determinants
"If we allow large changes in b or c, the solution behaves in a very jumpy way. As",linear_prog
Problems 45–49 use column-row multiplication and block multiplication.,gauss elim
3. For the Fibonacci matrix A =,eigenvec_val
only the entries strictly below the diagonal will be involved:,computations
"a p by q block of zeros prevents a matching if p+q > n. Here women 3, 4 could marry",linear_prog
To produce y∗ we return to the simplex method—which has already computed x∗. Our,linear_prog
"Example 2. The matrix that projects onto the line through a = (1,1,1) is",orthogonality
Problem: Prove also that rank(AB) ≤ rank(B).,vector spaces
"deﬁnite matrix, since c(A) = c(RT)c(R).",computations
rows of A. Its dimension is also r.,vector spaces
direction. The next elimination step will produce zeros in row 3 of the matrix. This,vector spaces
"3M If A has independent columns, then ATA is square, symmetric, and invert-",orthogonality
"different columns. By deciding even or odd, compute detA and detB.",determinants
"rows have n components and so does the vector x in Ax = 0. We have to show, using",orthogonality
tiplication by real numbers. Addition and multiplication must produce vectors in the,vector spaces
or prove that there is no such matrix.,orthogonality
Sn is the Fibonacci number F2n+2 by proving F2n+2 = 3F2n − F2n−2. Keep using,determinants
"Linear dependence is easy to visualize in three-dimensional space, when all vectors",vector spaces
"including the origin, are fakes.",linear_prog
7.2 Matrix Norm and Condition Number,computations
should run. Insofar as our linear model reﬂects the true economy. x∗ and y∗ represent,linear_prog
7.4 Iterative Methods for Ax = b,computations
"That makes Ax = b solvable, so b is in the column space. All columns of A pass this",vector spaces
"11. Write P, Q and R in the form λ1x1xH",eigenvec_val
"2. Find the matrix A whose eigenvalues are 1 and 4, and whose eigenvectors are",eigenvec_val
Remark 3. Other matrices S will not produce a diagonal Λ. Suppose the ﬁrst column,eigenvec_val
means that the product U−1,eigenvec_val
"20. What is the axis and the rotation angle for the transformation that takes (x1,x2,x3)",vector spaces
with FF = nI to ﬁnd F2 and F4 for the n by n Fourier matrix.,orthogonality
"a small residual, sometimes a small δx.",computations
From U back to A,gauss elim
"a line is carried out by a projection matrix P, and written in this new order we can see",orthogonality
"3−1,1), the two masses oscillate together—but the ﬁrst mass",pos_def_matrices
0 ∗ ∗ ∗,computations
you construct a 3 by 3 example with all |aij| ≤ 1 whose last pivot is 4? This is the,gauss elim
"Zero is a triple eigenvalue for A and B, so it will appear in all their Jordan blocks. There",eigenvec_val
multiples of the ﬁrst equation from the other equations. The goal is to eliminate u from,gauss elim
"16. If A is positive deﬁnite and a11 is increased, prove from cofactors that the determinant",pos_def_matrices
Suppose we have four women and four men. Some of those sixteen couples are compat-,linear_prog
"Since those projections are orthogonal, Pythagoras should still be correct. The square",orthogonality
3)2 to cancel the 9.,pos_def_matrices
"unforgettable as the formula in equation (1), but it relates the lengths of the sides of any",orthogonality
16. F is symmetric. So transpose equation (14) to ﬁnd a new Fast Fourier Transform!,orthogonality
add to zero. Every vector b in the column space has yTb = 0—the potential differences,vector spaces
neutrally stable if some |λi| = 1 and all the other |λi| < 1; and,eigenvec_val
22. Let S be the subspace of R4 containing all vectors with x1 +x2 +x3 +x4 = 0. Find a,orthogonality
∥x∥2 = xTx. This connection we want to preserve. The inner product must be modiﬁed,eigenvec_val
told which conditions the vectors in the space must satisfy. (Example: The nullspace,vector spaces
"as A, but different eigen",eigenvec_val
the program. We denote optimal vectors by an asterisk.,linear_prog
That gives the formula for the number �x and the projection p:,orthogonality
ce+dg c f +dh,determinants
columns of F are orthogonal.,orthogonality
"onal matrix, and a real orthogonal matrix? (The ﬁrst answer is the sum of the",eigenvec_val
C + 2D = 3,orthogonality
"tors. This number, which is shared by all bases and expresses the number of",vector spaces
then those eigenvectors are linearly independent.,eigenvec_val
"Example 3. Suppose A has rank 1, so its row space and column space are lines:",orthogonality
Thus A−1Ax = x. The matrix A−1 times A is the identity matrix. Not all matrices have,gauss elim
"corner is the optimal vector x∗, and the method stops.",linear_prog
from the rows above. This produces zeros above the diagonal as well as below. When it,gauss elim
"to 1. We might be talking about the value of steel and food and labor, instead of the",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"For a positive deﬁnite matrix, the solution x = A−1b and the error δx =",computations
"38. Find by experiment the number of positive, negative, and zero eigenvalues of",pos_def_matrices
", use SΛS−1 to",eigenvec_val
"(1,2,3). So would an exchange of 3 and 2, then 3 and 1, and then 2 and 1. In both",determinants
"12. If A is invertible, which properties of A remain true for A−1?",gauss elim
0 0 1 −1,gauss elim
11 12 13 14,determinants
Chapter 7 Computations with Matrices,computations
P! Find two vectors in P and check that their sum is not in P.,vector spaces
them). But the block sizes don’t match and J is not similar to K:,eigenvec_val
"bases. The algebra is almost straightforward. Suppose we have a basis v1,...,vn. The",eigenvec_val
2. x+(y+z) = (x+y)+z.,vector spaces
"of its q1, q2, q3 components: c = (qT",orthogonality
16. Suppose all r pivot variables come last. Describe the four blocks in the m by n,vector spaces
"This discussion is ﬁnished once we decide how to compute steps 1, 3, and 4:",linear_prog
smaller matrix Fm. The central problem is to recover y from the half-size vectors y′ and,orthogonality
j q j = 0. On the diagonal,orthogonality
(c) Why is the inner product of Px with Py again the same? What is the angle,orthogonality
updates the basis matrix correctly.,linear_prog
That is the ﬁrst equation in ATy = f. Similarly at the other three nodes—conservation,vector spaces
"Using cofactors of the last row of B4, show that |B4| = 2|B3|−|B2| = 1:",determinants
transformed to Ax (A is 2 by 2).,vector spaces
3. Explain why |λn/λn−1| controls the convergence of the usual power method. Con-,computations
"were even theorems to demonstrate it, but they did not allow for all possible methods.)",gauss elim
"Test this pattern for L = eye(5) − diag(1:5)\diag(1:4,−1) and inv(L).",gauss elim
"Writing the two components separately, we have v(0) = 8 and w(0) = 5:",eigenvec_val
Function Spaces and Fourier Series,orthogonality
"Starting from (3,2,1), a single exchange of 3 and 1 would achieve the natural order",determinants
elimination steps do not change the determinant. By rule 6 the zero row means a zero,determinants
to 1. Therefore a Markov process is neutrally stable.,eigenvec_val
"At the end is the triangular system, ready for back-substitution. You may prefer this",gauss elim
properties of a basis implies the other.,vector spaces
since we only have to worry about the plane triangle Oab.,orthogonality
"48. If P has 1s on the antidiagonal from (1,n) to (n,1), describe PAP.",gauss elim
"With this simple scaling, the power method uk+1 = Auk/αk converges to a multiple of",computations
Please look closely to see what actually happens at the moment when yb = cx. Some,linear_prog
Solve Rx = 0 to ﬁnd xn (its free variable is x2 = 1). Solve Rx = d to ﬁnd xp (its free,vector spaces
"minimum, f is allowed to vanish only at x = y = 0. When f(x,y) is strictly positive at",pos_def_matrices
"because the rows are orthogonal. Using the product and transposing rules,",determinants
change does that produce in the solution x?,computations
Each of the following tests is a necessary and sufﬁcient condition for a,pos_def_matrices
variable leaves the basis and which one enters. The basic variables are computed by,linear_prog
2. Bernadelli studied a beetle “which lives three years only. and propagates in as third,eigenvec_val
3. The matrix is positive deﬁnite. This extra property says that the pivots are positive.,gauss elim
The theory of Markov processes is illustrated by that California example:,eigenvec_val
The second term in equation (2) disappears to leave only,pos_def_matrices
u+v = 2 −→ .0001u+v = 1,gauss elim
"23. If A has eigenvalues 0, 1, 2, what are the eigenvalues of A(A−I)(A−2I)?",eigenvec_val
"row 2 = (c, 1)",determinants
"may involve negative or complex numbers, we take absolute values: |λ||z| = |Az| ≤ A|z|",eigenvec_val
"w z], then CD = −DC yields 4 equations Ax = 0:",determinants
What will be the pivots? Will a row exchange be required?,gauss elim
and minimize it. But setting its derivatives to zero will not give linear equations for the,orthogonality
"x2 = (1,−1) with frequency",eigenvec_val
"(b) λ −1 is an eigenvalue of A−1, as in Problem 21.",eigenvec_val
"eigenvalues of the corresponding A, and sketch the ellipse.",pos_def_matrices
"(b) If a 3 by 4 matrix has rank 3, what are its column space and left nullspace?",vector spaces
the parabolic boundary line between real and complex eigenvalues. The reason for the,eigenvec_val
that gives the minimum will be the solution u(x). The differential equation has been,pos_def_matrices
5E The eigenvalues of Ak are λ k,eigenvec_val
"40. The vectors v = (1,i,1), w = (i,1,0) and z =",eigenvec_val
"the cost happened to be x+2y, the whole edge between B and A would be optimal. The",linear_prog
"the triangular A has 1s along the diagonal, then detA = 1.",determinants
all Reλ < 0,eigenvec_val
"2. Give an example in R2 of linearly independent vectors that are not orthogonal. Also,",orthogonality
"In short, the matrix carries all the essential information. If the basis is known, and the",vector spaces
Another constraint is fundamental to linear programming: x and y are required to be,linear_prog
three inverse matrices in the right order:,gauss elim
"Ay = (0.3, 0.7)",eigenvec_val
"equals (9,2) instead of (8,1). With scalar multiplication unchanged, which rules",vector spaces
2.5 Graphs and Networks,vector spaces
nullspaces of A and AHA are,eigenvec_val
Those positive pivots in D multiply perfect squares to make xTAx positive. Thus condi-,pos_def_matrices
Special Matrices and Applications,gauss elim
"5U If A has s independent eigenvectors, it is similar to a matrix with s blocks:",eigenvec_val
This block Ji will enter when λ is a triple eigenvalue with a single eigenvector. Its,eigenvec_val
Rn (n real components),eigenvec_val
"1.14 Do the vectors (1,1,3), (2,3,6), and (1,4,3) form a basis for R3?",vector spaces
"4. Solve −u′′ = 2 with a single hat function, but place its node at x = 1",pos_def_matrices
equations are really one vector equation:,gauss elim
Negative components of r correspond to edges on which the cost goes down.,linear_prog
invertible. Then M−1JM = K is impossible.,eigenvec_val
1. Solve [1 x],orthogonality
Chapter 2 Vector Spaces,vector spaces
14. Find the rank and all four eigenvalues for both the matrix of ones and the checker,eigenvec_val
equals (AT)−1; for a symmetric matrix this is just A−1. A−1 equals its own transpose; it,gauss elim
"3,3). The new cost −91",linear_prog
v = v−h jnq j,computations
a11(x1)k+1 = (−a12x2 −a13x3 −···−a1nxn)k +b1.,computations
"9. If B is positive deﬁnite, show from the minimax principle (12) that the second small-",pos_def_matrices
. It can’t be the zero vector Z!,vector spaces
"4.16 The circular shift permutes (1,2,...,n) into (2,3,...,1). What is the corresponding",determinants
"We know that C1 j depends on rows 2,...,n. Row 1 is already accounted for by",determinants
Then the stiffness matrix is actually tridiagonal:,pos_def_matrices
This constant equals Pmin because the term before it is never negative. (Why?),pos_def_matrices
31. This 4 by 4 matrix needs which elimination matrices E21 and E32 and E43?,gauss elim
The transpose of a lower triangular matrix is upper triangular. The transpose of AT brings,gauss elim
"3w(w−1)(3n−2w+1). For a full matrix with w = n, we recover",gauss elim
"Note. This “zeroing” is not so easy to continue, because the rotations that produce",eigenvec_val
"If λ1 > 1, (I −A)−1 fails to be nonnegative.",eigenvec_val
case” because the third column is,gauss elim
60 coefﬁcient matrix A?,gauss elim
"If we are asking the columns to be a basis for the whole space Rn, then the matrix must",vector spaces
34. Put bases for the orthogonal subspaces V and W into the columns of matrices V and,orthogonality
"34. If A and B have the exactly the same eigenvalues and eigenvectors, does A = B? With",eigenvec_val
"oft a number after 16 bits, that is an error, But when a problem is so excruciatingly",computations
"is far from over, and we will identify the spectral radius that controls the speed of con-",computations
ﬁnd the Jacobi iteration matrix S−1T = −D−1(L +U) and its eigenvalues µi. Find,computations
problem is to show that the method stopped in the right place for the dual problem (even,linear_prog
row exchange. It comes from exchanging the rows of I:,gauss elim
"In a large problem, a departing variable might reenter the basis later on. But the cost",linear_prog
"span, and they are independent. So they form a basis. Notice again that a vector space",vector spaces
eigenvalues are positive. The test brings together three of the most basic ideas in the,pos_def_matrices
(d) The columns of a matrix are a basis for the column space.,vector spaces
"tried to make the row and column spaces the same size, with equal dimension r.",orthogonality
(c) what subspace is spanned by the positive matrices (all aij > 0)?,vector spaces
it would be wrong to leave it there because the rank has a simple and intuitive meaning:,vector spaces
Symmetric: AT = A,eigenvec_val
"For the third one, draw the column vectors (2,1) and (0,3). Multiplying by (1,1)",gauss elim
"column of B. In Figure 1.7, the 3, 2 entry of AB comes from row 3 and column",gauss elim
ann(xn)k+1 = (−an1x1 −an2x2 −···−ann−1xn−1)k+1 +bn.,computations
smallest subspace containing A?,vector spaces
In the right-hand ﬁgure there is a multiplication by 2 (and if it had been −2 the vector,gauss elim
Then S−1AS = Λ becomes special—it is Q−1AQ = Λ or,eigenvec_val
"not positive deﬁnite, because f(1,1) = −8. The conditions a > 0 and c > 0 ensure that",pos_def_matrices
That row exchange recovers LU—but now ℓ31 = 1 and ℓ21 = 2:,gauss elim
Example 1. The inverse of a sum matrix is a difference matrix:,determinants
Example 3. A =,eigenvec_val
"too that L and U are transposes of one another, as expected from the symmetry. The",gauss elim
AAT = (UΣV T)(VΣTUT) = UΣΣTUT,pos_def_matrices
"34. With 2 by 2 blocks, you cannot always use block determinants!",determinants
"of the feasible set, and computing their costs. In practice this is impossible. There",linear_prog
"f(x), say its values at n equally spaced points x = h,x = 2h,...,x = nh. We compute",gauss elim
take the nonzero rows of U.,vector spaces
Row picture: Intersection of planes,gauss elim
means that even Gauss-Seidel will require a great many iterations. But since sinπh =,computations
This matrix H has the remarkable property H2 = I. Two reﬂections bring back,vector spaces
"squares, and it is harder. We would still form E2, the sum of the squares of the errors,",orthogonality
The system is singular if row 3 of A is a,gauss elim
"There are simple formulas for the best left and right inverses, if they exist:",vector spaces
"3 tells which variable hits zero ﬁrst, and must",linear_prog
"4If everybody outside moves in and everybody inside moves out, then the populations are reversed every year",eigenvec_val
the duality gap sx is generally below 10−8 after 20–80 Newton steps. This algorithm,linear_prog
"Here is a more direct proof that this A is not diagonalizable. Since λ1 = λ2 = 0, Λ",eigenvec_val
"2. Find the condition on b1, b2, b3 to have a solution.",vector spaces
"Remark 5. The second derivatives at (0,0) are decisive:",pos_def_matrices
"2. When the column space doesn’t contain every b in Rm, we need the conditions on",vector spaces
j to ﬁnd a formula for a j. The,computations
matrix. Show that any vector z equals (vH,eigenvec_val
b = 0 and also b = 2 at t = z = 0 (and b = 2 at t = z = 1).,pos_def_matrices
"x0 = 0, r0 = b, p0 = r0",computations
4x + 8y = 6.,gauss elim
“average” the experiments and ﬁnd an optimal line. That line is not to be confused with,orthogonality
span S⊥. This is the same as solving Ax = 0 for which A?,orthogonality
The ordinary least-squares problem leading to �xW comes from changing Ax = b to,orthogonality
"whatever the original matrix A was, which we do not even know in this example—are a",vector spaces
gives the best x.,orthogonality
right-hand side to zero and ﬁnd all solutions xn.,vector spaces
"Karmarkar proposed a method based on two simple ideas, and in his experiments it",linear_prog
"transformation is the identity) is −v1sinθ +v2cosθ, producing column 2:",eigenvec_val
columns. It will be an “m by n matrix.”,gauss elim
M = I −uvT,gauss elim
"reason for EG ̸= GE. It is worth taking one more step, to see what happens with all",gauss elim
n by m left-inverse B such that BA = In. This is possible only if m ≥ n.,vector spaces
"2 and eigenvalues 1, 0. Then 1",eigenvec_val
y j+m = y′,orthogonality
"For the difference equation uk+1 = Auk, we emphasize the main point. Every eigen-",eigenvec_val
14. Prove that the condition number ∥A∥∥A−1∥ is at least 1.,computations
because they multiply A to give the zero rows in U.,vector spaces
It is crucial to keep these matrices in the right order. If Λ came before S (instead of,eigenvec_val
λmax = ωopt −1.,computations
27. Substitute y = eλt into y′′ = 6y′ − 9y to show that λ = 3 is a repeated root. This is,eigenvec_val
"4. By applying row operations to produce an upper triangular U, compute",determinants
Every linear programming problem falls into one of three possible categories:,linear_prog
− 2y + 2z =,gauss elim
projects onto the plane. What is the nullspace of P?,orthogonality
"2. Solve the triangular system of Problem 1 by back-substitution, y before x. Verify",gauss elim
"r , but it is n by n.",pos_def_matrices
"One ﬁnal note, Skew-Hermitian matrices satisfy KH = −K, just as skew-symmetric",eigenvec_val
(a+ib)(c+id) = ac+ibc+iad +i2bd,eigenvec_val
Therefore BA = I (which is not so obvious!).,vector spaces
example before ﬁnding A+ in general.,pos_def_matrices
detA = a1 jC1 j +a2 jC2 j +···+anjCnj.,determinants
(b) Compute uk = SΛkS−1u0 for any a and b.,eigenvec_val
answer. Here the matrices come in just the right order so that their product can be,gauss elim
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
9. (a) Find the special solutions to Ux = 0. Reduce U to R and repeat:,vector spaces
The sign ±1 depends on whether the number of row exchanges is even or odd.,determinants
3.39 Under what condition on the columns of A (which may be rectangular) is ATA in-,orthogonality
A(t) all with positive determinants. (The straight path A(t) = A+t(I −A) does go,determinants
"L2, L3 (with diagonal 1s), U1, U2, U3, and A1, A2, A3?",determinants
"Show that this matrix has λ = 3,3 and only one line of eigenvectors. Trouble here",eigenvec_val
One solution is x = (1,pos_def_matrices
An Example of Gaussian Elimination,gauss elim
separate projections onto the x- and y-axes:,orthogonality
"26. The matrix Bn is the −1, 2, −1 matrix An except that b11 = 1 instead of a11 = 2.",determinants
"This is not an easy problem. It was attacked by John von Neumann, who was the",gauss elim
You must notice that the word “dimensional” is used in two different ways. We speak,vector spaces
The solution Aku0 approaches a multiple of x1—which is the steady state,eigenvec_val
dt = Ap −→,vector spaces
"to make b ≥ 0, consider the auxiliary problem of minimizing w1 + w2 + ··· + wm,",linear_prog
by any x gives a combination of the columns; it is a vector Ax in the column space.,vector spaces
"vectors, and compute the solution that starts from y(0) = 2, y′(0) = 0.",eigenvec_val
"7. Find a vector x orthogonal to the row space of A, and a vector y orthogonal to the",orthogonality
"This leads to the Schwarz inequality in equation (6), which is the most important",orthogonality
"the cost varies, these planes sweep out the whole n-dimensional space. The optimal x∗",linear_prog
then so is Q−1AQ. No entry can become dangerously large because Q preserves lengths.,computations
in common (Figure 1.5c). Changing the right sides will move the planes in Figure 1.5b,gauss elim
"powers Bk approach zero (convergence). This is no surprise, since |λ|max is below",computations
(b) If v is perpendicular to u show that Pv = zero vector. Then λ = 0.,eigenvec_val
BA = SΛ2S−1SΛ1S−1 = SΛ2Λ1S−1.,eigenvec_val
Figure 6.5: Minimizing 1,pos_def_matrices
most important feature of practically any dynamical system.,eigenvec_val
"be sure that a minus sign goes with the reverse diagonal, as the next exercises show.",determinants
(ad −bc)/a depends only on the 2 by 2 corner submatrix A2. The rest of A does not enter,determinants
0 0 2 0 0,vector spaces
that det(A−I) = 0. Show by example that this does not imply detA = 1.,determinants
44. Suppose T is reﬂection across the x-axis and S is reﬂection across the y-axis. The,vector spaces
corresponding f = xTAx:,pos_def_matrices
duality: cx∗ = y∗b.,linear_prog
"Special cases of normal matrices, all with orthonormal eigenvectors:",eigenvec_val
"If j = 1, we are maximizing R(x) over one constraint xTv = 0. That maximum is between",pos_def_matrices
"key to its success is the convolution rule. In matrix language, all “circulant matrices”",orthogonality
"We are dividing by the determinant, and A is invertible exactly when detA is nonzero.",determinants
In matrix notation this is multiplication by L. So A = LU and b = Lc.,gauss elim
0 0 0 1,linear_prog
"41. What are the three equations for A, B, C if the parabola Y = A + Bx +Cx2 equals 4",vector spaces
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"Example 2. Now A has rank 2, and AAT =",pos_def_matrices
7.4 Iterative Methods for Ax = b,computations
"39. Suppose Ax = λx. If λ = 0, then x is in the nullspace. If λ ̸= 0, then x is in the",eigenvec_val
7. For a general 2 by 2 matrix,computations
no change in the eigenvalues:,computations
symmetric matrices of rank 1.,eigenvec_val
"c’s need not be unique, because the spanning set might be excessively large—it could",vector spaces
"47. If A = ones(4,4) and b = rand(4,1), how does MATLAB tell you that Ax = b has",gauss elim
if AC = CA,determinants
"L, D, and U, that are determined by the upper-left corner of A:",determinants
34. Multiply these matrices in the orders EF and FE and E2:,gauss elim
equation det(A−λI) = 0 and show that both eigenvalues are positive.,pos_def_matrices
(d) The column space of A−I equals the column space of A.,vector spaces
0 2 8 0,vector spaces
"sign. These signs alternate as in detM11, −detM12, detM13:",determinants
11. These equations are certain to have the solution x = y = 0. For which values of a is,gauss elim
∗ ∗ ∗ ∗,computations
13. The product (AB)C of linear transformations starts with a vector x and produces,vector spaces
"2. The effective rank The rank of a matrix is the number of independent rows, and",pos_def_matrices
This suggests a way to construct the maximal ﬂow: Check whether any path has,linear_prog
1000 equations in Chapter 1.,gauss elim
"When neighbors are exchanged, N changes by +1 or −1. Any exchange can be",determinants
Guess the form of Jk. Set k = 0 to ﬁnd J0. Set k = −1 to ﬁnd J−1.,eigenvec_val
10. Draw a graph with numbered and directed edges (and numbered nodes) whose inci-,vector spaces
"Section 4.2 explains these three deﬁning properties of the determinant, and their most",determinants
"dimensional ellipses.) In mechanics the eigenvectors give the principal directions, along",eigenvec_val
obvious. It also says something about square matrices: If the rows of a square matrix,vector spaces
A row exchange comes with the same column exchange to maintain symmetry.,pos_def_matrices
equation or minimize the quadratic.,pos_def_matrices
"food can be replaced by its vitamin equivalent, with no increase in cost, all adequate",linear_prog
0 2 0 6,vector spaces
"exchanges can never produce the natural order beginning with (3,2,1).",determinants
actly. The Rayleigh-Ritz principle produces an n-dimensional problem by choosing only,pos_def_matrices
39. Write A =,eigenvec_val
order of multiplication must be correct—if ABx = y then Bx = A−1y and x = B−1A−1y.,gauss elim
0 1 0 0,linear_prog
"speaking, those approximations stay on the same side of the solution x. An overrelax-",computations
"example, there are two pure exponentials to be combined:",eigenvec_val
"Suppose you are given three independent vectors a, b, c. If they are orthonormal, life is",orthogonality
so �x is shortest when xn = 0.,pos_def_matrices
"both too high or both too low is zero, but the chance of opposite errors is 1",orthogonality
"What about the operations that are carried out during elimination? In our example, the",gauss elim
units). The price must be nonnegative or the druggist will not sell. Since four units of,linear_prog
are even simpler when we produce an orthogonal basis for S.,orthogonality
"2Orthonormal matrix would have been a better name, but it is too late to change. Also, there is no accepted word",orthogonality
left-inverse B or a right-inverse C or a two-sided A−1.,vector spaces
Chapter 7 Computations with Matrices,computations
Solve c1v1 +···+c4v4 = 0 or Ac = 0. The v’s go in the columns of A.,vector spaces
6. The Schwarz inequality has a one-line proof if a and b are normalized ahead of time,orthogonality
u + 1.0001v = 2.0001,gauss elim
4.2 Properties of the Determinant,determinants
5O A real symmetric matrix can be factored into A = QΛQT. Its orthonormal,eigenvec_val
7. Compute H−1 in two ways for the 3 by 3 Hilbert matrix,gauss elim
4. N(AT) = left nullspace of A; dimension m−r.,vector spaces
"parabola for y = x2). There is a second meaning, completely different, which is closer to",vector spaces
"any entry in r is negative, the cost can still be reduced. We can make rxN negative,",linear_prog
"A graph becomes a network when numbers c1,...,cm are assigned to the edges. The",vector spaces
5. Choose a right-hand side which gives no solution and another right-hand side which,gauss elim
"unused capacity. If so, add ﬂow along that “augmenting path.” Then compute the re-",linear_prog
(sign = −1). A row exchange reverses sign. The ﬁnal value of sign is the determinant,gauss elim
(AB)(B−1A−1) = ABB−1A−1 = AIA−1 = AA−1 = I,gauss elim
"We may use geometry or calculus to determine �x. In n dimensions, we prefer the",orthogonality
are normalized to have length 1. They are now orthonormal. If these eigenvectors are,eigenvec_val
on determinants. Therefore we ask what part determinants play. It is not enough to,pos_def_matrices
added only if they have the same shape:,gauss elim
"the subspace at p. Geometrically, that gives the distance between points b and subspaces",orthogonality
"upper triangular. Therefore xk+1 can still replace xk, component by component, as soon",computations
"If we include the third quadrant along with the ﬁrst, scalar multiplication is all right.",vector spaces
x ̸= 0. Therefore λ = xHAx/xHx must be real. Our example has λ = 8 and λ = −1:,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"We conclude that the products (3) and (4) are the same, and detA = detAT. This",determinants
"equation to a matrix equation. The continuous problem asks for u(x) at every x, and a",gauss elim
"(b) If A = AT is real symmetric, then Λ is real and U = Q is orthogonal.",eigenvec_val
"23. If S and T are linear with S(v) = T(v) = v, then S(T(v)) = v or v2?",vector spaces
is a reﬂection matrix determined by one vector v:,computations
that subtract Sn−1 from the determinant Sn? Show that the determinants become the,determinants
"1-sided inverse of a square matrix is automatically a 2-sided inverse. To see why,",gauss elim
The row space is spanned by the rows. The deﬁnition is made to order. Multiplying A,vector spaces
"which happen to be 2 and 3, we use ﬁve exchanges (an odd number) of neighbors:",determinants
result is achieved if we multiply b by this elementary matrix (or elimination matrix):,gauss elim
"Fundamental Theorem of Linear Algebra, Part II",orthogonality
"(1,2,3). The left nullspace must be the perpendicular plane y1 + 2y2 + 3y3 = 0. That",orthogonality
"n+m, and the new x has n+m components. We keep this much of the original notation",linear_prog
This is the equation of an ellipsoid. Its axes have lengths 1/,pos_def_matrices
"The sum might or might not be invertible. Instead, it is the inverse of their product",gauss elim
"object of this section is to ﬁnd the answer, yes. This is the foundation of an approach",vector spaces
"actually computed. Now we look brieﬂy at a different approach, which omits the simplex",linear_prog
22. Find the ranks of AB and AM (rank 1 matrix times rank 1 matrix):,vector spaces
good edge. Then the cost decreases again.,linear_prog
Every vector in the plane is the sum of its q1 and q2 components. Similarly c is the sum,orthogonality
"This is just a difference equation. It starts with the initial error e0, and after k steps",computations
"C +Dx between x = 0 and x = 1. There are at least three ways of ﬁnding that line, and",orthogonality
if you compare them the whole chapter might become clear!,orthogonality
= real+real+(sum of complex conjugates).,eigenvec_val
These two eigenvectors are orthogonal:,eigenvec_val
31. Why isn’t R2 a subspace of R3?,vector spaces
"For the angle β, the sine is b2/∥b∥ and the cosine is b1/∥b∥ . The cosine of θ = β −α",orthogonality
Any two bases for a vector space V contain the same number of vec-,vector spaces
and at the same time to keep the geometry of ordinary Euclidean space. Ellipses become,orthogonality
The real numbers a and the imaginary numbers ib are special cases of complex num-,eigenvec_val
Our analysis so far has applied to symmetric matrices with positive eigenvalues. We,computations
Optimization needs a whole book. We stop while it is pure linear algebra.,pos_def_matrices
(3) If A and B are invertible then BA is invertible.,gauss elim
We want to show that Fc and F−1y can be done quickly. The key is in the relation of,orthogonality
matrix A. Find a solution to Ax = 0 and describe all other vectors in the nullspace of,vector spaces
and the column space C(A) are orthogonal complements. Their dimensions add up to,orthogonality
19. Every 2 by 2 matrix with trace zero can be written as,eigenvec_val
This is the basic statement of the problem. Note that it is a ﬁrst-order equation—no,eigenvec_val
Problems 25–28 are about the diagonalizability of A.,eigenvec_val
"same, the eigenvectors are the problem):",eigenvec_val
used in every section of this chapter. The eigenvectors diagonalize a matrix:,eigenvec_val
"(e) If eigenvectors x and y correspond to distinct eigenvalues, then xHy = 0.",eigenvec_val
"angle θ, the expression ∥b−a∥2 is (b−a)T(b−a), and equation (3) becomes",orthogonality
is also a combination of the columns of A.,vector spaces
to Ax = b; otherwise not.,linear_prog
The product of the pivots is ad − bc. That is the determinant of the diagonal matrix D.,determinants
"and in the extreme cases, by the origin alone or the whole space. The subspace {0}",orthogonality
"The best solution (�C, �D) is the �x that minimizes the squared error E2:",orthogonality
"sum; the condition for that is λmax < 1. Add up the inﬁnite series, and conﬁrm that",eigenvec_val
"Apply steps 1, 2, 3 in that order;",gauss elim
6. What 3 by 3 matrices represent the transformations that,vector spaces
"Compare the resulting equations with ATA�x = ATb, conﬁrming that calculus as well",orthogonality
0 0 0 1,eigenvec_val
"ination and the four subspaces, this solvability question was answered in a completely",linear_prog
The ﬁrst question is: How do we replace the derivative d2u/dx2? The ﬁrst derivative,gauss elim
0 0 3 3,vector spaces
"1002. Since c represents an upper bound, the condition number must be at least 10,000.",computations
6. Choose a coefﬁcient b that makes this system singular. Then choose a right-hand,gauss elim
m−r columns of U: left nullspace of A,pos_def_matrices
Positive Deﬁnite Matrices and Least Squares,pos_def_matrices
These formulas express detA as a combination of determinants of order n − 1. We,determinants
"One case has special importance. Let the n vectors have m components, so that A is an",vector spaces
"Elimination can simplify, one entry at a time, the linear system Ax = b. Fortunately it",vector spaces
cosθ cosϕ −sinθ sinϕ,vector spaces
"9. The quadratic f(x1,x2) = 3(x1 + 2x2)2 + 4x2",pos_def_matrices
"and they are linearly independent. Roughly speaking, no vectors in that set are wasted.",vector spaces
"This system is solved backward, bottom to top. The last equation gives w = 2. Sub-",gauss elim
"Show that a = d and b = c = 0. If AB = BA for all matrices B, then A is a multiple",gauss elim
", conﬁrm that CTAC has eigenvalues of the same signs",pos_def_matrices
"Sketch A′ = (1,6), B′ = (−2,7), C′ = (0,0) and their relation to A, B, C.",determinants
"has A2 = −I, and use this in the series for eAt:",eigenvec_val
(a) Why is the ﬁrst statement true? Somehow B doesn’t enter.,determinants
A = (pivot columns of A)(ﬁrst r rows of R) = (COL)(ROW).,vector spaces
of A (combinations of rows that give zero).,vector spaces
"0. This problem is inﬁnite-dimensional (the vector b is replaced by a function f, and",pos_def_matrices
1 0 0 0,orthogonality
"When elimination is applied to a symmetric matrix, AT = A is an advantage. The smaller",gauss elim
and ﬁnd one of its square roots—a matrix such that,eigenvec_val
∗ ∗ ∗ ∗,computations
"L, but the third plane doesn’t",gauss elim
1.28 By experiment or the Gauss-Jordan method compute,gauss elim
and we try never to let both appear on the same page.,orthogonality
"Proof. Put the eigenvectors xi in the columns of S, and compute AS by columns:",eigenvec_val
(d) a skew-symmetric matrix: aij = −a ji for all i and j.,gauss elim
Chapter 2 Vector Spaces,vector spaces
"and x ≥ 0 into equations, and ﬁnding the intersection of these n planes.",linear_prog
(V) There is a matrix R with independent columns such that A = RTR.,pos_def_matrices
"terms (equation (2) of the next section), three going parallel to the main diagonal and",determinants
"By rule 2, there is nothing special about the ﬁrst row.",determinants
"is divided by ω. Write a program for SOR on an n by n matrix. Apply it with ω = 1,",computations
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
Which of the four subspaces contains q?,orthogonality
Chapter 1 Matrices and Gaussian Elimination,gauss elim
the plane P0 through the origin parallel to P? Are P and P0 subspaces of R3?,vector spaces
giving a positive deﬁnite matrix A + εI. Then let ε approach zero. Since the determi-,pos_def_matrices
dimensional problems like −uxx = f. A tridiagonal system Ax = b is already easy. It is,computations
(d) If AT = −A then the row space of A equals the column space.,vector spaces
45. Write all known relations between r and m and n if Ax = b has,vector spaces
Can you ﬁnd a time T at which the solution u(T) is guaranteed to return to the,eigenvec_val
(8) that otherwise detA = 0 for exactly one value of a11.,determinants
"In the nonsingular case, there is a permutation matrix P that reorders",gauss elim
The row space and column space both became clear after elimination on A. Now,vector spaces
1 2 0 1,determinants
n − 1; you see the three 2 by 2 submatrices. The submatrix M1j is formed by throwing,determinants
Each of the ﬁrst ﬁve chapters will give a different (but equivalent) test for invertibility.,gauss elim
"3.11 If Q is orthogonal, is the same true of Q3?",orthogonality
"(b) What matrix N transforms (a,c) and (b,d) to (1,0) and (0,1)?",vector spaces
"2. For the previous matrix, write the general solution to du/dt = Au, and the speciﬁc",eigenvec_val
write down M and also M2. Why does (M2)ij count the number of 2-step paths from,vector spaces
"A space has inﬁnitely many different bases, but there is something common to all of",vector spaces
columns of U: column space of A,pos_def_matrices
to linear algebra—starting with property (1) and developing its consequences—that is,vector spaces
they are forced to be orthonormal—but they are.,orthogonality
“change of variables” u = Mv introduces the new unknown v:,eigenvec_val
Hermitian instead of symmetric and Q becomes unitary instead of orthogonal. In the,pos_def_matrices
(b) Verify that (Ax)Ty agrees with xT(ATy)—six terms in both.,gauss elim
(b) Verify that P = x1xT,eigenvec_val
of the permutation matrix P in Example 6.,eigenvec_val
"inside that cone, say b = (3,2), what is the feasible vector x? If b lies outside, say",linear_prog
"For AdiffAint, the composite transformation was the identity (and AintAdiff annihilated",vector spaces
corner Q in Figure 8.3.,linear_prog
for A. You have found bases for which spaces?,vector spaces
of matrices. Our second example is the famous Fibonacci sequence:,eigenvec_val
"actually meet), Often that number is reasonable. When it is enormous, we may have to",computations
17. The less familiar form A = LPU exchanges rows only at the end:,gauss elim
"A−1 exists it is also symmetric. From formula (ii) above, the transpose of A−1 always",gauss elim
A(xp +xn) = b.,vector spaces
"2. A is arbitrary: The columns of M include “generalized eigenvectors” of A, and the",eigenvec_val
Sometimes the tests extend to rectangular matrices and one-sided inverses: Chapter 2,gauss elim
"2bTb, what system of equations do",pos_def_matrices
vectors of the corresponding matrix. (Section 6.2 connects symmetric matrices to n-,eigenvec_val
"If you change that corner entry from 4 to 100, why is detA unchanged?",determinants
For the eigenvector x1(,pos_def_matrices
neighboring hat functions overlap. It is exactly this situation that brings in the general-,pos_def_matrices
"1.12 If A is an n by n − 1 matrix, and its rank is n − 2, what is the dimension of its",vector spaces
(b) This second-order equation y′′ = −y produces a vector equation u′ = Au:,eigenvec_val
"above the main diagonal are zero). Conﬁrm this with a 3 by 3 example, and then",gauss elim
Max ﬂow-min cut theorem. The maximal ﬂow in a network equals the,linear_prog
"34. If you know that detA = 6, what is the determinant of B?",determinants
positive. But we also have to deal with every upper left submatrix Ak. The trick is to,pos_def_matrices
"The principles (i) and (ii) were there, but they could not be implemented.",pos_def_matrices
transformations T(x) are not possible with Ax:,vector spaces
"as for ﬁnite differences, they are now an average of f around that point: b j =",pos_def_matrices
"the nullspace of A. Those are the only vectors in the nullspace, since Ax = 0 means equal",vector spaces
"The problem is illustrated by the geometry of Figure 8,2. The family of costs 2x+3y",linear_prog
Write b as a combination b = x1q1 +x2q2 +···+xnqn.,orthogonality
0 0 0 d,vector spaces
The comparison will be perfect if we keep the same A:,eigenvec_val
accounts for the missing 2 between 2·104 and the extreme possibility c = 4·104.,computations
requires b to be perpendicular to the left nullspace.,orthogonality
vector (along the line where they meet). Two planes in R3 cannot be orthogonal!,orthogonality
59. Suppose R is rectangular (m by n) and A is symmetric (m by m).,gauss elim
"is that of soldiers going over a bridge.1 Traditionally, they stop marching and just walk",eigenvec_val
(c) For which A is it a line?,vector spaces
"The column form of the equations immediately gives what solution for (u,v,w)?",gauss elim
has the same eigenvalues as BA plus,eigenvec_val
"through the mirror, Linearity decides the rest.",vector spaces
linearly dependent ﬁnd one eigenvalue and one eigenvector of A:,eigenvec_val
ance like 10−6 and count the singular values above it—that is the effective rank. The,pos_def_matrices
"of the dual problem. For an economist or an executive, these questions about marginal",linear_prog
multiple of equation 2 will be subtracted from equation 3.,gauss elim
"1. The columns span Rn, so Ax = b has at least one solution for every b.",vector spaces
Problems 33–36 are about the solution of Ax = b. Follow the steps in the text to,vector spaces
"A(ATA)−1AT, and here it simpliﬁes to",orthogonality
"the ﬁve edges. Since AT is 4 by 5, the equations ATy = 0 give four conditions on those",vector spaces
The idea of elimination is deceptively simple—you will master it after a few exam-,gauss elim
That good method is Gaussian Elimination. This is the algorithm that is constantly,gauss elim
"contains an identity matrix, in the four pivot rows and four pivot columns. From R",vector spaces
we must be able to solve du/dt = Ku. The characteristic polynomial λ 2 +1 should still,eigenvec_val
4.15 If C =,determinants
the projection matrix is P = QQT.,orthogonality
"Why do you know, without computing, that eAt will be an orthogonal matrix and",eigenvec_val
"hand side b = 0 always allows the solution x = 0, but there may be inﬁnitely many other",vector spaces
The major axis has y1 = 1/,pos_def_matrices
the characteristic polynomial is factored into,eigenvec_val
29. (a) What 3 by 3 matrix E13 will add row 3 to row 1?,gauss elim
"somehow goes through the origin and out the opposite side, when",vector spaces
than m independent rows or n independent columns. There is not space for more than m,vector spaces
"A. The equation w = Ax−b, or Ax−w = b, goes into matrix form:",linear_prog
(c) If the row space equals the column space then AT = A.,vector spaces
"5. (a) Draw the triangle with vertices A = (2,2), B = (−1,3), and C = (0,0). By",determinants
We are looking for the point on the ellipsoid xTAx = 1 farthest from the origin—the,pos_def_matrices
"Our Hilbert space has become a function space. The vectors are functions, we have a",orthogonality
10. Check that the tridiagonal A =,computations
65. Construct a 2 by 2 matrix whose nullspace equals its column space.,vector spaces
matrix A—which gives information about the connections.,vector spaces
"related by y j = Rx j. The properties of CTAC lead directly to thc properties of Ax = λMx,",pos_def_matrices
The ﬁrst m columns of A form a square matrix B (the basis matrix for that corner). The,linear_prog
"In the ﬁrst, b = 1 dominates a = c = 0. In the second, a = 1 and c = −1 have opposite",pos_def_matrices
onal to the nullspace. A vector z can’t be orthogonal to the nullspace but outside the row,orthogonality
1 2 1 0 1 0,gauss elim
are the same. So A = B.,eigenvec_val
Positive Matrices and Applications in Economics,eigenvec_val
"singular, but now it suffers from too many solutions instead of too few.",gauss elim
"ampliﬁcation is greatest when λ1 is near zero, and A is nearly singular.",computations
In either case H is both symmetric and orthogonal:,computations
4.4 Applications of Determinants,determinants
"2 contains the powers of w2 = −1,",orthogonality
"combination a(1,4) + b(1,5) that equals (1,0) has (a,b) = (",vector spaces
brings us to w = (1+i)/,orthogonality
skew-symmetric—which means that KT = −K. Find these matrices A and K when,gauss elim
ij = A ji,eigenvec_val
not A!). Suppose a combination of these pivot columns produced zero:,vector spaces
and 1. The Fibonacci equation is unstable. So is the compound interest equation Pk+1 =,eigenvec_val
"The ﬁnal answer is given by the Jordan form, with which the chapter ends.",eigenvec_val
The situation is changed when a third column is a combination of the ﬁrst two:,vector spaces
more unknowns) move steadily up until they intersect the feasible set. The ﬁrst contact,linear_prog
complex numbers c? Find a real matrix with A+rI invertible for all real r.,eigenvec_val
"If A is a positive matrix, so is its largest eigenvalue: λ1 > all other |λi|.",eigenvec_val
"and a2 = (1,2,0,0), Express a1 and a2 as combinations of q1 and q2, and ﬁnd the",orthogonality
"21. To multiply C times x, when C = FEF−1, we can multiply F(E(F−1x)) instead. The",orthogonality
Those are the constants in uk = c1λ k,eigenvec_val
"−x3 must sooner or later pull F toward −∞. For f(x,y), with no higher terms, all the",pos_def_matrices
columns of Q are orthonormal,eigenvec_val
This allows us to redraw Figure 3.5 with a correct formula for p (Figure 3.7).,orthogonality
"Thus λ 2 is an eigenvalue of A2, with the same eigenvector x. If the ﬁrst multiplication",eigenvec_val
diagonal if possible. The basic step is no longer to subtract a multiple of one row from,eigenvec_val
"(1,...,n). The permutation gives the column numbers as we go down the matrix. The is",determinants
"All but these n! determinants are zero, because a column is repeated. (There are",determinants
9. (a) Under what conditions is the following product nonsingular?,gauss elim
0 1 0 0,determinants
"w = 2, but the ﬁrst equation cannot decide both u and v.",gauss elim
12. Find the norms λmax and condition numbers λmax/λmin of these positive deﬁnite,computations
"Then (Px)T(Py) = xTy says that PTP = I for any permutation. With x = (1,2,3) and",gauss elim
"the right order PA, any nonsingular matrix is ready for elimination.",gauss elim
tion can give. R reveals all solutions immediately.,vector spaces
x + 2y =,gauss elim
"arrow from the origin. You can choose the arrow, or the point, or the three numbers. In",gauss elim
the space). Then linearity determines Ax:,vector spaces
Notice the zeros in the last matrix! R is upper triangular because of the way Gram-,orthogonality
5)/2 ≈ 1.618. Since,eigenvec_val
"space, which is haywire. The nullspace should be orthogonal to the row space. But",orthogonality
(b) A is diagonalizable.,eigenvec_val
nothing. The ﬁrst two rows are a basis for the row space. A similar rule applies to every,vector spaces
"pounds of rubber, and 2x1 +50x2 months of labor. If the unit costs y1, y2, y3 are $700",gauss elim
one corner of the feasible set. The heart of the method goes from corner to corner,linear_prog
we look for the particular combination (call it U) that minimizes P(V). This is the key,pos_def_matrices
0 3 8 5,gauss elim
"approximate values u1,...,un for the true solution u at these same points. At the ends",gauss elim
"(c) CD = −DC, not allowing the case CD = 0.",gauss elim
"2. a column vector b with m components, and",linear_prog
"direction from the one in the y direction, A recent choice is S = L0U0, in which small",computations
(b) Suppose all rows of B are [1 2 4]. Show by example that all rows of EB are not,gauss elim
4 5 6 b2,vector spaces
"Proof. Since the vectors are feasible, they satisfy Ax ≥ b and yA ≤ c. Because feasi-",linear_prog
the others. The edges that go forward across the cut have total capacity 2+3+1 = 6—,linear_prog
"from both sides of the diagonal, or to store both L and U.",gauss elim
"is x = (1,0,...,0). So a11 (on the main diagonal) is between λ1 and λn. You can see this",pos_def_matrices
somewhere else—they are inside the standard spaces Rn. We want to describe them,vector spaces
Example 3. f = 2x2 +4xy+y2 and A =,pos_def_matrices
"solutions to du/dt = Au and uk+1 = Auk, starting from u = (9,4).",eigenvec_val
1 1 1 2,determinants
"The matrix can differentiate that p(t), because matrices build in linearity!",vector spaces
Example 2. A =,eigenvec_val
8. Suppose the rabbit population r and the wolf population w are governed by,eigenvec_val
"41. If A = R+iS is a Hermitian matrix, are the real matrices R and S symmetric?",eigenvec_val
i that multiplies these components must be zero.,linear_prog
The pseudoinverse in Figure 6.3 starts with b and comes back to x+. It inverts A where,pos_def_matrices
found a vector y in which subspace? The inner product yTb is 1.,orthogonality
"will be checked in the next section, because here we are most interested in property 3:",determinants
2.5 Graphs and Networks,vector spaces
"2, and choose the �xW that minimizes the weighted sum of squares:",orthogonality
rank(A) that the rank of A is n. So A is invertible and B must be its two-sided inverse.,vector spaces
terms and each term needs,determinants
(c) Find the minimum value of 1,pos_def_matrices
"that point, what is the new x?",linear_prog
difference between Chapter 1 and Chapter 2. This chapter studies the most important,gauss elim
and we need to know when both eigenvalues of that matrix have negative real parts.,eigenvec_val
answer is no. A large cross term 2bxy can pull the graph below zero.,pos_def_matrices
"1.23 How can you construct a matrix that transforms the coordinate vectors e1,e2,e3 into",vector spaces
12. Give a reason if true or a counterexample if false:,eigenvec_val
. Verify that the trace,eigenvec_val
"The fourth root is at θ = 90°, which is 1",orthogonality
1. If a 4 by 4 matrix has detA = 1,determinants
"With orthonormal eigenvectors and S = Q, the eigenvalue problem is perfectly condi-",computations
2.1 Vector Spaces and Subspaces,vector spaces
Proof. Suppose xi is the largest component of x. Then Ax = λx leads to,computations
and columns. The zeros in A make all 24 terms zero.,linear_prog
Leake (Notre Dame) gave a full analysis in Management Science in Sports (1976).,vector spaces
"all constants). For rotations, the order of multiplication does not matter. Then U =",vector spaces
2t2eλt. When λ has multi-,eigenvec_val
"region is covered by a mesh, and u′′ = f(x) became u j+1 − 2u j + u j−1 = h2 f j. The",pos_def_matrices
"into every node is zero. The numbers f1, f2, f3, f4 are current sources into the nodes. The",vector spaces
"After studying Rn, it is natural to think of the space R∞. It con-",orthogonality
That produces twice the area 1,pos_def_matrices
0 0 2 6,determinants
33. Suppose V is known to have dimension k. Prove that,vector spaces
What are the pivots? List the three operations in which a multiple of one row is,gauss elim
Calculus would be enough to ﬁnd our conditions Fxx > 0 and FxxFyy > F2,pos_def_matrices
and overﬂow the computer. But still Σ is as good as possible. It reveals exactly what is,pos_def_matrices
of A−1. The ﬁnal count of multiplications for computing A−1 is n3:,gauss elim
9. What test on b1 and b2 decides whether these two equations allow a solution? How,gauss elim
"7. Find the projection matrix P onto the space spanned by a1 = (1,0,1) and a2 =",orthogonality
I have to admit that the front wall and side wall of a room look like perpendicular,orthogonality
loses the digits 2 and 3.,gauss elim
"This sum is guaranteed to converge, and for any two vectors it still obeys the Schwarz",orthogonality
"17. Show that if U and V are unitary, so is UV. Use the criterion UHU = I.",eigenvec_val
"One proof is to multiply the left side by w8, which leaves it unchanged. (It yields w8 +",orthogonality
"25. For a circulant C = FΛF−1, why is it faster to multiply by F−1, then Λ, then F (the",eigenvec_val
(ii) Each column of AB is the product of a matrix and a column:,gauss elim
"food diets must cost at least as much as vitamins. This is only a one-sided inequality,",linear_prog
−1 −3 3 4,vector spaces
s [ cs ],vector spaces
"elimination algorithm, the computer automatically looks for the largest pivot. Then the",computations
"the equations when f = (1,1,6). With those currents entering nodes 1, 2, 3 of the",vector spaces
2 1 0 1,vector spaces
Arnoldi for symmetric matrices (all coded in ARPACK).,computations
"(a) Show that this same x is an eigenvector of B = A − 7I, and ﬁnd the eigenvalue.",eigenvec_val
"(b) Starting from D1 = 1 and D2 = 0, ﬁnd D3,D4,...,D8. By noticing how these",determinants
"and cA are lower triangular if A and B are lower triangular, and they are symmetric if A",vector spaces
The condition number of A is c = ∥A∥∥A−1∥. The relative error satisﬁes,computations
the number of columns in A has to equal the number of rows in B. Then A can be,gauss elim
Solve for x and y after ﬁxing the second breakdown by a row exchange.,gauss elim
5k +1 5k −1,eigenvec_val
There are two ways to multiply a matrix A and a vector x. One way is a row at a,gauss elim
(a) Write its inverse.,eigenvec_val
"unique (see Problem 17), LT must be identical to U.",gauss elim
"26. Find the lengths of u = (1+i,1−i,1+2i) and v = (i,i,i). Also ﬁnd uHv and vHu.",eigenvec_val
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"subject to x ≥ 0, w ≥ 0, Ax + w = b. Whenever Ax = b has a nonnegative solution,",linear_prog
we can only make a single choice of ω. The largest µ gives the largest value of ω and,computations
We see again that the length of the rows (the number of columns in A) must match,gauss elim
When does equality hold?,orthogonality
"F, and how many from F−1?",orthogonality
1. Computation of A−1. The 2 by 2 case shows how cofactors go into A−1:,determinants
"have n roots. This is uniqueness, and it implies existence: Given any values b1,...,bn,",vector spaces
Two subspaces V and W of the same space Rn are orthogonal if every,orthogonality
Maybe QR is not as beautiful as LU (because of the square roots). Both factoriza-,orthogonality
"eigenvectors x1,...,xn, the vector uk will be given by the usual formula:",computations
matrix. We are allowing three types of elementary matrices:,gauss elim
One fact is unavoidable: The orthogonal case is the most important. Suppose we,orthogonality
"As it stands, the QR algorithm is good but not very good. To make it special, it needs",computations
1.5 Triangular Factors and Row Exchanges,gauss elim
Chapter 2 Vector Spaces,vector spaces
leading mathematician at the time when computers suddenly made a million operations,gauss elim
2.5 Graphs and Networks,vector spaces
"are exchanged, two equal columns (or a column of zeros) produce a zero determinant,",determinants
"When the rank is as large as possible, r = n or r = m or r = m = n, the matrix has a",vector spaces
Remark 2. U and V give orthonormal bases for all four fundamental subspaces:,pos_def_matrices
kx + 3y =,gauss elim
How do we know that the three columns lie in the same plane? One answer is to ﬁnd a,gauss elim
"two functions: If f(x) = sinx and g(x) = cosx, then their inner product is",orthogonality
3x + ky = −6.,gauss elim
x+ = A+b is shortest,pos_def_matrices
"the same coordinate direction, one will be a multiple of the other, and",determinants
0 1 0 0,eigenvec_val
"and accuracy, the key idea that makes ﬁnite elements successful is the use of piecewise",pos_def_matrices
3.2 Cosines and Projections onto Lines,orthogonality
"when a pivot entry will be zero, requiring a row exchange. From the formula determi-",determinants
"reduced by the ﬁrst QR steps, and another two steps are sufﬁcient to ﬁnd λ2. This gives",computations
of (−λ)n−1 and compare.,eigenvec_val
Remark. Our description of a Markov process was deterministic: populations moved in,eigenvec_val
"could be added to any solution, since the second derivative ofC+Dx contributes nothing.",gauss elim
40. Find the determinant of this cyclic P by cofactors of row 1. How many exchanges,determinants
"8. If w1, w2, w3 are independent vectors, show that the sums v1 = w2+w3, v2 = w1+w3,",vector spaces
"Suppose you invest $1000 at 6% interest. Compounded once a year, the principal P",eigenvec_val
The optimal solution of Ax = b is the minimum length solution of ATA�x = ATb.,pos_def_matrices
1. Compute the products,gauss elim
f1 + f2 + f3 + f4 = 0,vector spaces
(the best form we can get) is the reduced row echelon form R:,vector spaces
"2.3 Linear Independence, Basis, and Dimension",vector spaces
"u2 = λ 2x,...",eigenvec_val
"9%. We can buy amounts x, y, z not exceeding a total of $100,000. The problem is to",linear_prog
10. Show that the eigenvalues of B =,computations
11. The fundamental theorem is often stated in the form of Fredholm’s alternative: For,orthogonality
16. Find a point with z = 2 on the intersection line of the planes x + y + 3z = 6 and,gauss elim
"“snake” down the rows of the matrix). In other words, there are n! ways to permute the",determinants
1.4 Matrix Notation and Matrix Multiplication,gauss elim
The rank is 4− 1 = 3.,vector spaces
A to the upper triangular U. We will see it again in the next section.,gauss elim
"speciﬁc example with b3 −2b2 +5b1 = 0, choose b = (1,5,5):",vector spaces
3.7 The system Ax = b has a solution if and only if b is orthogonal to which of the four,orthogonality
we make an easy conversion to the complex case.,eigenvec_val
"2. (Recommended) On the preceding feasible set, what is the minimum value of the",linear_prog
matrix N). Everything is revealed by Rx = d.,vector spaces
jections. Why is P = a1aT,orthogonality
"Example 6. In nuclear engineering, a reactor is called critical when it is neutrally",eigenvec_val
"If A = I or even if A = I/10, its condition number is c = λmax/λmin = 1. By compari-",computations
1 2 1 1,determinants
a third matrix that connects them.,orthogonality
8.3 The Dual Problem,linear_prog
"v+w,u+v,u), Describe what T −1 does to the point (x,y,z).",vector spaces
"matrices whose column space is Rn. Now we allow singular matrices, and rectangu-",vector spaces
7. Consider any A and a “Givens rotation” M in the 1–2 plane:,eigenvec_val
"Choose a basis consisting of eigenvectors. The standard basis led to A, which was not",eigenvec_val
also done on the right-hand side—because both sides are there together.,gauss elim
"4)2 ≈ .07. If ω is further increased, the eigenvalues become a",computations
31. Tridiagonal matrices have zero entries except on the main diagonal and the two,gauss elim
postmultiplication by a permutation matrix.) The difﬁculty with being so conservative,gauss elim
1∆. In the case,linear_prog
2u + 3v − 4w =,gauss elim
izontal axis. That axis is the column space of A. The y-axis that,vector spaces
multiplication by 2 + 3t? The columns of the 5 by 4 matrix A come from applying,vector spaces
(t1 − ¯t)2 +···+(tm − ¯t)2,orthogonality
the pivots to reduce [A I] to [I A−1]:,gauss elim
of length 1/N. The discrete system with N unknowns is governed by,eigenvec_val
"change of variables was nonlinear, but the simplest transformation is just a rescaling by",linear_prog
"would give −10,000. Every trace of the entry 1 would disappear:",gauss elim
"are x = (1,1,1) and x = (0,6,−3.6).",gauss elim
Every matrix of rank 1 has the simple form A = uvT = column times row.,vector spaces
"to show how it can be broken into simple pieces. For linear algebra, the simple pieces",vector spaces
We pay attention only when the rows point in different directions. The nonzero terms,determinants
0 2 2 2,vector spaces
8.3 The Dual Problem,linear_prog
"every step, so we are left with the bare minimum:",gauss elim
1.5 Factor the preceding matrices into A = LU or PA = LU.,gauss elim
x = c1x1 +···+cnxn,vector spaces
One viewpoint is this: The determinant provides an explicit “formula” for each entry,determinants
"The true x is (1,1). Make a table to show the error for each ε. Exchange the two",computations
is the length ∥x∥ we want:,orthogonality
What is the minimum value E2 = e2,orthogonality
f(x). This brings b1sinx as close as possible to f(x). All the terms in the series are,orthogonality
+ w = 1.,gauss elim
40. Put A =,eigenvec_val
Λ. Then set x = Qy and the quotient becomes simple:,pos_def_matrices
"4.12 In analogy with the previous exercise, what is the equation for (x,y,z) to be on the",determinants
"determinant is their product, so all upper left determinants are positive.",pos_def_matrices
(λ1x)Hy = (Ax)Hy = xHAy = xH(λ2y).,eigenvec_val
"the right-inverse by Note 2, so every nonsingular matrix is invertible.",gauss elim
There are many right-inverses because the last row of C is completely arbitrary. This is,vector spaces
the whole truth. N(A) contains every vector orthogonal to the row space. The nullspace,orthogonality
2.2 Solving Ax = 0 and Ax = b,vector spaces
"These columns e1,...,en represent unit vectors in the coordinate directions; in R4,",vector spaces
"If x is in the nullspace then Ax = 0. If v is in the row space, it is a combination of the",orthogonality
Chapter 8 Linear Programming and Game Theory,linear_prog
"has λ = 1 and .6, and the power method uk = Aku0",computations
mal corner x∗. Interior point methods start inside the feasible set (where the constraints,linear_prog
identities (and they give a new way to remember those identities). It was no accident,vector spaces
bers; they lie on the axes. Two complex numbers are easy to add:,eigenvec_val
the basis in U to the basis in W. If we distinguish the transformation A from its,vector spaces
normal length a by least squares.,orthogonality
AATAv j = σ 2,pos_def_matrices
There is one more consequence of detA = detAT. We can expand in cofactors of a,determinants
∂x = 4(x+y)−3x2 = 0,pos_def_matrices
λ x = A−1x.,eigenvec_val
1.10 Invent a vector space that contains all linear transformations from Rn to Rn. You,vector spaces
moderate. Then h = 1,computations
"Example 2. Three measurements b1, b2, b3 are marked on Figure 3.9a:",orthogonality
32. Diagonalize A and compute SΛkS−1 to prove this formula for Ak:,eigenvec_val
"that gave place to the tests on a, b, c:",pos_def_matrices
ax + 2y = 0,gauss elim
0 0 0 0,vector spaces
"The elimination steps on this A are easy: (i) E subtracts ℓ21 times row 1 from row 2, (ii)",gauss elim
between the smallest eigenvalue µ of the new matrix and the original λ’s?,pos_def_matrices
potential differences agree with the b’s? You are ﬁnding (from Kirchhoff or from,vector spaces
"The eigenvectors are (1,−i) and (1,i). and the solution is",eigenvec_val
D yields the Cholesky factor-,pos_def_matrices
(c) turn every vector counterclockwise through 90°.,gauss elim
Does the square of Qθ equal Q2θ (rotation through a double angle)? Yes.,vector spaces
Just as for difference equations. the eigenvalues decide how u(t) behaves as t → ∞.,eigenvec_val
The constants ci that match the initial conditions u(0) are c = S−1u(0).,eigenvec_val
m + n halfspaces. This feasible set has ﬂat sides; it may be unbounded. and it may be,linear_prog
1.9 Write down a 2 by 2 system with inﬁnitely many solutions.,gauss elim
vectors go into the columns of S:,eigenvec_val
2.1 Vector Spaces and Subspaces,vector spaces
one requires an unnatural patience with matrix multiplication. The ﬁrst row of (AB)T is,gauss elim
Now suppose the two observations are not trusted to the same degree. The value,orthogonality
(a) A and AT have the same number of pivots.,vector spaces
"If the angles are not 90°, the volume is not the product of the lengths. In the plane",determinants
eigenvectors are not changed. The Hermitian example on the previous pages would lead,eigenvec_val
eventually have to do the same multiplications).,gauss elim
"we can list its components as b = (5,−2,9), or we can represent it geometrically by an",gauss elim
line stays where it is. The perpendicular line reverses direction; all points go straight,vector spaces
Figure 1.6: Singular cases: b outside or inside the plane with all three columns.,gauss elim
and the chain of matrices is C(t) = tQ+(1−t)QR. The family C(t) goes slowly through,pos_def_matrices
terval from −π to π? What is the closest straight line c+dx?,orthogonality
ﬁnd and to integrate.,pos_def_matrices
Chapter 2 Vector Spaces,vector spaces
34. There are sixteen 2 by 2 matrices whose entries are 1s and 0s. How many of them,gauss elim
"misses the 1s in the other columns.) Therefore detPdetPT = detI = 1, and P and PT",determinants
2 from aTb/aTa instead of,orthogonality
57. Suppose the ﬁrst and last columns of a 3 by 5 matrix are the same (nonzero). Then,vector spaces
"20. In Hilbert space, ﬁnd the length of the vector v = (1/",orthogonality
"Roughly speaking, A cannot be too large. If production consumes too much, nothing is",eigenvec_val
22. Which values of c give a bowl and which give a saddle point for the graph of z =,pos_def_matrices
(c) Prove that N (called a “nilpotent” matrix) cannot be symmetric.,eigenvec_val
"dependent rows, Ax = b may have no solution. That happens when b is outside the",pos_def_matrices
"Since W is invertible, no vector is assigned length zero (except the zero vector). All",orthogonality
"a ﬁxed interval, say 0 ≤ x ≤ 1. The space includes f(x) = x2, g(x) = sinx, their",vector spaces
. The second basis vector is re-,vector spaces
"proved) that all deterministic algorithms must take exponentially long to ﬁnish, in the",linear_prog
"Suppose the vectors x1,...,xn are a basis for the space V, and vectors",vector spaces
"Canceling bTb and aTa on both sides of this equation, you recognize formula (2) for the",orthogonality
"Since W n = 1, the left side is zero. But W is not 1, so the last factor must be zero. The",orthogonality
13. Diagonalize the matrix A =,eigenvec_val
decided by r and α. This step begins with the current basis matrix B and the current,linear_prog
1 and λ k,eigenvec_val
All symmetric matrices are combinations of one-dimensional projections—which are,eigenvec_val
ABx = BAx = Bλx = λBx.,eigenvec_val
is the meeting point of n different planes. Each plane is given by one equation—just,linear_prog
1. Start from any node s and repeat the following step:,linear_prog
"matrix. Change A(1,1) to 1 so detA = 1. Predict the entries of A−1 based on n = 3",determinants
describes the solution to du/dt = Au: It goes around in a circle.,eigenvec_val
"F is symmetric and orthogonal (apart from a factor √n), and it has only one drawback:",orthogonality
we start by computing the eigenvalues and eigenvectors; there is no shortcut to avoid,eigenvec_val
by θ = 60° the area is,determinants
The constrained minimum of P = 1,pos_def_matrices
vector in each subspace. Which vectors are orthogonal?,orthogonality
H = 2P − I =,vector spaces
ending in 0 come before numbers ending in 1).,orthogonality
6.5 The Finite Element Method,pos_def_matrices
"pinpoint one central cause: If you differentiate eikx, or integrate it, or translate x to",orthogonality
"In other words, the product of a 1 by n matrix (a row vector) and an n by 1 matrix (a",gauss elim
"24. What three elimination matrices E21, E31, E32 put A into upper triangular form",gauss elim
c1x1 +c2x2 = u(0),eigenvec_val
68. Show by example that these three statements are generally false:,vector spaces
"Of course the transpose is not the inverse! AT moves the spaces correctly, but not the",orthogonality
14. What matrix P projects every point in R3 onto the line of intersection of the planes,orthogonality
4. Mark all the sixth roots of 1 in the complex plane. What is the primitive root w6?,orthogonality
"1.14 If possible, ﬁnd 3 by 3 matrices B such that",gauss elim
"edge (otherwise Mi j = 0). For the graph in Problem 6 with 6 nodes and 4 edges,",vector spaces
(b) How do you know that ATy = 0 has a nonzero solution?,vector spaces
Suppose A = AT can be factored into A = LDU without row exchanges.,gauss elim
. Find two combinations of the columns,gauss elim
"7. Show, by forming bTb directly, that Pythagoras’s law holds for any combination",orthogonality
"We start by simplifying this 3 by 4 matrix, ﬁrst to U and then further to R:",vector spaces
"has trace a+d, and determinant ad −bc",eigenvec_val
2p2 and Ap4 is 3p3. The nullspace contains p1 (the derivative of a constant is zero).,vector spaces
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
"31. Choose a, b, c, so that det(A−λI) = 9λ −λ 3. Then the eigenvalues are −3, 0, 3:",eigenvec_val
mation (from U to W). Its matrix is the product of the individual matrices,vector spaces
(f) the column space of A.,vector spaces
unchanged. Every other W changes the length and inner product.,orthogonality
a symmetric 2 by 2 matrix!,pos_def_matrices
Line W perpendicular to plane V,orthogonality
"druggist’s price ≤ grocer’s price. It is called weak duality, and it is easy to prove for",linear_prog
orthonormal case we only need QT.,orthogonality
"Its eigenvalues are λ1 = λ2 = 0, since it is triangular with zeros on the diagonal:",eigenvec_val
C = AB =,gauss elim
"so are its multiples ib; b is real. The sum a+ib is a complex number, and it is plotted in",eigenvec_val
18. Draw the projection of b onto a and also compute it from p = �xa:,orthogonality
"vector” is g(x) = x. Keep the usual scalar multiplication cf(x), and ﬁnd two rules",vector spaces
"and y′′, and reconstruct y from equation (13).",orthogonality
Remark. Not all differential equations come to us as a ﬁrst-order system du/dt = Au.,eigenvec_val
solved in reverse order: Substitute each newly computed value into the equations that,gauss elim
jth column of A comes from applying T to v j:,eigenvec_val
"at the start of equation (2), by increasing a component of xN. That will be our next step.",linear_prog
tiplication to describe the operations that make it simpler. Notice that three different,gauss elim
Suppose the n by n matrix A has n linearly independent eigenvectors.,eigenvec_val
2 is below λ1(B). So λ1(B) is caught between.,pos_def_matrices
c(A) = ∥A∥∥A−1∥ ≈ (1.618)2,computations
cannot also have λ 1λ2 = 1. Thus xHy = 0 and the eigenvectors are orthogonal.,eigenvec_val
There is a simple way to change the inequality x+2y ≥ 4 to an equation. Just introduce,linear_prog
deﬁnite. That guarantees that the real parts of the eigenvalues are positive. But it is not,pos_def_matrices
"In the dual, the druggist is selling vitamin pills at prices yi ≥ 0. Since food j",linear_prog
"a ﬁnite number of steps (n3/3 for a full matrix, less than that for the large matrices we",computations
x1 +x2 +x3 −x4 = 0.,orthogonality
3. Each pivot lies to the right of the pivot in the row above. This produces the staircase,vector spaces
The basis for the nullspace is,vector spaces
1q j = 0),orthogonality
"matrix operations are allowed, and what are the results?",gauss elim
we start with three possibilities:,computations
"Therefore the best case is when A is symmetric, or more generally when AAT = ATA.",computations
e = b − Ax,vector spaces
This can be solved by back-substitution (since Ji is triangular). The last equation du3/dt =,eigenvec_val
17. (Constrained minimum) Suppose the unconstrained minimum x = A−1b happens to,pos_def_matrices
"This is already a unit vector, so it is q3. I went to desperate lengths to cut down the num-",orthogonality
"vectors x are eigenvectors. We can watch the behavior of each eigenvector, and then",eigenvec_val
Then A is a normal matrix; its diagonalizing S is an orthogonal Q (Section 5.6).,computations
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"The matrix ATA is certainly symmetric. Its transpose is (ATA)T = ATATT, which is ATA",orthogonality
Figure 4.2: Volume (area) of the parallelogram = ℓ times h = |detA|.,determinants
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"clear (in fact, probably clearer) if we omit some of the details.",linear_prog
"that x times (2,10) plus y times (3,9) equals (1,11). If the right-hand side changes",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"Find the Jacobian matrix of 9 partial derivatives: ∂x/∂ρ, ∂x/∂φ, ∂x/∂θ are in row",determinants
Thus CD = −DC is only possible if C or D is singular.,determinants
", compare three inverse power steps to",computations
"ﬁrst you see nothing, and suddenly you recognize everything. The cost is in computing",pos_def_matrices
2. Find the largest possible number of independent vectors among,vector spaces
Row Exchanges and Permutation Matrices,gauss elim
67. The reduced form R of a 3 by 3 matrix with randomly chosen entries is almost sure,vector spaces
"(m − r) + r = m, This completes the second half of the fundamental theorem of linear",orthogonality
7.3 Computation of Eigenvalues,computations
"27. Prove that AHA is always a Hermitian matrix, Compute AHA and AAH:",eigenvec_val
"10, an individual outside California moves in. If inside,",eigenvec_val
"Transposing PA = LDU gives ATPT = UTDTLT, and again by rule 9,",determinants
The ﬁrst step is straightforward: We want to ﬁnd the “parabola” P(x) whose minimum,pos_def_matrices
"substitution, verify Cayley-Hamilton: A2 −(a+d)A+(ad −bc)I = 0.",eigenvec_val
yb ≤ y(Ax) = (yA)x ≤ cx.,linear_prog
"directions of q1, q2, q3.",orthogonality
"3.17 What words describe the equation ATA�x = ATb, the vector p = A�x = Pb, and the",orthogonality
5. The following system has no solution:,orthogonality
"The sum of squares matches xTx—and the length of x = (1,2,−3) is",orthogonality
"(V′) There is a matrix R, possibly with dependent columns, such that A = RTR.",pos_def_matrices
contains the variances σ2,orthogonality
The other axes are along the other eigenvectors. Their lengths are 1/,pos_def_matrices
"The column space contains p1, p2, p3 (the derivative of a cubic is a quadratic). The",vector spaces
Our ﬁrst move beyond the eigenvector matrix M = S is a little bit crazy: Instead of a,eigenvec_val
fascinating stage was the ﬁrst—the tableau—which dominated the subject for so many,linear_prog
Basis for a Vector Space,vector spaces
"32. What is the nullspace matrix N (of special solutions) for A, B, C?",vector spaces
0 0 0 3,vector spaces
All solutions approach zero if and only if all eigenvalues have Reλ < 0.,eigenvec_val
"6. When b = (2,5,7), ﬁnd a solution (u,v,w) to equation (4) different from the solution",gauss elim
"Gram-Schmidt, from QR to Q. It is invertible, because Q is invertible and the triangular",pos_def_matrices
the eigenvalues of AB plus 4 zeros; G has the eigenvalues of BA plus 6 zeros. AB,eigenvec_val
"28. Solve u′ = Ju by back-substitution, solving ﬁrst for u2(t):",eigenvec_val
"to r′′ = |λ1 −α|/|λ2 −α|. If α is a good approximation to λ1, r′′ will be very small",computations
minimum? Again the answer is no; the sign of b is of no importance! Even though its,pos_def_matrices
"vations x = b1 and x = b2. Unless b1 = b2, we are faced with an inconsistent system of",orthogonality
25. Construct a matrix with the required property or say why that is impossible.,orthogonality
(AB)−1�T comes from (A−1)T and (B−1)T. In what order?,gauss elim
∗ ∗ 0 0,computations
1. All vectors perpendicular to the column space lie in the left nullspace. Thus the,orthogonality
"3.16 Suppose the vectors q1,...,qn are orthonormal. If b = c1q1 + ··· + cnqn, give a",orthogonality
"(a) What inequalities (< or ≤) must be true between m, n, and r?",vector spaces
"the span of a, b, c, you add three projections. All calculations require only the inner",orthogonality
"case takes care of (x1,x2,0) = (1,2,0) across the base. This forms a right angle with the",orthogonality
"Maximize 4p, subject to p ≤ 2, 2p ≤ 3, and p ≥ 0.",linear_prog
(a) Is this result the same as separately applying C then B then A?,vector spaces
The transpose AT can be deﬁned by the following property: The inner,orthogonality
"26. In the Gram-Schmidt formula (10), verify that C is orthogonal to q1 and q2.",orthogonality
3. Show directly from the rows that every vector f in the row space will satisfy f1 +,vector spaces
Special solutions to Ax = 0,vector spaces
its direction is outside our control. The solution is changed from x to x+δx:,computations
positive deﬁnite) with special attention to condition III: How is det(−A) related to,pos_def_matrices
"(c) Construct a matrix whose column space is spanned by (1,1,2) and whose row",vector spaces
"at this rate. The other solutions will be mixtures of these pure solutions, and the mixture",eigenvec_val
(a) Why is its column space perpendicular to its nullspace?,orthogonality
"(c) Conclude that the jth eigenvalue of CTAC, from its minimax principle, is also",pos_def_matrices
no is that every M−1AM has the same number of independent eigenvectors as A (each,eigenvec_val
(b) A has a repeated eigenvalue.,eigenvec_val
using square roots. Notice the 1,orthogonality
5. Addition and scalar multiplication are required to satisfy these eight rules:,vector spaces
"from 0 to 1, from the eigenvalues of CTAC to the eigenvalues of QTAQ. Because C(t) is",pos_def_matrices
"This lemma applies to all matrices, with no assumption that A is diagoalizable. We",eigenvec_val
"If we have a subspace of Rn, the standard vectors ei might not lie in that subspace.",orthogonality
"There is a “double inﬁnity” of solutions, with v and y free and independent. The com-",vector spaces
achieved by an odd number of exchanges of neighbors. This will complete the proof;,determinants
We emphasize that V and W can be orthogonal without being complements. Their,orthogonality
would have gone in the reverse direction):,gauss elim
"get a good answer. It is very tempting to try the same reasoning, hoping to prove what",eigenvec_val
To locate the ellipse we compute λ1 = 1 and λ2 = 9. The unit eigenvectors are,pos_def_matrices
The ﬁrst two subspaces (the two lines) had dimensions 1+1 = 2 in the space R2. The,orthogonality
Chapter 2 Vector Spaces,vector spaces
"21. If you extend Problem 20 following the 1, 2, 1 pattern or the −1, 2, −1 pattern, what",gauss elim
−6 0 0 1 0,gauss elim
"termines each column of A−1. The ﬁrst column of A−1 is multiplied by A, to yield the",gauss elim
trices are diagonalizable. The standard example of a “defective matrix” is,eigenvec_val
"5.22 If A2 = −I, what are the eigenvalues of A? If A is a real n by n matrix show that n",eigenvec_val
the zero entries in 4I). The n by n matrices will have FF = nI. Then the inverse of F is,orthogonality
4. Explain why the Schwarz inequality becomes an equality in the case that a and b,orthogonality
there a whole line of solutions?,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"the number of marriages when brides have a veto. That may not be the real problem, but",linear_prog
the new system WAx = Wb. This changes the solution from �x to �xW. The matrix W TW,orthogonality
it is far from sufﬁcient to look only at the diagonal entries.,pos_def_matrices
c2P2 + c3P3. (Its eigenvector matrix is again the Fourier matrix.) Write out also,eigenvec_val
"sum (f +g)(x) = x2 +sinx, and all multiples like 3x2 and −sinx. The vectors are",vector spaces
A matrix with n distinct eigenvalues can be diagonalized. This is the typical case.,eigenvec_val
Another way to count: There are ℓ steps from n = 2ℓ to n = 1. Each step needs n/2,orthogonality
The situation is the same when we are given a plane (or any subspace S) instead of a,orthogonality
"21. A positive deﬁnite matrix cannot have a zero (or even worse, a negative number) on",pos_def_matrices
"(b) All sequences (x1,x2,...) with x j = 0 from some point onward.",vector spaces
1 3 3 2,vector spaces
Solution. (Notice how the right-hand side is included as an extra column!),vector spaces
"the column picture, plus a vector b on the right side. The equations ask for a linear com-",gauss elim
Many applications add extra equations Cx = d on top of the minimization problem.,pos_def_matrices
9. How many multiplications to ﬁnd an n by n determinant from,determinants
another look. These were the rules:,eigenvec_val
"DO 10 J = 1, N",gauss elim
"This list can be made much longer, especially if we look ahead to later chapters. Every",vector spaces
rate between two adjacent segments is the difference in concentrations. Within each,eigenvec_val
"a1j. Furthermore, a1j also accounts for the jth column, so its cofactor C1 j must depend",determinants
"3. Find the echelon form U, the free variables, and the special solutions:",vector spaces
unknowns u and v are not reversed in a row exchange.,gauss elim
and making a clever choice of λ.,eigenvec_val
the projection of b is (qT,orthogonality
We can ﬁnally discover when elimination is possible,determinants
1 4 10 20,determinants
U and V leave lengths unchanged in ∥Ax∥ = ∥UΣV Tx∥. So the largest ∥Ax∥/∥x∥ comes,computations
ω. The key step was to connect the eigenvalues λ of L to the eigenvalues µ of the,computations
Section 7.2 attempts to measure the,computations
18. Move the third plane in Problem 17 to a parallel plane 2x + 3y + 2z = 9. Now the,gauss elim
The inverses come in reverse order.,gauss elim
sinθ cosϕ +cosθ sinϕ,vector spaces
We accept the computed λ ′,computations
actually get the same number back again: (xHAx)H = xHAHxHH = xHAx. So that number,eigenvec_val
37. The ﬁrst component of Ax is ∑a1 jx j = a11x1 + ··· + a1nxn. Write formulas for the,gauss elim
(lines or planes) in (two or three)-dimensional space. The column picture is in (two,gauss elim
"add 1 (Ap = p+1), or to keep the positive coefﬁcients (A(t −t2) = t). It will be linear",vector spaces
Notice that �C = (y1 + y2 + y3)/3 is the mean of the data.,orthogonality
33. Find three 2 by 2 matrices that have λ1 = λ2 = 0. The trace is zero and the determi-,eigenvec_val
"The ﬁrst column of H1A equals −∥x∥z. It is zero below the main diagonal, and it",computations
0 1 0 0,eigenvec_val
"our lives are examples of Markov processes, but I hope not.",eigenvec_val
"Remark 3. The zero-order terms F(0,0) = 7 and f(0,0) = 0 have no effect on the an-",pos_def_matrices
1 1 1 1,eigenvec_val
into each node equals the ﬂow out. That is Kirchhoff’s current law:,linear_prog
(c) If AB and BA are deﬁned then AB and BA are square.,gauss elim
"combination of the columns that adds to zero. After some calculation, it is u = 3, v = 1,",gauss elim
This Cholesky decomposition has the pivots split evenly between L and LT.,pos_def_matrices
equations. There most be a formula for y (and also x) It is a “ratio of determinants”,gauss elim
Finally the third plane intersects this line in a point. The plane (not drawn) represents ,gauss elim
the square root of 1. Note especially that half the entries in F∗,orthogonality
"2. Below each pivot is a column of zeros, obtained by elimination.",vector spaces
3. Explain why A is never similar to A+I.,eigenvec_val
4u − 6v = −2 (vertical plane),gauss elim
then we can imagine that those exchanges are done ﬁrst (by P). The matrix PA will not,gauss elim
"Like other new ideas in scientiﬁc computing, Karmarkar’s method succeeded on some",linear_prog
"rewrite that as sX = θe. Creating the diagonal matrix S from s, this is eSX = θe. If we",linear_prog
"The series always converges, and its sum eAt has the right properties:",eigenvec_val
"9. Show that the trace equals the sum of the eigenvalues, in two steps. First, ﬁnd the",eigenvec_val
Lax gives four striking examples in his book Linear Algebra.,eigenvec_val
"to the problem. Demand and production, y and p, are nonnegative. Since p is (1−A)−1y,",eigenvec_val
"1q1 = 1, we have found x1 = qT",orthogonality
"resents (IłA)−1. It is nonnegative when A is nonnegative, provided it has a ﬁnite",eigenvec_val
jection PR onto the row space? (It is not PT,orthogonality
timated the ﬁnal roundoff error. It was Wilkinson who found the right way to answer the,gauss elim
solution to Ax = 0:,vector spaces
Diagonalizability of A depends on enough eigenvectors.,eigenvec_val
"W is the line spanned by w = (0,0,4,5), then w is orthogonal to both v’s. The line W",orthogonality
of P and it does not depend on the order of the row exchanges.,gauss elim
You see how the rank r is crucial. It counts the pivot rows in the “row space” and the,vector spaces
every instant. Their total number v + w is constant. That comes from adding the two,eigenvec_val
"You have to be careful with L. Suppose elimination subtracts row 1 from row 2,",gauss elim
or δA. This was the basis of Wilkinson’s error analysis. Since elimination actually,computations
B has the same column space as A. The new column lies in the plane of Figure 2.1; it is,vector spaces
"Many simplex codes use the product form of the inverse, which saves these simple",linear_prog
This is the characteristic equation. Each λ is associated with eigenvectors x:,eigenvec_val
4.4 Applications of Determinants,determinants
(b) Deduce that the eigenvalues of C must come in plus-minus pairs.,eigenvec_val
"(b) In parallel with the upper bound (8) on the error, prove a lower bound:",computations
eigenvector add to zero.,eigenvec_val
"5. Elimination can be completed: PA = LDU, with all n pivots.",vector spaces
terriﬁc for numerically stable computations. because U and V are orthogonal matrices.,pos_def_matrices
systems that are actually met in practice. The next chapter turns to the existence and the,gauss elim
"this book, when the basic problem was Ax−b, the solution was real when A and b were",eigenvec_val
Example 2. The eigenvalues themselves are not so clear for a rotation:,eigenvec_val
det < 0 gives λ1 < 0 and λ2 > 0: real and unstable,eigenvec_val
a11C21 +a12C22 +···+a1nC2n = 0.,determinants
Two simple examples will illustrate three important points about roundoff error. The,gauss elim
It should be said that after computing a similar matrix Q−1AQ with more zeros than,computations
"ﬁve components, one for each edge. These numbers represent currents ﬂowing along",vector spaces
using Problems 36–37 and 34:,eigenvec_val
38. (a) Find all functions that satisfy dy,vector spaces
"By deﬁnition, pivots cannot be zero. We need to divide by them.",gauss elim
tic; A\b; toc and tic; A\B; toc (which solves for 9 right sides).,gauss elim
"and −1. When matrices are multiplied, there is usually no direct way to read off the",gauss elim
half-axes with lengths a =,pos_def_matrices
C11 = detA3 = det,determinants
"u(0) = a1x1 +···+anxn,",eigenvec_val
0 1 0 0,determinants
(b) Show by example that equality fails (as shown) when C enters.,determinants
"The discovery that such an improvement could be produced so easily, almost as if by",computations
2A(un+1 +un) or un+1 = (I − 1,eigenvec_val
(a) Why is the inner product of x with Py equal to the inner product of Px with y?,orthogonality
(c) A multiplies itself to produce A2? Here m = n.,gauss elim
This is the answer we wanted. The fractions and square roots look surprising because,eigenvec_val
Differentiation is a left-inverse of integration. Rectangular matrices cannot have two-,vector spaces
leaves every vector unchanged. The elementary matrix Eij subtracts ℓ times,gauss elim
square of c(A). Forming ATA can turn a healthy problem into a sick one. It may be,computations
"cut in half (plus the cost of reassembling the results). What makes this true, and possible",orthogonality
"formula for the determinant, and to put that formula to use.",determinants
"The pivots for C are 1 and .9999, much better than .0001 and −9999 for B.",gauss elim
(d) the second pivot.,gauss elim
AT and which are in the nullspace of A?,vector spaces
"z = f(x,y) will then be shaped like a bowl, resting on the origin (Figure 6.1). If the",pos_def_matrices
"and explain why they are important. Geometrically, think of the usual three-dimensional",vector spaces
0 0 ∗ ∗,computations
3(9−x2 −x3) into the cost function and the,linear_prog
"2. We move to matrix notation, writing the n unknowns as a vector x and the n equa-",gauss elim
0 (a0 +···+antn)dt = a0t +···+,vector spaces
1.4 Matrix Notation and Matrix Multiplication,gauss elim
"solutions. Substituting v = eλty and w = eλtz into the equation, we ﬁnd",eigenvec_val
"2. Portfolio Selection. Federal bonds pay 5%, municipals pay 6%, and junk bonds pay",linear_prog
(c) Find as many linearly independent vectors b as possible for which Ax = b has a,vector spaces
pendent. The v’s and w’s need not be column vectors—the proof was all about the matrix,vector spaces
decide on a basis. For the polynomials of degree 3 there is a natural choice for the four,vector spaces
1. When is an upper triangular matrix nonsingular (a full set of pivots)?,gauss elim
A1 = R0Q0 =,computations
end of the section. and we explain them as we go.,eigenvec_val
"All they will allow is a few simple checks on the eigenvalues, after they have been",eigenvec_val
3 −2x1x2 −2x1x3 −4x2x3.,pos_def_matrices
8. Explain why the system,gauss elim
"H2 is of order n−2. When it is embedded in U2, it produces",computations
determinant. This means: When a triangular matrix is singular (because of a zero on the,determinants
14. Prove that T 2 is a linear transformation if T is linear (from R3 to R3).,vector spaces
0 0 0 0,vector spaces
"n = 1. Instead of (1,0,...,0),",pos_def_matrices
x1 −x2 = b1,orthogonality
"products aTv, bTv, and cTv. But to make this true, we are forced to say, “If they are",orthogonality
"just as it applied to matrix equations Ax = 0. The nullspace is always a subspace, and",eigenvec_val
a a a a,vector spaces
"derivative of a combination like p = 2 +t −t2 −t3 is decided by linearity, and there is",vector spaces
Example 3. du/dt =,eigenvec_val
ax + by +,determinants
r = m = n. Each of these conditions is a necessary and sufﬁcient test:,vector spaces
the ﬁrst three columns. The other three columns are the same as L−1. (This is the effect,gauss elim
"nothing new. At the same time, because every step can be reversed, nothing is lost; the",vector spaces
cos2 θ +sin2θ = 1.,eigenvec_val
"u = (cost,sint). Suppose we approximate du/dt by forward, backward, and centered",eigenvec_val
"57. Write the inner product of (1,4,5) and (x,y,z) as a matrix multiplication Ax. A has",gauss elim
times 2000 numbers instead of a million (25 to 1 compression).,pos_def_matrices
detA = a11C11 +a12C12 +···+a1nC1n.,determinants
"Our description will be in electrical terms. On edge i, the conductance is ci and the",vector spaces
(b) If AB and BA are deﬁned then A and B are square.,gauss elim
"B−1u is the column of B−1N in the reduced tableau R, above the most negative entry in",linear_prog
The matrix N is normal if it commutes with NH: NNH = NHN. For,eigenvec_val
"that case (b1,b2) is perpendicular to the vector",orthogonality
0 0 3 1,vector spaces
Example 4. The columns of the n by n identity matrix are independent:,vector spaces
17. Diagonalize ATA to ﬁnd its positive deﬁnite square root S = VΣ1/2V T and its polar,pos_def_matrices
"Example 1. In equation (1), the exponential of A =",eigenvec_val
"That is the general rule, that F−1 comes from the complex number w−1 = w. It lies at",orthogonality
− 3z = 5.,gauss elim
lower right-hand corner of a full-size matrix U1:,computations
that their ratios give the second and third pivots.,pos_def_matrices
"orthogonal to a plane, but a plane cannot be orthogonal to a plane.",orthogonality
"25. The eigenvalues of A are 1 and 9, the eigenvalues of B are ł1 and 9:",eigenvec_val
(d) Find a basis for the column space of A.,vector spaces
method uk+1 = Auk three times to the initial guess u0 =,computations
that some combination gives zero:,pos_def_matrices
matrix for the Current Law.,vector spaces
is just one combination that uses only the values at x and x±h:,gauss elim
2xTAx − xTb reaches its,pos_def_matrices
those three) are fundamental; the full list is given in Problem 5 at the end of this section.,vector spaces
way near the origin. F has a minimum if and only if f has a minimum. I am going to,pos_def_matrices
"There are two eigenvalues, because a quadratic has two roots. Every 2 by 2 matrix",eigenvec_val
Diagonalization of a Matrix,eigenvec_val
"by differentiation brings back the original function. To make that happen for matrices,",vector spaces
The ﬁrst m and the last m components of the vector y = Fnc are,orthogonality
A small pivot forces a practical change in elimination. Normally we,gauss elim
This section has done its best while requiring M to be a unitary matrix U. We got,eigenvec_val
element becomes J drdθ dz. The Jacobian determinant is the three-dimensional ana-,determinants
32. (Recommended) Draw Figure 3.4 to show each subspace for,orthogonality
hat function overlaps itself and only two neighbors:,pos_def_matrices
old friend of chemists.),orthogonality
the same cofactors as A (because the second row is thrown away to ﬁnd those cofactors).,determinants
cosθ—and that is impossible.,eigenvec_val
= −1 and 2.,eigenvec_val
"One more comment on this example: It is a discrete approximation, with only two",eigenvec_val
x1 +c1y = 0,pos_def_matrices
"tions). The computer can do those quickly, but not many trillions. And already",gauss elim
0 0 6 6,vector spaces
2 1 1 1,determinants
H = I −2 vvT,computations
"take our place in carrying out the elimination. Since all the steps are known, we should",gauss elim
The row space component goes to the column space: Axr = Ax.,orthogonality
tiplication takes the inner products of the rows of Q. (For QTQ it was the columns.),orthogonality
"like at least p men, a complete matching is possible. That is Hall’s condition. No block",linear_prog
4 4 8 8,determinants
zero? How far can it be increased until the equations force x3 or x4 down to zero? At,linear_prog
When is (I −A)−1 a nonnegative matrix?,eigenvec_val
rotates the xy-plane by the angle θ.,eigenvec_val
"is the symmetric positive deﬁnite square root of ATA, and Q is AS−1. In fact, A could be",pos_def_matrices
"6.1 Minima, Maxima, and Saddle Points",pos_def_matrices
"4. (a) If A is invertible and AB = AC, prove quickly that B = C.",gauss elim
"number are again complex numbers. You must allow combinations x + iy, with a real",orthogonality
b not in place,gauss elim
"third. If E comes after G, then the third equation feels no effect from the ﬁrst. You will",gauss elim
"2yd as predicted in equation (5), since b = 0 and Pmin = 0.",pos_def_matrices
"23. The columns of A are n vectors from Rm. If they are linearly independent, what is",vector spaces
"stay the same. Geometrically, we shifted the feasible set a little (by changing b), and we",linear_prog
"matrix and the column vector (3,4,5). What are the outputs from A ∗ v and v’ ∗ v?",gauss elim
opposite sides of the origin?,orthogonality
"The determinant of every permutation matrix is detP = ±1. By row exchanges, we can",determinants
31. The Hadamard matrix H has orthogonal rows. The box is a hypercube!,determinants
"0, the right sides divided by the x4 column, make",linear_prog
The coefﬁcient b1 is the least squares solution of the inconsistent equation b1sinx =,orthogonality
We have to prove this fact: All possible bases contain the same number of vectors.,vector spaces
"S is invertible, because its columns (the eigenvectors) were assumed to be independent.",eigenvec_val
The laws of diffusion led to a ﬁrst-order system du/dt = Au. So do a lot of other appli-,eigenvec_val
Diagonalizable matrices share the same eigenvector matrix S if and only,eigenvec_val
why x∗ still remains the optimal solution.,linear_prog
Figure 6.1: A bowl and a saddle: Deﬁnite A =,pos_def_matrices
each other. Then every matrix will split into A = UΣV T.,pos_def_matrices
(c) What happens to row 3 in elimination?,gauss elim
3. to recognize the Fourier series as a sum of one-dimensional projections (the orthog-,orthogonality
"Now the edges come in the order 1, 2, 3, 4, 6 (again rejecting 5), and 7. They are the",linear_prog
C(A). Requirements (i) and (ii) for a subspace of Rm are easy to check:,vector spaces
"From now on, we will work with equation (6). It has a very regular coefﬁcient matrix,",gauss elim
zero vector will belong to every subspace. That comes from rule (ii): Choose the scalar,vector spaces
12. The identity transformation takes every vector to itself: Tx = x. Find the corre-,eigenvec_val
10. If R = [ p q,pos_def_matrices
cannot have an inverse. To repeat: No matrix can bring 0 back to x.,gauss elim
"Suppose we know the SVD. The key is in the singular values (in Σ). Typically, some",pos_def_matrices
"matrix is the pseudoinverse A+ of our diagonal A. Based on this example, we know Σ+",pos_def_matrices
"The answer is: We are computing the determinant of a new matrix B, with a new row 2.",determinants
The minimum of R(x) subject to xTx1 = 0 is λ2. The minimum of R(x),pos_def_matrices
"eigenvectors in the real world R3, or in Rn, we look in C3 or Cn. The space Cn contains",eigenvec_val
elements. The equation can be −u′′ = f(x) with boundary conditions u(0) = u(1) =,pos_def_matrices
"x1,...,x4. But that is impossible to do! We can raise or lower all the potentials by the",vector spaces
"from, you can backtrack to ﬁnd the path for extra ﬂow.",linear_prog
"when ε = 10−3,10−6,10−9,10−12,10−15:",computations
"nonsingular as long as degeneracy is forbidden, the simplex method has produced the",linear_prog
19. Compute the symmetric LDLT factorization of,gauss elim
"31. Find a counterexample to the following statement: If v1, v2, v3, v4 is a basis for the",vector spaces
vector b in the column space comes from exactly one vector xr in the row,orthogonality
all solutions to Ax = 0 is from the special solutions:,vector spaces
"(b) a second plane that also contains the points (6,0,0) and (2,2,0).",gauss elim
more vectors if necessary.,vector spaces
"15. If every row of A adds to zero, prove that detA = 0. If every row adds to 1, prove",determinants
"between x and y, show that Az is halfway between Ax and Ay.",vector spaces
transformed back—to reconstruct. What is crucially important is that F and F−1 can be,orthogonality
the statistical variances are σ2,orthogonality
The Theory of Inequalities,linear_prog
1 0 0 0,eigenvec_val
"gives the exact answer y more slowly than elimination, but you can go part way and then",linear_prog
"column rank.” It expresses a result that, for a random 10 by 12 matrix, is not at all",vector spaces
in Figure 1.3. It contains the points (5,gauss elim
"Whenever you see LDU or LDV, it is understood that U or V has is on the diagonal—",gauss elim
"9. (Transportation problem) Suppose Texas, California, and Alaska each produce a mil-",linear_prog
exchange it is totally vulnerable to roundoff.,gauss elim
13. By trial and error ﬁnd examples of 2 by 2 matrices such that,gauss elim
4 4 8 8,determinants
"occurs at component k, the kth column of the current B will leave.",linear_prog
"tains all vectors v = (v1,v2,v3,...) with an inﬁnite sequence of components. This space",orthogonality
The entering variable xi corresponds to the most negative component of r.,linear_prog
"invertible. We show in Chapter 3 that ATA does have an inverse if the rank is n, and AAT",vector spaces
+ 2v + 2w = 10,gauss elim
subspace of Rm. We illustrate by a system of m = 3 equations in n = 2 unknowns:,vector spaces
"they did. In the rectangular case m > n, they don’t. They give the projection p and",orthogonality
length: ∥x∥2 = x2,eigenvec_val
The values λ = −1 and λ = 2 lead to a solution of Ax = λx or (A−λI)x = 0. A matrix,eigenvec_val
The Law of Inertia,pos_def_matrices
"In this case we can produce any B that has the correct eigenvalues. It is an easy case,",eigenvec_val
d4u/dx4 = f(x)—for which the condition number grows as n4.1,computations
"vectors contain the crucial information. In the primal problem, x∗ tells the purchaser",linear_prog
We need to see how y = Fnc (a vector with n components) can be recovered from two,orthogonality
"Hint: Starting with u and v, add and subtract for (a). Try cu and cv for (b).",vector spaces
c [ cs ],vector spaces
"11. Solve as two triangular systems, without multiplying LU to ﬁnd A:",gauss elim
"only one inverse, namely B = C = A−1. Existence implies uniqueness and uniqueness",vector spaces
"When the equation was Ax = 0, the particular solution was the zero vector! It ﬁts the pat-",vector spaces
"third approach is to describe each individual entry in AB and hope for the best. In fact,",gauss elim
(c) Every diagonalizable matrix can be inverted.,eigenvec_val
"(Computer not needed!) If you ask for v ∗ A, what happens?",gauss elim
Multiply them to ﬁnd AB and BA.,gauss elim
values positive. Here all the entries aij are positive.,eigenvec_val
starts on an invertible matrix A but breaks down at column 3:,gauss elim
(d) Why is eKt unitary?,eigenvec_val
", construct the rank-1 projection matrix P = uuT.",eigenvec_val
powers are spaced evenly around the circle. That spacing assures that the sum of all n,eigenvec_val
15. Solve the second-order equation,eigenvec_val
"5. In n dimensions, what angle does the vector (1,1,...,1) make with the coordinate",orthogonality
"counted high cards to win at blackjack (Las Vegas follows ﬁxed rules, and a true matrix",linear_prog
only the errors ek = x−xk:,computations
12. Find the matrix that projects every point in the plane onto the line x+2y = 0.,orthogonality
"To repeat, the row space contains everything orthogonal to the nullspace. The column",orthogonality
the transformation of every vector is settled.,vector spaces
i y j = 0.,pos_def_matrices
xp and xn. Reduce the augmented matrix [A b].,vector spaces
its own characteristic equation: (T −λ1I)(T −λ2I)(T −λ3I) = 0.,eigenvec_val
in place (ℓ21 is 1 and ℓ31 is 2 when A = LPU).,gauss elim
"14. Write down all six of the 3 by 3 permutation matrices, including P = I. Identify their",gauss elim
"look at band matrices, to see how concentration near the diagonal speeds up elimination.",gauss elim
"x1 ≥ x2 ≥ 0, and draw a picture of this set.",linear_prog
be hopeless; a much more concise record is needed.,gauss elim
The third derivatives are drawn into the problem when the second derivatives fail to,pos_def_matrices
", compute A2, A3, and A4. Then use the text and",eigenvec_val
"This example chose v = (0,0,1) so the constraint xTv = 0 knocked out the third com-",pos_def_matrices
vectors are a basis.,vector spaces
"14, �D = 5",orthogonality
is singular by ﬁnding a combination of the three equations that adds up to 0 = 1.,gauss elim
The growth of uk is governed by the λ k,eigenvec_val
"A of coefﬁcients. In fact we can see this general result: In a subspace of dimension k,",vector spaces
"3. The row space of A is the column space of AT. It is C(AT), and it is spanned by the",vector spaces
(b) ﬁnd their sum and product.,eigenvec_val
"is Hermitian (complex b), ﬁnd its pivots and determinant.",pos_def_matrices
"is the ﬁrst column of R. The second step works with the second column of H1A,",computations
37. Use Gauss-Jordan elimination on [A I] to solve AA−1 = I:,gauss elim
"1.12 True or false, with reason if true or counterexample if false:",gauss elim
Pivot Variables and Free Variables,vector spaces
"If n is at all large, a good estimate for the number of operations is 1",gauss elim
46. Apply Gauss-Jordan elimination (right-hand side becomes extra column) to Ux = 0,vector spaces
"and their eigenvalues, and the numbers ωopt and λmax for SOR.",computations
"geometry, because the lines that give the cost function (or the planes, when we get to",linear_prog
their potentials. They wouldn’t even have to play the game! There would be complete,vector spaces
Make a Fibonacci guess for S4 and verify that you are right.,determinants
Section 1.5 will discuss row exchanges when the system is not singular. Then the,gauss elim
"tion is zero at x = (0,...,0), and its ﬁrst derivatives are zero. The tangent is ﬂat; this is",pos_def_matrices
"23. Show that the best least-squares ﬁt to a set of measurements y1,...,ym by a horizontal",orthogonality
"43. Suppose (x,y,z) is a linear combination of (2,3,1) and (1,2,3). What determinant",determinants
A. The entry in the ith row and jth column is always denoted by aij. The ﬁrst subscript,gauss elim
direction and contributes nothing new.,vector spaces
In mathematics the formula A = QΛQT is known as the spectral theorem. If we,eigenvec_val
"the pivot columns 1 and 3 are independent. No set of three columns is independent, and",vector spaces
"2. Projection Figure 2.10 also shows the projection of (1,0) onto the θ-line. The",vector spaces
Example 1. The uk approach the eigenvector,computations
2x2+xy+y2−3y and P2 = 1,pos_def_matrices
and certainly not of those for Vice-President—then W has off-diagonal terms. The best,orthogonality
"The matrices E for step 1, F for step 2, and G for step 3 were introduced in the",gauss elim
"(b) T(v1,v2) = (v1,v2,v1 +v2)",vector spaces
for the pivot variables. This x is a special solution.,vector spaces
"24. Which three matrices E21, E31, E32 put A into triangular form U?",gauss elim
b1 −b2 +b3 = 0,vector spaces
(c) Show by example that the answer det(AD−CB) is also wrong.,determinants
ered — or is that already built in?,vector spaces
ces are AB and BA? “Northwest” and “southeast” mean zeros below and above the,gauss elim
"(c) all of A, B, and A+B are invertible.",gauss elim
to be somewhere. That brings us back to the two fundamental properties of a Markov,eigenvec_val
"number of units, The vector p represents prices instead of production levels.",eigenvec_val
4. What is the smallest subspace of 3 by 3 matrices that contains all symmetric matrices,vector spaces
"50. (Recommended) Suppose all vectors x in the unit square 0 ≤ x1 ≤ 1, 0 ≤ x2 ≤ 1 are",vector spaces
"powers of w—all the nth roots of 1—is zero. Algebraically, the sum 1+w+···+wn−1",eigenvec_val
"reached from the source by additional ﬂow, without exceeding any capacities. Those",linear_prog
"38. (a) How do M and N in Problem 37 yield the matrix that transforms (a,c) to (r,t)",vector spaces
(iii) Each row of AB is the product of a row and a matrix:,gauss elim
"because P is symmetric, its row and column spaces are the same.",orthogonality
"To summarize, the three topics basic to this section are:",orthogonality
"two lines, write the two equations for the x1 and x2 that minimize ∥x1a1 −x2a2 −b∥.",orthogonality
. Give an example in which the column space gets larger and,vector spaces
Notice te5t in the ﬁrst component u1(t).,eigenvec_val
A hypercube computer has parallel processors at the corners with connections along,determinants
"bility also includes x ≥ 0 and y ≥ 0, we can take inner products without spoiling those",linear_prog
The last formulas are like p = A�x and P = A(ATA)−1AT. When the columns are orthonor-,orthogonality
3.3 Projections and Least Squares,orthogonality
"4v, what does the ratio v/w approach",eigenvec_val
the Schwarz inequality is the same as |cosθ| ≤ 1. In some ways that is a more easily,orthogonality
Suppose p0 is a vector of prices. Then Ap0 multiplies prices by amounts to give the,eigenvec_val
algorithms and hardware are both much faster now.,pos_def_matrices
6. Write down the 2 by 2 matrices A and B that have entries aij = i+ j and bij = (−1)i+j.,gauss elim
"reﬂected separately. The same could be done for projections: split, project separately,",vector spaces
(onto the plane of q1 and q2).,orthogonality
24. Diagonalize B and compute SΛkS−1 to prove this formula for Bk:,eigenvec_val
and not a similarity transformation with S−1. The main point is easy to summarize: As,pos_def_matrices
"42. Suppose (x,y,z), (1,1,0), and (1,2,1) lie on a plane through the origin. What deter-",determinants
(b) Compute the products AB and BA and A2.,gauss elim
We count the free variables for the nullspace. We count the pivot variables for the column,vector spaces
they are on the unit circle if UHU = I. The eigenvectors can be scaled to unit length and,eigenvec_val
(a) Which three cofactors of L are zero? Then L−1 is lower triangular.,determinants
from the normal equations ATA�x = ATb. If A has dependent columns then ATA is not,pos_def_matrices
"c = (c0,c1,c2,c3) and x = (x0,x1,x2,x3).",eigenvec_val
a+ib = r(cosθ +isinθ) = reiθ.,eigenvec_val
"food, and the ith row of Ax ≥ b forces the diet to include at least bi of that vitamin. If ci",linear_prog
"pivots, so u and w are the pivot variables. The other group is made up of the free",vector spaces
of A; it normally changes the eigenvalues.,eigenvec_val
"the arguments apply to larger matrices. Here we give one more example, and then put",gauss elim
u + 1.0001v = 2,computations
"1. True or false: If m = n, then the row space of A equals the column space. If m < n,",vector spaces
"those new coordinates of (1,0) related to M or M−1?",vector spaces
"matrix, and each exchange reverses the sign of the determinant:",determinants
"reﬂects every point (x,y) into (y,x), its mirror image across the 45° line. Geometrically,",orthogonality
stability of differential equations,eigenvec_val
matrix A is 2 by 2. The right side requires integration of the hat function times f(x) = 2.,pos_def_matrices
Notice our emphasis on the word space. A subspace is a subset that is “closed” under,vector spaces
"N(A) = (C(AT))⊥. At the same time, the row space contains all vectors that are orthog-",orthogonality
solved in one pass.,linear_prog
53. (a) The row vector xT times A times the column y produces what number?,gauss elim
5.2 Find the determinants of A and A−1 if,eigenvec_val
1. Consider the system Ax = b given by,pos_def_matrices
and maximum will be equal. We have to show that this y∗ satisﬁes the dual constraints,linear_prog
"cost is −∞ (this doesn’t happen here). Our example will go from the corner P to Q, and",linear_prog
5.17 (a) Find the eigenvalues and eigenvectors of A =,eigenvec_val
"work. To subtract a multiple ℓ of equation j from equation i, put the number −ℓ into",gauss elim
dx = Ax−b = 0.,pos_def_matrices
"Always detS = detD because L lies below the diagonal, and detT = det(1 − ω)D be-",computations
2∥x∥2 for all x on the constraint line 2x1 −x2 = 5.,pos_def_matrices
be square and invertible.,vector spaces
19. Why are these statements false?,orthogonality
axes of the ellipse.,pos_def_matrices
"empty, or one is empty and the other problem is unbounded (the maximum is +∞ or the",linear_prog
3.5 The Fast Fourier Transform,orthogonality
"At the same time the columns of A become the rows of AT, If A is an m by n matrix, then",gauss elim
programming than determinants or eigenvalues.,linear_prog
(b) Explain why C +D�t = �b comes from the ﬁrst equation in ATA�x = ATb.,orthogonality
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
divide by pivots →,gauss elim
4.2 Properties of the Determinant,determinants
3. The smallest ratio 9,linear_prog
left-hand side and T = −L −U on the right-hand side. Gauss-Seidel chose S = D + L,computations
"equations and by e0t = 1 in differential equations, and it doesn’t move.",eigenvec_val
�� times A =,gauss elim
We must expect a violent change in the solution from ordinary changes in the data.,computations
Each loop produces a vector y in the left nullspace. The component +1 or −1 indicates,vector spaces
Triangular Factors and Row Exchanges,gauss elim
(b) Every basis for R6 can be reduced to a basis for S by removing one vector.,vector spaces
"(b) if Ax = 0 and ATy = b, then xTb = 0.",orthogonality
"2 is positive. Find its matrix A, factor it",pos_def_matrices
"1. If e = (1,1,1), verify that MTe = e. By Problem 11, λ = 1 is also an eigenvalue",eigenvec_val
Here is the most important unitary matrix by far.,eigenvec_val
(b−a)(c−a)(c−b). Then V4 = (x−a)(x−b)(x−c)V3.,determinants
"is the component of b in the column space, and the error e = b − Pb is the component",orthogonality
column comes from the basis vector that is projected to zero.,vector spaces
4x2 +12xy+cy2? Describe this graph at the borderline value of c.,pos_def_matrices
3 0 6 9,vector spaces
"could be billions of corners, and we cannot compute them all. Instead we turn to the",linear_prog
U? Multiply by E−1,gauss elim
of A (not of U!):,vector spaces
continuous Markov process; λ = 0 in a differential equation corresponds to λ = 1 in,eigenvec_val
u+w+z = 4 and u+w = 2 (all in four-dimensional space). Is it a line or a point or,gauss elim
"Those conditions would come from elimination, but here they have a meaning on the",vector spaces
problem automatically brings in orthogonality.,orthogonality
I +At + (At)2,eigenvec_val
of the Fn have been published in the Fibonacci Quarterly. Apparently Fibonacci brought Arabic numerals into,eigenvec_val
30. Without multiplying A =,pos_def_matrices
", ﬁnd the powers Ak (including A0) and show explicitly that their sum",eigenvec_val
Books on numerical linear algebra give more information about this remarkable algo-,computations
"are linearly dependent, since the second column is three times the ﬁrst. The combination",vector spaces
Every important quantity appears in the fully reduced tableau R. We can decide whether,linear_prog
"(c) If the third equation contains 0u and 0v, then no multiple of equation 1 or equa-",gauss elim
They are the columns of the Fourier matrix! Its eigenvalues must have absolute value 1.,eigenvec_val
"The limiting vector u∞ is now a multiple of the other eigenvector (1,1).",computations
unstable and eAt is unbounded if any eigenvalue has Reλi > 0.,eigenvec_val
"To multiply A times x in n dimensions, we need a notation for the individual entries in",gauss elim
"ﬁnd the essential information inside the 1000 by 1000 matrix, and send only that.",pos_def_matrices
pivots are computed in 2n steps instead of 1,pos_def_matrices
Vector Spaces and Subspaces,vector spaces
"other symmetric matrices, any negative eigenvalues in Λ become positive in Σ. For",pos_def_matrices
This triangular form will show that any symmetric or Hermitian matrix—whether its,eigenvec_val
"For block elimination the pivot is C−1, the multiplier is ATC, and subtraction knocks out",vector spaces
4.4 Applications of Determinants,determinants
"giving the densities of bone and tissue at each point. Mathematically, the problem",orthogonality
Problems 27–31 introduce basic ideas of statistics—the foundation for least,orthogonality
"L’s with unit diagonal), then L1 = L2 and U1 = U2. The LU factorization is",gauss elim
"related to Fk? The law Fk+2 = Fk+1 +Fk is still in force, so F−1 = 1.",eigenvec_val
"i x j, without the Σ. Not being",gauss elim
Cn (n complex components),eigenvec_val
�� → L−1A =,gauss elim
left-hand side is qT,orthogonality
T 2 = 4D,eigenvec_val
"is positive semideﬁnite, and",pos_def_matrices
is continuous in time but discrete in space; the unknowns are v(t) and w(t) in the two,eigenvec_val
Example 2 (Leontief’s input-output matrix).,eigenvec_val
P2b is the projection of Pb—and Pb is already on the line! So P2b = Pb. This matrix P,orthogonality
"4, to solve −u′′ = 2 with u(0) = u(1) = 0. Verify",pos_def_matrices
This is the place to recognize one extremely important theorem. Suppose a matrix has,vector spaces
case these eigenvalues are 2−,computations
of A and B—are the same. Here are matrices B similar to A.,eigenvec_val
"17. If V is the orthogonal complement of W in Rn, is there a matrix with row space V",orthogonality
Chapter 7 Computations with Matrices,computations
has λ1 = 2 and the economy is lost,eigenvec_val
32. Write these ancient problems in a 2 by 2 matrix form Ax = b and solve them:,gauss elim
"21. (Remarkable) If A and B are square matrices, show that I −BA is invertible if I −AB",gauss elim
xn = xn−1 +αnpn−1,computations
1] and B = [2,gauss elim
24. Find cofactors and then transpose. Multiply CT,determinants
"24. (a) Show by direct multiplication that every triangular matrix T, say 3 by 3, satisﬁes",eigenvec_val
A can be safely diagonalized. Nevertheless it is true (see Section 5.6) that even with,eigenvec_val
"61. Wires go between Boston, Chicago, and Seattle. Those cities are at voltages xB, xC,",gauss elim
"There was an exercise in Chapter 1, about moving in and out of California, that is worth",eigenvec_val
"C and C(1) = Q. Then the eigenvalues of C(t)TAC(t) will change gradually, as t goes",pos_def_matrices
the column space will satisfy b1 +b2 −b3 = 0. Derive the same thing from the three,vector spaces
"direction. Similarly, ﬁx x = 0 and look in the y direction where f(0,y) = cy2:",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"by (1,1,1), what is S⊥? If S is spanned by (2,0,0) and (0,0,3), what is S⊥?",orthogonality
"free hand with the vectors in the basis (they are independent). When those are settled,",vector spaces
"If A is m by n, and B is n by p, then multiplication is possible. The product AB will",gauss elim
"The method is simple. We are given a, b, c and we want q1, q2, q3. There is no",orthogonality
"4.2 If B = M−1AM, why is detB = detA? Show also that detA−1B = 1.",determinants
Figure 3.10: The qi component of b is removed; a and B normalized to q1 and q2.,orthogonality
useful test for theoretical purposes. Each test is enough by itself.,pos_def_matrices
� (v′(x))2dx is positive like xTAx.,pos_def_matrices
"Write these factors with 1, w, w2 in D and 1, w2, w4 in F3. Multiply!",orthogonality
(b) What matrix adds row 1 to row 3 and at the same time adds row 3 to row 1?,gauss elim
We note that many authors transpose the matrix so its rows add to 1.,eigenvec_val
gradually appear on the main diagonal.,computations
"We do the same for integration. That goes from cubics to quartics, transforming",vector spaces
Problems 17–26 ask for projections onto lines. Also errors e = b− p and matri-,orthogonality
smaller mass goes much further.,pos_def_matrices
Do the multiplication SΛkS−1�1,eigenvec_val
(Note that i is not included; the imaginary part is a real number.) Thus w2 = cos2θ +,orthogonality
"How far is the plane x1 + x2 − x3 − x4 = 8 from the origin, and what point on it is",orthogonality
∂y = 4x+2y = 0.,pos_def_matrices
3.5 The Fast Fourier Transform,orthogonality
how can you tell in advance that A is symmetric? What are its trace and determi-,eigenvec_val
Triangular Forms with a Unitary M,eigenvec_val
Chapter 2 Vector Spaces,vector spaces
"The new matrix in the equation is M−1AM. In the special case M = S, the system is",eigenvec_val
(a) If the third equation starts with a zero coefﬁcient (it begins with 0u) then no,gauss elim
"as yTA = 0, those components multiply the rows of A to produce the zero row:",vector spaces
"is n+1 (because with the constant term, there are n+1 coefﬁcients).",vector spaces
when there are several equations and only one unknown:,orthogonality
times the ﬁnal equation 2.,gauss elim
of Ux. The energy in state space equals the energy in transform space. The energy is,eigenvec_val
"formula does is to show how A−1 depends on the n2 entries of A, and how it varies when",determinants
The recursion |Bn| = 2|Bn−1|−|Bn−2| is the same as for the A’s. The difference is in,determinants
-dimensional space. The equations have no,gauss elim
(b) Show why RTR has no negative numbers on its diagonal.,gauss elim
"which means that A equals AH. If A is an m by n matrix, then AH is n by m:",eigenvec_val
"Thus the three steps are: split c into c′ and c′′, transform them by Fm into y′",orthogonality
The valuable thing for linear algebra is that the extension to n dimensions is so,vector spaces
det(B−λI) = detM−1det(A−λI)detM = det(A−λI).,eigenvec_val
(d) Every row is orthogonal to every column (A is not the zero matrix).,orthogonality
"(d) If you are given any three vectors in R6 and any three vectors in R5, is there a",vector spaces
"12. Suppose the matrix A is ﬁxed, except that a11 varies from −∞ to +∞. Give examples",determinants
2. Off the diagonal the c’s appear with minus signs. The edges to the grounded node,vector spaces
to U (and also takes b to c). It is lower triangular (zeros are omitted):,gauss elim
50. A matrix with orthonormal eigenvectors has the form A = UΛU−1 = UΛUH. Prove,eigenvec_val
"Algebraically, we can diagonalize the symmetric A by an orthogonal matrix: QTAQ =",pos_def_matrices
"vector (1,1,−1) and automatically contains any multiple (c,c,−c):",vector spaces
or genius it is an astonishing success. The steps of the simplex method are summarized,linear_prog
"source f1 must balance −y1−y2, which is the ﬂow leaving node 1 (along edges 1 and 2).",vector spaces
"timality conditions, the derivative of lnxi gives 1/xi. If we create a diagonal matrix X",linear_prog
∥x∥2 = max xTATAx,computations
"Since the left-hand sides are identical, we have weak duality yb ≤ cx.",linear_prog
(e) A defective matrix (nondiagonalizable).,eigenvec_val
"18. For equation (16) in the text, with ω = 1 and",eigenvec_val
that Buniakowsky’s claim is genuine.,orthogonality
"reﬂection, the order makes a difference.",vector spaces
Figure 3.5: The projection p is the point (on the line through a) closest to b.,orthogonality
"2, −1 matrices A of order 10, 20, 50, with b = (1,0,...,0).",computations
2x − y =,gauss elim
(2) BA = 2B for every A.,gauss elim
is so large and the coding is so awkward that the new method is largely (or entirely) of,gauss elim
tation �P with �P4 ̸= I.,gauss elim
x+ = Vy+ = VΣ+UTb = A+b.,pos_def_matrices
meet but don’t cross. This happens for the last 2 by 2 matrix below.,eigenvec_val
"a perpendicular projection. The determinant is the volume of a box. Now, for a positive",pos_def_matrices
Show that zTAz = λ1a2,pos_def_matrices
the set of lower triangular matrices. Another is the set of symmetric matrices. A + B,vector spaces
these choices. The number of basis vectors is a property of the space itself:,vector spaces
but the E’s are different.,gauss elim
"diagonals add to 15. The ﬁrst row could be 8, 3, 4. What is M times (1,1,1)? What",gauss elim
eventually comes back to its original place.,determinants
Suppose the most negative reduced cost is ri. Then the ith component of xN is the,linear_prog
in the row space of A. Remember that any vector �x can be split into a row space compo-,pos_def_matrices
"norms and condition numbers. Because b = Ax and δx = A−1δb, equation (6) gives",computations
unavoidable even by a perfect mathematician or a perfect computer. A blunder is much,computations
"There are many possibilities for a basis, but we propose a speciﬁc choice: The columns",vector spaces
"x = 0, w = b, and its optimal vector. Find the corner of the feasible set x1−x2 = 3,",linear_prog
"Any combination c1x1 +c2x2 has c1 as its v component, and c2 as its y component. The",vector spaces
1+i2 is zero. Then w8 = i4 = 1. There has to be a system here.,orthogonality
ﬁrst column below this pivot. The bad news appears in column 2:,vector spaces
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Remark 2. A is the limit of symmetric matrices with distinct eigenvalues. As the limit,eigenvec_val
The conditions a > 0 and ac > b2 are just right. They guarantee,pos_def_matrices
are an orthogonal basis for,eigenvec_val
Example 4. Suppose A = I. Then CTAC = CTC is positive deﬁnite. Both I and CTC,pos_def_matrices
orthogonalization applied to the Krylov matrix: KN = QNRN. The eigenvalues of,computations
Multiplication of a Matrix and a Vector,gauss elim
"(a) Explain why eA(t+T) = eAteAT, using the formula eAt = SeΛtS−1.",eigenvec_val
"For Problems 21–26, consult the accompanying ﬁgures.",orthogonality
"In short, we have a matrix formula for splitting any b into two perpendicular compo-",orthogonality
"42. Suppose v1, v2, v3 are eigenvectors for T. This means T(vi) = λivi for i = 1,2,3.",vector spaces
"Heisenberg’s uncertainty principle comes from noncommuting matrices, like posi-",eigenvec_val
"14. Multinational companies in the Americas, Asia, and Europe have assets of $4 trillion.",eigenvec_val
"we try to write b as a combination of the basis vectors for S, it cannot be done—the",orthogonality
"invertible, the factorization is unique.",gauss elim
"4. Suppose the cost function in Example 3 is x − y, so that after rearrangement c =",linear_prog
21 to factor A into LU = E−1,gauss elim
with third plane = solution,gauss elim
A−a j jI has a,pos_def_matrices
Property 1′ (Ux)H(Uy) = xHUHUy = xHy and lengths are preserved by U:,eigenvec_val
"metric. In matrix language, PA loses the symmetry of A but",gauss elim
ﬁes T(cv) = cT(v) but not T(v+w) = T(v)+T(w).,vector spaces
Check that λ1 +λ2 +λ3 equals the trace and λ1λ2λ3 equals the determinant.,eigenvec_val
"Thus, if N is normal, the triangular T =U−1NU must be diagonal. Since T has the same",eigenvec_val
basis matrix M is a,vector spaces
"34. When the edge vectors a, b, c are perpendicular, the volume of the box is ∥a∥ times",determinants
eigenvectors” and factors like teλt. (To compute this defective case we can use the Jor-,eigenvec_val
Our goal is to read off all the solutions to Rx = 0. The pivots are crucial:,vector spaces
0 0 ∗ ∗,computations
0 f(t)dt has no eigenvalues (here −∞ < x < ∞).,eigenvec_val
cx = 0 goes through the origin. The planes cx = constant give all possible costs. As,linear_prog
"product. Replacing x by x, the inner product becomes",eigenvec_val
(b) Prove that the determinant of any Hermitian matrix is real.,eigenvec_val
y2 = 0 give the same value 6y1 +7y2 = 3. These vectors must be optimal.,linear_prog
we do next—to ﬁnd the inverse matrix A−1 and the solution x = A−1b.,gauss elim
2. What can you say about,eigenvec_val
"y and b is greater than 90°, so yb < 0. The angle between y and every column of A is",linear_prog
"(which means they are linearized). The nonlinear term is s = θeX−1. To avoid 1/xi,",linear_prog
j = 0. This leaves us with y∗b = cx∗ in equation (3). This equality,linear_prog
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"Solve for x if a1 = (1,1,0), a2 = (0,1,0), b = (2,1,4).",orthogonality
"Its entries are −1 or +1 or (mostly) zero, and pivot steps involve only additions and",linear_prog
just choose wi = T(vi). Why must T be invertible?,vector spaces
But the eigenvalues of D are just its diagonal entries (the pivots). Thus the number of,pos_def_matrices
dt2 +2v−w = 0,pos_def_matrices
"5. If A and B are positive deﬁnite, then A+B is positive deﬁnite. Pivots and eigenvalues",pos_def_matrices
"(c) the entry in row 3, column 4 of AB?",gauss elim
column are sure to produce 9 as the fourth pivot?,gauss elim
"For these special “tridiagonal matrices,” the operation count drops from n2 to 2n. You",gauss elim
"for that space. If the vectors are the columns of an m by n matrix,",vector spaces
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
(What is the rank of A?),determinants
"3. By computing B = LU, and updating these LU factors instead of B−1.",linear_prog
32 . Find L and U:,gauss elim
series gives the coordinates of the “vector” f (x) with respect to a set of (inﬁnitely many),orthogonality
2x1 +3x2 = 1,determinants
"If the components of x are v and w, the Jacobi step Sxk+1 = Txk +b is",computations
The rank counts the number of genuinely independent rows in the matrix A. We want,vector spaces
x j = detB j,determinants
2 0 0 1,gauss elim
"5. Let Fn be the determinant of the 1, 1, −1 tridiagonal matrix (n by n):",determinants
A rectangular matrix cannot have both existence and uniqueness. If m is different from,vector spaces
Figure 1.4: The column picture: linear combination of columns equals b.,gauss elim
has λ1 = 1,eigenvec_val
they are unit vectors—each has length ∥ei∥ = 1. They point along the coordinate axes. If,orthogonality
for λ3 = −1.,eigenvec_val
space comes from one and only one vector xr in the row space. All we are doing is to,pos_def_matrices
"The difference across edge 1 is x2 −x1, from the ±1 in the ﬁrst row.",vector spaces
of the corner entries.,pos_def_matrices
"constraints x ≥ 0, y ≥ 0. Then we have only equations and simple nonnegativity con-",linear_prog
"Those are the 3! = 6 permutations of (1,2,3); the ﬁrst one is the identity.",determinants
orthogonal subspaces are necessarily orthogonal complements:,orthogonality
force the simplex method to try every corner—at exponential cost.,linear_prog
There is only one way of choosing four nonzero entries from different rows and,determinants
Application of the SVD,pos_def_matrices
−1 −3 3 4,vector spaces
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
(d) what subspace is spanned by the invertible matrices?,vector spaces
(c) A2 = A.,gauss elim
ﬁrms the geometry of the previous section. The error vector e connecting b to p must be,orthogonality
remarkable combination (A−1)T = (AT)−1.,orthogonality
3) and q2 = (−1,orthogonality
30. If A = QR then ATA = RTR =,orthogonality
are diagonalized by F. So they reduce to two FFTs and a diagonal matrix.,orthogonality
"The product U = U1U2U3 is still a unitary matrix, and U−1AU = T.",eigenvec_val
0 1 2 2,determinants
the corner is optimal by looking at r = cN −cBB−1N in the middle of the bottom row. If,linear_prog
"exchanging rows, since eventually the small pivot would catch up with us. But what",determinants
Analysis of the Finite Element Method (see www.wellesleycambridge.com) written,pos_def_matrices
"n, some cut must be responsible.",linear_prog
"matrix is replaced by u, column k of B−1 is replaced by v = B−1u. To recover the identity",linear_prog
times row 1 from row 2. The reverse step adds ℓ21,gauss elim
is in R3. The nullspace is a plane in R3 and the left nullspace is a line in R2:,vector spaces
hard in m-dimensional space if m > n. We assume it in what follows.,orthogonality
"n, we cannot have r = m and r = n.",vector spaces
"i go to zero, and",eigenvec_val
"irreversible, and the heat equation cannot run backward.)",eigenvec_val
The pivots di are not to be confused with the eigenvalues. For a typical positive,pos_def_matrices
"of ﬁnite differences that jumped ahead, because it is easy to “discretize” a differential",pos_def_matrices
(this is really mathematical induction) we end up with a multiple of x1 that produces,eigenvec_val
"5. Show that for any two different vectors of the same length, ∥x∥ = ∥y∥, the House-",computations
next section does better by ﬁnding a physical meaning for y from Kirchhoff’s Current,vector spaces
the equation Ax = b is the same as Sx = Tx+b. Therefore we can try,computations
lambda because of the small lambdas for the eigenvalues on its diagonal.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
", compute ∥A−1∥ = 1/λ1, ∥A∥ = λ2, and c(A) =",computations
and the row space has dimension r. That makes it easy to deal with the original matrix,vector spaces
0 3 0 0,gauss elim
"(c) Solve uk+1 = Puk, starting from u0 = (9,9,0).",eigenvec_val
"The result is the identity matrix U = I. The inverses of E, F, and G will bring back A:",gauss elim
"by c. Similarly the projection of (0,1) has length s, and falls at s(c,s) = (cs,s2), That",vector spaces
"If the outputs are s1, f1, ℓ1, then the required inputs are",eigenvec_val
"The nullspace is also called the kernel of A, and its dimension n−r is the nullity.",vector spaces
multiplication as columns times rows:,pos_def_matrices
3 by 2 system,gauss elim
solution for an m by n system. Then that circle of ideas will be complete.,vector spaces
tion to an overdetermined system. The vector b represents the data from experiments,orthogonality
up to 1. Nobody is gained or lost.,eigenvec_val
"on differential equations, you would substitute y = eλt into y′′′ −3y′′ +2y′ = 0:",eigenvec_val
the original one because Q−1,computations
"Multiply the second equation by L to give LUx = Lc, which is Ax = b. Each triangular",gauss elim
is clear: A cannot have a whole column of zeros. The inverse could never multiply a,gauss elim
Suppose A is singular (rank 1). Its column space is a line. The vector Ax has to stay on,eigenvec_val
"later, and ﬁrst we try to explain them.",linear_prog
3. Suppose the time direction is reversed to give the matrix −A:,eigenvec_val
n. The pair aij = a ji combines into,pos_def_matrices
Nullspace is a line,vector spaces
its success depends on the following identity Hx = −σz:,computations
"1.13 Use elimination to ﬁnd the triangular factors in A = LU, if",vector spaces
∂y2 = 4+ysiny−2cosy = 2,pos_def_matrices
ΛQT is its symmetric pos-,pos_def_matrices
(b) Find a basis for the orthogonal complement S⊥.,orthogonality
"is p = (b1,b2,0). The best we can do with Ax = b is to solve the ﬁrst two equations,",pos_def_matrices
"7. For which numbers a does elimination break down (a) permanently, and (b) tem-",gauss elim
(b) Find the other eigenvalues of A (it is Markov).,eigenvec_val
36. Diagonalize this orthogonal matrix to reach Q = UΛUH. Now all λ’s are,eigenvec_val
41. Suppose V is the whole space R4. Then V⊥ contains only the vector,orthogonality
when the vectors and matrices are real. We have listed these changes in a table at the,eigenvec_val
entering variable will be x5.,linear_prog
a time. In fact orthogonal subspaces are unavoidable: They are the fundamental sub-,orthogonality
Chapter 7 Computations with Matrices,computations
which Ax = b has no solution.,vector spaces
from the center. In the original x-space they point along the eigenvectors of A.,pos_def_matrices
so they also produce linear transformations.,vector spaces
70. I = eye(1000); A = rand(1000); B = triu(A); produces a random triangular matrix,gauss elim
be m by p. We now ﬁnd the entry in row i and column j of AB.,gauss elim
Back-substitution in Ux = 0,vector spaces
"Even though there are four vectors, their combinations only ﬁll out a plane in three-",vector spaces
ampliﬁed by the eigenvalues λi. By combining these special solutions to match u0—that,eigenvec_val
"n independent eigenvectors, we do have A = B. Find A ̸= B when λ = 0,0 (repeated),",eigenvec_val
or matrix powers or eigenvalues—just as elimination steps were natural for Ax = b.,eigenvec_val
2 4 6 4,vector spaces
frequency ω1 = 1 is the same as for a single spring and a single mass. In the faster mode,eigenvec_val
eigenvectors are in the orthogonal matrix Q and its eigenvalues are in Λ.,eigenvec_val
"36. If A is a 64 by 17 matrix of rank 11, how many independent vectors satisfy Ax = 0?",vector spaces
. This vector satisﬁes Ax = 0.,vector spaces
Here is a light-hearted question about permutations. How many exchanges does it,determinants
"Our goals are to deﬁne the inverse matrix and compute it and use it, when A−1",gauss elim
Chapter 7 Computations with Matrices,computations
(b) Find the limit as n → ∞ of the matrices An = SΛnS−1.,eigenvec_val
49. Find AT and A−1 and (A−1)T and (AT)−1 for,gauss elim
14. If A =,linear_prog
Ak = (MJM−1)(MJM−1)···(MJM−1) = MJkM−1.,eigenvec_val
0. In this case b projects to the zero vector:,orthogonality
"origin. Like all the other possible choices, this intersection point will only be a genuine",linear_prog
(a) write out u′,eigenvec_val
. The rows of A with coefﬁ-,vector spaces
"poses. On one side, IT = I. On the other side, we know from part (i) the transpose of a",gauss elim
dead hut is suddenly very much alive (Problem 33 gives the steps). It is direct rather,computations
bring a tremendous simpliﬁcation to Gaussian elimination.,gauss elim
"space, if A is n by n. That demands a closer look.",vector spaces
30. Find L and U for the nonsymmetric matrix,gauss elim
"In mechanics, x and y become displacements and stresses. In ﬂuids, the unknowns",vector spaces
The least squares solution to WAx = Wb is �xW:,orthogonality
"everyone married? If linear algebra can work in 20-dimensional space, it can certainly",linear_prog
2. By updating B−1 when column u taken from N replaces column k of B.,linear_prog
"On the left-hand side they reduce L to I, as in Example 4. (The ﬁrst step subtracts ℓ21",gauss elim
Example 5. U = 1,eigenvec_val
That new corner is feasible because we still have x ≥ 0. It is basic because we again,linear_prog
60. Factor these symmetric matrices into A = LDLT. The matrix D is diagonal:,gauss elim
"42. If Ax = b has inﬁnitely many solutions, why is it impossible for Ax = B (new right-",vector spaces
24. Use row operations to simplify and compute these determinants:,determinants
Figure 1.8: A band matrix A and its factors L and U.,gauss elim
to all the rows. The nullspace and row space are perpendicular lines in R2:,orthogonality
all |λ| = 1,eigenvec_val
"version was 50 times faster than the simplex method. (His algorithm, outlined in 8.2,",linear_prog
determined the pivot variables in the special solutions (the columns of N).,vector spaces
"ments contain concentrations v(0) and w(0) of a chemical. At each time t, the diffusion",eigenvec_val
23. Construct an unsymmetric 2 by 2 matrix of rank 1. Copy Figure 3.4 and put one,orthogonality
numbers cycle around (with what period?) ﬁnd D1000.,determinants
14. Find a left-inverse and/or a right-inverse (when they exist) for,vector spaces
too. Show that the second solution is y = te3t.,eigenvec_val
eigenvalues and eigenvectors of these three matrices.,eigenvec_val
"in the feasible set, and is a complete fake. Our example with n = 2 variables and m = 2",linear_prog
dimensions a line requires two equations; in n dimensions it will require n − 1.,gauss elim
unitary UHU = I or UH = U−1,eigenvec_val
"components in the directions q1,...,q j−1 that are already settled:",orthogonality
∆b produces a large change ∆x.,computations
what are those lengths and the maximum determinant?,determinants
inner segments S1 and S2.,eigenvec_val
and ﬁnd the “generalized eigenvalue problem” that must be solved for the frequency,eigenvec_val
Example 2. The solution of,determinants
0 0 1 0,determinants
"20. Find the eigenvalues of the “periodic” −1, 2, −1 matrix C. The −1s in the corners",orthogonality
"14. The vectors a1 = (1,1,0) and a2 = (1,1,1) span a plane in R3. Find the projection",orthogonality
"Then the condition number, and the sensitivity of A will follow from multiplying the",computations
1 2 0 1,vector spaces
"D2, U1 = U2. The LDU factorization and the LU factorization are uniquely",gauss elim
(a+c)+i(b+d) = (a+c)−i(b+d) = (a+ib)+(c+id).,eigenvec_val
This section has two goals. The ﬁrst is to explain one way in which large linear systems,gauss elim
4x + 8y =,gauss elim
And the last equation in the iteration step will use new values exclusively:,computations
eAt = I +SΛS−1t + SΛ2S−1t2,eigenvec_val
"(c) P exchanges rows 1 and 2, then rows 2 and 3.",gauss elim
2.4 The Four Fundamental Subspaces,vector spaces
40. What rows or columns or matrices do you multiply to ﬁnd,gauss elim
"Equations (10) and (11) are the same! The same three exponents appear: λ = 0, λ = 1,",eigenvec_val
identical with the matrix-vector product Ax. More than that: When B contains several,gauss elim
"look at their combinations c1v1 + c2v2 + ··· + ckvk. The trivial combination, with all",vector spaces
"to the 1s in the matrix, and the capacities are 1 marriage. There is no edge between the",linear_prog
(b) P23 exchanges rows 2 and 3 and then E31 subtracts row I from row 3. What,gauss elim
"3. In Example 3, suppose the cost is 3x + y. With rearrangement, the cost vector is",linear_prog
"n marriages. When a complete matching is impossible, and the maximal ﬂow is below",linear_prog
PC/min = Pmin + 1,pos_def_matrices
"is positive deﬁnite, test A−1 = [ p q",pos_def_matrices
"That is a quick proof of symmetry for RTR. Its i, j entry is the inner product of row i",gauss elim
39. Challenge problem: Is there a real 2 by 2 matrix (other than I) with A3 = I? Its,eigenvec_val
The norm of A measures the largest amount by which any vector (eigenvector or not),computations
"53. True or False? (Give reason if true, or counterexample to show it is false.)",vector spaces
Complex Numbers and Their Conjugates,eigenvec_val
only had A). Row 1 shows the edge from node 1 to node 2. Row 5 comes from the ﬁfth,vector spaces
17. The ﬁrst of these equations plus the second equals the third:,gauss elim
better visualized in matrix form. We choose h = 1,gauss elim
20. Normally 4 “planes” in four-dimensional space meet at a,gauss elim
Chapter 2 Vector Spaces,vector spaces
5.12 Show that every matrix of order > 1 is the sum of two singular matrices.,eigenvec_val
"(d) v = (a,b).",vector spaces
"Its entries are complex numbers. That is a small price to pay, and we pay it below. The",orthogonality
a1? Sketch a ﬁgure.,orthogonality
"All inner products and lengths are preserved, when the space is rotated or reﬂected.",orthogonality
"span could have dimension 1, 2, or 3. Give an example of y1, y2, y3 to show each",vector spaces
"postmultiply by S−1, to deduce falsely that A = 0. There is no invertible S.",eigenvec_val
1.18 Suppose A is the 4 by 4 identity matrix except for a vector v in column 2:,gauss elim
"(b) A2 and B2 can be similar when A and B are not similar (try λ = 0,0).",eigenvec_val
Next we ﬁnd matrices that represent differentiation and integration. First we must,vector spaces
factors). The diagonalization that displays those eigenvalues is the natural choice of,pos_def_matrices
"6n3. Elimination becomes fast, and the search",pos_def_matrices
"1.10 Find inverses if they exist, by inspection or by Gauss-Jordan:",gauss elim
"ﬁrst to see how a, b, c, and d are related. The question will lead to:",gauss elim
special solution? What is the nullspace?,vector spaces
for eigenvalues (by halving the intervals) becomes simple. The current favorite is the,pos_def_matrices
hand sides that allows the three lines to intersect at the same point?,gauss elim
λ1 and λ2 = .7 :,eigenvec_val
Problems 1–10 are about linear independence and linear dependence.,vector spaces
(a) The complete solution is any linear combination of xp and xn.,vector spaces
"The columns of S are the eigenvectors of A. Writing S−1u0 = c, the solution",eigenvec_val
"5. Which pairs are orthogonal among the vectors v1, v2, v3, v4?",orthogonality
"possible to see why it has properties 1–3. For A = I, every product of the aij will be",determinants
27. (Important) A is an m by n matrix of rank r. Suppose there are right-hand sides b for,vector spaces
"except for very special starting conditions the solution will blow up. Furthermore, the",eigenvec_val
Ax = b does not have a unique solution (if it has a solution at all). Any “constant vector”,vector spaces
"It is convenient to work with a symmetrically placed interval like −1 ≤ x ≤ 1, because",orthogonality
2E Suppose c1v1 +···+ckvk = 0 only happens when c1 = ··· = ck = 0. Then,vector spaces
(λ +ω −1)2 = λω2µ2.,computations
"Every column is a multiple of a, and so is Pb = �xa. The vectors that project to p = 0",orthogonality
line of all solutions x = xp + xn,vector spaces
"We are very close to answering an important question, so we keep going: For which",eigenvec_val
the expected value of (error in bi) times (error in b j). Then the main diagonal of C−1,orthogonality
"B, since each row of A multiplies B to give a zero row.)",vector spaces
Chapter 1 Matrices and Gaussian Elimination,gauss elim
and add the projections. These rules apply to any transformation that comes from a,vector spaces
"inite, when a < 0. The preﬁx semi allows the possibility that f can equal zero, as it will",pos_def_matrices
"elimination. (Gauss is recognized as the greatest of all mathematicians, but certainly not",gauss elim
“primitive” nth root of unity is,orthogonality
(a) the ﬁrst pivot.,gauss elim
"It has properties (i) and (ii) in 3N. By choosing u = a/∥a∥, P becomes the projection",orthogonality
"more difﬁcult than Ax = b. With linear systems, a ﬁnite number of elimination steps",eigenvec_val
"At the corner P in Figure 8.3, x = 0 intersects 2x+y = 6. To be organized, we exchange",linear_prog
when the block B (of order 1,pos_def_matrices
"subspace. The line L through z = (0,0,5,−4) is perpendicular to V and W. Then the",orthogonality
"(c) Show directly that if Cx = λx, then C(Dx) = −λ(Dx).",eigenvec_val
agreement that the team with highest potential is the best.,vector spaces
look at all nonzero vectors whose last n−k components are zero:,pos_def_matrices
"above λ2. The highest frequency is decreased, but not below λn−1.",pos_def_matrices
"straightforward. For a vector in R7 we just need the seven components, even if the",vector spaces
"u′(0) = b1ω1x1 +···+bnωnxn. Substituting the a’s and b’s into the formula for u(t), the",eigenvec_val
"(b) With A = [1 1] and b = [3], write out the auxiliary problem, its Phase I vector",linear_prog
"can have any value,) When elimination breaks down, we want to ﬁnd every possible",gauss elim
dt = Au =,eigenvec_val
without row exchanges. The key observation is that the ﬁrst k pivots are completely,determinants
the iteration could not converge.) We also get a clue to the behavior of the eigenvalues:,computations
writing out all matrices in full !). For computational purposes (except for small problems,linear_prog
1 0 0 0,determinants
because b = −10 overwhelms a and c.,pos_def_matrices
worry about repeated eigenvalues and a possible shortage of eigenvectors. All we say,eigenvec_val
"on the material, and stores up elastic energy.",pos_def_matrices
15. What is the quadratic f = ax2 +2bxy+cy2 for each of these matrices? Complete the,pos_def_matrices
is the shortest distance ∥e∥ from b to the column space of A?,orthogonality
(d) If AT is invertible then A is invertible.,gauss elim
"11. If the eigenvalues of A are 1, 1, 2, which of the following are certain to be true? Give",eigenvec_val
"This is the one idea of the whole Gram-Schmidt process, to subtract from every new",orthogonality
39. The functions e−ix and e−ix are orthogonal on the interval 0 ≤ x ≤ 2π because their,eigenvec_val
x + 2y = 3,gauss elim
third equation is the sum of the ﬁrst two. In that case the three planes have a whole line,gauss elim
����� = cb−ad = −,determinants
"the second row is virtually full, whereas near the end it has zeros from the upward row",gauss elim
S−1T controls the convergence of xk+1 = (1−A)xk +b?,computations
to construct accurate difference equations on irregular meshes. The essential thing is the,pos_def_matrices
38. Generally eAeB is different from eBeA. They are both different from eA+B. Check this,eigenvec_val
(b) If U is upper triangular then (U−1)T is,gauss elim
matrices also span Rn!,vector spaces
"3. Build trees from all n nodes, by repeating the following step:",linear_prog
4 0 0 0,gauss elim
(b) A symmetric matrix can’t be similar to a nonsymmetric matrix.,eigenvec_val
"changing the solutions. The system Ax = 0 is reduced to Ux = 0, and this process is",vector spaces
2. the test xTy = 0 for perpendicular vectors; and,orthogonality
30. Multiply [a b,gauss elim
11. Find the minimum values of,pos_def_matrices
u + 1.0001v = 2,gauss elim
"capacity across the minimal cut. The problem, here and in all of duality, is to show that",linear_prog
Least-Squares Fitting of Data,orthogonality
situation becomes impossible if we add a few more axes. It is virtually hopeless to solve,orthogonality
in matrix terms the construction of a perpendicular line from b to the column space of,orthogonality
"disappears when vectors are orthonormal. The projections onto the axes are uncoupled,",orthogonality
Chapter 8 Linear Programming and Game Theory,linear_prog
28. Figure out how to write my′′ +by′ +ky = 0 as a vector equation Mu′ = Au.,eigenvec_val
commutes with B1 =,gauss elim
0 0 0 0,vector spaces
complex inner product is,eigenvec_val
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
Every v comes from w’s,vector spaces
"2. If a 3 by 3 matrix has detA = −1, ﬁnd det(1",determinants
"ATA is 1 by 1, whereas AAT is 3 by 3. They both have eigenvalue 9 (whose square root",pos_def_matrices
of the basic operations; vectors are multiplied by numbers and then added. The result is,gauss elim
ci = 1/Ri (because in elimination you divide by the pivots). We also show the fourth,vector spaces
have the same band structure of three essential diagonals (3n−2 parameters) as A. Note,gauss elim
economy can grow. The eigenvalues of A−1 will govern this growth. But again there is,eigenvec_val
which is Wx = 0. A combination of the w’s gives zero! The w’s could not be a basis—so,vector spaces
The key was in the eigenvalues λ and eigenvectors x. Eigenvalues are important in,eigenvec_val
One key to this chapter is to see the geometric meaning of linear inequalities. An,linear_prog
F1 and the saddle point of F2 (look where ﬁrst derivatives are zero).,pos_def_matrices
"determinant not only has all the properties found earlier, it even exists.",determinants
(Qx)T(Qx) = xTQTQx = xTx.,orthogonality
detA4 = 2(detA3)−detA2 = 2(4)−3 = 5,determinants
"What is L is this case? Comparing with PA = LU in Box 1J, the multipliers now stay",gauss elim
"The eigenvectors are still orthogonal, and we still have K = UΛUH—with a unitary U",eigenvec_val
"keep the error matrix E as small as possible—usually by insisting, as in the next section,",computations
is in the opposite direction: Does every linear transformation lead to a matrix? The,vector spaces
"stage, when k equaled n. Altogether, the total number of operations is the sum of k2 −k",gauss elim
8. If A =,pos_def_matrices
"This is pure linear algebra, based on orthogonality. The input is a sequence of num-",orthogonality
"(d) EF = 0, although no entries of E or F are zero.",gauss elim
�x = b1 +b2,orthogonality
uk = c1λ k,computations
(AH)i j = A ji.,eigenvec_val
"programming, we are interested in the special class for which A is an incidence matrix.",linear_prog
"equation, just as SΛkS−1u0 solved the difference equation:",eigenvec_val
(III) All the upper left submatrices Ak have positive determinants.,pos_def_matrices
"There must be inﬁnitely many solutions, since any multiple cx will also satisfy A(cx) =",vector spaces
(a) any k independent vectors in V form a basis;,vector spaces
"One possibility is to choose the n equations x1 = 0,...,xn = 0, and end up at the",linear_prog
"The interest is added on at every instant, and the difference equation breaks down. You",eigenvec_val
"squares problem with several parameters, and it is solved in Section 3.3. The formulas",orthogonality
We now introduce the space Cn of vectors with n complex components. Addition and,eigenvec_val
multiplications. Compare with 53 = 125 for,determinants
0 0 0 1,orthogonality
32. Solve the triangular system Lc = b to ﬁnd c. Then solve Ux = c to ﬁnd x:,gauss elim
constructing Q is to work with the whole ﬁrst column of A:,computations
matrix on the orthogonal complement (the nullspace of QT).,orthogonality
16. (Turning a robot hand) A robot produces any 3 by 3 rotation A from plane rotations,computations
"The two nullspaces are identical. In particular, if A has independent columns (and only",orthogonality
"17. Find a matrix A that has V as its row space, and a matrix B that has V as its nullspace,",vector spaces
"complicated for real engineering problems, like the stresses on an airplane. The real",pos_def_matrices
The natural generalization will involve all n of the upper left submatrices of A:,pos_def_matrices
(normal equation). The minimizing equation Ax = b changes into the,pos_def_matrices
"A similar reasoning applies to the rows of A, which are also independent. Suppose",vector spaces
Matrix multiplication is not commutative: Usually FE ̸= EF.,gauss elim
"condition is fulﬁlled. If there is an overpricing (y∗A)j < c j, it must be canceled through",linear_prog
the difference as a slack variable w = x + 2y − 4. This is our equation! The old con-,linear_prog
A particular case of this rule gives the determinant of A−1. It must be 1/detA:,determinants
"For another matrix A, the dimensions in Figure 2.1 may be very different. The small-",vector spaces
"problem with q1: it can go in the direction of a. We divide by the length, so that q1 =",orthogonality
from the center (1,orthogonality
= λmax(ATA) = ∥A∥2.,computations
A discrete Markov matrix has its column sums equal to λmax = 1. A continuous,eigenvec_val
xk that drops to zero is the one that gives the minimum ratio in equation (3):,linear_prog
1 1 1 1,eigenvec_val
44. Review: Which of the following are bases for R3?,vector spaces
16. (Recommended) From the zero submatrix decide the signs of the n eigenvalues:,pos_def_matrices
"piece of b has a simple formula, and recombining the pieces gives back b:",orthogonality
It is only a repeated λ that may (or may not!) require an off-diagonal 1 in J.,eigenvec_val
"We will pick a few important applications, after emphasizing one key point. The SVD is",pos_def_matrices
0 0 0 0,vector spaces
0 0 ∗ ∗,computations
false proof that AB has the eigenvalue µλ:,eigenvec_val
"8. For the same matrix H, compare the right-hand sides of Hx = b when the solutions",gauss elim
"4. Show from the eigenvalues that if A is positive deﬁnite, so is A2 and so is A−1.",pos_def_matrices
"m − r of them to end up with a basis. According to 2L, we could have done so. But it",vector spaces
"It is the size of b, compared to a and c, that must be controlled. We now want a",pos_def_matrices
a b c d,gauss elim
is carried to the zero vector. Every Ax is in the column space. Nothing is carried to the,orthogonality
"linear combination in the ﬁrst row of AB. Then rule 3 for the determinant of AB,",determinants
"turn, is 1. Then x = xp +(any combination xn of special solutions).",vector spaces
to make basic and the column of B to make free?,linear_prog
corner—is the ﬁrst to approach an eigenvalue. That entry is the simplest and most pop-,computations
"matrix, elimination will multiply the old B−1 by",linear_prog
i = j ±1,pos_def_matrices
"(iii) For rotations, the matrix is not changed. Those lines are still rotated through θ, and",vector spaces
assume for convenience that the eigenvalues of A are distinct—the eigenspaces are all,eigenvec_val
5. The matrix A =,vector spaces
A + δA = L′U′ instead of the right matrix A = LU. He proved that partial pivoting,computations
"12. If A is a 5 by 5 matrix with all |aij| ≤ 1, then detA ≤",determinants
a positive deﬁnite matrix. The eigenvalues are in the diagonal matrix Λ. The eigenvector,pos_def_matrices
"a waste of time to compute them. In a larger problem, hundreds of columns would be",linear_prog
Chapter 2 Vector Spaces,vector spaces
A is nearly singular whereas B is far from singular. If we slightly change the last entry,gauss elim
j is zero and Aij = 0. Each,pos_def_matrices
"and y steaks is constrained by x+2y ≥ 4, as well as by x ≥ 0 and y ≥ 0. (We cannot have",linear_prog
"vector x = (1,0,...,0) contains equal amounts of every frequency component, and its",eigenvec_val
except the ﬁrst term. We are left with,orthogonality
5k −1 5k +1,eigenvec_val
"Left Nullspace: To solve ATy = 0, we ﬁnd its meaning on the graph. The vector y has",vector spaces
Eigenvectors weighted by λ k,computations
"goes into the zero vector. All vectors go into the column space, since Ax is always a",vector spaces
"for some x′. Then A(x + x′) = b + b′, so that b + b′ is also a combination of the",vector spaces
(a) project every vector onto the x-y plane?,vector spaces
j position—then these multipliers give a complete record of elimination.,gauss elim
"The second component of the product Ax is 4u−6v+0w, from the second row of A. The",gauss elim
"column u will enter the basis, none of the other columns above r will be used. It was",linear_prog
"Other vectors like x = (1,5) are mixtures x1 +5x2 of the two eigenvectors, and when A",eigenvec_val
"The row space of A contains vectors in R4, but not all vectors. Its",vector spaces
"(d) rotate the x-y plane, then x-z, then y-z, through 90°?",vector spaces
The answer is twice column 1 plus 5 times column 2. It corresponds to the “column,gauss elim
away! We go from b to c by forward elimination (this uses L) and we go from c to x by,gauss elim
The determinant is zero at x =,determinants
Verify that A(θ1)A(θ2) = A(θ1+θ2) from the identities for cos(θ1+θ2) and sin(θ1+,gauss elim
"Instead of the average of b1 and b2 (for w1 = w2 = 1), �xW is a weighted average of the",orthogonality
because each one is E times,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
The second approach looks at the columns of the linear system. The two separate,gauss elim
m = 1k = 1.,orthogonality
product of both sides with v1. Orthogonality of the v’s leaves only one term:,orthogonality
This matrix H is ill-conditioned and row exchanges don’t help.,gauss elim
"Above the parabola, the number under the square root is negative—so λ is not real. On",eigenvec_val
"(c) 0 or ∞, depending on b.",vector spaces
"means that the error is squared at every step, as in Newton’s method xk+1 −xk = −f(xk)/ f ′(xk) for solving f(x) =",computations
guarantees to match the grocer on every food—and even undercuts on expensive foods,linear_prog
"square of w8 is w4, which will be essential in the Fast Fourier Transform. The roots add",orthogonality
"one solution, or no solution, or an inﬁnity of solutions?—are much easier to answer",vector spaces
Chapter 8 Linear Programming and Game Theory,linear_prog
Gk+1 and Gk. Then Gk+2 = 1,eigenvec_val
In this section we escape for the ﬁrst time from linear equations. The unknown x will not,pos_def_matrices
"5. Write the Jacobi matrix J for the diagonally dominant A of Problem 4, and ﬁnd the",computations
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"across its main diagonal; the rows of A become the columns of AT, and vice versa. The",orthogonality
3.1 Orthogonal Vectors and Subspaces,orthogonality
3. Multiplying any a+ib by its conjugate a−ib produces a real number a2 +b2:,eigenvec_val
Least Squares Problems with Several Variables,orthogonality
66. Why does no 3 by 3 matrix have a nullspace that equals its column space?,vector spaces
"do not, We could have two matrices with zero eigenvalues, while AB has λ = 1:",eigenvec_val
to give a basis for that solution space.,vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
Matrix multiplication is associative: (AB)C = A(BC). Just write ABC.,gauss elim
w4 = cos π,orthogonality
"14. Find R for each of these (block) matrices, and the special solutions:",vector spaces
plete solution is a combination of two special solutions:,vector spaces
"Example 6. This is the previous matrix A with a right-hand side b = (1,1,1,1).",gauss elim
"7. Solve the 4 by 4 system (6) if the right-hand sides are y0 = 2, y1 = 0, y2 = 2, y3 = 0.",orthogonality
"There is a MATLAB demo (just type eigshow), displaying the eigenvalue problem for a",eigenvec_val
The work of elimination is reduced from n3/3 to n3/6. There is no need to store entries,gauss elim
"From the row space to the column space, A is actually invertible. Every",orthogonality
that separates c into c′ and c′′. At the left is the matrix that multiplies by w j,orthogonality
Each corner of the feasible set comes from turning n of the n + m inequalities Ax ≥ b,linear_prog
The eigenvalues of K are i and −i; their squares are −1 and −1; their reciprocals are,eigenvec_val
(b) What steps lead to the middle equation?,determinants
tions are a basis for the subspace of 3 by 3 matrices with row and column sums all,vector spaces
"19. Why does the nullspace of ATA contain (1,1,1,1)? What is its rank?",vector spaces
"In this special case, the best we can do is to solve the ﬁrst two equations of Ax = b. Then",orthogonality
"All eigenvectors of this A are multiples of the vector (1,0):",eigenvec_val
h jn = qT,computations
The matrix QT is just as much an orthogonal matrix as Q.,orthogonality
The matrices (I −A)−1 in those two cases are −1,eigenvec_val
It remains to ﬁnd the determinant of P. Row exchanges transform it to the identity,determinants
"(a) If A and B are identical except that b11 = 2a11, then detB = 2detA.",determinants
One more preliminary remark. The two parts of this hook were linked by the chapter,pos_def_matrices
"2. For the same 3 by 3 matrix, show directly from the columns that every vector b in",vector spaces
28. (a) E21 subtracts row 1 from row 2 and then P23 exchanges rows 2 and 3. What,gauss elim
Write the 3 by 4 matrix A at the start of this section as the product of the 3 by 2,vector spaces
Crossing those axes are the two ways that stability is lost.,eigenvec_val
order of the eigenvectors in S and the eigenvalues in Λ is automatically the same.,eigenvec_val
"3, ﬁnd the motion if the ﬁrst mass is",eigenvec_val
Our goal is to verify this formula for A−1. We have to see why ACT = (detA)I:,determinants
"animation (with sound) to the course page web.mit.edu/18.06, to show the power method",computations
pTAp0 = 0 (search directions are A-orthogonal). The iteration solves Ax = b by,computations
"Astonishingly, that guess has been proved wrong. There now exists a method that re-",gauss elim
21. (Move to 3 by 3) Forward elimination changes Ax = b to a triangular Ux = c:,gauss elim
Example 5. If A =,pos_def_matrices
"17. The four types of subspaces of R3 are planes, lines, R3 itself, or Z containing only",vector spaces
(b) The vectors for which x1 +x2 +x3 = 0 and x3 +x4 = 0.,vector spaces
Determinants give a transition from Ax = b to Ax = λx. In both cases the determinant,eigenvec_val
were trying to discover the decay rates λ and µ. This is a problem in nonlinear least,orthogonality
back-substitution because R is triangular. The real cost is the mn2 operations of Gram-,orthogonality
2x1 +x2 ≥ 6,linear_prog
"parameter is changed in an experiment, or one observation is corrected, the “inﬂuence",determinants
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
18. We want to ﬁt a plane y = C +Dt +Ez to the four points,orthogonality
and the plane b3 = 0).,vector spaces
factorization at each step is very quick.,computations
It is astonishing how many natural laws can be expressed as minimum principles. Just,pos_def_matrices
and [0 0 1 2] in [U c]:,gauss elim
How many are there?,eigenvec_val
will not allow them to take over the book.,eigenvec_val
vectors x and b. What command would test whether or not Ax = b?,gauss elim
"Rotations, reﬂections, and projections act on n-dimensional space. The transformation",eigenvec_val
"The section begins with w and its properties, moves on to F−1, and ends with the",orthogonality
10. Is the projection matrix P invertible? Why or why not?,orthogonality
"make block multiplication possible, then it is allowed. Replace these x’s by numbers",gauss elim
"3. Solve Ax = b by least squares, and ﬁnd p = A�x if",orthogonality
4.3 Formulas for the Determinant,determinants
48. The lines 3x+y = b1 and 6x+2y = b2 are,orthogonality
0 0 0 0,vector spaces
"method to an end, was r = cN −cBB−1N ≥ 0.",linear_prog
UQ is unitary. Start from UHU = I and QTQ = I.,eigenvec_val
"2. The error vector must be perpendicular to each column a1,...,an of A:",orthogonality
"Ax = λx, but it is useless in solving differential equations. The goal is to build u(t) out",eigenvec_val
(e) What is the dimension of the left nullspace of A?,vector spaces
The best solution is �C = 9,orthogonality
constraints come from the basic property of symmetric matrices: xj is perpendicular to,pos_def_matrices
"fact might have been recognized from Gaussian elimination, but it never was.",determinants
in the domain and f(x) is in the range. In our case the function is f(x) = Ax. Its domain,vector spaces
the opposite side of a mirror. In this example the mirror is the,vector spaces
1. What are F2 and F4 for the 4 by 4 Fourier matrix F?,orthogonality
easy if the basic variables x1 and x5 stand by themselves (as x1 and x2 originally did).,linear_prog
20. Find the inverse of,gauss elim
Maximum where Axn = λnxn,pos_def_matrices
Show that this also implies c(AB) ≤ c(A)c(B).,computations
illustrates four transformations that come from matrices:,vector spaces
"rows and columns from 0 to n − 1, instead of 1 to n. The ﬁrst row has j = 0, the ﬁrst",orthogonality
"row was only a combination of the ﬁrst two. After elimination it became a zero row, It",vector spaces
is really a block multiplication to ﬁnd AB:,gauss elim
Then two ﬁnal Householder transformations quickly achieve the bidiagonal form:,computations
We hope this example reinforced the reader’s understanding of elimination (which,gauss elim
"were trying to make x small and b = Ax large. When A is not symmetric, the maximum of",computations
s c ] as before.,vector spaces
data. This average is closer to b1 than to b2.,orthogonality
"In a more complicated example, the approximation will not be exact at the nodes.",pos_def_matrices
"and sixth—are linearly independent, they must be a basis (for the column space of U,",vector spaces
"13. If B has eigenvalues 1, 2, 3, C has eigenvalues 4, 5, 6, and D has eigenvalues 7, 8, 9,",eigenvec_val
(d) If A is invertible there is no solution xn in the nullspace.,vector spaces
"If we take the inner product of x = (1+3i,3i) with itself, we are back to ∥x∥2:",eigenvec_val
"35. Suppose A and B have the same eigenvalues λ1,...,λn with the same independent",eigenvec_val
"9. Find the orthogonal complement of the plane spanned by the vectors (1,1,2) and",orthogonality
"5.3 If A has eigenvalues 0 and 1, corresponding to the eigenvectors",eigenvec_val
"exchanges to change (6,5,4,3,2,1) to (1,2,3,4,5,6)? One is even and the other is",gauss elim
Measuring sensitivity entirely by λ1 has a serious drawback. Suppose we multiply all,computations
"16. Use 8H to show that the following equation has no solution, because the alternative",linear_prog
7). Those are the vertical errors in,orthogonality
"eigenvalues as N, it must be Λ. The eigenvectors of N are the columns of U, and they",eigenvec_val
"state (eigenfunction) of an atom, minimization was used to get a rough approximation",pos_def_matrices
"Test on the tridiagonal −1, 2, −1 matrices.",pos_def_matrices
the rows of I get reordered twice.,gauss elim
"space. Adding z as an extra row of A would enlarge the row space, but we know that",orthogonality
"26. If the entries of a 3 by 3 matrix are chosen randomly between 0 and 1, what are the",vector spaces
(a) Which vectors span the column space of A?,vector spaces
"allow us to ﬁnd the projection p. Even though a and b are not orthogonal, the distance",orthogonality
−1 −3 3 4,vector spaces
5.4 Differential Equations and eAt,eigenvec_val
"42. Find a solution x(t), y(t) of the ﬁrst system that gets large as t → ∞. To avoid this",eigenvec_val
"It is easy to write the system in matrix form. Let the unknown vector be u(t), with",eigenvec_val
2 is the same for both pairs.,eigenvec_val
"The pivots are the ratios d1 = 2, d2 = 3",pos_def_matrices
"It is orthogonal to the column space, and it is a typical vector in the left nullspace:",orthogonality
This invertibility is fundamental for differential equations. If n solutions are linearly,eigenvec_val
"multiply a vector, the length is not changed. The vector u(0) is just rotated, and that",eigenvec_val
Maximize yb subject to yA ≤ c. Show that the one-sided inequality yb ≤ cx still,linear_prog
"divided by the ﬁxed quantity detB, leads to rule 3 for the ratio d(A). Thus d(A) =",determinants
q r ] for positive deﬁniteness.,pos_def_matrices
"29. (a) The corners of a triangle are (2,1), (3,4), and (0,5). What is the area?",determinants
"an eigenvalue and it might not. If it is, then its eigenvectors satisfy Ax = 0x. Thus x is",eigenvec_val
columns of S are independent,eigenvec_val
"tion algorithm a remarkably distinguished history, although even von Neumann overes-",gauss elim
of ﬁnite elements has become an important part of engineering education. It is treated,pos_def_matrices
"see it in our example. Subtract equation 2 from equation 1, and then divide equation 2",vector spaces
Robert Freund’s notes for his MIT class pin down the (quadratic) convergence rate and,linear_prog
"To prove that P is also symmetric, take its transpose. Multiply the transposes in",orthogonality
It also means that the combination is unique: If v = a1v1 + ··· + akvk and also v =,vector spaces
5.3 Difference Equations and Powers Ak,eigenvec_val
3u + 3v − w =,gauss elim
Ultimately we want to know which matrices are invertible and which are not. This,gauss elim
"5. If the product AB is the zero matrix, AB = 0, show that the column space of B is",vector spaces
and x+ for any diagonal matrix Σ:,pos_def_matrices
45. Multiply AB using columns times rows:,gauss elim
"Even though it is the hardest to apply to a single matrix, eigenvalues can be the most",pos_def_matrices
Here I +Jit +(Jit)2/2!+··· produces 1+λt +λ 2t2/2!+··· = eλt on the diagonal.,eigenvec_val
"The columns, rows, and eigenvectors of Q and U are orthonormal, and every |λ| = 1",eigenvec_val
10. The ellipse u2 +4v2 = 1 corresponds to A =,pos_def_matrices
"n. The relation between y and c is linear, so it must be given by a matrix. This is the",orthogonality
determinant. So the same equality must have held for the original rows.,determinants
all lead to J =,eigenvec_val
The ﬁrst and third columns of U are a basis for its column space. They are the,vector spaces
still the maximum is always found at a corner of the feasible set (with only m nonzero,linear_prog
"equal detA. For example, d(I) = detB/detB = 1; rule 1 is satisﬁed. If two rows",determinants
0 0 0 0,vector spaces
1 0 0 0,vector spaces
The Fourier series is linear algebra in inﬁnite dimensions. The “vectors” are functions,orthogonality
a)2 + ( y,pos_def_matrices
(b) Factor A = LDLT when b is in the range for positive deﬁniteness.,pos_def_matrices
There is an obvious difference between Fibonacci numbers and Markov processes. The,eigenvec_val
The graph in Figure 2.6 has 4 nodes and 5 edges. It does not have an edge between,vector spaces
(f) A singular matrix.,eigenvec_val
0 0 0 ∗,computations
"(The whole algorithm is like elimination, but slightly slower.) The result of n − 1",computations
x1—and when ∥x∥ is small. The true solution x should be as small as possible compared,computations
two functions like sinx and cosx—whose inner product is zero—will be called orthogo-,orthogonality
Example 3. The columns of this triangular matrix are linearly independent:,vector spaces
13. Apply the Gram-Schmidt process to,orthogonality
"The same is true of x = (1,1,1,1) in the nullspace, and all the entries in PA = LDU! The",vector spaces
the line is only a point. Therefore p = 0 is the only candidate for the projection. But,orthogonality
"and then multiply by A−1, you are back where you started:",gauss elim
"comes the fourth fundamental subspace, which has been keeping quietly out of sight.",vector spaces
matrix applications in economics.,eigenvec_val
"change in the solution. Our x and δb make 45° angles with the worst cases, which",computations
h = |b − p|,determinants
thogonal to 1. Now the one-dimensional projections add to the best line:,orthogonality
"picture” of linear equations. If the right-hand side b has components 7, 6, 7, then the",gauss elim
The Complete FFT and the Butterﬂy,orthogonality
equation for P0? Find two vectors in P0 and check that their sum is in P0.,vector spaces
A matrix with orthonormal columns will be called Q.,orthogonality
4. For the positive deﬁnite A =,computations
"differential equations! If you can differentiate xn, sinx, and ex, you know enough. As a",eigenvec_val
"There are many other choices, square or rectangular, and we can see why. If you multiply",pos_def_matrices
subspace (two in the nullspace). Which vectors are orthogonal?,vector spaces
"Example 2. Suppose V is the plane spanned by v1 = (1,0,0,0) and v2 = (1,1,0,0). If",orthogonality
"inﬁnite-dimensional ellipsoids, and perpendicular lines are recognized exactly as before.",orthogonality
Elimination still innocently subtracts 4 times the ﬁrst equation from the second. But,gauss elim
"Some edges lead away from the optimal but unknown x∗, and others lead gradually",linear_prog
"20. If the eigenvectors of A are the columns of I, then A is a",eigenvec_val
The equation z = 2 in Ux = c comes from the original x+3y+6z = 11 in Ax = b by,gauss elim
22. A diagonal entry a j j of a symmetric matrix cannot be smaller than all λ’s. If it,pos_def_matrices
be larger—because the class of trial functions is restricted to the V’s. This step was,pos_def_matrices
Step 3. Subtract −1 times the second equation from the third.,gauss elim
35. Suppose A is the sum of two matrices of rank one: A = uvT +wzT.,vector spaces
(b) BA = 4B.,gauss elim
We turn to the columns. This time the vector equation (the same equation as (1)) is,gauss elim
"words, over which S2 is the maximum of R(x) equal to λ2?",pos_def_matrices
solution y1V1 +y2V2 +y3V3.,pos_def_matrices
"r = b, then A(xr −x′",orthogonality
"The picture may contain 1000 by 1000 “pixels”—a million little squares, each with a",pos_def_matrices
linearly independent. So are the r columns that contain pivots.,vector spaces
Many authors have made this theorem the climax of their linear algebra course.,eigenvec_val
Spanning Trees and the Greedy Algorithm,linear_prog
"cannot pretend that its row is insigniﬁcant. The third has two pivots and its rank is 2, but",pos_def_matrices
"The coordinate vectors e1,...,en in Rn are the most important orthogonal vectors.",orthogonality
"5.7 Would you prefer to have interest compounded quarterly at 40% per year, or annu-",eigenvec_val
7. (a) Evaluate this determinant by cofactors of row 1:,determinants
"Construct the corresponding quadratic P(x1,x2,x3), compute its partial derivatives",pos_def_matrices
"51. The nullspace of a 3 by 4 matrix A is the line through (2,3,1,0).",vector spaces
"whole interval. To ﬁnd the length of such a vector, the usual rule of adding the squares",orthogonality
3.3 Projections and Least Squares,orthogonality
8. Show that starting from A0 =,computations
"Now comes R. We can go further than U, to make the matrix even simpler. Divide",vector spaces
to a real number. A unitary (or orthogonal) matrix can be compared to a number on,eigenvec_val
for positive b and no solution for negative b. Which of the following transformations,vector spaces
u(t) = c1eλ1tx1 +c2eλ2tx2,eigenvec_val
b)2 = 1. What are a and,pos_def_matrices
"possible subspaces are easy to describe: R3 itself, any plane through the origin, any line",vector spaces
AT below the pivot. The result is,vector spaces
possible. These are the extreme cases in the following inequalities:,computations
place of n3/3 operations we need only 2n. Tridiagonal systems Ax = b can be solved,gauss elim
"(a) To prove that the p +q vectors x1,...,xp, Cy1,...,Cyq are independent, assume",pos_def_matrices
Problem Maximize 5x+6y+9z subject to,linear_prog
"More precisely, it is hopeless to solve this by Gaussian elimination; every roundoff",orthogonality
the right-hand side. Therefore y is orthogonal to every combination of the columns.,orthogonality
far below the n3/3 steps needed to factor A on the left-hand side.,gauss elim
"3. If the system starts from rest, u′(0) = 0, the terms in bsinωt will disappear:",eigenvec_val
Fibonacci’s rule Fk = Fk−1 +Fk−2.,determinants
"B and M, it gives the projection matrix in the standard basis of v’s:",eigenvec_val
"a smaller set of “dimension n − 2.” Assuming all goes well, every new plane (every new ",gauss elim
"give no solution (Figure 1.5a shows an end view). In two dimensions, parallel lines",gauss elim
"If the ﬁrst step is a row exchange, the pivots are c and (−detA)/c.",determinants
"This time there is no nullspace (except for the zero vector, as always!) but integration",vector spaces
x+y+t = 0 and x−t = 0?,orthogonality
. Find its eigenvalue and,eigenvec_val
of elimination (which is U = I).,gauss elim
"(b) w1 = (1,2,0), w2 = (2,5,0), w3 = (0,0,2), w4 = (0,0,0), and any b?",vector spaces
"on the other side. As before, equality holds when t reaches tmax—which is the eigenvalue",eigenvec_val
"independent, but it fails to span R2. The three vectors v1, v2, v3 certainly span R2, but",vector spaces
the Kuhn-Tucker conditions of nonlinear programming:,linear_prog
"45. Suppose T is reﬂection across the 45° line, and S is reﬂection across the y-axis, If",vector spaces
c0 +i2c1 +i4c2 +i6c3,orthogonality
is to the family of matrices A − λI. The parameter λ is subtracted all along the main,determinants
Solve Rx = 0 (free variable = 1). What are the solutions to Rx = d?,vector spaces
1.6 Find bases for the four fundamental subspaces associated with,vector spaces
"1.17 If x is a vector in Rn, and xTy = 0 for every y, prove that x = 0.",vector spaces
cleared out below the second pivot. The forward step is ﬁnished when the system is,gauss elim
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"This deﬁnition gives us another (better) way to verify the formula (AB)T = BTAT, Use",orthogonality
19. A fourth way to multiply matrices is columns of A times rows of B:,gauss elim
"of each row from the rows beneath, It reached the “triangular” system (3), which is",gauss elim
C11 = ∑(a2β ···anv)detP = det(submatrix of A).,determinants
"side c = (0,6,0) becomes d = (−9,3,0). Then −9 and 3 go into xp:",vector spaces
"and of people, is not computation but intelligence.",vector spaces
1b. On the right-hand side all terms disappear (because qT,orthogonality
ectb. This u = ectv is a particular solution. How does it break down when c is an,eigenvec_val
"Certainly the ﬁrst pivot depended only on the ﬁrst row and column, The second pivot",determinants
matrix multiplication follow the same rules as before. Length is computed differently.,eigenvec_val
λmax/λmin is the product ∥A∥∥A−1∥—which is the condition number.,computations
"c = (0,1,3,0). Show that r ≥ 0 and, therefore, that corner P is optimal.",linear_prog
c0 +c1eix +c2e2ix +c3e3ix =,orthogonality
It is natural to represent the three unknowns by a vector:,gauss elim
0 1 1 0,vector spaces
4x + 8y = 6,gauss elim
"2. With a special choice of M, what special form can be achieved by M−1AM?",eigenvec_val
Certainly BA = I and AC = I. What is not so certain is that ATA and AAT are actually,vector spaces
"(iii) If ax2 +2bxy+cy2 stays positive, then necessarily ac > b2.",pos_def_matrices
"ponent at a time. Then xk can be destroyed as fast as xk+1 is created, The ﬁrst component",computations
3 and conﬁrm that u′,eigenvec_val
"Up to now, we have hardly thought about the signs of the eigenvalues. We couldn’t",pos_def_matrices
complex polynomial of degree n has a full set of n roots (possibly complex and possibly,orthogonality
instability a scientist thought of exchanging the two equations!,eigenvec_val
Row exchanges are unnecessary in theory and in practice. This is in contrast to the,gauss elim
Step 2. Subtract −1 times the ﬁrst equation from the third;,gauss elim
"If a+ib is an eigenvalue of a real matrix, so is a−ib. (If A = AT then b = 0.)",eigenvec_val
2xTAx − xTb = 1,pos_def_matrices
The x-y plane in Figure 2.4 has two vectors in every basis; its dimension is 2. In three,vector spaces
2. The nullspace of A is denoted by N(A). Its dimension is n−r.,vector spaces
"29. In Figure 3.4, how do we know that Axr is equal to Ax? How do we know that this",orthogonality
"15. If the r pivot variables come ﬁrst, the reduced R must look like",vector spaces
"dimension is the rank r = 3. Elimination will ﬁnd three independent rows, and we can",vector spaces
λ1 and λ2 = 1,eigenvec_val
The larger eigenvalue λ = 1,eigenvec_val
24. For which s and t do A and B have all λ > 0 (and are therefore positive deﬁnite)?,pos_def_matrices
"are pressure and ﬂow rate. In statistics, e is the error and x is the best least-squares ﬁt to",vector spaces
xi = α = smallest ratio(B−1b)j,linear_prog
", and there exists a",vector spaces
"from the second. The lower right entry becomes −9999, but roundoff to three places",gauss elim
"Maximize yb, subject to y ≥ 0 and yA ≤ c.",linear_prog
inequalities w ≥ 0.,linear_prog
"3(−1), and the inverse of the covariance matrix is W TW:",orthogonality
7 = 0. It is orthogonal to the column,orthogonality
29. What is wrong with this proof that projection matrices have detP = 1?,determinants
"product. You see how (A−1)T is the inverse of AT, proving (ii):",gauss elim
"chosen to be the columns of S, then we have S−1AS = Λ as always. The diagonalizing",eigenvec_val
This sum takes us along the ith row of A. The column index j takes each value from 1,gauss elim
"(a) X is twice as old as Y and their ages add to 39,",gauss elim
strictly speaking it is in the column space of AT.,vector spaces
2 stays home and 1,eigenvec_val
erties it possesses. This suggests the natural place to begin. The determinant can be (and,determinants
"vector b = (b1,...,bn) is a combination of those columns. In this example the weights",vector spaces
transform gives y′ = Fmc′ and y′′ = Fmc′′. Those are the two multiplications by the,orthogonality
is where c came from—we recover the correct solution uk = SΛkS−1u0.,eigenvec_val
is faced by a druggist. Protein pills compete with steak and peanut butter. Immediately,linear_prog
"A = λ1P1+···+λkPk, where Pi is the projection onto the eigenspace for λi. Since there is",eigenvec_val
"The most important ease, and the simplest, is when the number of unknowns equals the",gauss elim
the minimizing trial function U = y1V1 +···+ynVn. Connecting all these heights y j by,pos_def_matrices
"odd then c is odd. The vector f is odd if fn−j = −f j; for n = 4 that means f0 = 0,",orthogonality
form and a solution.,gauss elim
"error δx, coming from A−1, is in the direction of the ﬁrst eigenvector x1:",computations
"that decays at this slower rate corresponds to the eigenvector (1,1). Therefore the two",eigenvec_val
(λ −aii)xi = ∑,computations
The ﬁrst formula has already appeared. Row operations produce the pivots in D:,determinants
0 A0Q0 = Q−1,computations
"1. Choose the trial functions V1,...,Vn.",pos_def_matrices
1 2 0 1,vector spaces
3.5 The Fast Fourier Transform,orthogonality
In the n by n case we suggest two possible proofs—since this is the least obvious rule.,determinants
decrease x1 while keeping x1 +2x5 = 8. Then x1 will be down to zero when x5 reaches,linear_prog
1 2 0 2 −9,vector spaces
Ax = b is consistent (has a solution) when b satisﬁes b2 =,vector spaces
"The columns all end with zero. In the column space, the closest vector to b = (b1,b2,b3)",pos_def_matrices
the system has no solution.) Which b’s allow an xp?,vector spaces
The basic variables will stand alone when elimination multiplies by B−1:,linear_prog
"through the origin, or the origin (the zero vector) alone.",vector spaces
Our example with cost 2x + 3y can be put into words. It illustrates the “diet problem”,linear_prog
write xTAx as a sum of two squares and xTBx as one square.,pos_def_matrices
7.4 Iterative Methods for Ax = b,computations
aT(b− �xa) = aTb− aTb,orthogonality
18. Test to see if ATA is positive deﬁnite in each case:,pos_def_matrices
"Remark 7. Suppose A has only one column, containing a. Then the matrix ATA is the",orthogonality
"gives the row number, and the second subscript indicates the column. (In equation (4),",gauss elim
"Applying the length formula (1), this test for orthogonality in Rn becomes",orthogonality
Suppose xi is the entering variable and u is column i of N:,linear_prog
Show again that the 2 by 2 matrix in the upper left corner is invertible.,vector spaces
sion of detA. That is the clue we need: A−1 divides the cofactors by detA.,determinants
Start from the corner at which x1 = 8 and x2 = 9 are the basic variables. At that corner,linear_prog
trace and determinant tell us everything:,eigenvec_val
"tains the ﬁrst (take c = 0 to get A0 = 0). We saw rule (iii) in action when (4,0) was",vector spaces
A → H1A =,computations
variables and then proceed as for the three-fold left-hand set to derive values,determinants
Jordan form is independent of this triangular form.),eigenvec_val
the number of pieces of the graph—and there will be no way to rank one piece against,vector spaces
The next section contains the tests to decide whether xTAx is positive (the bowl goes,pos_def_matrices
The last is symmetric about x and it is the most accurate. For the second derivative there,gauss elim
"If A is singular, elimination leads to a zero row in U. Then detA = detU = 0. If A",determinants
edge will produce a loop.,vector spaces
"The result is a two-point boundary-value problem, describing not a transient but a steady-",gauss elim
Our true goal is to look beyond two or three dimensions into n dimensions. With n,gauss elim
of one row from another. Such a transformation preserved the nullspace and row space,eigenvec_val
norms of A and A−1. The matrices in this chapter are square.,computations
"dinate axes are mutually orthogonal. That is just about optimal, and the one possible",orthogonality
very seldom that we want only the transpose of A. It is the conjugate transpose AH that,eigenvec_val
"22. Project b = (1,0,0) onto the lines through a1 and a2 in Problem 21 and also onto",orthogonality
(b) A system Ax = b has at most one particular solution.,vector spaces
(d) What is a basis for the column space of A?,vector spaces
3] and also the matrix P2,orthogonality
"easy. To project a vector v onto the ﬁrst one, you compute (aTv)a. To project the same",orthogonality
"If Ax = λx, then xHAx = λxHx and xHAHx = λxHx.",pos_def_matrices
many solutions; the three columns can be combined in inﬁnitely many ways to produce,gauss elim
maximizes or minimizes a certain cost function like 2x + 3y. The problem in linear,linear_prog
"Finally, trigonometry connects the sides a and b to the hypotenuse r by a = rcosθ",eigenvec_val
We need ℓ − k exchanges of neighbors to move the entry in place k to place ℓ. Then,determinants
Jordan form can be reached. I hope the following table will be a convenient summary.,eigenvec_val
−1 column for every edge:,linear_prog
orthogonal matrix Q. Every Hermitian matrix can be diagonalized by a unitary,eigenvec_val
"reverse order, and use symmetry of (ATA)−1, to come back to P:",orthogonality
2(A+AT) should be positive,pos_def_matrices
5.6 Solve for both initial values and then ﬁnd eAt:,eigenvec_val
"(b) If Nx = λx, show that λ must be zero.",eigenvec_val
Each of the four fundamental subspaces has a meaning in terms of the graph. We can,vector spaces
A complete matching is possible if (and only if) Hall’s condition holds.,linear_prog
"A square matrix is the opposite. If m = n, we cannot have one property without the",vector spaces
"As always, the constants are found from the initial conditions. This is easier to do (at the",eigenvec_val
The fundamental equation ATA�x = ATb simpliﬁes to a triangular system:,orthogonality
"for Fk must give an integer. In fact, since the second term [(1 −",eigenvec_val
"15. Find an orthonormal set q1, q2, q3 for which q1, q2 span the column space of",orthogonality
"rows: v = ATz for some vector z. Now, in one line:",orthogonality
"Starting from the classical Rayleigh-Ritz principle, I will introduce the new idea of ﬁnite",pos_def_matrices
How does this row picture extend into n dimensions? The n equations will con-,gauss elim
is λ 2 −(a+d)λ +(ad −bc). By direct,eigenvec_val
"(b) If C(A) contains only the zero vector, then A is the zero matrix.",vector spaces
"Now we have two free parameters c1 and c2, and it is reasonable to hope that they can",eigenvec_val
"negative component, choose u = column i of N to enter the basis.",linear_prog
"to decide whether they are positive deﬁnite, positive semideﬁnite, or indeﬁnite.",pos_def_matrices
The cost cBxB +cNxN has been turned into,linear_prog
∂L/∂x = 0 :,pos_def_matrices
is to ordinary differential equations. We shall not assume that the reader is an expert on,eigenvec_val
"plement of the subspace spanned by (1,−1,0,0,0) and (2,−2,3,4,−4).",orthogonality
0 1 0 0,vector spaces
0 0 0 0,eigenvec_val
"at directly, Ax = b requires b to be in the column space. Looked at indirectly. Ax = b",orthogonality
+ 2v + 2w = 11,gauss elim
", each overrelaxation step is",computations
3. Construct an indeﬁnite matrix with its largest entries on the main diagonal:,pos_def_matrices
"vector is orthogonal to the ﬁrst column (1,1,1), since −2",orthogonality
"If the ordering goes a row at a time, every point must wait a whole row for the neigh-",computations
1 3 1 0,gauss elim
the possibility of a speedier method...,determinants
"(a) 0 or 1, depending on b.",vector spaces
"times the column y = (0,1,0).",gauss elim
"line of multiples of (1,1,1,1).",vector spaces
25. L is lower triangular and S is symmetric. Assume they are invertible:,determinants
it is the product of two nonzero determinants. Both matrices on the right-hand side are,eigenvec_val
"r drdθ dz; this element is our little box. (It looks curved if we try to draw it, but proba-",determinants
The Geometry of Linear Equations,gauss elim
components from the nullspace. The matrix that produces x+ from b = [18] is the pseu-,pos_def_matrices
"The matrix Σ is m by n, with r nonzero entries σi. Its pseudoinverse Σ+ is n by m, with",pos_def_matrices
∥R∥ ≤ ∥A∥. Then ∥A∥ = ∥Q∥∥R∥. Find an example of A = LU with ∥A∥ < ∥L∥∥U∥.,computations
8. Under what conditions on b1 and b2 (if any) does Ax = b have a solution?,vector spaces
Chapter 1 Matrices and Gaussian Elimination,gauss elim
the plane 2u+v+w = 0 goes through the origin.,gauss elim
cofactors and each 4 by 4 cofactor contains,determinants
minant is zero? What equation does this give for the plane?,determinants
Chapter 2 Vector Spaces,vector spaces
8. (Second proof of A = LU) The third row of U comes from the third row of A by,gauss elim
"3.8 Which straight line gives the best ﬁt to the following data: b = 0 at t = 0, b = 0 at",orthogonality
when n independent squares are multiplied by positive pivots.,pos_def_matrices
"rule 5. Because the matrix will now have two identical rows, detA = 0 by rule 4.",determinants
"in Rn. Those are not the only spaces. By deﬁnition, any vector space allows the com-",vector spaces
"of a parabola, so the error λ1 − αk is roughly the square of the error in the eigenvector.",computations
"In fact y1 −y2 = (1,−1,0,1,−1) gives the big loop around the outside of the graph.",vector spaces
0 ∗ ∗ 0,computations
"contains (−1,1) and all its multiples xn = (−c,c):",vector spaces
"The nullspace of B is the line of all points x = c, y = c, z = −c. (The line goes through",vector spaces
bination of the n columns that equals b. For certain equations that will be impossible.,gauss elim
The Cross-Product Matrix ATA,orthogonality
The ﬁrst gives the length of the vector (squared).,gauss elim
1 c 2 2,vector spaces
(b) inﬁnity of solutions,gauss elim
"1. The inﬁnite-dimensional space R∞. Its vectors have inﬁnitely many components, as",vector spaces
this idea of symmetry has to be extended. The right generalization is not to matrices that,eigenvec_val
the only matrices with ∥A∥ = 1 and ∥A−1∥ = 1 must have ATA = I. They are,computations
"time, Each row of A combines with x to give a component of Ax. There are three inner",gauss elim
"Thus b−Pb is orthogonal to the space, and Pb is the projection onto the column space.",orthogonality
"32. Find the numbers a and b that give the inverse of 5 ∗ eye(4) − ones(4,4):",gauss elim
"2. It is impossible to get both errors small, because when",eigenvec_val
"ity” u′(0). To match these conditions, there will be 2n pure exponential solutions.",eigenvec_val
26. Suppose S is a ﬁve-dimensional subspace of R6. True or false?,vector spaces
"4. Solve du/dt = Pu, when P is a projection:",eigenvec_val
"combine to produce the m−r zero rows of U. Start from PA = LU, or L−1PA = U. The",vector spaces
10. Find the limiting values of yk and k (k → ∞) if,eigenvec_val
ignore the rest; this is hard to justify if all m equations come from the same source.,orthogonality
value of P(y) = 1,pos_def_matrices
"constant. It just keeps passing around the system. The general solution to d2u/dt2 = Au,",eigenvec_val
x j = 0 as a separate case):,determinants
"They are the numbers 1,w,...,wn−1 (or 1,i,i2,i3 in this 4 by 4 ease). It is a real matrix,",eigenvec_val
The components of uk = Aku0 specify the probability that the individual is outside,eigenvec_val
"will break down—either the equations were written in the wrong order, which is",gauss elim
"When the masses were equal, m1 = m2 = 1, this was the old system u′′ +Au = 0. Now",pos_def_matrices
there is a y such that yA = 0 and yb ̸= 0.,linear_prog
∗ ∗ ∗ ∗,computations
These are the columns of Q. Splitting A = QΛQT into 3 columns times 3 rows gives,eigenvec_val
The Matrix Form of One Elimination Step,gauss elim
"after elimination, We need to devote one more section to those questions, to ﬁnd every",vector spaces
"19. If PC = A(ATA)−1AT is the projection onto the column space of A, what is the pro-",orthogonality
Properties of Eigenvalues and Eigenvectors,eigenvec_val
At the end of elimination we have a full set of r independent rows. Those r edges,vector spaces
Application of A = QS:,pos_def_matrices
"be zero because d2 ̸= 0, and ﬁnally c1 = 0. This establishes independence and completes",vector spaces
possible. In fact the combination of Gauss and von Neumann gives the simple elimina-,gauss elim
as if it were some kind of pancake. We have to do better than that.,orthogonality
are a basis for R2:,vector spaces
The underlying linear transformations of the x-y plane are also simple. But rotations,vector spaces
Rectangular Matrices with Orthogonal Columns,orthogonality
"It is this combination, b = xn and δb = εx1, that makes the relative error as large as",computations
"American money stays home, and 1",eigenvec_val
negative entries. The powers Ak are all nonnegative.3,eigenvec_val
x + 2y = 4,linear_prog
What we see in the ﬁrst two rows and columns is exactly the factorization of the corner,determinants
second pivot? What is y? The second pivot is missing when ad = bc.,gauss elim
"The identity matrix I, with 1s on the diagonal and 0s everywhere else,",gauss elim
"pivots for MATLAB’s lu(rand(3,3)). The average of the ﬁrst pivot from abs(A(1,1))",gauss elim
and the addition is to bring back the identity matrix:,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
x + ky = 1,gauss elim
matrix. If the eigen-,eigenvec_val
the orthogonal complement S⊥. What are P+Q and PQ? Show that P−Q is its own,orthogonality
"1.30 (a) If A is square, show that the nullspace of A2 contains the nullspace of A.",vector spaces
This number ad − bc is the determinant of A. A matrix is invertible if its determinant,gauss elim
2] produce RTR =,gauss elim
29. Prove that a matrix with a column of zeros cannot have an inverse.,gauss elim
are in the right places to give PPT = I.,gauss elim
18. The “ℓ1 norm” is ∥x∥1 = |x|1+···+|x|n. The “ℓ∞ norm” is ∥x∥∞ = max|xi|. Compute,computations
2y1 +5y2 ≤ 1,linear_prog
(and pronounced “A inverse”). The fundamental property is simple: If you multiply by A,gauss elim
"(ii) If the vector x goes to x′, then 2x must go to 2x′. in general cx must go to cx′, since",vector spaces
�x1 = 2 and �x2 = 1. The error in the equation 0x1 +0x2 = 6 is sure to be 6.,orthogonality
"zontal axis, one eigenvalue is zero (because the determinant is λ1λ2 = 0). On the vertical",eigenvec_val
"plicity m with only one eigenvector, the extra factor t appears m−1 times.",eigenvec_val
p = A�x =,orthogonality
The candidate for the second pivot has become zero: unacceptable. We look below that,vector spaces
1. The feasible set is empty.,linear_prog
rows of a square matrix are orthonormal whenever the columns are. The rows point,orthogonality
∗ ∗ ∗ ∗,computations
Its component p in this direction is exactly b1sinx.,orthogonality
experiment t is the load and b is the reading from the strain gauge. Unless the load,orthogonality
"to the second column (−1,1,2), because −2",orthogonality
exchanges nothing). The product of two permutation matrices is another permutation—,gauss elim
"identical springs (Figure 5.3). The ﬁrst mass is held at v(0) = 1, the second mass is held",eigenvec_val
0 0 0 3 0,vector spaces
vectors produce the whole nullspace.,vector spaces
"the 4 by 4 matrix from the standard basis 1, t, t2, t3. Find its nullspace and column",vector spaces
Give a 2 by 2 example of this important rule for matrix multiplication.,gauss elim
avoid a contradiction is to have m = n. This completes the proof that m = n. To repeat:,vector spaces
"tor. These pure solutions satisfy the differential equation, since d/dt(ceλtx) = A(ceλtx).",eigenvec_val
are symmetric but share the same rank as A. Their eigenvalues—the singular values,pos_def_matrices
"matrices of any size, we go back to elimination on a symmetric matrix: A = LDLT.",pos_def_matrices
by 2 mass matrix Mi j =,pos_def_matrices
"6.7a). It is concentrated in a small interval around its node, and it is zero everywhere",pos_def_matrices
Note 6. A diagonal matrix has an inverse provided no diagonal entries are zero:,gauss elim
complete matching is impossible.,linear_prog
"Remember that our goal is to produce a discrete problem—in other words, a problem",gauss elim
To ﬁnish the complex case we need the analogue of a real orthogonal matrix—and you,eigenvec_val
smallest possible vector space: the empty set is not allowed. At the other extreme. the,vector spaces
The ﬁrst stage of the elimination process produces zeros below the ﬁrst pivot:,gauss elim
"To repeat: Forward elimination produced the pivots 2, −8, 1. It subtracted multiples",gauss elim
"(b) Are the two angles the same? Find their cosines if a = (1,1,−1), x = (2,0,1),",orthogonality
(a) a basis for the orthogonal complement V⊥.,orthogonality
"a difference equation, since e0t = 1.)",eigenvec_val
"Of course Σ could multiply by a large σ or (more commonly) divide by a small σ,",pos_def_matrices
b. The new problem Ax = λx will still be solved by simplifying a matrix—making it,eigenvec_val
keeping us inside the subspace. The eight required properties are satisﬁed in the larger,vector spaces
"To ﬁnd the right multiple of x1, use the fact that the total population stays the same. If",eigenvec_val
"to explain how x is actually found, but I do not think there is.",gauss elim
"coefﬁcient matrix A, the matrices E for elimination and P for row exchanges, and the",gauss elim
1 0 1 0,eigenvec_val
This should conﬁrm Exercise 3.,eigenvec_val
"DO 10 I = 1, N",gauss elim
"the x-y plane or three-dimensional space or Rn, the axes are there. They are usually per-",orthogonality
in the form SΛS−1. Find eAt from SeΛtS−1.,eigenvec_val
3.10 Which constant function is closest to y = x4 (in the least-squares sense) over the,orthogonality
3. The matrix A =,vector spaces
11. Reduce the equation 3u2 − 2,pos_def_matrices
"matrix B at the end of this section, which is not positive deﬁnite. Without a row",gauss elim
E32E31E21A = U? Multiply by E−1,gauss elim
5. Decide the dependence or independence of,vector spaces
"minima. When the variables are x1,...,xn, they go into a column vector x. For any",pos_def_matrices
"10. If Q is an orthogonal matrix, so that QTQ = I, prove that detQ equals +1 or −1.",determinants
24. Every column of AB is a combination of the columns of A. Then the dimensions of,vector spaces
"the unit circle—a complex number of absolute value 1. The λ’s are real if AH = A, and",eigenvec_val
section goes onward from U to a reduced form R—the simplest matrix that elimina-,vector spaces
"(T) From T to J, the job is to change 2 to 1. and a diagonal M will do it:",eigenvec_val
Remark 1. The maximin principle extends to j-dimensional subspaces S j:,pos_def_matrices
29. A linear transformation from V to W has an inverse from W to V when the range,vector spaces
6.6 for a 2 by 2 positive deﬁnite matrix to see it clearly.,pos_def_matrices
"16. If there is an edge between every pair of nodes (a complete graph), how many edges",vector spaces
y1 +y2 ≤ 3? Solve both this problem and its dual.,linear_prog
2(x − A−1b)TA(x − A−1b) + constant.,pos_def_matrices
Chapter 8 Linear Programming and Game Theory,linear_prog
Chapter 2 Vector Spaces,vector spaces
"(a) closed under vector addition and subtraction, but not scalar multiplication.",vector spaces
0 0 1 2,vector spaces
(c) Find the projection matrix P2 that projects vectors in R3 onto V.,orthogonality
"Choose any matrix R, probably rectangular. Multiply RT times R. Then the product RTR",gauss elim
"other two, because A = QΛQT.)",eigenvec_val
vitamin for a total income of y1b1 +···+ymbm = yb—to be maximized.,linear_prog
main diagonal) its determinant is zero.,determinants
"4. If Q1 and Q2 are orthogonal matrices, so that QTQ = I, show that Q1Q2 is also",orthogonality
"thogonal; the powers 1, x, x2 are not. When f(x) is written as a combination of sines and",orthogonality
12. Find the eigenvalues and eigenvectors for,eigenvec_val
"In place of the shopper, who buys enough protein at minimal cost, the dual problem",linear_prog
determinant is a polynomial of degree n. It starts with (−λ)n.,eigenvec_val
"stationary point of F is at x = α, y = β, the only change would be to use the second",pos_def_matrices
16. (a) Find an orthogonal Q so that Q−1AQ = Λ if,eigenvec_val
(3) BA has the ﬁrst and last rows of A reversed.,gauss elim
i xi. The eigenvectors xi are,eigenvec_val
11. Apply elimination (circle the pivots) and back-substitution to solve,gauss elim
(a) Find a basis for V and a basis for V⊥.,orthogonality
(a) How do you know that K −I is invertible?,eigenvec_val
"To describe overrelaxation, let D, L, and U be the parts of A on, below, and above",computations
by n) are the square roots of the nonzero eigenvalues of both AAT and ATA.,pos_def_matrices
The columns of Q (or U) contain orthonormal eigenvectors of A.,eigenvec_val
1(c1v1 +···+ckvk) = c1vT,orthogonality
on this plane have the following form (ﬁll in the ﬁrst components):,vector spaces
"In the 2 by 2 case, the standard LDU factorization is",determinants
functions of mathematics should come together in such a graceful way. Our best answer,orthogonality
Sxk+1 = Txk +b.,computations
"The cofactors C11, C12, C13 are the 2 by 2 determinants in parentheses.",determinants
2. Its square is itself: P2 = P.,orthogonality
35. The trace of S times ΛS−1 equals the trace of ΛS−1 times S. So the trace of a diago-,eigenvec_val
Hx = x− 2vvTx,computations
"x1 ≥ 0, x2 ≥ 0",linear_prog
"fact practically doubles our list of properties, because every rule that applied to the rows",determinants
special solution: There are more solutions than the trivial x = 0.,vector spaces
sulting matrix is what we now mean by A: its n−1 columns are independent. The square,vector spaces
For which values of λ is A−λI a singular matrix?,determinants
"numbers Fk become larger and larger, while by deﬁnition any “probability” is between 0",eigenvec_val
(Figure 6.2). We will show that the axes of the ellipse point toward the eigenvector of,pos_def_matrices
"makes this combination zero. The next rotation P32 is chosen in a similar way, to remove",computations
"elimination—from A to U as usual, and from U to D by upward elimination. The",determinants
this section—to the triangle inequality for vectors.) The Schwarz inequality seems to,orthogonality
"That could seem a little mysterious, unless you already know about 2 by 2 determi-",gauss elim
has this arbitrary constant c (like the +C when we integrate in calculus).,vector spaces
"transformations, and only those, that lead us back to matrices.",vector spaces
x + 2y = 3,gauss elim
The unknowns are the ﬂows xij from node i to node j. The capacity constraints are,linear_prog
matrix M = E31P23 does both steps at once? Explain why the M’s are the same,gauss elim
1. The determinant of the identity matrix is 1.,determinants
2; the eigenvalues were already sitting along the main diagonal.,eigenvec_val
= volume of a hypercube in R4?,determinants
F2 boxes. The new factor w4 = i is the square of the old factor w = w8 = e2πi/8. The,orthogonality
The point about the last two subspaces is that they come from AT. If A is an m by n,vector spaces
Here is our ﬁrst big theorem in linear algebra:,vector spaces
"It would be a waste of time, since we only need back-substitution for x (and forward",gauss elim
"value. The coefﬁcient of w is zero, but this remains a plane in 3 -space. (The equation ",gauss elim
combinations of the rows are identical: same space!,vector spaces
"This is an overdetermined system, with m equations and only two unknowns. If errors",orthogonality
The problem is to ﬁnd the combination of the column vectors on the left side that,gauss elim
solving the third-order equation is quicker.,eigenvec_val
"This is the replacement for equation (3), when A is not symmetric. In the symmetric case,",computations
is solvable if and only if b1 +b2 +b3 = 0.,orthogonality
r) = b−b = 0. This puts xr −x′,orthogonality
W. Why does V TW = zero matrix? This matches vTw = 0 for vectors.,orthogonality
"Au1. If A is small (as it is), then production does not consume everything—and the",eigenvec_val
"(d) If A and B are diagonalizable, so is AB.",eigenvec_val
The m−r solutions to yTA = 0 are hiding somewhere in elimination. The rows of A,vector spaces
"I mistakenly thought; that vector has length 1 (it is the rotation), so we must multiply",vector spaces
"beautifully and simply that it will be a delight to see, that the fundamental subspaces",orthogonality
nullspace of A contains only z =,eigenvec_val
2. Techniques for Solving Ax = λx.,computations
can take no steps or two steps or about a third of a million steps:,gauss elim
����� = λ 2 −(trace)λ +determinant,eigenvec_val
"spring), or its conductance (if it contains a resistor). Those numbers go into a diagonal",vector spaces
"matrix, with zero determinant, has one or more of its eigenvalues equal to zero.",eigenvec_val
0 ∗ ∗ ∗,computations
"answer, �x = (a1b1 +···+ambm)/(a2",orthogonality
add to zero around all loops. In a moment we link x to y by a third law (Ohm’s law for,vector spaces
Which column is sure to have no pivot (and which variable is free)? What is the,vector spaces
"18. Find a normal matrix (NNH = NHN) that is not Hermitian, skew-Hermitian, unitary,",eigenvec_val
"knowing A−1 than knowing L and U. Multiplying A−1 by b takes n2 steps, whereas 4n",gauss elim
End with U and c.,gauss elim
Under what circumstances could the process break down? Something must go wrong,gauss elim
16. Find these 4 by 4 determinants by Gaussian elimination:,determinants
No pivot in column 3,gauss elim
10. The matrix A =,vector spaces
"the x direction combine with −1, 2, −1 in the y direction to give a main diagonal of",computations
"solutions agree, remember that each power (SΛS−1)k telescopes into Ak = SΛkS−1 (be-",eigenvec_val
"The result was an equivalent system Ux = c, with a new coefﬁcient matrix U:",gauss elim
1 1 1 1,determinants
"the formula for �x becomes a meaningless 0/0, and correctly reﬂects the fact that �x is",orthogonality
"by an m by n matrix! The original vector x has n components, and the transformed vector",vector spaces
"Any spanning set in V can be reduced to a basis, by discarding vectors if",vector spaces
detAk = detLk detDk detUk = detDk = d1d2···dk.,determinants
"∂ f/∂x = ∂ f/∂y = 0. A local minimum would also be a global minimum, The surface",pos_def_matrices
"enters regression analysis. In geodesy, the U.S. mapping survey tackled 2.5 million",orthogonality
adjacent diagonals. Factor these into A = LU and A = LDV:,gauss elim
"(from the real numbers R1 to the real numbers R1) are invertible? None are linear,",vector spaces
"5. Find a particular solution to Ax = (0,6,−6) and the complete xp +xn.",vector spaces
was optimal remains optimal. The choice of basic variables does not change; B and N,linear_prog
permutation matrix P = [0 1,gauss elim
gives the minimum. Substitute x = A−1b into P(x):,pos_def_matrices
ponent of xB dropped to zero (the other components of xB remain positive). The leaving,linear_prog
the complex vector space Cn contains all vectors x with n complex components:,eigenvec_val
You recognize the two copies of F2 in the center. At the right is the permutation matrix,orthogonality
of B. Eigenvalues of AB (are)(are not) equal to eigenvalues of BA.,eigenvec_val
1.5 Triangular Factors and Row Exchanges,gauss elim
1.6 Inverses and Transposes,gauss elim
The left nullspace N(AT) and column space C(A) are subspaces of Rm.,vector spaces
or in vector notation,eigenvec_val
15. The space of all 2 by 2 matrices has the four basis “vectors”,vector spaces
0 0 1 2,vector spaces
(c) All decreasing sequences: x j+1 ≤ x j for each j.,vector spaces
"We use parentheses and commas when the components are listed horizontally, and",gauss elim
compare each pivot with all possible pivots in the same column. Exchanging,gauss elim
"m11c1 +m12c2 = d1 and m21c1 +m22c2 = d2, the vectors c1V1 +c2V2 and d1v1 +d2v2",eigenvec_val
1 2 0 1,vector spaces
"upper triangular with ones on the diagonal, exactly like U.) Since the factorization is",gauss elim
28. True or false (with a counterexample if false)?,vector spaces
Elimination produces those pivots before the determinant appears.,gauss elim
(a) Every subspace of R4 is the nullspace of some matrix.,vector spaces
2. Suppose the values b1 = 1 and b2 = 7 at times t1 = 1 and t2 = 2 are ﬁtted by a line,orthogonality
"(c) Any two of these statements imply the third: A is Hermitian, A is unitary, A2 = I.",eigenvec_val
Singular case ac = b2:,pos_def_matrices
So far we have a convenient shorthand Ax = b for the original system of equations.,gauss elim
computers will partly relieve it.,computations
each resistor). First we stay with the matrix A for an application that seems frivolous,vector spaces
2 2 0 1,gauss elim
many solutions will they have? Draw the column picture.,gauss elim
Those are three-dimensional column vectors. The vector b is identiﬁed with the point,gauss elim
"2 − b1x1 − b2x2. The usual approach, by",pos_def_matrices
"vector x of greatest length. From our earlier description of the ellipsoid, its longest axis",pos_def_matrices
(c) Find three independent eigenvectors of P all with eigenvalue λ = 0.,eigenvec_val
We have ﬁnally arrived at the fundamental algorithm of numerical linear algebra:,gauss elim
The eigenvalues are λ = trace±,eigenvec_val
"primal, c is in the cost function and b is in the constraint, In the dual, b and c are",linear_prog
cost are the most important.,linear_prog
25. From the unit vector u =,eigenvec_val
"The triangular factorization can be written A = LDU, where L and U have 1s on",gauss elim
λ = 1 and λ = 9 are outside the squares. The eigenvectors are inside. This is different,pos_def_matrices
"20. If P is the projection onto the column space of A, what is the projection onto the left",orthogonality
square to write f as a sum of one or two squares d1(,pos_def_matrices
3 equal to I?,orthogonality
"it was at t = 0). Another plane is z = 0, which is also three-dimensional; it is the ordinary ",gauss elim
"optimal because it minimizes the cost function, and the minimum cost 6 is the value of",linear_prog
This is exactly like the vector inner product f Tg. It is still related to the length by,orthogonality
30 Jacobi steps: (.99)30 = .75.,computations
"edges and n nodes, A is m by n (and normally m > n). Its transpose is the “node-edge”",vector spaces
"ranked Tennessee fourth, with Miami signiﬁcantly lower.",vector spaces
5. Galerkin’s method starts with the differential equation (say −u′′ = f(x)) instead of,pos_def_matrices
"than that, the subspaces are orthogonal complements.",orthogonality
"What conditions on a, b, and c ensure that the quadratic f(x,y) = ax2 +2bxy+cy2",pos_def_matrices
(c) Complex λ’s with real part a > 0.,eigenvec_val
1 1 1 c,gauss elim
"(The example had r = .7, and it was still slow.) If r = 1, which means |λn−1| = |λn|, then",computations
"The old way, the vector in C2 with components (1,i) would have zero length: 12+i2 = 0,",eigenvec_val
to write the equations that would hold if a line could go through all three points. Then,orthogonality
mean when the f’s are currents into the nodes?,vector spaces
"Note 2. In the least-squares equation ATAx = ATb, the condition number c(ATA) is the",computations
0 0 0 0 0,vector spaces
"Proof. Every b in the column space is a combination Ax of the columns. In fact, b is",orthogonality
22. Compute the eigenvalues and eigenvectors of A and A2:,eigenvec_val
trouble without being parallel.,gauss elim
that some ordering of the rows of A leaves no zeros on the diagonal. (Don’t use P,determinants
(c) An m by n matrix has no more than n pivot variables.,vector spaces
"new. Now we cannot avoid them. A real matrix has real coefﬁcients in det(A−λI), but",eigenvec_val
symmetric A) is P = 1,gauss elim
"long cost vector [c 0] into [cB cN]. The stopping condition, which brought the simplex",linear_prog
"The dimension of the column space C(A) equals the rank r, which also",vector spaces
(b) QTAQ is symmetric positive deﬁnite.,pos_def_matrices
making six vectors in all.,vector spaces
and i+i3 = 0). But the three cube roots of 1 also add to zero.,orthogonality
"∥�x∥2 = ∥xr∥2 +∥xn∥2,",pos_def_matrices
"28. Apply Gram-Schmidt to (1,−1,0), (0,1,−1), and (1,0,−1), to ﬁnd an orthonormal",orthogonality
xTy = x1y1 +···+xnyn.,eigenvec_val
"cosines, that is a Fourier series. Each term is a projection onto a line—the line in func-",orthogonality
have to decide on a rule for addition. What is its dimension?,vector spaces
"an echelon matrix U or a reduced R, we will ﬁnd a basis for each of the subspaces",vector spaces
"The determinant should depend linearly on the ﬁrst row a11,a12,...,a1n.",determinants
These inequalities mean that roundoff error comes from two sources. One is the,computations
Linear algebra recognizes this P(x) as 1,pos_def_matrices
"11. With the last column removed from the preceding A, and with the numbers 1. 2, 2, 1",vector spaces
ellipse of all Ax,computations
inner products and cosines,orthogonality
for the corresponding Markov process,eigenvec_val
"1.2 By giving a basis, describe a two-dimensional subspace of R3 that contains none of",vector spaces
This is a question in sensitivity analysis. It allows us to squeeze extra information out,linear_prog
vectors as P in Problem 30. Find its eigenvalues.,eigenvec_val
"the new pair of basic variables, and ﬁnd the cost at the new corner.",linear_prog
1.2 The Geometry of Linear Equations,gauss elim
"the minus sign). That number C12 goes in row 2, column 1!",determinants
"after), then λ1 would multiply the entries in the ﬁrst row. We want λ1 to appear in the",eigenvec_val
sider a typical situation with rank r = 3. The echelon matrix U certainly has three,vector spaces
"cB. At regular intervals (maybe every 40 simplex steps), B−1 is recomputed and the E−1",linear_prog
"The feasible set is governed by these m equations and the n+m simple inequalities x ≥ 0,",linear_prog
have solutions—and what is one of the solutions?,gauss elim
(a) Every basis for S can be extended to a basis for R6 by adding one more vector.,vector spaces
"This triangular T must be diagonal, because it is also Hermitian when A = AH:",eigenvec_val
"A block of zeros is preventing a complete matching! The 2 by 3 submatrix in rows 3,",linear_prog
a fourth equation that leaves us with no solution.,gauss elim
Two orthogonal axes in R3,orthogonality
". On your computer, solve by elimination",computations
Linearity in row 1 gives |Bn| = |An|−|An−1| =,determinants
(b) The ﬁrst 3 rows of U are a basis for the row space of A—true or false?,vector spaces
U (after elimination) to the four spaces for A:,vector spaces
"explicitly form, and in actual computation should not form, these matrices L−1 and U−1.",gauss elim
"is our leading example of an orthogonal matrix. The columns have length 1, their inner",eigenvec_val
"same number of positive entries, negative entries, and zero entries.",pos_def_matrices
x2 −x3 = b2,orthogonality
(a) λ1 = 1 is an eigenvalue of A.,eigenvec_val
7. If A is triangular then detA is the product a11a22···ann of the diagonal entries. If,determinants
"(d) If A is orthogonal or unitary, then all |λi| = 1 are on the unit circle.",eigenvec_val
(i) The diet has x∗,linear_prog
A permutation matrix P has the same rows as the identity (in some order). There is,gauss elim
"to show, by systematically using these properties, how the determinant can be computed.",determinants
"f(x,y) has a minimum at a point where ∂F/∂x = ∂F/∂y = 0 with",pos_def_matrices
The Geometry: Movement Along Edges,linear_prog
"by a constant, and remains an eigenvector. We can multiply the columns of S by any",eigenvec_val
speciﬁc and manageable examples.,eigenvec_val
"the real case, we would write M = Q). Unless the eigenvectors of Λ are orthogonal, a",eigenvec_val
"b = (x,y,z) onto the x-y plane. Its projection is p = (x,y,0), and this is the sum of the",orthogonality
"the ellipse come from eigenvectors x of ATA, not of A.",computations
b j−1q j−1 +a jq j +b jq j+1 (with q0 = 0). Multiply by qT,computations
"adding to zero, and (1,1,1,1) would be in its nullspace.",vector spaces
i x j = 0,eigenvec_val
masses move together and the spring in the middle is never stretched (Figure 5.3a). The,eigenvec_val
Therefore every b is in C(A) for a nonsingular matrix.,vector spaces
"7. If w1, w2, w3 are independent vectors, show that the differences v1 = w2 −w3, v2 =",vector spaces
the diagonal and D is the diagonal matrix of pivots.,gauss elim
"27. Show, by trying for an M and failing, that no two of the three Jordan forms in equa-",eigenvec_val
1.4 Matrix Notation and Matrix Multiplication,gauss elim
the projections p1 and p2 and add p1 + p2. The projections do not add to b because,orthogonality
"du/dt = Au), and the goal is to understand how this switch to second derivatives alters",eigenvec_val
"dard basis (1,0) and (0,1) by the matrices",vector spaces
Problems 60–66 ask for matrices (if possible) with speciﬁc properties.,vector spaces
"matrices or functions x(t) and y(t). As long as the transformation satisﬁes equation (1),",vector spaces
terms control F near the stationary point:,pos_def_matrices
"hyperplane,” which has the vector b on one side and the whole cone on the other side.",linear_prog
inside California move out. We start with y0 people outside and z0 inside.,eigenvec_val
"We want to rewrite the three equations with three unknowns u, v, w in the simpliﬁed",gauss elim
led from A to U. The result is an upper triangular system Ux = c:,vector spaces
elimination) the conditions that make Ax = b solvable.,vector spaces
"Schmidt. If you need the eigenvalues of a large matrix, don’t use det(A−λI)!",computations
or vertical lines. That equals the maximum number of marriages.,linear_prog
"The voltage law allows us to assign potentials x1,...,xn to the nodes. Then the dif-",vector spaces
holds: If Ax = 0 then A(cx) = 0. Both requirements fail if the right-hand side is not zero!,vector spaces
4.3 Formulas for the Determinant,determinants
"inverses are equal: B = B(AC)(BA)C = C. Now, from the rank of a matrix, it is easy to",vector spaces
Figure 8.7: A network and a shortest spanning tree of length 23.,linear_prog
"Remark 4. Not all matrices possess n linearly independent eigenvectors, so not all ma-",eigenvec_val
"By themselves, the numbers m and n give an incomplete picture of the true size of a",vector spaces
7. Find the products FGH and HGF if (with upper triangular zeros omitted),gauss elim
"Pascal’s lower triangular A = abs(pascal(4,1)) and test inv(S) = inv(A’) ∗ inv(A).",gauss elim
"hardly even seems interesting. If it is there, it is practically impossible to miss. Nev-",vector spaces
Eigenvalues of AB (are equal to)(are not equal to) eigenvalues of A times eigenvalues,eigenvec_val
or x·y. We will use the name inner product and keep the notation xTy.,orthogonality
= x2 − 1,orthogonality
3. Solve Ay = b to ﬁnd U(x) = y1V1(x)+···+ynVn(x).,pos_def_matrices
inverse of each step adds back the multiple that was subtracted. These inverses come in,vector spaces
"is on the same line as the column a = (2,3,4).",orthogonality
7. c(x+y) = cx+cy.,vector spaces
5.4 Differential Equations and eAt,eigenvec_val
matrix can achieve both existence and uniqueness. Only a square matrix has a two-sided,vector spaces
"go further would make x2 negative, so the leaving variable is x2. The new corner has",linear_prog
x2) = (x3 −x1). Those differences are b1 +b3 = b2. To circle the lower loop and arrive,vector spaces
"The simplex method moves along edges of the feasible set, eventually reaching the opti-",linear_prog
The plane consists of all vectors perpendicular to a ﬁxed vector y. The angle between,linear_prog
Again this illustrates the Fundamental Theorem: The row space is perpendicular to the,vector spaces
logue of the stretching factor dx/du:,determinants
"and contribute nothing new. Equivalently, it is the plane of all vectors b that satisfy",vector spaces
If A is skew-symmetric (AT = −A) then eAt is an orthogonal matrix.,eigenvec_val
We are guaranteed a minimum.,pos_def_matrices
"back. For physical and mathematical reasons the exponentials are special, and we can",orthogonality
"average of opinions, and it sometimes becomes vague after the top dozen colleges. We",vector spaces
7 ) is in the column space. This vector,orthogonality
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Problems 27–36 are about area and volume by determinants.,determinants
9. Find a 1 by 3 matrix whose nullspace consists of all vectors in R3 such that x1 +,vector spaces
"differential equation, and any speciﬁc solution u(t) is some combination",eigenvec_val
Chapter 2 Vector Spaces,vector spaces
Chapter 2 Vector Spaces,vector spaces
3 and x = 2h = 2,pos_def_matrices
is �b = 1,orthogonality
Positive deﬁnite: xTAx > 0,eigenvec_val
The answer n+1 agrees with the product of pivots at the start of this section.,determinants
"the columns are independent, we solve Ax = 0. Spanning involves the column space,",vector spaces
15. Why is I the only symmetric positive deﬁnite matrix that has λmax = λmin = 1? Then,computations
for n = 1 to N −1,computations
"form a tree—a graph with no loops. Our graph has r = 3, and edges 1, 2, 4 form one",vector spaces
1 − x1x2 + x2,pos_def_matrices
2. Complete the square in P = 1,pos_def_matrices
5x1 +3x2 ≥ 7.,linear_prog
0 A0Q0 = QT,computations
"falls on the unit circle in the complex plane. As θ varies from 0 to 2π, this number eiθ",eigenvec_val
Maximize x61 subject to Ax = 0 and 0 ≤ xij ≤ cij.,linear_prog
is to ﬁnd the coefﬁcients of the basis vectors:,orthogonality
steps in solving Ax = λx:,eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
Find a special solution for each free variable and describe every solution to Ax = 0,vector spaces
"25. In Problem 24, the projection of b onto the plane of a1 and a2 will equal b. Find",orthogonality
you say that det(I +M) = detI +detM.,determinants
the real question is about the matrix that multiplies y:,eigenvec_val
0 0 1 0,eigenvec_val
3x + 2y =,gauss elim
"D’s are diagonal matrices with no zeros on the diagonal, then L1 = L2, D1 =",gauss elim
powers of x and produce the Legendre polynomials.,orthogonality
"full problem is not hard to explain, and not easy to solve.",linear_prog
"x2 +x3 ≥ 1, x1 ≥ 0, x2 ≥ 0, x3 ≥ 0.",linear_prog
"For any matrix M, compare JM with MK. If they are equal, show that M is not",eigenvec_val
"vulnerable. It is a blunder to accept .0001 as the ﬁrst pivot, and we must insist on a larger",computations
This gives the determinant of increasingly bigger matrices. At every step the determinant,determinants
"If the old guess xk happened to coincide with the true solution x, then the new guess xk+1",computations
tors are unchanged. By following those eigenvectors we will solve difference equations,eigenvec_val
b when the equation is written as λ1x2 + λ2y2 = 1? The ellipse 9x2 + 16y2 = 1 has,pos_def_matrices
"variance, gets a heavier weight.",orthogonality
The result is now a great way to get help from the dual problem in solving the primal,linear_prog
the nonzero rows of the reduced matrices R:,vector spaces
(ii) An upper triangular T that is unitary must be diagonal. Thus T = Λ.,eigenvec_val
52. Show that A2 = 0 is possible but ATA = 0 is not possible (unless A = zero matrix).,gauss elim
"To compute a coefﬁcient like b1, multiply both sides by the corresponding function sinx",orthogonality
(c) The column space of 2A equals the column space of A.,vector spaces
"equations. We reproduce the four values y = 2,4,6,8 when Fc = y:",orthogonality
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Figure 2.5: The four fundamental subspaces (lines) for the singular matrix A.,vector spaces
hypotenuse in the triangle OaQ. So the sine and cosine of α are,orthogonality
"As time goes on, the motion is “almost periodic.” If the ratio ω1/ω2 had been a",eigenvec_val
"xk = (1,λk,...,λ n−1",eigenvec_val
(a) A is invertible.,eigenvec_val
"are inconsistent, as Gaussian elimination will systematically discover.",gauss elim
Cn = 0 (odd n),determinants
"It is like ﬁnding a permutation matrix within the nonzero entries of A. On the graph, this",linear_prog
∥Ux∥2 = xHUHUx = ∥x∥2.,eigenvec_val
ωopt = 1+λmax = 1.75.,computations
"29. Without computing A, ﬁnd bases for the four fundamental subspaces:",vector spaces
"with ﬁve corners at (0,0,0,0) and the rows of I has what volume?",determinants
(b−Pb)TPc = bT(I −P)TPc = bT(P−P2)c = 0.,orthogonality
0 0 1 2,determinants
"is grounded, and the nth column of the original incidence matrix is removed. The re-",vector spaces
3. CTAC has orthogonal eigenvectors y j. So the eigenvectors of Ax = λMx have,pos_def_matrices
What kind of box is formed from the rows (or columns) of Q?,determinants
f = ax2 +2bxy+cy2 = a,pos_def_matrices
"would use the million numbers on the left sides correctly, but not efﬁciently. We will",gauss elim
why this produces the zero matrix. We are substituting the matrix A for the number,eigenvec_val
Note 4. Roundoff error also enters Ax = λx. What is the condition number of the eigen-,computations
equation says that AQ = QT where T is a,computations
A−1 is large—A is nearly singular. The change in x is especially large when δb points in,computations
"23. How many permutations of (1,2,3,4) are even and what are they? Extra credit:",determinants
"6. Suppose u1,...,un and v1,...,vn are orthonormal bases for Rn. Construct the matrix",pos_def_matrices
"You have to listen closely to distinguish that name from the phrase “A is Hermitian,”",eigenvec_val
Ax = c1Ax1 +···+cnAxn = c1λ1x1 +···+cnλnxn.,pos_def_matrices
a much simpler matrix to use in the iterative steps.,computations
cannot exceed the grocer’s price c j. That is the jth constraint in yA ≤ c. Working within,linear_prog
This can’t be negative since A is positive deﬁnite—and it is zero only if y−x = 0. At all,pos_def_matrices
In the last example all pivots were di = 1. In that case D = I. But that was very excep-,gauss elim
The left nullspace N(AT) has dimension m−r.,vector spaces
"31. If you know the average �x9 of 9 numbers b1,...,b9, how can you quickly ﬁnd the",orthogonality
"easy). The favorite for large sparse problems is the conjugate gradient method, which",linear_prog
n in equation (1),computations
"eAt from the series I +At +··· and write the solution eAtu(0) starting from y(0) = 3,",eigenvec_val
", whose entries are all √−1.",eigenvec_val
2. A triangular T that is normal must be diagonal! (See Problems 19–20 at the end of,eigenvec_val
same edges—although that will not always happen. Their total length is the same—and,linear_prog
and all lower triangular matrices? What is the largest subspace that is contained in,vector spaces
are orthogonal to the columns. it is much easier to check those one or two conditions,orthogonality
basis just about optimal: The vectors should have length 1. For an orthonormal basis,orthogonality
Find eigenvectors of A and PAP for λ = 11:,eigenvec_val
"enth spot above Tennessee (9-1-2). A few days after publication, packages",vector spaces
columns of A are only in,gauss elim
true effect of a matrix—what is happening inside the multiplication Ax. The nullspace,orthogonality
"Every term is a product of n = 3 entries aij, with each row and column represented once.",determinants
a matrix and a vector. The new deﬁnition should be consistent with that one. When,gauss elim
also the sums down the main diagonal and the four other parallel diagonals?,orthogonality
"x = (2,0,0,0,3). The cost is down to −9.",linear_prog
4. Dimension of a subspace (a number).,vector spaces
0 0 1 1,vector spaces
"This section began with 90° rotations, projections onto the x-axis, and reﬂections through",vector spaces
efﬁcient way to substitute the entries of an n by n matrix into the formula. What the,determinants
The determinant ad −bc must be positive.,eigenvec_val
any simple way to create zeros. That is the goal of the following paragraphs.,computations
"matrix (call that [A]), then the product rule 2V becomes extremely concise: [AB] =",vector spaces
The important number that is beginning to emerge (the true size) is the rank r. The,vector spaces
"the usual xy and xc, is a vector space. What is the “zero vector”?",vector spaces
3 and the multipliers are −1,orthogonality
combination of the columns. You will soon see something beautiful—that A takes its,vector spaces
step. That produces a real unitary U—an orthogonal matrix.,eigenvec_val
"beyond symmetry, as we certainly want to do, there will have to be a major change. This",computations
"columns with pivots. Every other column is a combination of those two. Furthermore,",vector spaces
transformation that doesn’t come from a matrix?,vector spaces
"Ax has m components. The rule of linearity is equally satisﬁed by rectangular matrices,",vector spaces
problem is to recognize a minimum point. This arises throughout science and engi-,pos_def_matrices
pivot = −8 →,gauss elim
", then B =",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
�x = 2b1 +3b2 +4b3,orthogonality
"We claim that U also has three independent columns, and no more, The columns have",vector spaces
the relative error ∥δx∥/∥x∥.,computations
space and nullspace of A and the solution to Ax = b:,vector spaces
produce any ﬁve-dimensional vector b. This is not at all special to the identity matrix.,vector spaces
Note that yTx is different from xTy; we have to watch the order of the vectors.,eigenvec_val
"lutions of the special form ceλtx, where λ is an eigenvalue of A and x is its eigenvec-",eigenvec_val
"an m-dimensional vector). Summations are simpler than writing everything out in full,",gauss elim
"still ride a horse). In case you switched to a car, think of a road going over a mountain",pos_def_matrices
"38. Spherical coordinates ρ, φ, θ give x = ρ sinφ cosθ, y = ρ sinφ sinθ, z = ρ cosφ.",determinants
"with respect to the standard basis v1 = (1,0), v2 = (0,1), and also with respect to",eigenvec_val
"13. In the graph above with 4 nodes and 6 edges, ﬁnd all 16 spanning trees.",vector spaces
Positive deﬁniteness brings this whole course together (in Chapter 6)!,gauss elim
will highlight the parts that look new:,pos_def_matrices
"For linear programming, the important alternatives come when the constraints are",linear_prog
"since the sum 0 + 0 is in this one-point space, and so are all multiples c0. This is the",vector spaces
ize them again—that is a single step of the method—the convergence ratio becomes,computations
"Cn = 1 (n = 4,8,...)",determinants
not invertible. An example will lead you to x.,gauss elim
MT for every M. Show that no matrix A will do it. To professors: Is this a linear,vector spaces
Computing (see www.wellesleycambridge.com). We mention that Ax = λMx,pos_def_matrices
1. Find the inverses (no special system required) of,gauss elim
T = T H,eigenvec_val
The most important special case is when r = 1. Then a + ib is eiθ = cosθ + isinθ. It,eigenvec_val
"amounts are C and D, then the Geiger counter readings would behave like the sum of",orthogonality
"a left-inverse. Solving AA−1 = I has at the same time solved A−1A = I, but why? A",gauss elim
Chapter 8 Linear Programming and Game Theory,linear_prog
"is consistent with the usual idea of the range, as the set of all possible values f(x); x is",vector spaces
Find the Jacobi matrix D−1(−L −U) and the Gauss-Seidel matrix (D + L)−1(−U),computations
u(t) = eλ2tx2 = e2t,eigenvec_val
"AS, which by matrix multiplication is Ay, then y must be an eigenvector: Ay = λ1y. The",eigenvec_val
This is sometimes called the revised simplex method to distinguish it from the oper-,linear_prog
also with inv(A) and A\I for the full matrix A.),gauss elim
0 0 2 0,vector spaces
3 is irrational. The,eigenvec_val
This is a linear equation for the unknown function u(x). Any combination C + Dx,gauss elim
(a) the parallel plane through the origin.,gauss elim
1.20 The n by n permutation matrices are an important example of a “group.” If you,gauss elim
"projects to (0,0) is the nullspace.",vector spaces
"To ﬁnd a basis for the column space C(A), we use what is already done for U. The",vector spaces
∥x∥2 = 12 +22 +32,orthogonality
"becomes appropriate, and xH is the row vector [x1 ··· xn].",eigenvec_val
"16. P0 is the plane through (0,0,0) parallel to the plane P in Problem 15. What is the",vector spaces
perpendicular line are projected onto the origin; that line is the nullspace of P. Points,vector spaces
"3If Gram thought of it ﬁrst, what was left for Schmidt?",orthogonality
"Look ﬁrst at r = [−1 1] in the bottom row. It has a negative entry in column 3, so the",linear_prog
"34. Check that e = b − p = (−1,3,−5,3) is perpendicular to both columns of A. What",orthogonality
"17. With a friction matrix F in the equation u′′ + Fu′ − Au = 0, substitute a pure expo-",eigenvec_val
0 0 1 0,orthogonality
some nonnegative vector x (other than x = 0). We are allowing inequality in Ax ≥ tx in,eigenvec_val
6.3 Singular Value Decomposition,pos_def_matrices
is automatically a square symmetric matrix:,gauss elim
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"We saw this change of order when the elimination matrices E, F, G were inverted to",gauss elim
Length ∥ f∥ of function,orthogonality
6. Describe the set of attainable right-hand sides b (in the column space) for,vector spaces
edible amount of egg. The condition xegg ≥ 0 is changed to xegg ≥ δ. How does this,linear_prog
j−1a j)q j−1 +∥A j∥q j = Q times column j of R.,orthogonality
"changes). By the law of inertia, A has the same number of positive eigenvalues as D.",pos_def_matrices
15. Find the SVD and the pseudoinverse VΣ+UT of,pos_def_matrices
"Fourier matrix F, and the whole technology of digital signal processing depends on it.",orthogonality
this term over to the left side:,eigenvec_val
"(c) Find b1 in S and b2 in S⊥ so that b1 +b2 = b = (1,1,1,1).",orthogonality
(ABx)Ty = (Bx)T(ATY) = xT(BTATy).,orthogonality
"The rules for multiplying, like (e2)(e3) = e5, continue to hold when the exponents iθ are",orthogonality
gular) for that d? Which d makes this system singular (no third pivot)?,gauss elim
"Find the eigenvalues of I +A, (IłA)−1, and (I − 1",eigenvec_val
The column space and left nullspace are closely related. The left nullspace contains,vector spaces
three columns lie in a plane. Then every combination is also in the plane (which goes,gauss elim
Scientiﬁc Computing. (See www.wellesleycambridge.com.),vector spaces
detA = ±detU = ±d1d2···dn.,determinants
would be easy to answer. We would have a scalar instead of a vector equation:,eigenvec_val
10. Compare the pivots in direct elimination to those with partial pivoting for,gauss elim
and the determinant depends linearly on each individual column. The proof is just to,determinants
w ≥ 0. We now have equality constraints and nonnegativity.,linear_prog
P. Multiply PF to ﬁnd the eigenvalues λ0 to λ3:,orthogonality
as it is computed. A typical step is,computations
"cx = (cx1,cx2), which of the eight conditions are not satisﬁed?",vector spaces
Now work with the 3 by 3 submatrix in the lower right-hand corner. It has a unit,eigenvec_val
vector its components in the directions that are already settled. That idea is used over,orthogonality
was formed from all solutions to Ax = 0.,orthogonality
"This is much more stable, to subtract the projections one at a time.",orthogonality
in general uk+1 = Auk. Each step is a matrix-vector multiplication. After k steps it,computations
6 +C + 1,orthogonality
What is the nullspace matrix N of special solutions? What is its shape?,vector spaces
. One particular vector in that nullspace is,orthogonality
"California started with all 90 million people out, it ended with 60 million out and 30",eigenvec_val
"29. When you multiply a Hermitian matrix by a real number c, is cA still Hermitian? If",eigenvec_val
total capacity across the minimal cut.,linear_prog
Young’s equation (7) is exactly our 2 by 2 example! The best ω makes the two roots λ,computations
all λ > 0,eigenvec_val
"If Ax = 0 has more unknowns than equations (n > m), it has at least one",vector spaces
"2. Find a permutation P of the columns of F that produces FP = F (n by n), Combine",orthogonality
1 AU1 starts in the right form:,eigenvec_val
transpose: D = DT. We only have to show that detP = detPT.,determinants
"ATy = 0, and it is written N(AT). Its dimension is",vector spaces
LDLT. This symmetry of A reﬂects the symmetry of d2u/dx2. An odd derivative,gauss elim
1. The columns of U are left singular vectors (unit eigenvectors of,pos_def_matrices
"1. Compute the determinant of A − λI. With λ subtracted along the diagonal, this",eigenvec_val
(ii) It equals its transpose: PT = P.,orthogonality
"chosen to be orthonormal. For most matrices that is not true, and for rectangular matrices",pos_def_matrices
"40. The triangle with corners (0,0), (6,0), and (1,4) has area",determinants
(III′) detA = 0 and smaller determinants are positive.,pos_def_matrices
20. Construct the projection matrices P1 and P2 onto the lines through the a’s in Problem,orthogonality
Every stage needs 1,orthogonality
"that every component of x is nonnegative (the vector inequality x ≥ 0), this adds n more",linear_prog
"regarding it as half of a parallelogram, explain why its area equals",determinants
"without forcing their lengths to equal one. Then square roots enter only at the end, when",orthogonality
0 0 0 0,eigenvec_val
15. Find the PA = LDU factorizations (and check them) for,gauss elim
require that the determinant of A is positive. If a = c = −1 and b = 0. then detA = 1,pos_def_matrices
"These determinants give the volumes—or areas, since we are in two dimensions—drawn",determinants
both λ > 0,eigenvec_val
What we are really doing is splitting the determinant into the following sum:,determinants
"(n − 1)-dimensional plane in n dimensions, The second plane intersects it (we hope) in ",gauss elim
"convolution rule), than to multiply directly by C?",eigenvec_val
"14. If B is square, show that A = B+BT is always symmetric and K = B−BT is always",gauss elim
there is a y ≤ 0 with yA ≥ 0 and yb < 0.,linear_prog
minus what it saves. Computing r is called pricing out the variables. If the direct cost,linear_prog
"last n columns give an m by n matrix N. The cost vector c splits into [cB cN], and the",linear_prog
"4. Show that the following problem is feasible but unbounded, so it has no optimal",linear_prog
B by A and B!,determinants
"unit for each barrel-mile, what linear program with ﬁve equality constraints must be",linear_prog
"imum, but they can prevent it from being a global minimum. In our example the term",pos_def_matrices
"can’t—except for the zero vector, which is useless. But there must be eigenvalues, and",eigenvec_val
"example to the halfspace x+2y ≤ 4, keeping x ≥ 0 and y ≥ 0, we get the small triangle",linear_prog
"5. If the inverse of A2 is B, show that the inverse of A is AB. (Thus A is invertible",gauss elim
No zeros on the diagonal,vector spaces
"the pivot equation are multiplied by ℓ, and then subtracted from another equation.",gauss elim
solve the least-squares problem Ax = b.,orthogonality
2. (For Marriage Matrices) The 1s in the matrix can be covered by three horizontal,linear_prog
"2], compute ATB, BTA, ABT, and BAT.",gauss elim
determinant would this give? Construct A.,eigenvec_val
(b) A and AT have the same free variables.,vector spaces
What is the alternative if b lies outside the cone? Figure 8.4 also shows a “separating,linear_prog
"and integrate from 0 to 2π. (The function f(x) is given on that interval.) In other words,",orthogonality
1Invertibility is perhaps in second place as an important property.,vector spaces
the line 3x+y =,orthogonality
v − w = 2,gauss elim
m = n. The number of vectors is the same.,vector spaces
"nentials). If A can be diagonalized, the powers of A = SΛS−1 are easy: Ak = SΛkS−1. In",eigenvec_val
Thus Ak is positive deﬁnite. Its eigenvalues (not the same λ1!) must be positive. Its,pos_def_matrices
Notice how the formula for (AB)T resembles the one for (AB)−1. In both cases we,gauss elim
"ti determinants of hilb(1),hilb(2),...,hilb(10). Hilbert matrices are hard to work",determinants
"23. (a) If you know x is an eigenvector, the way to ﬁnd λ is to",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
together they satisfy QP − PQ = I. The uncertainty principle follows directly from the,eigenvec_val
actions table” of consumption and production for each one. The theory also reaches,eigenvec_val
"� ViVjdx, and solve the eigenvalue problem Ax = λMx.",pos_def_matrices
two exponentials (and not like a straight line):,orthogonality
"linear algebra, in a systematic way.",orthogonality
must make that choice.,linear_prog
"In addition to unequal reliability, the observations may not be independent. If the",orthogonality
"This description involves nothing more than a restatement of Ax = b, by columns:",vector spaces
"addition and scalar multiplication. Those operations follow the rules of the host space,",vector spaces
3. The Gram-Schmidt process and its interpretation as a new factorization A = QR.,orthogonality
"called a linear combination, and this combination solves our equation:",gauss elim
"5.19 If K is a skew-symmetric matrix, show that Q = (I −K)(I +K)−1 is an orthogonal",eigenvec_val
"Otherwise, if Ax ≥ tmaxx is not an equality, multiply by A. Because A is positive, that",eigenvec_val
ogy. It counts zero-dimensional nodes minus one-dimensional edges plus two-dimensional,vector spaces
y = 0. Hint: What is the rank?,vector spaces
showing that the ﬁrst derivatives are zero at that point).,pos_def_matrices
comes ﬁrst. Please check that A−1 would be U−1GFE.,gauss elim
Ellipsoids in n Dimensions,pos_def_matrices
"Every entry of A contributes to xHAx. Try the 2 by 2 case with x = (u,v):",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
but z = 6 will disappear. That is conﬁrmed by solving the normal equations:,orthogonality
from the origin to x+y. The triangle inequality asserts that this distance cannot,orthogonality
0]. The row space is the line through [1 0 0]T. It,vector spaces
length squared of column j of R is a j j. Use determinant = volume.),pos_def_matrices
trouble; we need a second solution after e3t. The matrix equation is,eigenvec_val
xij ≤ cij. The ﬂows are nonnegative: xij ≥ 0 going with the arrows. By maximizing the,linear_prog
"3 − 1 ≈ .73. In the fastest mode, the components of x2 = (1 +",pos_def_matrices
Chapter 2 Vector Spaces,vector spaces
"26. If A and B have the same λ’s with the same full set of independent eigenvectors, their",eigenvec_val
matrix. If you keep the basis vectors in order but change,vector spaces
"(b) This equation is M−1FM = G, so F and G have the same 10 eigenvalues. F has",eigenvec_val
3 −2x1x2 −2x1x3 +2x2x3,pos_def_matrices
direct use of the determinant formula for 1000 equations would be a total disaster. It,gauss elim
"implies existence, when the matrix is square. The condition for invertibility is full rank:",vector spaces
1. Factor (from A ﬁnd its factors L and U).,gauss elim
matrices satisfy KT = −K. Their properties follow immediately from their close link to,eigenvec_val
"than iterative, but unlike elimination, it can be stopped part way. And needless to say,",computations
"pick out the columns that contain the pivots, they also are linearly independent. In our",vector spaces
Since Λ1Λ2 = Λ2Λ1 (diagonal matrices always commute) we have AB = BA.,eigenvec_val
The main point of this section is S−1AS = A. The eigenvector matrix S converts A into,eigenvec_val
"plant can assemble a Chevrolet in 1 minute, a Buick in 2 minutes, and a Cadillac in 3",linear_prog
"y) = A−1x + A−1y? If A is represented by the matrix M, explain why A−1 is repre-",vector spaces
�� has E31b =,gauss elim
6. The true solution to Ax = b is slightly different from the elimination solution to,computations
"detAB/detB coincides with detA, which is our product formula.",determinants
on the same list.,gauss elim
"(b) Find A−1, which has the same form as A.",gauss elim
5. Find two points on the line of intersection of the three planes t = 0 and z = 0 and,gauss elim
Sum of eighth roots,orthogonality
"Since the v’s form a basis, they must span the space. Every w j can be written as a",vector spaces
replaces edge 2 by a new edge “1 minus 2”: That elimination step destroys an edge and,vector spaces
"reﬂected across the 45° line. It was split into (2,2) + (2,−2) and the two parts were",vector spaces
"the angle, but the cosine of the angle, that is directly related to inner products. We look",orthogonality
"even oil exploration). The signals are transformed by the matrix F, and later they can be",orthogonality
"In the example, x2 = 0 because the second food is too expensive. Its price exceeds the",linear_prog
the result onto the x-axis? What matrix represents projection onto the x-axis followed,vector spaces
change more rows. There are n! = (n)(n−1)···(1) permutations of size n. Row 1 has,gauss elim
"61. Construct a matrix whose nullspace consists of all multiples of (4,3,2,1).",vector spaces
concentrations will become nearly equal (typical for diffusion) as t → ∞.,eigenvec_val
"1. The eigenvalues for Ax = λMx are real, because CTAC is symmetric.",pos_def_matrices
"this product is always p(A) = zero matrix, even if A is not diagonalizable.",eigenvec_val
if du/dt = Au then u(t) = eAtu(0) = MeJtM−1u(0).,eigenvec_val
i x j = 0,eigenvec_val
The particular solution in equation (4) comes from solving the equation with all free,vector spaces
"If the vectors x and y are feasible and cx = yb, then x and y are optimal.",linear_prog
"The second row of A−λI is (1,−λ). To get (A−λI)x = 0, the eigenvector is x = (λ,1),",eigenvec_val
The nullspace of this A is the one-dimensional space of constants: da0/dt = 0. The,vector spaces
0 0 0 0,vector spaces
13. (a) In what range of a and b is the following equation a Markov process?,eigenvec_val
This is an initial-value problem. The unknown is speciﬁed at time t = 0 by the given,eigenvec_val
"the column vectors. The whole idea can be seen in that ﬁgure, where 2 times column",gauss elim
solution can easily become worthless. We will devote two pages (entirely optional in,gauss elim
Problems 38–42 are about spaces in which the “vectors” are functions.,vector spaces
"48. Suppose we have two bases v1,...,vn and w1,...,wn for Rn. If a vector has coefﬁ-",vector spaces
"the geometry, computationally through the simplex method, or algebraically through",linear_prog
"converted to a minimum principle, and it only remains to integrate by parts:",pos_def_matrices
Example 2. The columns of the matrix,vector spaces
point of the function f = xTAx.,pos_def_matrices
5.28 (a) For which numbers c and d does A have real eigenvalues and orthogonal eigen-,eigenvec_val
aii(xi)k+1 = aii(xi)k +ω[(−ai1x1 −···−aii−1xi−1)k+1 +(−aiixi −···−ainxn)k +bi].,computations
the matrix shorthand of equation (8). Start from ATy = 0.,orthogonality
"1. For the 3-node triangular graph in the ﬁgure following, write the 3 by 3 incidence",vector spaces
and computing c =,orthogonality
it gives a chance to see rectangular matrices in action—and how the square symmetric,vector spaces
8. Write down the dimensions of the four fundamental subspaces for this 6 by 4 inci-,vector spaces
of ± signs for cofactors does not give detP.,determinants
"Without having made any special choice of the perturbation, there was a relatively large",computations
using any computer code for linear equations. Then change an entry of b by .0001,gauss elim
"2. Which “famous” inequality gives ∥(A+B)x∥ ≤ ∥Ax∥+∥Bx∥, and why does it follow",computations
F(x) = F(0)+xT(grad F)+ 1,pos_def_matrices
4. The Markov matrix A =,computations
"parallel moment, Ax = λx (twice in the second ﬁgure).",eigenvec_val
term. One reasonable deﬁnition is that the symmetric part 1,pos_def_matrices
The “Perron-Frobenius theorem” gives the key properties of a positive matrix—not,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
22. From the formula ACT = (detA)I show that detC = (detA)n−1.,determinants
"FFT—the fast transform. The great application in signal processing is ﬁltering, and the",orthogonality
"2. If we have a basis for the subspace S, is there a formula for the projection p?",orthogonality
"27. What matrix E31 subtracts 7 times row 1 from row 3? To reverse that step, R31 should",gauss elim
− 3w = 5.,gauss elim
then we send only the corresponding 20 columns of U and V. The other 980 columns,pos_def_matrices
Which rule for matrix multiplication makes this row 3 of L times U?,gauss elim
Chapter 8 Linear Programming and Game Theory,linear_prog
when the time step gets shorter.,eigenvec_val
The decisive matrix S−1T has eigenvalues ±1,computations
with row 1 and column 1 removed:,determinants
pivots in columns 1 and 3. Therefore its free variables are the second and fourth v and y.,vector spaces
This law can only be satisﬁed if the total current from outside is f1 + f2 + f3 +,vector spaces
Problems 56–60 are about symmetric matrices and their factorizations.,gauss elim
"31. If c is not an eigenvalue of A, substitute u = ectv and ﬁnd v to solve du/dt = Au −",eigenvec_val
There will be 24 permutation matrices of order n = 4. There are only two permutation,gauss elim
1. The column space of A is denoted by C(A). Its dimension is the rank r.,vector spaces
"has n eigenvalues, This is a fact that follows from the determinant formula, and not from",determinants
"number ci can be the length of edge i, or its capacity, or its stiffness (if it contains a",vector spaces
30. (Recommended) Execute the six steps following equation (6) to ﬁnd the column,vector spaces
"the four components of the matrix-vector product Cx, which is the convolution of",eigenvec_val
"exchanges in [L, U] = lu(pascal(5)) spoil Pascal’s pattern!",gauss elim
0 0 0 0,vector spaces
"Then ﬁnd a second pair of orthonormal eigenvectors x1, x2 for λ = 0.",eigenvec_val
c1 = 0. The only combination to produce the zero vector is the trivial combination. The,vector spaces
"1The number of operations is bounded by powers of m and n, as in elimination. For integer programming and",linear_prog
"of columns produces b (say Ax = b), then multiplying that combination by c will",vector spaces
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
"Eliminating y1, y2, y3, y4 leaves three equations ATCAx = −f for x1, x2, x3. Solve",vector spaces
j = 0 when food j is priced above its vitamin equivalent.,linear_prog
a|x1|2 +2Rebx1x2 +c|x2|2 = a|x1 +(b/a)x2|2 +,pos_def_matrices
"1, then the diet cost (from druggist or grocer) will go up by y∗",linear_prog
"of one column and one unknown x, the matrix now has n columns. The number m of",orthogonality
2 0 1 2,determinants
"28. If the product M = ABC of three square matrices is invertible, then A, B, C are",gauss elim
"You can guess what happened. The old methods came back, with a new idea and a",pos_def_matrices
different way by Problem 11 in Section 3.1:,linear_prog
"numbers underneath it, to ﬁnd out the right multipliers.",gauss elim
"The second plane is 4u − 6v = −2. It is drawn vertically, because w can take any ",gauss elim
is stable. It has λ < 0. Comment on this craziness.,eigenvec_val
of the ﬁrst equation to produce zeros below the ﬁrst pivot. Then the second column is,gauss elim
"These second derivatives 4, 4, 2 contain the answer. Since they are the same for F and",pos_def_matrices
also gives a great example of the four fundamental subspaces:,orthogonality
Chapter 8 Linear Programming and Game Theory,linear_prog
"almost unbelievable (and computing A3 requires twice as many, as far as we can see).",gauss elim
(b) Rows of A times matrix B.,gauss elim
"Ax = b by the transpose matrix, to give ATA�x = ATb. Now the normal equations are",orthogonality
8. The methane molecule CH4 is arranged as if the carbon atom were at the center of a,orthogonality
reiθ times Reiα has absolute value rR and angle θ +α.,eigenvec_val
5. With h = 1,gauss elim
"the second row from the third, we arrive at U:",vector spaces
"of A is independent, then so are the corresponding columns of U, and vice versa.",vector spaces
1I am convinced that plants and people also develop in accordance with minimum principles. Perhaps civilization,pos_def_matrices
rows—the equations in the system Ax = b. What does that mean about potential,vector spaces
"4, x = 1",gauss elim
�� → H1AH(1) =,computations
u = Cx. Then rule 2V applies AB to u and reaches (AB)Cx.,vector spaces
"goes gradually toward a triangular form, and the eigenvalues gradually appear on the",eigenvec_val
"1. After reaching Rx = 0, identify the pivot variables and free variables.",vector spaces
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
3) are the columns of,orthogonality
Thus EG ̸= GE. A random example would show the same thing—most matrices don’t,gauss elim
by the number detA (if it is not zero!) gives A−1 = CT/detA.,determinants
u (eigenvalues i and −i) goes around in a circle:,eigenvec_val
"The vector x must be an eigenvector of A, exactly as before. The corresponding eigen-",eigenvec_val
after each simplex step you see it repeat several times before the simplex method ﬁnds a,linear_prog
"f is like a vector with a whole continuum of components, the values of sinx along the",orthogonality
solutions to Ax = 0 form a vector space—the nullspace of A.,vector spaces
"attainable right-hand sides b, but also with the solutions x that attain them. The right-",vector spaces
(b) Check by subtracting column 1 from the other columns and recomputing.,determinants
"by n, or 5 by 4. It comes from applying integration to each basis vector of V:",vector spaces
1.21 What is the rank of the n by n matrix with every entry equal to 1? How about the,vector spaces
"39. A is 3 by 5, B is 5 by 3, C is 5 by 1, and D is 3 by 1. All entries are 1. Which of these",gauss elim
from the second row down—and row 1 of A is a linear combination of the ﬁrst rows of,determinants
tions appear when the nullspace contains more than the zero vector and/or the column,vector spaces
"−1, 2, −1 matrix",determinants
1v1 ̸= 0 and therefore c1 = 0. The same is true of every ci.,orthogonality
) and A−1 = (,eigenvec_val
saddle. The graph of z = −x2 − y2 is a bowl opening downward. What is a test on,pos_def_matrices
"solution: Maximize x+y, subject to x ≥ 0, y ≥ 0, −3x+2y ≤ −1, x−y ≤ 2.",linear_prog
"(a) If V is orthogonal to W, then V⊥ is orthogonal to W⊥.",orthogonality
The value of this determinant is J = r. It is the r in the cylindrical volume element,determinants
row.) Later stages are faster because the equations are shorter.,gauss elim
contained in the nullspace of A. (Also the row space of A is in the left nullspace of,vector spaces
"are ±σi, the singular values of A. Hint: Try",computations
(b) Complete the square for xHAx. Now xH = [x1 x2] can be complex,pos_def_matrices
from completing the square to 5(u+ 4,pos_def_matrices
Show that the columns are orthogonal.,orthogonality
The order is right for the ℓ’s to fall into position. This always happens! Note that,gauss elim
are below zero then the grounded team ranks ﬁrst.) The dimension of the nullspace is,vector spaces
(a) A multiplies a vector x with n components?,gauss elim
"for n = 50,100,200,400? (And what does “Inf” mean in MATLAB?)",determinants
Do the original numbers lie inside or outside the unit circle?,eigenvec_val
The L and U factors of a tridiagonal matrix are bidiagonal. The three factors together,gauss elim
The pivots were the numbers outside the squares. To see how that happens for symmetric,pos_def_matrices
For proof we just insert V TV = I into the middle of the SVD:,pos_def_matrices
1.22 (a) If A is invertible what is the inverse of AT?,gauss elim
(a) the third column of AB?,gauss elim
"vector matrix S is triangular, then S−1 is triangular and A is triangular.",eigenvec_val
since no matrices are multiplying on the right to spoil the zeros. The ﬁrst step in,computations
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
11. Write a 2 by 2 system Ax = b with many solutions xn but no solution xp. (Therefore,vector spaces
that projects onto the line perpendicular to a.,orthogonality
minant of the lower right corner M11.,determinants
"is an orthogonal matrix (because QTQ =VUTUV T = I). In the complex case, S becomes",pos_def_matrices
"The unknowns are x and y. I want to describe two ways, elimination and determinants,",gauss elim
"Except for very special cases, uk will approach the corresponding eigenvector4. In",eigenvec_val
"C11 comes from erasing row 1 and column 1, which leaves the −1, 2, −1 pattern:",determinants
other. A square matrix has a left-inverse if and only if it has a right-inverse. There is,vector spaces
"3. Find the SVD from the eigenvectors v1, v2 of ATA and Avi = σiui:",pos_def_matrices
(b) What single matrix L has the same effect as these three reverse steps? Add row,gauss elim
"matrices of order 2, namely",gauss elim
The pivot for the second stage of elimination is −8. We now ignore the ﬁrst equation.,gauss elim
2. The space of 3 by 2 matrices. In this case the “vectors” are matrices! We can add,vector spaces
"nation gives zero, and check entries to prove each term is zero.) The ﬁve permuta-",vector spaces
points) and Figure 3.9a is in two dimensions (or n dimensions if there are n parameters).,orthogonality
"32. If the columns of a 4 by 4 matrix have lengths L1, L2, L3, L4, what is the largest",determinants
"columns of A that correspond, in U, to the columns containing pivots.",vector spaces
"zero vector in this space, the vector 1",vector spaces
"column of zeros to produce a column of I. In a less extreme case, suppose elimination",gauss elim
7.2 Matrix Norm and Condition Number,computations
"the proof. Since Ax = 0 if and only if Ux = 0, the ﬁrst, fourth, and sixth columns of A—",vector spaces
"sided inverses! In the opposite order, AintAdiff = I cannot be true. The 5 by 5 product has",vector spaces
"many entries are nonzero, this method is a mistake. Therefore we ask whether there is",computations
a basis. Therefore Cn is a complex vector space of dimension n.,eigenvec_val
"34. Following the method of Problem 33, reduce A to echelon form and look at zero",vector spaces
29. Suppose A is an m by n matrix of rank r. Its reduced echelon form is R. Describe,vector spaces
2Please forgive this enthusiasm: I know the method may not be immortal.,pos_def_matrices
"P has the same effect on b, exchanging b1 and b2. The new system is PAx = Pb. The",gauss elim
on A: Step 1,gauss elim
"Since λ1 ̸= λ2 and x1 ̸= 0, we are forced into c1 = 0. Similarly c2 = 0, and the two",eigenvec_val
xTAx = (xTQ)Λ(QTx) = yTΛy = λ1y2,pos_def_matrices
"2,0,...,0). The center is at the origin, because if x satisﬁes xTAx = 1,",pos_def_matrices
That minimum length solution will be called x+. It is our preferred choice as the best,pos_def_matrices
"(a) Show that detA = 0 if a+d = 0. Solve for u, v, w, z, the entries of D.",determinants
The four rows of A are a basis for the row space of A—true or false?,vector spaces
"(e) The columns add up to a column of 0s, the rows add to a row of 1s.",orthogonality
"is q1, the denominator is 1, and the projection is (qT",orthogonality
I can’t see this clearly in three or four dimensions.,linear_prog
"with ∥b∥ = 1, how large can ∥x∥ be? If b has roundoff error less than 10−16, how",computations
are sufﬁcient for the forward elimination and back-substitution that produce x =U−1c =,gauss elim
"24. For which vectors (b1,b2,b3) do these systems have a solution?",vector spaces
The vector that A ampliﬁes the most is the corresponding eigenvector of ATA:,computations
"y3 = t2, y4 = t3, y5 = t4, spanning the polynomials of degree 4. The matrix A will be m",vector spaces
"(e) the “reverse-triangular” matrix that results from row exchanges,",determinants
"maining capacities and decide whether the sink is cut off from the source, or additional",linear_prog
2x + 3y = 0,linear_prog
"unknown, then solving for the next to last, and eventually for the ﬁrst.",gauss elim
invertible. Find a formula for B−1 that involves M−1 and A and C.,gauss elim
the correction term is zero.,pos_def_matrices
"(ii) the portfolio’s average quality must be no lower than municipals, so x ≥ z.",linear_prog
"by A, and after k steps there are k multiplications:",eigenvec_val
"20. For F1(x,y) = 1",pos_def_matrices
"between them like at least two men, and so on, to p = n.",linear_prog
"T, B, and J, which are triangular, the eigenvalues are on the diagonal. We want to show",eigenvec_val
of length w acts on w − 1 rows below. Elimination on the n columns of a band matrix,gauss elim
2.2 Solving Ax = 0 and Ax = b,vector spaces
intersect. What can go wrong? One possibility is that two planes may be parallel. The,gauss elim
a1x1 +···+apxp = b1Cy1 +···+bqCyq,pos_def_matrices
"vertible. That is the real action of A. It is partly hidden by nullspaces and left nullspaces,",vector spaces
p = A(ATA)−1ATb = AA−1(AT)−1ATb = b.,orthogonality
"It makes an angle θ with the horizontal. The whole plane enters in Chapter 5, where",orthogonality
"(c) 0 or ∞, depending on b.",vector spaces
value of each product. That is a new set of prices which the system uses for the next,eigenvec_val
"equations for x, y, z, t are you solving?",gauss elim
differences around a loop?,vector spaces
"Then (AB)c = c1Ab1 +···+cnAbn, equals A(c1b1 +···+cnbn) = A(Bc).",gauss elim
The uncertainty left by these two arbitrary constants C and D is removed by a “boundary,gauss elim
"In minimization, our big application is least squares. The best �x is the vector that mini-",pos_def_matrices
"side is real by Property 1, and the right-hand side xHx = ∥x∥2 is real and positive, because",eigenvec_val
(It is comparable to the Fast Fourier Transform.) Before that came the iterative methods,computations
Step 1. Subtract 2 times the ﬁrst equation from the second;,gauss elim
for every vector x.,orthogonality
the smallest eigenvalue λ1. R(x) reaches that minimum at the ﬁrst eigenvector,pos_def_matrices
33. Show by a picture how a rectangle with area x1y2 minus a rectangle with area x2y1,determinants
chosen to make the difference between −u′′ and f orthogonal to every Vj:,pos_def_matrices
25. What is the closest straight line to the parabola y = x2 over −1 ≤ x ≤ 1?,orthogonality
DLT. Show directly from equation (12) that the condition number of c(R) is,computations
13. Change the 2s to 3s and ﬁnd the eigenvalues of S−1T for both methods:,computations
"and then equation (10) deﬁnes the determinants of 2 by 2 matrices, 3 by 3 matrices,",determinants
(b) All vectors whose components add to zero.,vector spaces
Chapter 1 compared the equations Ax = b and Ax′ = b′:,computations
"solution that matches u(0) = (3,1). What is the steady state as t → ∞? (This is a",eigenvec_val
3. Two lines in the plane are perpendicular when the product of their slopes is −1.,orthogonality
The rank always satisﬁes r ≤ m and also r ≤ n. An m by n matrix cannot have more,vector spaces
"F′′ = 0. In two dimensions, a very important possibility still remains: The combination",pos_def_matrices
"“big formula” with n! terms, and a formula “by induction”. In Section 4.4 the determi-",determinants
2nlogn. There is an amaz-,orthogonality
"In this example all four subspaces are lines. That is an accident, coming from r = 1 and",vector spaces
zeros to appear. The main point is to see the analogies with the positive deﬁnite case.,pos_def_matrices
What other properties are shared by A and CTAC? The answer is given by Sylvester’s,pos_def_matrices
corner if it also satisﬁes the m remaining inequality constraints. Otherwise it is not even,linear_prog
"part is producing oscillations, but the amplitude comes from the real part.",eigenvec_val
Chapter 2 Vector Spaces,vector spaces
allowed one nonzero diagonal below the main diagonal (Hessenberg form). Therefore,computations
Note 4. (Important) Suppose there is a nonzero vector x such that Ax = 0. Then A,gauss elim
"Now we are ready for the serious step, to project b onto a subspace—rather than just",orthogonality
It remains to ﬁnd an R For which A = RTR. We have almost done this twice already:,pos_def_matrices
"(b) Check that ∥column 1∥2 = 2, ∥B∥2 = 3",orthogonality
8.2 The Simplex Method,linear_prog
"of the components becomes impossible. This summation is replaced, in a natural and",orthogonality
"(b) A 5 by 7 matrix never has linearly independent columns,",vector spaces
(b) Add columns 1 + 2 + (3 + 3),gauss elim
"Then Hx = −σz = (−σ,0,...,0). The vector Hx ends in zeros as desired.",computations
c jl = ∑,gauss elim
"century. In engineering problems like plate bending, or physics problems like the ground",pos_def_matrices
Which previous problem justiﬁes the middle step?,orthogonality
3 and x = 1.,pos_def_matrices
"6, to get a 5 by 5 matrix A:",gauss elim
The Feasible Set and the Cost Function,linear_prog
"Remark 2. Since QT = Q−1, we also have QQT = I. When Q comes before QT, mul-",orthogonality
one quadratic by another one:,pos_def_matrices
(b) (Much harder!) If you put 1s and 0s at random into the entries of a 10 by 10,gauss elim
The constant bTb raises the whole graph—this has no effect on the best �x. The other,pos_def_matrices
A major use of the polar decomposition is in continuum,pos_def_matrices
These subspaces cannot be orthogonal.,orthogonality
"m), that geometry did earlier:",orthogonality
"spanned by (0,0,0,1), what is S⊥? What is (S⊥)⊥?",orthogonality
Elimination in a Nutshell: PA = LU,gauss elim
∂x = 4x+4y = 0,pos_def_matrices
means four edges with no nodes in common. The maximal ﬂow is less than 4 exactly,linear_prog
44. Find a 3 by 3 permutation matrix with P3 = I (but not P = I). Find a 4 by 4 permu-,gauss elim
This shows the fundamental operation on A:,pos_def_matrices
There is another law of trigonometry that leads directly to the same result. It is not so,orthogonality
Cramer’s rule: The jth component of x = A−1b is the ratio,determinants
"27. Find an orthonormal basis for the subspace spanned by a1 = (1,−1,0,0), a2 =",orthogonality
holds. Why was y ≥ 0 needed in equation (1) but not here? This weak duality can be,linear_prog
(in a few theoretical paragraphs) that the pivot test succeeds.,gauss elim
"If Q (square or rectangular) has orthonormal columns, then QTQ = I:",orthogonality
"cosωt = 1, leaving only",eigenvec_val
1. Linear independence or dependence.,vector spaces
Eigenvalues of A+B (are equal to)(are not equal to) eigenvalues of A plus eigenval-,eigenvec_val
0 ∗ ∗ ∗,computations
"(d) If A or any power of A has all positive entries, these other |λi| are below 1.",eigenvec_val
0 1 0 0,vector spaces
Every eigenvector x of A corresponds to an eigenvector M−1x of B.,eigenvec_val
itive deﬁnite square root. Why does R have positive eigenvalues? Compute R and,pos_def_matrices
"32. Cofactors of those 1, 3, 1 matrices give Sn = 3Sn−1 − Sn−2. Challenge: Show that",determinants
to the voltage drop ei:,vector spaces
(b) Describe the ﬁve types of subspaces of R4.,vector spaces
of S is y. Then the ﬁrst column of SΛ is λ1y. If this is to agree with the ﬁrst column of,eigenvec_val
"the matrix, a column at a time.",vector spaces
steady state x > 0,eigenvec_val
"For a 5 by 8 matrix with four pivots, Figure 2.3 shows the reduced form R. It still",vector spaces
b reduces to two triangular systems. This is the practical equivalent of the calculation,gauss elim
is in the group; and the law P1(P2P3) = (P1P2)P3 is true—because it is true for all,gauss elim
"equations in n unknowns, there are n planes in the row picture. There are n vectors in",gauss elim
(b) the conjugate of a number on the unit circle?,eigenvec_val
"orthogonal columns. Take t1 = −3, t2 = 0, and t3 = 3. Then the attempt to ﬁt y = C+Dt",orthogonality
Chapter 1 Matrices and Gaussian Elimination,gauss elim
column operations (for the ﬁrst time?) to make the whole third column zero. By sub-,gauss elim
"divisions and multiplication-subtractions that produce L, D, and U (without assuming a",gauss elim
We need Kirchhoff’s Voltage Law and Current Law to complete the framework:,vector spaces
2 satisﬁes the rule. The combination Fk = (λ k,eigenvec_val
"or else inﬁnitely many, and a full set of pivots cannot be found.",gauss elim
"4. Update B, B−1, or LU, and the solution xB = B−1b. Return to step 1.",linear_prog
A and f as Rayleigh-Ritz for symmetric problems.,pos_def_matrices
exchanges is even or odd:,determinants
Example 3. The 4 by 4 second difference matrix A4 has only two nonzeros in row 1:,determinants
Invertible = Nonsingular (n pivots),gauss elim
2u − 2v + 7w = 7.,gauss elim
from distinct eigenvalues are automatically independent.,eigenvec_val
"work. It does not allow us to multiply every pair of matrices. If they are square, they",gauss elim
is probably the most important ﬁgure in the book (Figure 3.4).,orthogonality
"v2 = (1,4)? The columns of M come from expressing V1 and V2 as combinations",eigenvec_val
0 0 0 0,eigenvec_val
"38. If AB = 0, the columns of B are in the nullspace of A. If those vectors are in Rn,",vector spaces
"steps is an upper triangular R, but the matrix that records the steps is not a lower",computations
"The columns of U (m by m) are eigenvectors of AAT, and the columns of V (n",pos_def_matrices
the other components. Of course if x is an eigenvector then so is 7x and so is −x. All,eigenvec_val
We want to study the behavior of uk+1 = Auk as k → ∞. Assuming that A can be,eigenvec_val
Note 1. The inverse exists if and only if elimination produces n pivots (row exchanges,gauss elim
"are m rows—and the index j goes from 1 to n. Altogether the matrix has mn entries, and",gauss elim
"If we add two matrices and then transpose, the result is the same as ﬁrst transposing",gauss elim
"standard eigenvalue problem Ay = λy. But our matrix M will be tridiagonal, because",pos_def_matrices
"points b that are not on a line by points p that are! Unable to solve Ax = b, we solve",orthogonality
changes the constraint and the cost:,linear_prog
"(e) If we exchange rows 1 and 2 of A, and then exchange columns 1 and 2, the",eigenvec_val
found in the corresponding column of R (with sign reversed).,vector spaces
matrices the pivots still give a simple test for positive deﬁniteness: xTAx stays positive,pos_def_matrices
yiy j. The components b j are,pos_def_matrices
what is the weighted average that replaces equation (9)? It is the best estimate when,orthogonality
"Note that we did not start with the m rows of A, which span the row space, and discard",vector spaces
Therefore the Gram-Schmidt process can begin by accepting v1 = 1 and v2 = x as the,orthogonality
eigenvalues always agrees with the sum of the diagonal entries (the trace of L):,computations
At the optimal ω the two eigenvalues are equal. They must both equal ω − 1 so their,computations
"5. Compute ATA and AAT, and their eigenvalues and unit eigenvectors, for",pos_def_matrices
matrix W so that the weighted length ∥Wu(t)∥ is always decreasing. If there exists,eigenvec_val
same number of zero eigenvalues.,pos_def_matrices
intersecting at the solution to ten equations—but somehow this is almost possible.,gauss elim
"To go from A to Q−1AQ, there are two main possibilities: We can produce one zero at",computations
3. The Volume of a Box. The connection between the determinant and the volume is,determinants
"A1 is positive deﬁnite, so F1 is concave up (= convex). Find the minimum point of",pos_def_matrices
"can now complete Chapter 3, by choosing a “best” (shortest) �x for every Ax = b.",pos_def_matrices
0 0 2 4,vector spaces
"maximization—anyway, both problems get solved at once.",linear_prog
"of the ﬁttest, and we want to describe some ideas that have survived so far. They fall",computations
"connection to P2 = P, which means that λ 2 = λ.)",eigenvec_val
"For y∗ = cBB−1, the ﬁrst half is an equality and the second half is cBB−1N ≤ cN. This is",linear_prog
"In the 3 by 3 case, I often set a component of x equal to 1 and solve (A−λI)x = 0 for",eigenvec_val
"19. Find the 3 by 3 matrix A and its pivots, rank, eigenvalues, and determinant:",pos_def_matrices
"19. In conjugate gradients, show that r1 is orthogonal to r0 (orthogonal residuals), and",computations
"There is nothing exceptional about λ = 0. Like every other number, zero might be",eigenvec_val
"A. The matrix that gives p is a projection matrix, denoted by P:",orthogonality
needed to create one unit of product i:,eigenvec_val
"whenever i = j,",orthogonality
The minimum over a subspace of trial functions is at the y nearest to A−1b. (That,pos_def_matrices
det(A−λI) = λ 2 −1.7λ +.7,eigenvec_val
"show that, with only these two constraints, there will be only two kinds of cars in the",linear_prog
"the original. A reﬂection is its own inverse, H = H−1, which is clear from the",vector spaces
1.6 (a) There are sixteen 2 by 2 matrices whose entries are 1s and 0s. How many are,gauss elim
. Find the eigenvectors of A−1. What does the inverse power,computations
"r > .9, which means that more than 20 iterations are needed to achieve one more digit.",computations
shares the fundamental ideas of linear algebra. The most far-reaching of those ideas is,linear_prog
35. An n-dimensional cube has how many corners? How many edges? How many (n−,determinants
"exchanges produce a full set of pivots. Chapter 2 admits the singular case, and limps",gauss elim
"cost is 2x∗ +3y∗ = 6. The vector (0,2) is feasible because it lies in the feasible set, it is",linear_prog
row to produce zeros in the second column:,linear_prog
"Elimination is a perfect algorithm, except",computations
"3.19 If v1,...,vn is an orthonormal basis for Rn, show that v1vT",orthogonality
The vectors v and w are orthogonal when their inner product is zero:,orthogonality
(U−1AU)H = UHAH(U−1)H = U−1AU.,eigenvec_val
"in the nullspace and the row space, which makes it orthogonal to itself. Therefore it is",orthogonality
exactly the reduced row echelon form of RT (not AT).,vector spaces
the intersection of the second plane with the ﬁrst. That intersection is a l ine. In three ,gauss elim
"Find two vectors in the nullspace of A, and the complete solution to Ax = b.",vector spaces
0 0 1 0,determinants
A similar rule holds with three or more matrices:,gauss elim
Fortunately for that college (from which I am writing these words) the graph is not,vector spaces
the arrows cannot be reversed. The ﬂow on the two edges into the sink cannot exceed,linear_prog
transpose the matrix and work with the rows.,determinants
"developed by Dantzig as a systematic way to solve linear programs, and either by luck",linear_prog
"Vectors with xTy = 0 are orthogonal. Now we allow inner products that are not zero,",orthogonality
"n choices, then row 2 has n−1 choices, and ﬁnally the last row has only one choice. We",gauss elim
to ﬁnd three of these:,eigenvec_val
"to approach zero. The difference ∆u can be forward, backward, or centered:",gauss elim
entering variable. The ratios of 2,linear_prog
"16. Find all vectors that are perpendicular to (1,4,4,1) and (2,9,8,2).",orthogonality
"(b) If this A is 2 by 2, and not I or −I, ﬁnd its trace and determinant.",eigenvec_val
ﬁrst equation. Altogether the right-hand side is responsible for n2 operations—much,gauss elim
rows of A and solving Ax = 0.,orthogonality
"must come from the eigenvector (1,−1). If the experiment admits only nonnegative",eigenvec_val
(d) Columns of A times rows of B.,gauss elim
all ﬁrst derivatives are zero. A is the “second derivative matrix” with entries aij =,pos_def_matrices
add row 1 to row 2 and multiply row 1 by −1 to reach B. Which rules show the,determinants
"(1,2,3), by taking these to be the rows of A and solving Ax = 0. Remember that the",orthogonality
"(a) λ 2 is an eigenvalue of A2, as in Problem 22.",eigenvec_val
"i2 = −1, i3 = −i, and i4 = 1. The other eighth roots are the powers w2",orthogonality
The ﬁrst step of the FFT changes multiplication by Fn to two multiplications by Fm.,orthogonality
"approach to n2 − n is this: All n2 entries need to be changed, except the n in the ﬁrst",gauss elim
3 by 3 zero matrix,determinants
"18. Show that a unitary matrix has |detU| = 1, but possibly detU is different from",eigenvec_val
"Since λ1 = λ2, those ﬁrst two projections x1xT",eigenvec_val
Is this graph a tree? (Are the rows of A independent?) Show that removing the last,vector spaces
"To describe this phase, rewrite Ax ≥ b in a form completely parallel to the n simple",linear_prog
and Py are orthogonal.,orthogonality
2vk+1 = wk +b1,computations
10. The transpose of A has the same determinant as A itself: detAT = detA.,determinants
compute AB and BA and A−1 and B−1 and (AB)−1.,gauss elim
(a) An invertible matrix can’t be similar to a singular matrix.,eigenvec_val
leaves a line. Finally a fourth plane leaves a single point. It is the intersection point of 4 ,gauss elim
13. Write down the ﬁve conditions for a 3 by 3 matrix to be negative deﬁnite (−A is,pos_def_matrices
Figure 3.4 summarizes the fundamental theorem of linear algebra. It illustrates the,orthogonality
2. Solve (from L and U and b ﬁnd the solution x).,gauss elim
ing rule for the overall permutation of c’s before entering the FFT: Write the subscripts,orthogonality
change from .99 to .75 that revolutionized the solution of Ax = b.,computations
exponential is in the solution to the corresponding differential equation:,eigenvec_val
In economics the difference equation is backward! Instead of u1 = Au0 we have u0 =,eigenvec_val
45. Find and check the inverses (assuming they exist) of these block matrices:,gauss elim
1 4 10 19,determinants
6.5 The Finite Element Method,pos_def_matrices
detK (always). Deduce that the determinant must be zero.,determinants
"the geometry of many-dimensional polyhedra, bad feasible sets are rare and the simplex",linear_prog
There is no solution unless b2 = 2b1. The column space of A contains only,vector spaces
The solution to this equation is the one thing you need to know:,eigenvec_val
but until recently no one knew how to solve it. Dozens of algorithms have been sug-,computations
for some nonsingular C.,pos_def_matrices
1 1 1 2,determinants
(a) Transpose RTAR to show its symmetry. What shape is this matrix?,gauss elim
"10. Conﬁrm the last exercise: If V1 = m11v1 + m21v2 and V2 = m12v1 + m22v2, and",eigenvec_val
ducing the velocity y′ as another unknown:,eigenvec_val
and indeﬁnite A =,pos_def_matrices
edge produces a spanning tree. Then the remaining rows are a basis for,vector spaces
Proof. Suppose the diagonal entries are nonzero. Then elimination can remove all the,determinants
column of BA is certain to be zero. The speciﬁc right-inverse C = AT(AAT)−1 chooses,vector spaces
"heart of the simplex method, and it can be organized in three different ways:",linear_prog
"V1 = (1,1), V2 = (1,−1). Show that those matrices are similar.",eigenvec_val
(b) If the third equation has zero as its second coefﬁcient (it contains 0v) then no,gauss elim
"(b) Solve du/dt = Au starting from u(0) = (100,100).",eigenvec_val
"This list is really a brief history of the simplex method, In some ways, the most",linear_prog
of the space spanned by the v’s.,vector spaces
"set, the maximum problem would have no solution! The cost can go arbitrarily high and",linear_prog
"A has positive eigenvalues, by our test. But we know that λmin is smaller than 2, because",pos_def_matrices
1 AU1 has the required ﬁrst column. At the second,computations
where f is negative.,pos_def_matrices
2. Solve to ﬁnd a combination of the columns that equals b:,gauss elim
matrix A is associated with P2?,pos_def_matrices
tor from deleting row i and column j of A goes into row j and column i of CT. Dividing,determinants
"in Introduction to Applied Mathematics, and also in my new book Applied Mathematics",pos_def_matrices
We know how a matrix moves subspaces around when we multiply by A. The nullspace,vector spaces
Notice that the ﬁrst part is not the false statement det(B+C) = detB+detC. You cannot,determinants
"There is more than one way to study duality. We quickly proved yb ≤ cx, and then",linear_prog
consistent. To obtain a ranking we can use least squares: Make Ax as close as possible,vector spaces
What are v and w at t = 1?,eigenvec_val
"very likely that RTR = RRT. Equality can happen, but it’s not normal.",gauss elim
The minimum length least-squares solution is x+ = A+b = VΣ+UTb.,pos_def_matrices
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"the ellipsoid. In the y variables we can see that it is an ellipsoid, because the equation",pos_def_matrices
1 1 1 1,eigenvec_val
0 0 0 4,vector spaces
The ith component of Ax is,gauss elim
whether the current goes with or against the arrow. The combinations of y1 and y2 ﬁll,vector spaces
"The identity matrix is symmetric, I2 = I, and the error b−Ib is zero.",orthogonality
13. Choose sinθ and cosθ to make P21AP−1,computations
"i , which are the average of (error in bi)2.",orthogonality
"v(t) = 3e−t +5e2t,",eigenvec_val
v + 2w =,vector spaces
"The nth power is at the angle nθ. When n = −1, the reciprocal 1/w has angle −θ. If",orthogonality
"choice for the last column v. All but one column will be used by that time, when we",determinants
"For this, the determinant gives a conclusive test.",eigenvec_val
"For an invertible matrix, the nullspace contains only x = 0 (multiply Ax = 0 by A−1).",vector spaces
"(a) What properties can be guaranteed for the corresponding unit eigenvectors u, v,",eigenvec_val
ﬁt perfectly or exact potentials cannot be found. Second difﬁculty: If A has nonzero,vector spaces
E). The new second component −12 appeared after the ﬁrst elimination step.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"In the differential equation, this produces the special solutions u = eλtx. They are the",eigenvec_val
That is both beautiful and practical. It is beautiful because it brings together (for,pos_def_matrices
"might be hard to decide which rows to keep and which to discard, so it was easier just to",vector spaces
"8. Compute the determinants of A2, A3, A4. Can you predict An?",determinants
"(e) carry out the same three rotations, but each one through 180°?",vector spaces
The computed u is completely mistaken. B is well-conditioned but elimination is vio-,gauss elim
"can be solved. Give two more right-hand sides in addition to b = (2,5,6) for which",gauss elim
"Chapter 2 studies the general case, where there may be many solutions or none. In",gauss elim
"be foolish to make x3 positive, because its cost coefﬁcient is +7 and we are trying to",linear_prog
"19. In Problem 17, ﬁnd the projection matrix P = aaT/aTa onto the line through each",orthogonality
"of columns with weights −3, 1, 0, 0 gives a column of zeros.",vector spaces
I am not entirely happy with the 3 by 4 matrix in the previous section. From a theoretical,vector spaces
result is always a tie. We will ﬁnd a similar “minimax theorem” in game theory. The,linear_prog
"The matrix W TW appears in the middle. In this new sense, the projection A�xW and the",orthogonality
an example in which it doesn’t. Why is Ax = b solvable exactly when the column,vector spaces
"5. Find all solutions to the equation eix = −1, and all solutions to eiθ = i.",orthogonality
Differential Equations and eAt,eigenvec_val
20. Let S be a subspace of Rn. Explain what (S⊥)⊥ = S means and why it is true.,orthogonality
"�x = 0, which is a more “symmetric” choice than any other number.",orthogonality
30. Explain why this Vandermonde determinant contains x3 but not x4 or x5:,determinants
points lie at the three “corners” of this set?,linear_prog
Those statements are not yet proved for unitary (including orthogonal) matrices.,eigenvec_val
"When row i of QT multiplies column j of Q, the result is qT",orthogonality
10x + 9y = 11.,gauss elim
2.2 Solving Ax = 0 and Ax = b,vector spaces
The general solution is a combination of pure exponential solutions. These are so-,eigenvec_val
"to solve these equations. Certainly x and y are determined by the numbers 1, 2, 3, 4, 5,",gauss elim
11. Prove again that detQ = 1 or −1 using only the Product rule. If |detQ| > 1 then,determinants
Multiply xk by A to ﬁnd the corresponding eigenvalue αk. Verify that in the 3 by 3,computations
"The algorithm is almost magically simple. It starts with A0, factors it by Gram-Schmidt",computations
"Orthogonal matrices are crucial to numerical linear algebra, because they introduce",orthogonality
and 0 and −1 and 1 lie in the “nonpivot columns” of R. Reverse their signs to ﬁnd the,vector spaces
"reorder 4, 1, 2, 3 into 1, 2, 3, 4? Is |P2| = +1 or −1?",determinants
step s that keeps e−sPc nonnegative.,linear_prog
"(c) The matrices that have (1,1,1) in their nullspace form a subspace.",vector spaces
"20. If N is normal, show that ∥Nx∥ = ∥NHx∥ for every vector x. Deduce that the ith row",eigenvec_val
11. Give examples of A and B such that,gauss elim
Those two λ’s add up to the trace; Exercise 9 gives ∑λi = trace for all matrices.,eigenvec_val
(c) For any A = MJM−1: Show that AT is similar to JT and so to J and to A.,eigenvec_val
"That basis is not unique (it never is), but some choice is necessary and this is the most",vector spaces
of unity. The two square roots of unity are 1 and −1. The fourth roots are the square,orthogonality
15. Which particular subspace S2 in Problem 13 gives the minimum value λ2? In other,pos_def_matrices
"work: Either S contains a positive vector x > 0, or S⊥ contains a nonzero y ≥ 0. When S",linear_prog
trying to get in line with the natural frequencies of the market. The eigenvalues are the,eigenvec_val
"row space into its column space, and on those spaces of dimension r it is 100 percent in-",vector spaces
Which corner do we go to next? We want to move along an edge to an adjacent,linear_prog
n = det(AAT) = (detA)(detAT) = (detA)2.,determinants
Schwarz inequality (Qx)T(Px) ≤ ∥Qx∥∥Px∥ of Section 3.2:,eigenvec_val
Example 5. Suppose E is the same but G adds row 2 to row 3. Now the order makes a,gauss elim
B. Compare the times for inv(B) and B\I. Backslash is engineered to use the zeros,gauss elim
"of these n! terms, and that sum is the explicit formula we are after:",determinants
11. Show that an orthogonal matrix that is upper triangular must be diagonal.,orthogonality
λi and eigenvectors xi.,eigenvec_val
5.24 A variation on the Fourier matrix is the “sine matrix”:,eigenvec_val
"Simple, I admit. If you move up to 2 by 2, it’s more interesting. The matrix",vector spaces
"This is the only case when we can take apart (ATA)−1, and write it as A−1(AT)−1. When",orthogonality
Iterative Methods for Ax = b,computations
one plane? Those are the components of ATy.,gauss elim
types of quantities appear in our example:,gauss elim
(b) Row space contains,orthogonality
0 0 · 1,orthogonality
differential equation that has useful applications.,eigenvec_val
This law of diffusion exactly matches our example du/dt = Au:,eigenvec_val
a−ib. It has three important properties:,eigenvec_val
"Reversing the signs, −Ax gives the drop in potential. Part of that drop may be due to a",vector spaces
"2, y = 0 (and also through (2,3) and all intermediate points). The second",gauss elim
"with sides (2,2) and (1,3). Find those areas from 2 by 2 determinants and say why",determinants
"(a) (1,2,0) and (0,1,−1).",vector spaces
factorizations of A into lower triangular times diagonal times upper triangular. (LT is,gauss elim
space—each edge in the tree corresponds to a row in the basis.,vector spaces
"Symmetric Products RTR, RRT, and LDLT",gauss elim
The condition x ≥ 0 restricts x to the positive quadrant in n-dimensional space. In,linear_prog
"column space, and a vector z orthogonal to the nullspace:",orthogonality
exchanging row i with row 1. Remember to delete row i and column j of A for Mij:,determinants
"(ii) If we multiply any vector x in the subspace by any scalar c, cx is in the subspace.",vector spaces
"At the initial time t = 0, u equals u(0) because e0 = 1. The derivative of eat has the",eigenvec_val
"3.30 Is there a matrix whose row space contains (1,1,0) and whose nullspace contains",orthogonality
"is not in general true. If λ is an eigenvalue of A and µ is an eigenvalue of B, here is the",eigenvec_val
"When a zero arises in the pivot position, exchange that equation for the one below it",gauss elim
gle Gauss-Seidel step is worth two Jacobi steps. Since both methods require the same,computations
(b) Those vectors (are)(are not)(might be) linearly independent.,vector spaces
2u + 3v − 4w =,gauss elim
1 1 1 0,determinants
"With x5 entering the basis, x1 or x2 must leave. In the ﬁrst equation, increase x5 and",linear_prog
By developing the Markov ideas we can ﬁnd a small gold mine (entirely optional) of,eigenvec_val
19. The powers Ak of this matrix A approaches a limit as k → ∞:,eigenvec_val
To ﬁnd detD we patiently apply rule 3. Factoring out a11 and then a22 and ﬁnally ann,determinants
"y1,...,yq of CTAC corresponding to eigenvalues µi < 0.",pos_def_matrices
"Therefore, we “pivot” by substituting x5 = 1",linear_prog
"equations, integral equations, and difference equations. Each frequency component goes",orthogonality
Vectors x and y are still added component by component. Scalar multiplication cx is,eigenvec_val
"6. What 5 by 5 system replaces (6) if the boundary conditions are changed to u(0) = 1,",gauss elim
There are two more things to be done with this example. One is to complete the,eigenvec_val
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"between them. The teams are the nodes, and the games are the edges. There are a few",vector spaces
p ≥ 0 and zTAz = µ1b2,pos_def_matrices
"We can always reach this echelon form U, with zeros below the pivots:",vector spaces
b = Mc? Start from,vector spaces
P−1 = PT =,orthogonality
"This follows from rule 2, since if the equal rows are exchanged, the determinant is sup-",determinants
"The largest |λi| will eventually be dominant, so the spectral radius ρ = |λmax| will govern",computations
"necessary, suppose that x1,...,xm are the basic (nonzero) variables at the current corner.",linear_prog
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
� (V ′)2dx and M =,pos_def_matrices
"2xTAx−xTb, and knows immediately that Ax = b",pos_def_matrices
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
creating ℓ21 = 1. Then suppose it exchanges rows 2 and 3. If that exchange is done in,gauss elim
"transaction over and over, the price tends to equilibrium.",eigenvec_val
"symmetric matrices A, orthogonal matrices Q, all exponentials etA of a ﬁxed matrix",pos_def_matrices
"Looking at a and ac−b2, it is even possible to spot the appearance of the pivots. They",pos_def_matrices
R2 it is a quarter of the plane; it is an eighth of R3. A random vector has one chance,linear_prog
"The length of this eigenvector Avj is σ j, because",pos_def_matrices
"3. In Problem 2, show that xk = (sinkπh,sin2kπh,...,sinnkπh) is an eigenvector of A.",computations
feels no effect from row,gauss elim
"29. Construct a 3 by 3 matrix whose column space contains (1,1,0) and (1,0,1) but not",vector spaces
explained in Section 8.3. Here we stay with the diet problem and try to interpret its dual.,linear_prog
the column spaces give rank(AB) ≤ rank(A).,vector spaces
avoid adding 10 numbers. What coefﬁcient of �x9 correctly gives �x10?,orthogonality
The diagonal matrix U−1AU represents a key theorem in linear algebra.,eigenvec_val
to the jth man:,linear_prog
4u + av = 2.,gauss elim
are the only possibility for breakdown. But three planes in three dimensions can be in,gauss elim
(I −A)−1 = I +A+A2 +A3 +··· .,eigenvec_val
"1.23 By experiment with n = 2 and n = 3, ﬁnd",gauss elim
[1 2 4]. It is true that those rows are,gauss elim
The right-hand side b is the column vector of “inhomogeneous terms.” The left-hand,gauss elim
(a) λ1 < 0 and λ2 > 0.,eigenvec_val
Minimum where Ax1 = λx1,pos_def_matrices
"1.11 If E is 2 by 2 and it adds the ﬁrst equation to the second, what are E2 and E8 and",gauss elim
"for, the intersection has dimension zero. It is a point, it lies on all the planes, and its ",gauss elim
"completely undetermined. All values of x give the same error E = ∥0x − b∥, so E2 is",orthogonality
"(b) If A has independent rows, its right-inverse AT(AAT)−1 is A+.",pos_def_matrices
7. (a) What 3 by 3 symmetric matrices A1 and A2 correspond to f1 and f2?,pos_def_matrices
"43. If P is the plane of vectors in R4 satisfying x1 + x2 + x3 + x4 = 0, write a basis for",orthogonality
TT H = U−1NUUHNHU = U−1NNHU = U−1NHNU = UHNHUU−1NU = T HT.,eigenvec_val
This result is a little too high because of the zeros in the e j. Forward elimination,gauss elim
"The proof is to see that this corner can be settled ﬁrst, before even looking at other",determinants
"Changing to −uxx −uyy = f leads to the “ﬁve-point scheme.” The entries −1, 2, −1 in",computations
seeds chose an almost unbelievable ratio of F12/F13 = 144/233.2,eigenvec_val
exponential solutions eλitxi are now the pure powers λ k,eigenvec_val
"(e) If a square matrix A has independent columns, so does A2.",vector spaces
"eigenvectors, and that is a key question in quantum mechanics:",eigenvec_val
"vectors, and sketch the ellipse.",pos_def_matrices
Matrices of Rank 1,vector spaces
contain the “semipositive” vectors [1 0] and [0 1]. This slightly weaker alternative does,linear_prog
"The ﬁrst has rank 1, although roundoff error will probably produce a second pivot. Both",pos_def_matrices
6I Rayleigh’s Principle: The minimum value of the Rayleigh quotient is,pos_def_matrices
vectors are independent; only the trivial combination gives zero.,eigenvec_val
for every b if and only if the columns are linearly independent. Then A has an,vector spaces
0 1 2 2,determinants
"Λ). When A was symmetric, we wrote Q instead of S, choosing the eigenvectors to be",eigenvec_val
to be c = 0.,vector spaces
"5. For any symmetric matrix A, compute the ratio R(x) for the special choice x =",pos_def_matrices
if every week 1,eigenvec_val
projection p = A�x is so fundamental that we do it in two ways:,orthogonality
The diagonal entries are multiples of i (allowing zero). The eigenvalues are 8i and −i.,eigenvec_val
question is so important that it has many answers. See the last page of the book!,gauss elim
"(2,1,4,3) → (1,2,4,3) → (1,4,2,3) → (1,4,3,2) → (1,3,4,2) → (3,1,4,2).",determinants
(b) The number of columns minus the total number of rows.,vector spaces
"The small pivot .0001 brought instability, and the remedy is clear—exchange rows.",gauss elim
"That heat equation is approached by dividing the pipe into smaller and smaller segments,",eigenvec_val
A transformation need not go from Rn to the same space Rn. It is absolutely permitted,vector spaces
is a free variable. Find the special solution for this variable.,vector spaces
0 0 0 2,determinants
The corner is optimal when r = cN − cBB−1N ≥ 0. Its cost is cBB−1b.,linear_prog
"The smallest subspace Z contains only one vector, the zero vector. It is a “zero-",vector spaces
expense of one extra formula) by switching from oscillating exponentials to the more,eigenvec_val
corner (or at −∞ if the cost turns out to be unbounded). What is remarkable is the speed,linear_prog
"invertible. When m = n and all matrices are square, Q becomes an orthogonal",orthogonality
need a unitary matrix such that U−1AU is diagonal. Schur’s lemma has just found it.,eigenvec_val
identity matrix and 0 is the 3 by 2 zero matrix?,vector spaces
= λ 2 +1 = 0,eigenvec_val
"solutions. (There always are, if there are more unknowns than equations, n > m.) The",vector spaces
8.4 is about problems (like marriage) in which the solution is an integer. Section 8.5,linear_prog
"also the Gauss-Seidel matrix −(D+L)−1U and its eigenvalues λi, and decide whether",computations
2. The squared length of x is ∥x∥2 = xHx = |x1|2 +···+|xn|2.,eigenvec_val
1.21 Describe the rows of DA and the columns of AD if D = [2 0,gauss elim
"9b) with smaller and smaller barriers, given by the size of θ.",linear_prog
29. (Recommended) Compute L and U for the symmetric matrix,gauss elim
basis. Then compute B−1u and show from its sign that you will never meet another,linear_prog
"not bad to see these inverses now, before the next section. The ﬁnal problem is to undo",gauss elim
2aijxix j. Then f = a11x2,pos_def_matrices
no shortage of eigenvectors.,eigenvec_val
"the vectors v1,...,vk are linearly independent. If any c’s are nonzero, the v’s",vector spaces
The worst case is when ∥δx∥ is large—with δb in the direction of the eigenvector,computations
36. Follow the 3 by 3 text example but with plus signs in A. Eliminate above and below,gauss elim
"8. Minimize 2x1 +x2, subject to x1 +x2 ≥ 4, x1 +3x2 ≥ 12, x1 −x2 ≥ 0, x ≥ 0.",linear_prog
even if it is not exactly zero. Choosing a larger pivot reduces the roundoff error.,gauss elim
"Does either F(x,y) or f(x,y) have a minimum at the point x = y = 0?",pos_def_matrices
"����� = ad −bc,",determinants
The components of x are the inner products qT,orthogonality
7.2 Matrix Norm and Condition Number,computations
(a) E21 subtracts 5 times row 1 from row 2.,gauss elim
"already in the subspace, and P(Pb) is still Pb. In other words P2 = P. Two or three or",orthogonality
"An anti-diagonal P, with P13 = P22 = P31 = I, takes the x-y-z axes into the z-y-x axes—",orthogonality
x + 4y − 2z =,gauss elim
"(a) If B is formed from A by exchanging two rows, then B is similar to A.",eigenvec_val
Chapter 8 Linear Programming and Game Theory,linear_prog
"that PPT = I. (The 1 in the ﬁrst row of P matches the 1 in the ﬁrst column of PT, and",determinants
"8. In the feasible set for the General Motors problem, the nonnegativity x,y,z ≥ 0 leaves",linear_prog
"Every row is a multiple of the ﬁrst row, so the row space is one-dimensional. In fact, we",vector spaces
"separately). Show that p + q = n, so the number p of positive λ’s equals the",pos_def_matrices
"willing to try. Thus row i gives the choices of the ith woman, and column j corresponds",linear_prog
the basis vectors. The rest is determined by linearity. The requirement (1) for two vectors,vector spaces
"11. If A is m by n and B is n by m, explain why",determinants
"a = (a11, a12)",determinants
Example 1. Minimize P(x) = x2,pos_def_matrices
The sum of potential differences around a loop must be zero.,vector spaces
row space of A,pos_def_matrices
0 0 2 2 6,vector spaces
"The projection of b = (4,5,6) is p = (4,5,0)—the x and y components stay the same",orthogonality
One Linear System = Two Triangular Systems,gauss elim
"special points w, all of them on the unit circle, in order to solve wn = 1.",orthogonality
"This is solvable when b1, b2, b3 are in the ratio 2:3:4. The solution x will exist only if b",orthogonality
Chapter 1 Matrices and Gaussian Elimination,gauss elim
gence with S = A and T = 0; the ﬁrst and only step of the iteration would be Ax1 = b. In,computations
2 and we can produce anything.,eigenvec_val
δλk = ykExk +terms of order ∥E∥2.,computations
vectors of A and AT are not the same.,eigenvec_val
solutions—because the three parallel planes move over to become the same.,gauss elim
second. (Not twice the second row to the ﬁrst!) The result of doing both the subtraction,gauss elim
"dx2 +u = x,",gauss elim
"Remark 3. For the generalized problem Ax = λMx, the same principles hold if M is",pos_def_matrices
3. Then the expected error is zero and the variance is 2,orthogonality
"4x4 + x2y + y2 and F2(x,y) = x3 + xy − x, ﬁnd the second derivative",pos_def_matrices
"The ﬁrst equation gives u = 0, and the second equation then forces v = 0. The nullspace",vector spaces
multiplies by E31. The important question is: What happens to A on the left-,gauss elim
The difﬁculty is clear; no multiple of the ﬁrst equation will remove the coefﬁcient 3.,gauss elim
(d) the eigenvalues of (B+I)−1.,eigenvec_val
"with ω > 1, the method is known as successive overrelaxation (SOR). The optimal",computations
To repeat: We cannot invert the separate parts AT and A when those matrices are rectan-,orthogonality
positive vector x1 to a negative vector x1/(1 − λ1). In that case (I − A)−1 is deﬁnitely,eigenvec_val
at 0℃ and with a heat source f(x).,gauss elim
15. Suppose A is symmetric positive deﬁnite and Q is an orthogonal matrix. True or,pos_def_matrices
0 0 0 0,vector spaces
B as well as A. The proof with repeated eigenvalues is a little longer.,eigenvec_val
"19. True or false: If the n columns of S (eigenvectors of A) are independent, then",eigenvec_val
left nullspace of the new matrix.,vector spaces
The numbers ℓij ﬁt right into the matrix L that takes U back to A.,gauss elim
"since the third equation is 0 = b3. That error cannot be reduced, but the errors in the ﬁrst",pos_def_matrices
need to be modiﬁed for complex numbers. The new deﬁnitions coincide with the old,eigenvec_val
"which AC = I. For m < n, a right inverse is not a left inverse.",vector spaces
dimensional subspace; an example is the set of vectors in R6 whose ﬁrst and last com-,vector spaces
3. The column space of A is the plane containing all combinations of the pivot columns,vector spaces
"are present, it will have no solution. A has two columns, and x = (C,D):",orthogonality
Any 5 by 5 matrix that is nonsingular will have the whole of R5 as its column space.,vector spaces
matrix similar to J3 is M−10M = 0. A count of the eigenvectors will determine J when,eigenvec_val
"23. By applying row operations to produce an upper triangular U, compute",determinants
"Starting with a different basis (1,1) and (2,−1), this same A is also the only linear",vector spaces
"to ﬁnd A−1. The normal count for each new right-hand side is n2, half in the forward",gauss elim
13. The minimax principle for λj involves j-dimensional subspaces S j:,pos_def_matrices
The complete solution is x = xp + xn. One particular solution xp has all free,vector spaces
"of the components of x. Deduce that if Ax = λx with λ ̸= 1, the components of the",eigenvec_val
"to the true b. This means that the original problem Ax = b should be at the other extreme,",computations
(b) What is the shape of the feasible set?,linear_prog
(12 +···+n2)−(1+···+n) = n(n+1)(2n+1),gauss elim
else (including x = 0 and x = 1). Any combination y1V1 +···+ynVn must have the value,pos_def_matrices
Test for a maximum:,pos_def_matrices
"transform can do each multiplication in only 5·1024 steps. It is 200 times faster, because",orthogonality
"x1,...,xp of A corresponding to eigenvalues λi > 0. and the orthonormal eigenvectors",pos_def_matrices
So much for that application of linear algebra.,vector spaces
A2 has the same,eigenvec_val
r = |x| = 5.,eigenvec_val
"4n. If only one side is stretched, the volume and determinant go up by 4; that is rule 3.",determinants
"next eigenvalue, the QR algorithm continues with the smaller matrix (3 by 3, in the",computations
B = A−7I =,eigenvec_val
sign. The saddles 2xy and x2 − y2 are practically the same; if we turn one through 45°,pos_def_matrices
take to change VISA into AVIS? Is this permutation odd or even?,determinants
"roots are equally spaced around the unit circle, at intervals of 2π/n. Note again that the",orthogonality
solution. We know another way of answering the same question: Ax = b can be solved if,vector spaces
"Let me compare those two approaches, looking ahead to real problems when n is",gauss elim
deﬁnition allows other things to be “vectors”-provided that addition and scalar multipli-,vector spaces
2 1 0 1 0 0,gauss elim
"tors of A went into the columns of S, and that made S−1AS a diagonal matrix (called",eigenvec_val
row 1 of UH times column 2 of U is 1,eigenvec_val
(B−1A−1)(AB) = B−1A−1AB = B−1IB = B−1B = I.,gauss elim
"62. Producing x1 trucks and x2 planes requires x1 + 50x2 tons of steel, 40x1 + 1000x2",gauss elim
C−1y + Ax =,vector spaces
Then the matrices on the right-hand side are x1xT,eigenvec_val
"25. Which of these transformations satisfy T(v + w) = T(v) + T(w), and which satisfy",vector spaces
Of course c2 +s2 = cos2θ +sin2 θ = 1. A projection matrix equals its own square.,vector spaces
"2, d3 = 4",pos_def_matrices
36. Write A =,eigenvec_val
"column of A, which is a row of AT. Down column j of A,",determinants
x3 −x1 = b3,orthogonality
matrix ATA turns up in the end.,vector spaces
2. The pyramid with four,determinants
(b) Show that f1 is a single perfect square and not positive deﬁnite. Where is f1,pos_def_matrices
block row from the bottom row:,linear_prog
"we write out the dual, since yA ≤ c for row vectors means ATyT ≤ cT for columns.",linear_prog
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"That is how Ax is usually explained, but the second way is equally important. In fact it is",gauss elim
0 0 1 0,determinants
"reason, the eigenvalues of A+B generally have nothing to do with λ + µ.",eigenvec_val
i = 0 when vitamin i is oversupplied in the diet x∗.,linear_prog
"zero, there are solutions other than x = 0. Those are the eigenvectors.",eigenvec_val
18. (Lyapunov test for stability of M) Suppose AM + MHA = −I with positive deﬁnite,pos_def_matrices
"pk+1 = (1+.06∆t)pk. Moving pk to the left side and dividing by ∆t,",eigenvec_val
"x+y. The new system has four unknowns (x, y, and two slack variables):",linear_prog
"To project b onto a, multiply by the projection matrix P: p = Pb.",orthogonality
"“power methods,” and the preprocessing of a symmetric matrix to make it tridiagonal.",computations
"30. A doctor takes four readings of your heart rate. The best solution to x = b1,...,x = b4",orthogonality
"1. Use three hat functions, with h = 1",pos_def_matrices
4.1 Find the determinants of,determinants
For such a matrix we can solve Ax = b by Gaussian elimination; there are ﬁve pivots.,vector spaces
This is veriﬁed just by obeying the rule for multiplying a matrix and a vector:,gauss elim
and only if b lies in the column space of A. This subspace comes from the four columns,vector spaces
"see the point. We want a nonzero eigenvector x, The vector x = 0 always satisﬁes",eigenvec_val
"3. The determinant depends linearly on the ﬁrst row. Suppose A, B, C are the same",determinants
3.2 Cosines and Projections onto Lines,orthogonality
"continues periodically. The Discrete Fourier Series is best written in this complex form,",orthogonality
it by looking at a typical vector x. It has a “row space component” and a “nullspace,orthogonality
Which fundamental subspace contains q3? What is the least-squares solution of,orthogonality
"(c) If the ﬁrst row is (3,−1), what is the second row?",eigenvec_val
completely natural and inevitable: to apply the new ﬁnite element ideas to this long-,pos_def_matrices
"12. Which number d forces a row exchange, and what is the triangular system (not sin-",gauss elim
Thus U is a unitary matrix. Its inverse looks the same except that w is replaced by,eigenvec_val
k (QkRk +αkI)Qk = Ak+1.,computations
nothing new about that—it is the way we all differentiate. It would be crazy to memorize,vector spaces
"proved (for variants of the usual method) to be polynomial. For some reason, hidden in",linear_prog
The condition number is approximately c(A) = 1,computations
"In either order, EF or FE, this changes rows 2 and 3 using row 1.",gauss elim
Problems 1–2 compute the SVD of a square singular matrix A.,pos_def_matrices
"Schmidt was done. The ﬁrst vectors a and q1 fell on the same line. Then q1, q2 were in",orthogonality
and synthesis of signals—computing c from y and y from c—is a central part of scientiﬁc,orthogonality
"K, and split these matrices into A+K:",eigenvec_val
is ATy = 0.,vector spaces
(b) closed under scalar multiplication but not under vector addition.,vector spaces
x j = a j +ibj.,eigenvec_val
Some linear problems have a structure that makes their solution very quick. Band ma-,linear_prog
What are the dimensions of its four fundamental subspaces?,vector spaces
∥x∥2 = |x1|2 +···+|xn|2.,eigenvec_val
"matrix Q by a continuous chain of nonsingular matrices C(t). At t = 0 and t = 1, C(0) =",pos_def_matrices
"The change from x to y = QTx rotates the axes of the space, to match the axes of",pos_def_matrices
"(c) Find the area of the triangle with sides v, w, and w−v. Draw it.",determinants
stage of elimination admits the simpliﬁcations (a) and (b).,gauss elim
5. Show (if B is invertible) that BA is similar to AB.,eigenvec_val
inequalities (multiplying by negative numbers would reverse them):,linear_prog
"For any m by n matrix A there is a permutation P, a lower triangular L",vector spaces
Example 1. x = 3+4i times its conjugate x = 3−4i is the absolute value squared:,eigenvec_val
many free variables are there in the solution to Ax = b? How many free variables,vector spaces
Problems 20–31 compute the factorization A = LU (and also A = LDU).,gauss elim
"The matrix A2 is diagonalized by the same S, so the eigenvectors are unchanged. The",eigenvec_val
"numbers 1,2,...,n. The column numbers give the permutations:",determinants
entries of the true L and U are set to zero while factoring A. It is called incomplete LU,computations
"by every orthogonal matrix. It is not shared by projections, which are not orthogonal or",orthogonality
"ﬁed, and a halfspace in which it is not. A typical example is x +2y ≥ 4. The boundary",linear_prog
onto the line L at angle θ. This linear transformation is completely described without the,eigenvec_val
The one-sided inequality yb ≤ cx was easy to prove; it gave a quick test for optimal,linear_prog
and p is the sum p = (qT,orthogonality
"(a) Solve aTa�x = aTb to show that is the mean (the average) of the b’s,",orthogonality
dim(row space)+dim(nullspace) = number of columns.,orthogonality
step by checking the signs of the pivots.,pos_def_matrices
− 8v − 2w = −12,gauss elim
A closely related problem ﬁnds the shortest spanning tree—a set of n − 1 edges,linear_prog

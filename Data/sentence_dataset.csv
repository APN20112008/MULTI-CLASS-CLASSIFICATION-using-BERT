text,labels
Matrices and Gaussian Elimination,gauss elim
This book begins with the central problem of linear algebra: solving linear equations.,gauss elim
"The most important ease, and the simplest, is when the number of unknowns equals the",gauss elim
"number of equations. We have n equations in n unknowns, starting with n = 2:",gauss elim
1x + 2y =,gauss elim
Two unknowns 4x + 5y = 6.,gauss elim
"The unknowns are x and y. I want to describe two ways, elimination and determinants,",gauss elim
"to solve these equations. Certainly x and y are determined by the numbers 1, 2, 3, 4, 5,",gauss elim
6. The question is how to use those six numbers to solve the system.,gauss elim
1. Elimination Subtract 4 times the ﬁrst equation from the second equation. This,gauss elim
eliminates x from the second equation. and it leaves one equation for y:,gauss elim
Immediately we know y = 2. Then x comes from the ﬁrst equation 1x+2y = 3:,gauss elim
"Proceeding carefully, we cheek that x and y also solve the second equation. This",gauss elim
should work and it does: 4 times (x = −1) plus 5 times (y = 2) equals 6.,gauss elim
2. Determinants The solution y = 2 depends completely on those six numbers in the,gauss elim
equations. There most be a formula for y (and also x) It is a “ratio of determinants”,gauss elim
and I hope you will allow me to write it down directly:,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"That could seem a little mysterious, unless you already know about 2 by 2 determi-",gauss elim
"nants. They gave the same answer y = 2, coming from the same ratio of −6 to −3.",gauss elim
"If we stay with determinants (which we don’t plan to do), there will be a similar",gauss elim
"formula to compute the other unknown, x:",gauss elim
"Let me compare those two approaches, looking ahead to real problems when n is",gauss elim
much larger (n = 1000 is a very moderate size in scientiﬁc computing). The truth is that,gauss elim
direct use of the determinant formula for 1000 equations would be a total disaster. It,gauss elim
"would use the million numbers on the left sides correctly, but not efﬁciently. We will",gauss elim
"ﬁnd that formula (Cramer’s Rule) in Chapter 4, but we want a good method to solve",gauss elim
1000 equations in Chapter 1.,gauss elim
That good method is Gaussian Elimination. This is the algorithm that is constantly,gauss elim
used to solve large systems of equations. From the examples in a textbook (n = 3 is,gauss elim
close to the upper limit on the patience of the author and reader) too might not see much,gauss elim
difference. Equations (2) and (4) used essentially the same steps to ﬁnd y = 2. Certainly,gauss elim
x came faster by the back-substitution in equation (3) than the ratio in (5). For larger,gauss elim
n there is absolutely no question. Elimination wins (and this is even the best way to,gauss elim
The idea of elimination is deceptively simple—you will master it after a few exam-,gauss elim
"ples. It will become the basis for half of this book, simplifying a matrix so that we can",gauss elim
"understand it. Together with the mechanics of the algorithm, we want to explain four",gauss elim
deeper aspects in this chapter. They are:,gauss elim
1. Linear equations lead to geometry of planes. It is not easy to visualize a nine-,gauss elim
"dimensional plane in ten-dimensional space. It is harder to see ten of those planes,",gauss elim
intersecting at the solution to ten equations—but somehow this is almost possible.,gauss elim
"Our example has two lines in Figure 1.1, meeting at the point (x,y) = (−1,2).",gauss elim
"Linear algebra moves that picture into ten dimensions, where the intuition has to",gauss elim
imagine the geometry (and gets it right),gauss elim
"2. We move to matrix notation, writing the n unknowns as a vector x and the n equa-",gauss elim
tions as Ax = b. We multiply A by “elimination matrices” to reach an upper trian-,gauss elim
"gular matrix U. Those steps factor A into L times U, where L is lower triangular.",gauss elim
"I will write down A and its factors for our example, and explain them at the right",gauss elim
= L times U.,gauss elim
x + 2y = 3,gauss elim
4x + 5y = 6,gauss elim
"One solution (x, y) = (−1, 2)",gauss elim
x + 2y = 3,gauss elim
4x + 8y = 6,gauss elim
x + 2y = 3,gauss elim
4x + 8y = 12,gauss elim
Whole line of solutions,gauss elim
Figure 1.1: The example has one solution. Singular cases have none or too many.,gauss elim
First we have to introduce matrices and vectors and the rules for multiplication.,gauss elim
Every matrix has a transpose AT. This matrix has an inverse A−1.,gauss elim
3. In most cases elimination goes forward without difﬁculties. The matrix has an,gauss elim
inverse and the system Ax = b has one solution. In exceptional cases the method,gauss elim
"will break down—either the equations were written in the wrong order, which is",gauss elim
"easily ﬁxed by exchanging them, or the equations don’t have a unique solution.",gauss elim
That singular case will appear if 8 replaces 5 in our example:,gauss elim
1x + 2y =,gauss elim
4x + 8y = 6.,gauss elim
Elimination still innocently subtracts 4 times the ﬁrst equation from the second. But,gauss elim
look at the result!,gauss elim
This singular case has no solution. Other singular cases have inﬁnitely many solu-,gauss elim
"tions. (Change 6 to 12 in the example, and elimination will lead to 0 = 0. Now y",gauss elim
"can have any value,) When elimination breaks down, we want to ﬁnd every possible",gauss elim
4. We need a rough count of the number of elimination steps required to solve a sys-,gauss elim
tem of size n. The computing cost often determines the accuracy in the model. A,gauss elim
hundred equations require a third of a million steps (multiplications and subtrac-,gauss elim
"tions). The computer can do those quickly, but not many trillions. And already",gauss elim
"after a million steps, roundoff error could be signiﬁcant. (Some problems are sen-",gauss elim
"sitive; others are not.) Without trying for full detail, we want to see large systems",gauss elim
"that arise in practice, and how they are actually solved.",gauss elim
The ﬁnal result of this chapter will be an elimination algorithm that is about as efﬁ-,gauss elim
cient as possible. It is essentially the algorithm that is in constant use in a tremendous,gauss elim
"variety of applications. And at the same time, understanding it in terms of matrices—the",gauss elim
"coefﬁcient matrix A, the matrices E for elimination and P for row exchanges, and the",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
ﬁnal factors L and U—is an essential foundation for the theory. I hope you will enjoy,gauss elim
this book and this course.,gauss elim
The Geometry of Linear Equations,gauss elim
The way to understand this subject is by example. We begin with two extremely humble,gauss elim
"equations, recognizing that you could solve them without a course in linear algebra.",gauss elim
Nevertheless I hope you will give Gauss a chance:,gauss elim
2x − y =,gauss elim
x + y = 5.,gauss elim
We can look at that system by rows or by columns. We want to see them both.,gauss elim
The ﬁrst approach concentrates on the separate equations (the rows). That is the,gauss elim
"most familiar, and in two dimensions we can do it quickly. The equation 2x − y = 1 is",gauss elim
"represented by a straight line in the x-y plane. The line goes through the points x = 1,",gauss elim
y = 1 and x = 1,gauss elim
"2, y = 0 (and also through (2,3) and all intermediate points). The second",gauss elim
equation x+y = 5 produces a second line (Figure 1.2a). Its slope is dy/dx = −1 and it,gauss elim
crosses the ﬁrst line at the solution.,gauss elim
The point of intersection lies on both lines. It is the only solution to both equations.,gauss elim
That point x = 2 and y = 3 will soon be found by “elimination.”,gauss elim
x + y = 5,gauss elim
2x − y = 1,gauss elim
"(x, y) = (2, 3)",gauss elim
"(a) Lines meet at x = 2, y = 3",gauss elim
"(2, 1) = column 1",gauss elim
(b) Columns combine with 2 and 3,gauss elim
Figure 1.2: Row picture (two lines) and column picture (combine columns).,gauss elim
The second approach looks at the columns of the linear system. The two separate,gauss elim
equations are really one vector equation:,gauss elim
1.2 The Geometry of Linear Equations,gauss elim
The problem is to ﬁnd the combination of the column vectors on the left side that,gauss elim
"produces the vector on the right side. Those vectors (2,1) and (−1,1) are represented",gauss elim
by the bold lines in Figure 1.2b. The unknowns are the numbers x and y that multiply,gauss elim
"the column vectors. The whole idea can be seen in that ﬁgure, where 2 times column",gauss elim
1 is added to 3 times column 2. Geometrically this produces a famous parallelogram.,gauss elim
"Algebraically it produces the correct vector (1,5), on the right side of our equations.",gauss elim
The column picture conﬁrms that x = 2 and y = 3.,gauss elim
"More time could be spent on that example, but I would rather move forward to n = 3.",gauss elim
"Three equations are still manageable, and they have much more variety:",gauss elim
−2u + 7v + 2w =,gauss elim
"Again we can study the rows or the columns, and we start with the rows. Each equation",gauss elim
"describes a plane in three dimensions. The ﬁrst plane is 2u+v+w = 5, and it is sketched",gauss elim
in Figure 1.3. It contains the points (5,gauss elim
"2,0,0) and (0,5,0) and (0,0,5). It is determined",gauss elim
by any three of its points—provided they do not lie on a line.,gauss elim
"b(1, 1, 2) = point of intersection",gauss elim
with third plane = solution,gauss elim
4u − 6v = −2 (vertical plane),gauss elim
line of intersection: ﬁrst two planes,gauss elim
2u + v + w = 5 (sloping plane),gauss elim
Figure 1.3: The row picture: three intersecting planes from three linear equations.,gauss elim
"Changing 5 to 10, the plane 2u+v+w = 10 would be parallel to this one. It contains",gauss elim
"(5,0,0) and (0,10,0) and (0,0,10), twice as far from the origin—which is the center",gauss elim
"point u = 0, v = 0, w = 0. Changing the right side moves the plane parallel to itself, and",gauss elim
the plane 2u+v+w = 0 goes through the origin.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"The second plane is 4u − 6v = −2. It is drawn vertically, because w can take any ",gauss elim
"value. The coefﬁcient of w is zero, but this remains a plane in 3 -space. (The equation ",gauss elim
"4u = 3, or even the extreme case u = 0, would still describe a plane.) The ﬁgure shows ",gauss elim
the intersection of the second plane with the ﬁrst. That intersection is a l ine. In three ,gauss elim
dimensions a line requires two equations; in n dimensions it will require n − 1.,gauss elim
Finally the third plane intersects this line in a point. The plane (not drawn) represents ,gauss elim
"the third equation −2u+7v+2w = 9, and it crosses the line at u = 1, v = 1, w = 2. That ",gauss elim
"triple intersection point (1,1,2) solves the linear system.",gauss elim
How does this row picture extend into n dimensions? The n equations will con-,gauss elim
tain n unknowns. The ﬁrst equation still determines a “ plane.” It is no longer a two-,gauss elim
dimensional plane in 3-space; somehow it has “dimension” n − 1. It must be ﬂat and ,gauss elim
"extremely thin within n-dimensional space, although it would look solid to us.",gauss elim
"If time is the fourth dimension, then the plane t = 0 cuts through four-dimensional ",gauss elim
"space and produces the three-dimensional universe we live in (or rather, the universe as ",gauss elim
"it was at t = 0). Another plane is z = 0, which is also three-dimensional; it is the ordinary ",gauss elim
x-y plane taken over all time. Those three-dimensional planes will intersect! They share ,gauss elim
"the ordinary x-y plane at t = 0. We are down to two dimensions, and the next plane ",gauss elim
leaves a line. Finally a fourth plane leaves a single point. It is the intersection point of 4 ,gauss elim
"planes in 4 dimensions, and it solves the 4 underlying equations.",gauss elim
I will be in trouble if that example from relativity goes any further. The point is that ,gauss elim
linear algebra can operate with any number of equations. The ﬁrst equation produces an ,gauss elim
"(n − 1)-dimensional plane in n dimensions, The second plane intersects it (we hope) in ",gauss elim
"a smaller set of “dimension n − 2.” Assuming all goes well, every new plane (every new ",gauss elim
"equation) reduces the dimension by one. At the end, when all n planes are accounted ",gauss elim
"for, the intersection has dimension zero. It is a point, it lies on all the planes, and its ",gauss elim
coordinates satisfy all n equations. It is the solution!,gauss elim
Column Vectors and Linear Combinations,gauss elim
We turn to the columns. This time the vector equation (the same equation as (1)) is,gauss elim
Those are three-dimensional column vectors. The vector b is identiﬁed with the point,gauss elim
"whose coordinates are 5, −2, 9. Every point in three-dimensional space is matched to a",gauss elim
"vector, and vice versa. That was the idea of Descartes, who turned geometry into algebra",gauss elim
"by working with the coordinates of the point. We can write the vector in a column, or",gauss elim
"we can list its components as b = (5,−2,9), or we can represent it geometrically by an",gauss elim
"arrow from the origin. You can choose the arrow, or the point, or the three numbers. In",gauss elim
six dimensions it is probably easiest to choose the six numbers.,gauss elim
1.2 The Geometry of Linear Equations,gauss elim
"We use parentheses and commas when the components are listed horizontally, and",gauss elim
square brackets (with no commas) when a column vector is printed vertically. What,gauss elim
really matters is addition of vectors and multiplication by a scalar (a number). In Figure,gauss elim
"1.4a you see a vector addition, component by component:",gauss elim
In the right-hand ﬁgure there is a multiplication by 2 (and if it had been −2 the vector,gauss elim
(a) Add vectors along axes,gauss elim
columns 1 + 2,gauss elim
= linear combination equals b,gauss elim
(b) Add columns 1 + 2 + (3 + 3),gauss elim
Figure 1.4: The column picture: linear combination of columns equals b.,gauss elim
would have gone in the reverse direction):,gauss elim
Also in the right-hand ﬁgure is one of the central ideas of linear algebra. It uses both,gauss elim
of the basic operations; vectors are multiplied by numbers and then added. The result is,gauss elim
"called a linear combination, and this combination solves our equation:",gauss elim
"Equation (2) asked for multipliers u, v, w that produce the right side b. Those numbers",gauss elim
"are u = 1, v = 1, w = 2. They give the correct combination of the columns. They also",gauss elim
"gave the point (1,1,2) in the row picture (where the three planes intersect).",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
Our true goal is to look beyond two or three dimensions into n dimensions. With n,gauss elim
"equations in n unknowns, there are n planes in the row picture. There are n vectors in",gauss elim
"the column picture, plus a vector b on the right side. The equations ask for a linear com-",gauss elim
bination of the n columns that equals b. For certain equations that will be impossible.,gauss elim
"Paradoxically, the way to understand the good case is to study the bad one. Therefore",gauss elim
"we look at the geometry exactly when it breaks down, in the singular case.",gauss elim
Row picture: Intersection of planes,gauss elim
Column picture: Combination of columns,gauss elim
"Suppose we are again in three dimensions, and the three planes in the row picture do not",gauss elim
intersect. What can go wrong? One possibility is that two planes may be parallel. The,gauss elim
equations 2u + v + w = 5 and 4u + 2v + 2w = 11 are inconsistent—and parallel planes,gauss elim
"give no solution (Figure 1.5a shows an end view). In two dimensions, parallel lines",gauss elim
are the only possibility for breakdown. But three planes in three dimensions can be in,gauss elim
trouble without being parallel.,gauss elim
"Figure 1.5: Singular cases: no solution for (a), (b), or (d), an inﬁnity of solutions for (c).",gauss elim
The most common difﬁculty is shown in Figure 1.5b. From the end view the planes,gauss elim
"form a triangle. Every pair of planes intersects in a line, and those lines are parallel. The",gauss elim
"third plane is not parallel to the other planes, but it is parallel to their line of intersection.",gauss elim
"This corresponds to a singular system with b = (2,5,6):",gauss elim
"No solution, as in Figure 1.5b",gauss elim
3u + v + 4w = 6.,gauss elim
The ﬁrst two left sides add up to the third. On the right side that fails: 2+5 ̸= 6. Equation,gauss elim
1 plus equation 2 minus equation 3 is the impossible statement 0 = 1. Thus the equations,gauss elim
"are inconsistent, as Gaussian elimination will systematically discover.",gauss elim
1.2 The Geometry of Linear Equations,gauss elim
"Another singular system, close to this one, has an inﬁnity of solutions. When the",gauss elim
"6 in the last equation becomes 7, the three equations combine to give 0 = 0. Now the",gauss elim
third equation is the sum of the ﬁrst two. In that case the three planes have a whole line,gauss elim
in common (Figure 1.5c). Changing the right sides will move the planes in Figure 1.5b,gauss elim
"parallel to themselves, and for b = (2,5,7) the ﬁgure is suddenly different. The lowest",gauss elim
"plane moved up to meet the others, and there is a line of solutions. Problem 1.5c is still",gauss elim
"singular, but now it suffers from too many solutions instead of too few.",gauss elim
The extreme case is three parallel planes. For most right sides there is no solution,gauss elim
"(Figure 1.5d). For special right sides (like b = (0,0,0)!) there is a whole plane of",gauss elim
solutions—because the three parallel planes move over to become the same.,gauss elim
What happens to the column picture when the system is singular? it has to go wrong;,gauss elim
"the question is how, There are still three columns on the left side of the equations, and",gauss elim
we try to combine them to produce b. Stay with equation (3):,gauss elim
Singular case: Column picture,gauss elim
Three columns in the same plane,gauss elim
Solvable only for b in that plane,gauss elim
"For b = (2,5,7) this was possible; for b = (2,5,6) it was not. The reason is that those",gauss elim
three columns lie in a plane. Then every combination is also in the plane (which goes,gauss elim
"through the origin). If the vector b is not in that plane, no solution is possible (Figure",gauss elim
1.6). That is by far the most likely event; a singular system generally has no solution.,gauss elim
But there is a chance that b does lie in the plane of the columns. In that case there are too,gauss elim
many solutions; the three columns can be combined in inﬁnitely many ways to produce,gauss elim
b. That column picture in Figure 1.6b corresponds to the row picture in Figure 1.5c.,gauss elim
b not in place,gauss elim
(b) inﬁnity of solutions,gauss elim
Figure 1.6: Singular cases: b outside or inside the plane with all three columns.,gauss elim
How do we know that the three columns lie in the same plane? One answer is to ﬁnd a,gauss elim
"combination of the columns that adds to zero. After some calculation, it is u = 3, v = 1,",gauss elim
w = −2. Three times column 1 equals column 2 plus twice column 3. Column 1 is in,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
the plane of columns 2 and 3. Only two columns are independent.,gauss elim
"The vector b = (2,5,7) is in that plane of the columns—it is column 1 plus column",gauss elim
"3—so (1, 0, 1) is a solution. We can add an multiple of the combination (3,−1,−2) that",gauss elim
gives b = 0. So there is a whole line of solutions—as we know from the row picture.,gauss elim
"The truth is that we knew the columns would combine to give zero, because the rows",gauss elim
"did. That is a fact of mathematics, not of computation—and it remains true in dimension",gauss elim
"n. If the n planes have no point in common, or inﬁnitely many points, then the n",gauss elim
columns lie in the same plane.,gauss elim
"If the row picture breaks down, so does the column picture. That brings out the",gauss elim
difference between Chapter 1 and Chapter 2. This chapter studies the most important,gauss elim
problem—the nonsingular case—where there is one solution and it has to be found.,gauss elim
"Chapter 2 studies the general case, where there may be many solutions or none. In",gauss elim
both cases we cannot continue without a decent notation (matrix notation) and a decent,gauss elim
"algorithm (elimination). After the exercises, we start with elimination.",gauss elim
"1. For the equations x + y = 4, 2x − 2y = 4, draw the row picture (two intersecting",gauss elim
lines) and the column picture (combination of two columns equal to the column,gauss elim
"vector (4,4) on the right side).",gauss elim
2. Solve to ﬁnd a combination of the columns that equals b:,gauss elim
u − v − w =,gauss elim
v + w =,gauss elim
3. (Recommended) Describe the intersection of the three planes u+v+w+z = 6 and,gauss elim
u+w+z = 4 and u+w = 2 (all in four-dimensional space). Is it a line or a point or,gauss elim
an empty set? What is the intersection if the fourth plane u = −1 is included? Find,gauss elim
a fourth equation that leaves us with no solution.,gauss elim
4. Sketch these three lines and decide if the equations are solvable:,gauss elim
3 by 2 system,gauss elim
x + 2y =,gauss elim
What happens if all right-hand sides are zero? Is there any nonzero choice of right-,gauss elim
hand sides that allows the three lines to intersect at the same point?,gauss elim
5. Find two points on the line of intersection of the three planes t = 0 and z = 0 and,gauss elim
x+y+z+t = 1 in four-dimensional space.,gauss elim
1.2 The Geometry of Linear Equations,gauss elim
"6. When b = (2,5,7), ﬁnd a solution (u,v,w) to equation (4) different from the solution",gauss elim
"(1,0,1) mentioned in the text.",gauss elim
"7. Give two more right-hand sides in addition to b = (2,5,7) for which equation (4)",gauss elim
"can be solved. Give two more right-hand sides in addition to b = (2,5,6) for which",gauss elim
it cannot be solved.,gauss elim
8. Explain why the system,gauss elim
u + 2v + 3w = 1,gauss elim
+ 2w = 0,gauss elim
is singular by ﬁnding a combination of the three equations that adds up to 0 = 1.,gauss elim
What value should replace the last zero on the right side to allow the equations to,gauss elim
have solutions—and what is one of the solutions?,gauss elim
9. The column picture for the previous exercise (singular system) is,gauss elim
Show that the three columns on the left lie in the same plane by expressing the third,gauss elim
"column as a combination of the ﬁrst two. What are all the solutions (u,v,w) if b is",gauss elim
"the zero vector (0,0,0)?",gauss elim
"10. (Recommended) Under what condition on y1, y2, y3 do the points (0,y1), (1,y2),",gauss elim
"(2,y3) lie on a straight line?",gauss elim
11. These equations are certain to have the solution x = y = 0. For which values of a is,gauss elim
there a whole line of solutions?,gauss elim
ax + 2y = 0,gauss elim
2x + ay = 0,gauss elim
"12. Starting with x+4y = 7, ﬁnd the equation for the parallel line through x = 0, y = 0.",gauss elim
"Find the equation of another line that meets the ﬁrst at x = 3, y = 1.",gauss elim
Problems 13–15 are a review of the row and column pictures.,gauss elim
"13. Draw the two pictures in two planes for the equations x−2y = 0, x+y = 6.",gauss elim
"14. For two linear equations in three unknowns x, y, z, the row picture will show (2 or 3)",gauss elim
(lines or planes) in (two or three)-dimensional space. The column picture is in (two,gauss elim
or three)-dimensional space. The solutions normally lie on a,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"15. For four linear equations in two unknowns x and y, the row picture shows four",gauss elim
. The column picture is in,gauss elim
-dimensional space. The equations have no,gauss elim
solution unless the vector on the right-hand side is a combination of,gauss elim
16. Find a point with z = 2 on the intersection line of the planes x + y + 3z = 6 and,gauss elim
x−y+z = 4. Find the point with z = 0 and a third point halfway between.,gauss elim
17. The ﬁrst of these equations plus the second equals the third:,gauss elim
2x + 3y + 2z = 5.,gauss elim
"The ﬁrst two planes meet along a line. The third plane contains that line, because",gauss elim
"if x, y, z satisfy the ﬁrst two equations then they also",gauss elim
. The equations have,gauss elim
inﬁnitely many solutions (the whole line L). Find three solutions.,gauss elim
18. Move the third plane in Problem 17 to a parallel plane 2x + 3y + 2z = 9. Now the,gauss elim
three equations have no solution—why not? The ﬁrst two planes meet along the line,gauss elim
"L, but the third plane doesn’t",gauss elim
"19. In Problem 17 the columns are (1,1,2) and (1,2,3) and (1,1,2). This is a “singular",gauss elim
case” because the third column is,gauss elim
. Find two combinations of the columns,gauss elim
"that give b = (2,3,5). This is only possible for b = (4,6,c) if c =",gauss elim
20. Normally 4 “planes” in four-dimensional space meet at a,gauss elim
. Normally 4 col-,gauss elim
umn vectors in four-dimensional space can combine to produce b. What combina-,gauss elim
"tion of (1,0,0,0), (1,1,0,0), (1,1,1,0), (1,1,1,1) produces b = (3,3,3,2)? What 4",gauss elim
"equations for x, y, z, t are you solving?",gauss elim
"21. When equation 1 is added to equation 2, which of these are changed: the planes in",gauss elim
"the row picture, the column picture, the coefﬁcient matrix, the solution?",gauss elim
"22. If (a,b) is a multiple of (c,d) with abcd ̸= 0, show that (a,c) is a multiple of (b,d).",gauss elim
This is surprisingly important: call it a challenge question. You could use numbers,gauss elim
"ﬁrst to see how a, b, c, and d are related. The question will lead to:",gauss elim
has dependent rows then it has dependent columns.,gauss elim
"23. In these equations, the third column (multiplying w) is the same as the right side b.",gauss elim
"The column form of the equations immediately gives what solution for (u,v,w)?",gauss elim
6u + 7v + 8w =,gauss elim
4u + 5v + 9w =,gauss elim
2u − 2v + 7w = 7.,gauss elim
1.3 An Example of Gaussian Elimination,gauss elim
An Example of Gaussian Elimination,gauss elim
The way to understand elimination is by example. We begin in three dimensions:,gauss elim
−2u + 7v + 2w =,gauss elim
"The problem is to ﬁnd the unknown values of u, v, and w, and we shall apply Gaussian",gauss elim
"elimination. (Gauss is recognized as the greatest of all mathematicians, but certainly not",gauss elim
"because of this invention, which probably took him ten minutes. Ironically, it is the most",gauss elim
frequently used of all the ideas that bear his name.) The method starts by subtracting,gauss elim
multiples of the ﬁrst equation from the other equations. The goal is to eliminate u from,gauss elim
the last two equations. This requires that we,gauss elim
(a) subtract 2 times the ﬁrst equation from the second,gauss elim
(b) subtract −1 times the ﬁrst equation from the third.,gauss elim
− 8v − 2w = −12,gauss elim
8v + 3w =,gauss elim
The coefﬁcient 2 is the ﬁrst pivot. Elimination is constantly dividing the pivot into the,gauss elim
"numbers underneath it, to ﬁnd out the right multipliers.",gauss elim
The pivot for the second stage of elimination is −8. We now ignore the ﬁrst equation.,gauss elim
A multiple of the second equation will be subtracted from the remaining equations (in,gauss elim
this case there is only the third one) so as to eliminate v. We add the second equation to,gauss elim
"the third or, in other words, we",gauss elim
(c) subtract −1 times the second equation from the third.,gauss elim
"The elimination process is now complete, at least in the “forward” direction:",gauss elim
− 8v − 2w = −12,gauss elim
"This system is solved backward, bottom to top. The last equation gives w = 2. Sub-",gauss elim
"stituting into the second equation, we ﬁnd v = 1. Then the ﬁrst equation gives u = 1.",gauss elim
This process is called back-substitution.,gauss elim
"To repeat: Forward elimination produced the pivots 2, −8, 1. It subtracted multiples",gauss elim
"of each row from the rows beneath, It reached the “triangular” system (3), which is",gauss elim
solved in reverse order: Substitute each newly computed value into the equations that,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
Remark. One good way to write down the forward elimination steps is to include the,gauss elim
right-hand side as an extra column. There is no need to copy u and v and w and = at,gauss elim
"every step, so we are left with the bare minimum:",gauss elim
0 −8 −2 −12,gauss elim
0 −8 −2 −12,gauss elim
"At the end is the triangular system, ready for back-substitution. You may prefer this",gauss elim
"arrangement, which guarantees that operations on the left-hand side of the equations are",gauss elim
also done on the right-hand side—because both sides are there together.,gauss elim
"In a larger problem, forward elimination takes most of the effort. We use multiples",gauss elim
of the ﬁrst equation to produce zeros below the ﬁrst pivot. Then the second column is,gauss elim
cleared out below the second pivot. The forward step is ﬁnished when the system is,gauss elim
triangular; equation n contains only the last unknown multiplied by the last pivot. Back-,gauss elim
substitution yields the complete solution in the opposite order—beginning with the last,gauss elim
"unknown, then solving for the next to last, and eventually for the ﬁrst.",gauss elim
"By deﬁnition, pivots cannot be zero. We need to divide by them.",gauss elim
The Breakdown of Elimination,gauss elim
Under what circumstances could the process break down? Something must go wrong,gauss elim
"in the singular case, and something might go wrong in the nonsingular case. This may",gauss elim
"seem a little premature—after all, we have barely got the algorithm working. But the",gauss elim
possibility of breakdown sheds light on the method itself.,gauss elim
"The answer is: With a full set of n pivots, there is only one solution. The system is",gauss elim
"non singular, and it is solved by forward elimination and back-substitution. But if a zero",gauss elim
"appears in a pivot position, elimination has to stop—either temporarily or permanently.",gauss elim
The system might or might not be singular.,gauss elim
"If the ﬁrst coefﬁcient is zero, in the upper left corner, the elimination of u from the",gauss elim
other equations will be impossible. The same is true at every intermediate stage. Notice,gauss elim
"that a zero can appear in a pivot position, even if the original coefﬁcient in that place",gauss elim
"was not zero. Roughly speaking, we do not know whether a zero will appear until we",gauss elim
"try, by actually going through the elimination process.",gauss elim
"In many cases this problem can be cured, and elimination can proceed. Such a system",gauss elim
still counts as nonsingular; it is only the algorithm that needs repair. In other cases a,gauss elim
"breakdown is unavoidable. Those incurable systems are singular, they have no solution",gauss elim
"or else inﬁnitely many, and a full set of pivots cannot be found.",gauss elim
1.3 An Example of Gaussian Elimination,gauss elim
Example 1. Nonsingular (cured by exchanging equations 2 and 3),gauss elim
2u + 2v + 5w =,gauss elim
4u + 6v + 8w =,gauss elim
2v + 4w =,gauss elim
2v + 4w =,gauss elim
"The system is now triangular, and back-substitution will solve it.",gauss elim
Example 2. Singular (incurable),gauss elim
2u + 2v + 5w =,gauss elim
4u + 4v + 8w =,gauss elim
u + v +,gauss elim
There is no exchange of equations that can avoid zero in the second pivot position. The,gauss elim
equations themselves may be solvable or unsolvable. If the last two equations are 3w = 6,gauss elim
"and 4w = 7, there is no solution. If those two equations happen to be consistent—as in",gauss elim
3w = 6 and 4w = 8—then this singular case has an inﬁnity of solutions. We know that,gauss elim
"w = 2, but the ﬁrst equation cannot decide both u and v.",gauss elim
Section 1.5 will discuss row exchanges when the system is not singular. Then the,gauss elim
"exchanges produce a full set of pivots. Chapter 2 admits the singular case, and limps",gauss elim
"forward with elimination. The 3w can still eliminate the 4w, and we will call 3 the",gauss elim
second pivot. (There won’t be a third pivot.) For the present we trust all n pivot entries,gauss elim
"to be nonzero, without changing the order of the equations. That is the best case, with",gauss elim
The Cost of Elimination,gauss elim
Our other question is very practical. How many separate arithmetical operations does,gauss elim
"elimination require, for n equations in n unknowns? If n is large, a computer is going to",gauss elim
"take our place in carrying out the elimination. Since all the steps are known, we should",gauss elim
be able to predict the number of operations.,gauss elim
"For the moment, ignore the right-hand sides of the equations, and count only the",gauss elim
operations on the left. These operations are of two kinds. We divide by the pivot to,gauss elim
ﬁnd out what multiple (say ℓ) of the pivot equation is to be subtracted. When we do,gauss elim
"this subtraction, we continually meet a “multiply-subtract” combination; the terms in",gauss elim
"the pivot equation are multiplied by ℓ, and then subtracted from another equation.",gauss elim
"Suppose we call each division, and each multiplication-subtraction, one operation. In",gauss elim
"column 1, it takes n operations for every zero we achieve—one to ﬁnd the multiple ℓ,",gauss elim
and the other to ﬁnd the new entries along the row. There are n−1 rows underneath the,gauss elim
"ﬁrst one, so the ﬁrst stage of elimination needs n(n − 1) = n2 − n operations. (Another",gauss elim
"approach to n2 − n is this: All n2 entries need to be changed, except the n in the ﬁrst",gauss elim
row.) Later stages are faster because the equations are shorter.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"When the elimination is down to k equations, only k2 − k operations are needed to",gauss elim
clear out the column below the pivot—by the same reasoning that applied to the ﬁrst,gauss elim
"stage, when k equaled n. Altogether, the total number of operations is the sum of k2 −k",gauss elim
over all values of k from 1 to n:,gauss elim
(12 +···+n2)−(1+···+n) = n(n+1)(2n+1),gauss elim
Those are standard formulas for the sums of the ﬁrst n numbers and the ﬁrst n squares.,gauss elim
Substituting n = 1 and n = 2 and n = 100 into the formula 1,gauss elim
can take no steps or two steps or about a third of a million steps:,gauss elim
"If n is at all large, a good estimate for the number of operations is 1",gauss elim
"If the size is doubled, and few of the coefﬁcients are zero, the cost is multiplied by 8.",gauss elim
Back-substitution is considerably faster. The last unknown is found in only one oper-,gauss elim
"ation (a division by the last pivot). The second to last unknown requires two operations,",gauss elim
and so on. Then the total for back-substitution is 1+2+···+n.,gauss elim
Forward elimination also acts on the right-hand side (subtracting the same multiples,gauss elim
as on the left to maintain correct equations). This starts with n − 1 subtractions of the,gauss elim
ﬁrst equation. Altogether the right-hand side is responsible for n2 operations—much,gauss elim
less than the n3/3 on the left. The total for forward and back is,gauss elim
"Thirty years ago, almost every mathematician would have guessed that a general sys-",gauss elim
tem of order n could not be solved with much fewer than n3/3 multiplications. (There,gauss elim
"were even theorems to demonstrate it, but they did not allow for all possible methods.)",gauss elim
"Astonishingly, that guess has been proved wrong. There now exists a method that re-",gauss elim
quires only Cnlog2 7 multiplications! It depends on a simple fact: Two combinations of,gauss elim
"two vectors in two-dimensional space would seem to take 8 multiplications, but they can",gauss elim
"be done in 7. That lowered the exponent from log28, which is 3, to log27 ≈ 2.8. This",gauss elim
discovery produced tremendous activity to ﬁnd the smallest possible power of n. The,gauss elim
"exponent ﬁnally fell (at IBM) below 2.376. Fortunately for elimination, the constant C",gauss elim
is so large and the coding is so awkward that the new method is largely (or entirely) of,gauss elim
theoretical interest. The newest problem is the cost with many processors in parallel.,gauss elim
Problems 1–9 are about elimination on 2 by 2 systems.,gauss elim
1.3 An Example of Gaussian Elimination,gauss elim
1. What multiple ℓ of equation 1 should be subtracted from equation 2?,gauss elim
10x + 9y = 11.,gauss elim
"After this elimination step, write down the upper triangular system and circle the two",gauss elim
pivots. The numbers 1 and 11 have no inﬂuence on those pivots.,gauss elim
"2. Solve the triangular system of Problem 1 by back-substitution, y before x. Verify",gauss elim
"that x times (2,10) plus y times (3,9) equals (1,11). If the right-hand side changes",gauss elim
"to (4,44), what is the new solution?",gauss elim
3. What multiple of equation 2 should be subtracted from equation 3?,gauss elim
−x + 5y = 0.,gauss elim
"After this elimination step, solve the triangular system. If the right-hand side changes",gauss elim
"to (−6,0), what is the new solution?",gauss elim
4. What multiple ℓ of equation 1 should be subtracted from equation 2?,gauss elim
ax + by =,gauss elim
+ dy = g.,gauss elim
The ﬁrst pivot is a (assumed nonzero). Elimination produces what formula for the,gauss elim
second pivot? What is y? The second pivot is missing when ad = bc.,gauss elim
5. Choose a right-hand side which gives no solution and another right-hand side which,gauss elim
gives inﬁnitely many solutions. What are two of those solutions?,gauss elim
3x + 2y =,gauss elim
6x + 4y =,gauss elim
6. Choose a coefﬁcient b that makes this system singular. Then choose a right-hand,gauss elim
side g that makes it solvable. Find two solutions in that singular case.,gauss elim
2x + by = 16,gauss elim
4x + 8y =,gauss elim
"7. For which numbers a does elimination break down (a) permanently, and (b) tem-",gauss elim
ax + 3y = −3,gauss elim
4x + 6y =,gauss elim
Solve for x and y after ﬁxing the second breakdown by a row exchange.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
8. For which three numbers k does elimination break down? Which is ﬁxed by a row,gauss elim
"exchange? In each case, is the number of solutions 0 or 1 or ∞?",gauss elim
kx + 3y =,gauss elim
3x + ky = −6.,gauss elim
9. What test on b1 and b2 decides whether these two equations allow a solution? How,gauss elim
many solutions will they have? Draw the column picture.,gauss elim
3x − 2y =,gauss elim
6x − 4y = b2.,gauss elim
Problems 10–19 study elimination on 3 by 3 systems (and possible failure).,gauss elim
10. Reduce this system to upper triangular form by two row operations:,gauss elim
2x + 3y +,gauss elim
4x + 7y + 5z = 20,gauss elim
− 2y + 2z =,gauss elim
"Circle the pivots. Solve by back-substitution for z, y, x.",gauss elim
11. Apply elimination (circle the pivots) and back-substitution to solve,gauss elim
4x − 5y +,gauss elim
− 3z = 5.,gauss elim
List the three row operations: Subtract,gauss elim
"12. Which number d forces a row exchange, and what is the triangular system (not sin-",gauss elim
gular) for that d? Which d makes this system singular (no third pivot)?,gauss elim
2x + 5y + z =,gauss elim
4x + dy + z =,gauss elim
− z = 3.,gauss elim
13. Which number b leads later to a row exchange? Which b leads to a missing pivot?,gauss elim
"In that singular case ﬁnd a nonzero solution x, y, z.",gauss elim
x − 2y − z =,gauss elim
+ z = 0.,gauss elim
14. (a) Construct a 3 by 3 system that needs two row exchanges to reach a triangular,gauss elim
form and a solution.,gauss elim
"(b) Construct a 3 by 3 system that needs a row exchange to keep going, but breaks",gauss elim
1.3 An Example of Gaussian Elimination,gauss elim
"15. If rows 1 and 2 are the same, how far can you get with elimination (allowing row",gauss elim
"exchange)? If columns 1 and 2 are the same, which pivot is missing?",gauss elim
"16. Construct a 3 by 3 example that has 9 different coefﬁcients on the left-hand side, but",gauss elim
rows 2 and 3 become zero in elimination. How many solutions to your system with,gauss elim
"b = (1,10,100) and how many with b = (0,0,0)?",gauss elim
17. Which number q makes this system singular and which right-hand side t gives it,gauss elim
inﬁnitely many solutions? Find the solution that has z = 1.,gauss elim
x + 4y − 2z =,gauss elim
x + 7y − 6z =,gauss elim
3y + qz = t.,gauss elim
18. (Recommended) It is impossible for a system of linear equations to have exactly two,gauss elim
"(a) If (x,y,z) and (X,Y,Z) are two solutions, what is another one?",gauss elim
"(b) If 25 planes meet at two points, where else do they meet?",gauss elim
"19. Three planes can fail to have an intersection point, when no two planes are parallel.",gauss elim
The system is singular if row 3 of A is a,gauss elim
of the ﬁrst two rows. Find a third,gauss elim
equation that can’t be solved if x+y+z = 0 and x−2y−z = 1.,gauss elim
Problems 20–22 move up to 4 by 4 and n by n.,gauss elim
20. Find the pivots and the solution for these four equations:,gauss elim
"21. If you extend Problem 20 following the 1, 2, 1 pattern or the −1, 2, −1 pattern, what",gauss elim
is the ﬁfth pivot? What is the nth pivot?,gauss elim
22. Apply elimination and back-substitution to solve,gauss elim
4u + 5v +,gauss elim
− 3w = 5.,gauss elim
What are the pivots? List the three operations in which a multiple of one row is,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
23. For the system,gauss elim
u + 3v + 3w =,gauss elim
"u + 3v + 5w = 2,",gauss elim
"what is the triangular system after forward elimination, and what is the solution?",gauss elim
24. Solve the system and ﬁnd the pivots when,gauss elim
−u + 2v −,gauss elim
+ 2z = 5.,gauss elim
"You may carry the right-hand side as a ﬁfth column (and omit writing u, v, w, z until",gauss elim
the solution at the end).,gauss elim
25. Apply elimination to the system,gauss elim
3u + 3v − w =,gauss elim
+ w = −1.,gauss elim
"When a zero arises in the pivot position, exchange that equation for the one below it",gauss elim
"and proceed. What coefﬁcient of v in the third equation, in place of the present −1,",gauss elim
would make it impossible to proceed—and force elimination to break down?,gauss elim
26. Solve by elimination the system of two equations,gauss elim
3x + 6y = 18.,gauss elim
Draw a graph representing each equation as a straight line in the x-y plane; the lines,gauss elim
"intersect at the solution. Also, add one more line—the graph of the new second",gauss elim
equation which arises after elimination.,gauss elim
"27. Find three values of a for which elimination breaks down, temporarily or perma-",gauss elim
4u + av = 2.,gauss elim
Breakdown at the ﬁrst step can be ﬁxed by exchanging rows—but not breakdown at,gauss elim
28. True or false:,gauss elim
(a) If the third equation starts with a zero coefﬁcient (it begins with 0u) then no,gauss elim
multiple of equation 1 will be subtracted from equation 3.,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
(b) If the third equation has zero as its second coefﬁcient (it contains 0v) then no,gauss elim
multiple of equation 2 will be subtracted from equation 3.,gauss elim
"(c) If the third equation contains 0u and 0v, then no multiple of equation 1 or equa-",gauss elim
tion 2 will be subtracted from equation 3.,gauss elim
29. (Very optional) Normally the multiplication of two complex numbers,gauss elim
"involves the four separate multiplications ac, bd, be, ad. Ignoring i, can you compute",gauss elim
"ac−bd and bc+ad with only three multiplications? (You may do additions, such as",gauss elim
"forming a+b before multiplying, without any penalty.)",gauss elim
30. Use elimination to solve,gauss elim
+ 2v + 2w = 11,gauss elim
2u + 3v − 4w =,gauss elim
+ 2v + 2w = 10,gauss elim
2u + 3v − 4w =,gauss elim
31. For which three numbers a will elimination fail to give three pivots?,gauss elim
32. Find experimentally the average size (absolute value) of the ﬁrst and second and third,gauss elim
"pivots for MATLAB’s lu(rand(3,3)). The average of the ﬁrst pivot from abs(A(1,1))",gauss elim
Matrix Notation and Matrix Multiplication,gauss elim
"With our 3 by 3 example, we are able to write out all the equations in full. We can list",gauss elim
"the elimination steps, which subtract a multiple of one equation from another and reach",gauss elim
"a triangular matrix. For a large system, this way of keeping track of elimination would",gauss elim
be hopeless; a much more concise record is needed.,gauss elim
"We now introduce matrix notation to describe the original system, and matrix mul-",gauss elim
tiplication to describe the operations that make it simpler. Notice that three different,gauss elim
types of quantities appear in our example:,gauss elim
−2u + 7v + 2w =,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"On the right-hand side is the column vector b. On the left-hand side are the unknowns u,",gauss elim
"v, w. Also on the left-hand side are nine coefﬁcients (one of which happens to be zero).",gauss elim
It is natural to represent the three unknowns by a vector:,gauss elim
The unknown is x =,gauss elim
The solution is x =,gauss elim
"The nine coefﬁcients fall into three rows and three columns, producing a 3 by 3 matrix:",gauss elim
"A is a square matrix, because the number of equations equals the number of unknowns.",gauss elim
"If there are n equations in n unknowns, we have a square n by n matrix. More generally,",gauss elim
"we might have m equations and n unknowns. Then A is rectangular, with m rows and n",gauss elim
columns. It will be an “m by n matrix.”,gauss elim
"Matrices are added to each other, or multiplied by numerical constants, exactly as",gauss elim
vectors are—one entry at a time. In fact we may regard vectors as special cases of,gauss elim
"matrices; they are matrices with only one column. As with vectors, two matrices can be",gauss elim
added only if they have the same shape:,gauss elim
Multiplication of a Matrix and a Vector,gauss elim
"We want to rewrite the three equations with three unknowns u, v, w in the simpliﬁed",gauss elim
"matrix form Ax = b. Written out in full, matrix times vector equals vector:",gauss elim
Matrix form Ax = b,gauss elim
The right-hand side b is the column vector of “inhomogeneous terms.” The left-hand,gauss elim
side is A times x. This multiplication will be deﬁned exactly so as to reproduce the,gauss elim
original system. The ﬁrst component of Ax comes from “multiplying” the ﬁrst row of A,gauss elim
into the column vector x:,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
"The second component of the product Ax is 4u−6v+0w, from the second row of A. The",gauss elim
matrix equation Ax = b is equivalent to the three simultaneous equations in equation (1).,gauss elim
Row times column is fundamental to all matrix multiplications. From two vectors it,gauss elim
produces a single number. This number is called the inner product of the two vectors.,gauss elim
"In other words, the product of a 1 by n matrix (a row vector) and an n by 1 matrix (a",gauss elim
column vector) is a 1 by 1 matrix:,gauss elim
"This conﬁrms that the proposed solution x = (1,1,2) does satisfy the ﬁrst equation.",gauss elim
There are two ways to multiply a matrix A and a vector x. One way is a row at a,gauss elim
"time, Each row of A combines with x to give a component of Ax. There are three inner",gauss elim
products when A has three rows:,gauss elim
"That is how Ax is usually explained, but the second way is equally important. In fact it is",gauss elim
more important! It does the multiplication a column at a time. The product Ax is found,gauss elim
"all at once, as a combination of the three columns of A:",gauss elim
The answer is twice column 1 plus 5 times column 2. It corresponds to the “column,gauss elim
"picture” of linear equations. If the right-hand side b has components 7, 6, 7, then the",gauss elim
"solution has components 2, 5, 0. Of course the row picture agrees with that (and we",gauss elim
eventually have to do the same multiplications).,gauss elim
"The column rule will be used over and over, and we repeat it for emphasis:",gauss elim
Every product Ax can be found using whole columns as in equation (5).,gauss elim
Therefore Ax is a combination of the columns of A. The coefﬁcients are the,gauss elim
"To multiply A times x in n dimensions, we need a notation for the individual entries in",gauss elim
A. The entry in the ith row and jth column is always denoted by aij. The ﬁrst subscript,gauss elim
"gives the row number, and the second subscript indicates the column. (In equation (4),",gauss elim
"a21 is 3 and a13 is 6.) If A is an m by n matrix, then the index i goes from 1 to m—there",gauss elim
"are m rows—and the index j goes from 1 to n. Altogether the matrix has mn entries, and",gauss elim
amn is in the lower right corner.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
One subscript is enough for a vector. The jth component of x is denoted by x j. (The,gauss elim
"multiplication above had x1 = 2, x2 = 5, x3 = 0.) Normally x is written as a column",gauss elim
"vector—like an n by 1 matrix. But sometimes it is printed on a line, as in x = (2,5,0).",gauss elim
The parentheses and commas emphasize that it is not a 1 by 3 matrix. It is a column,gauss elim
"vector, and it is just temporarily lying down.",gauss elim
"To describe the product Ax, we use the “sigma” symbol Σ for summation:",gauss elim
The ith component of Ax is,gauss elim
This sum takes us along the ith row of A. The column index j takes each value from 1,gauss elim
to n and we add up the results—the sum is ai1x1 +ai2x2 +···+ainxn.,gauss elim
We see again that the length of the rows (the number of columns in A) must match,gauss elim
the length of x. An m by n matrix multiplies an n-dimensional vector (and produces,gauss elim
"an m-dimensional vector). Summations are simpler than writing everything out in full,",gauss elim
"but matrix notation is better. (Einstein used “tensor notation,” in which a repeated index",gauss elim
automatically means summation. He wrote aijx j or even a j,gauss elim
"i x j, without the Σ. Not being",gauss elim
"Einstein, we keep the Σ.)",gauss elim
The Matrix Form of One Elimination Step,gauss elim
So far we have a convenient shorthand Ax = b for the original system of equations.,gauss elim
"What about the operations that are carried out during elimination? In our example, the",gauss elim
"ﬁrst step subtracted 2 times the ﬁrst equation from the second. On the right-hand side,",gauss elim
2 times the ﬁrst component of b was subtracted from the second component. The same,gauss elim
result is achieved if we multiply b by this elementary matrix (or elimination matrix):,gauss elim
This is veriﬁed just by obeying the rule for multiplying a matrix and a vector:,gauss elim
"The components 5 and 9 stay the same (because of the 1, 0, 0 and 0, 0, 1 in the rows of",gauss elim
E). The new second component −12 appeared after the ﬁrst elimination step.,gauss elim
"It is easy to describe the matrices like E, which carry out the separate elimination",gauss elim
"steps. We also notice the “identity matrix,” which does nothing at all.",gauss elim
"The identity matrix I, with 1s on the diagonal and 0s everywhere else,",gauss elim
leaves every vector unchanged. The elementary matrix Eij subtracts ℓ times,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
"row j from row i. This Ei j includes −ℓ in row i, column j.",gauss elim
�� has Ib = b,gauss elim
�� has E31b =,gauss elim
Ib = b is the matrix analogue of multiplying by 1. A typical elimination step,gauss elim
multiplies by E31. The important question is: What happens to A on the left-,gauss elim
"To maintain equality, we must apply the same operation to both sides of Ax = b. In",gauss elim
"other words, we must also multiply the vector Ax by the matrix E. Our original matrix",gauss elim
"E subtracts 2 times the ﬁrst component from the second, After this step the new and",gauss elim
simpler system (equivalent to the old) is just E(Ax) = Eb. It is simpler because of the,gauss elim
zero that was created below the ﬁrst pivot. It is equivalent because we can recover the,gauss elim
original system (by adding 2 times the ﬁrst equation back to the second). So the two,gauss elim
systems have exactly the same solution x.,gauss elim
Now we come to the most important question: How do we multiply two matrices? There,gauss elim
"is a partial clue from Gaussian elimination: We know the original coefﬁcient matrix A,",gauss elim
"we know the elimination matrix E, and we know the result EA after the elimination step.",gauss elim
We hope and expect that,gauss elim
�� times A =,gauss elim
Twice the ﬁrst row of A has been subtracted from the second row. Matrix multiplication,gauss elim
is consistent with the row operations of elimination. We can write the result either as,gauss elim
"E(Ax) = Eb, applying E to both sides of our equation, or as (EA)x = Eb. The matrix",gauss elim
"EA is constructed exactly so that these equations agree, and we don’t need parentheses:",gauss elim
(EA times x) equals (E times Ax). We just write EAx.,gauss elim
This is the whole point of an “associative law” like 2 × (3 × 4) = (2 × 3) × 4. The law,gauss elim
seems so obvious that it is hard to imagine it could be false. But the same could be said,gauss elim
of the “commutative law” 2×3 = 3×2—and for matrices EA is not AE.,gauss elim
"There is another requirement on matrix multiplication. We know how to multiply Ax,",gauss elim
a matrix and a vector. The new deﬁnition should be consistent with that one. When,gauss elim
"a matrix B contains only a single column x, the matrix-matrix product AB should be",gauss elim
identical with the matrix-vector product Ax. More than that: When B contains several,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"columns b1, b2, b3, the columns of AB should be Ab1, Ab2, Ab3!",gauss elim
"Our ﬁrst requirement had to do with rows, and this one is concerned with columns. A",gauss elim
"third approach is to describe each individual entry in AB and hope for the best. In fact,",gauss elim
"there is only one possible rule, and I am not sure who discovered it. It makes everything",gauss elim
"work. It does not allow us to multiply every pair of matrices. If they are square, they",gauss elim
"must have the same size. If they are rectangular, they must not have the same shape;",gauss elim
the number of columns in A has to equal the number of rows in B. Then A can be,gauss elim
multiplied into each column of B.,gauss elim
"If A is m by n, and B is n by p, then multiplication is possible. The product AB will",gauss elim
be m by p. We now ﬁnd the entry in row i and column j of AB.,gauss elim
"The i, j entry of AB is the inner product of the ith row of A and the jth",gauss elim
"column of B. In Figure 1.7, the 3, 2 entry of AB comes from row 3 and column",gauss elim
(AB)32 = a31b12 +a32b22 +a33b32 +a34b42.,gauss elim
Figure 1.7: A 3 by 4 matrix A times a 4 by 2 matrix B is a 3 by 2 matrix AB.,gauss elim
Note. We write AB when the matrices have nothing special to do with elimination. Our,gauss elim
"earlier example was EA, because of the elementary matrix E. Later we have PA, or LU,",gauss elim
or even LDU. The rule for matrix multiplication stays the same.,gauss elim
"The entry 17 is (2)(1)+(3)(5), the inner product of the ﬁrst row of A and ﬁrst column",gauss elim
"of B. The entry 8 is (4)(2)+(0)(−1), from the second row and second column.",gauss elim
"The third column is zero in B, so it is zero in AB. B consists of three columns side by",gauss elim
"side, and A multiplies each column separately. Every column of AB is a combination",gauss elim
"of the columns of A. Just as in a matrix-vector multiplication, the columns of A are",gauss elim
multiplied by the entries in B.,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
Example 3. The 1s in the identity matrix I leave every matrix unchanged:,gauss elim
"Important: The multiplication AB can also be done a row at a time. In Example 1, the",gauss elim
ﬁrst row of AB uses the numbers 2 and 3 from the ﬁrst row of A. Those numbers give,gauss elim
"2[row 1] + 3[row 2] = [17 1 0]. Exactly as in elimination, where all this started, each",gauss elim
row of AB is a combination of the rows of B.,gauss elim
We summarize these three different ways to look at matrix multiplication.,gauss elim
(i) Each entry of AB is the product of a row and a column:,gauss elim
(AB)ij = (row i of A) times (column j of B),gauss elim
(ii) Each column of AB is the product of a matrix and a column:,gauss elim
column j of AB = A times (column j of B),gauss elim
(iii) Each row of AB is the product of a row and a matrix:,gauss elim
row i of AB = (row i of A) times B.,gauss elim
This leads hack to a key property of matrix multiplication. Suppose the shapes of,gauss elim
"three matrices A, B, C (possibly rectangular) permit them to be multiplied. The rows in",gauss elim
A and B multiply the columns in B and C. Then the key property is this:,gauss elim
Matrix multiplication is associative: (AB)C = A(BC). Just write ABC.,gauss elim
AB times C equals A times BC. If C happens to be just a vector (a matrix with only one,gauss elim
column) this is the requirement (EA)x = E(Ax) mentioned earlier. It is the whole basis,gauss elim
"for the laws of matrix multiplication. And if C has several columns, we have only to",gauss elim
"think of them placed side by side, and apply the same rule several times. Parentheses",gauss elim
are not needed when we multiply several matrices.,gauss elim
There are two more properties to mention—one property that matrix multiplication,gauss elim
"has, and another which it does not have. The property that it does possess is:",gauss elim
Matrix operations are distributive:,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
Of course the shapes of these matrices must match properly—B and C have the same,gauss elim
"shape, so they can be added, and A and D are the right size for premultiplication and",gauss elim
postmultiplication. The proof of this law is too boring for words.,gauss elim
The property that fails to hold is a little more interesting:,gauss elim
Matrix multiplication is not commutative: Usually FE ̸= EF.,gauss elim
Example 4. Suppose E subtracts twice the ﬁrst equation from the second. Suppose F,gauss elim
"is the matrix for the next step, to add row 1 to row 3:",gauss elim
These two matrices do commute and the product does both steps at once:,gauss elim
"In either order, EF or FE, this changes rows 2 and 3 using row 1.",gauss elim
Example 5. Suppose E is the same but G adds row 2 to row 3. Now the order makes a,gauss elim
"difference. When we apply E and then G, the second row is altered before it affects the",gauss elim
"third. If E comes after G, then the third equation feels no effect from the ﬁrst. You will",gauss elim
"see a zero in the (3,1) entry of EG, where there is a −2 in GE:",gauss elim
Thus EG ̸= GE. A random example would show the same thing—most matrices don’t,gauss elim
"commute. Here the matrices have meaning. There was a reason for EF = FE, and a",gauss elim
"reason for EG ̸= GE. It is worth taking one more step, to see what happens with all",gauss elim
three elimination matrices at once:,gauss elim
The product GFE is the true order of elimination. It is the matrix that takes the original,gauss elim
A to the upper triangular U. We will see it again in the next section.,gauss elim
"The other matrix EFG is nicer. In that order, the numbers −2 from E and 1 from F",gauss elim
and G were not disturbed. They went straight into the product. It is the wrong order for,gauss elim
elimination. But fortunately it is the right order for reversing the elimination steps—,gauss elim
which also comes in the next section.,gauss elim
Notice that the product of lower triangular matrices is again lower triangular.,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
1. Compute the products,gauss elim
"For the third one, draw the column vectors (2,1) and (0,3). Multiplying by (1,1)",gauss elim
just adds the vectors (do it graphically).,gauss elim
"2. Working a column at a time, compute the products",gauss elim
3. Find two inner products and a matrix product:,gauss elim
The ﬁrst gives the length of the vector (squared).,gauss elim
"4. If an m by n matrix A multiplies an n-dimensional vector x, how many separate",gauss elim
multiplications are involved? What if A multiplies an n by p matrix B?,gauss elim
5. Multiply Ax to ﬁnd a solution vector x to the system Ax = zero vector. Can you ﬁnd,gauss elim
more solutions to Ax = 0?,gauss elim
6. Write down the 2 by 2 matrices A and B that have entries aij = i+ j and bij = (−1)i+j.,gauss elim
Multiply them to ﬁnd AB and BA.,gauss elim
7. Give 3 by 3 examples (not just the zero matrix) of,gauss elim
(a) a diagonal matrix: aij = 0 if i ̸= j.,gauss elim
(b) a symmetric matrix: ai j = a ji for all i and j.,gauss elim
(c) an upper triangular matrix: aij = 0 if i > j.,gauss elim
(d) a skew-symmetric matrix: aij = −a ji for all i and j.,gauss elim
8. Do these subroutines multiply Ax by rows or columns? Start with B(I) = 0:,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"DO 10 I = 1, N",gauss elim
"DO 10 J = 1, N",gauss elim
"DO 10 J = 1, N",gauss elim
"DO 10 I = 1, N",gauss elim
"B(I) = B(I) + A(I,J) * X(J)",gauss elim
"B(I) = B(I) + A(I,J) * X(J)",gauss elim
The outputs Bx = Ax are the same. The second code is slightly more efﬁcient in,gauss elim
FORTRAN and much more efﬁcient on a vector machine (the ﬁrst changes single,gauss elim
"entries B(I), the second can update whole vectors).",gauss elim
"9. If the entries of A are aij, use subscript notation to write",gauss elim
(a) the ﬁrst pivot.,gauss elim
(b) the multiplier ℓi1 of row 1 to be subtracted from row i.,gauss elim
(c) the new entry that replaces aij after that subtraction.,gauss elim
(d) the second pivot.,gauss elim
10. True or false? Give a speciﬁc counterexample when false.,gauss elim
"(a) If columns 1 and 3 of B are the same, so are columns 1 and 3 of AB.",gauss elim
"(b) If rows 1 and 3 of B are the same, so are rows 1 and 3 of AB.",gauss elim
"(c) If rows 1 and 3 of A are the same, so are rows 1 and 3 of AB.",gauss elim
(d) (AB)2 = A2B2.,gauss elim
11. The ﬁrst row of AB is a linear combination of all the rows of B. What are the coefﬁ-,gauss elim
"cients in this combination, and what is the ﬁrst row of AB, if",gauss elim
12. The product of two lower triangular matrices is again lower triangular (all its entries,gauss elim
"above the main diagonal are zero). Conﬁrm this with a 3 by 3 example, and then",gauss elim
explain how it follows from the laws of matrix multiplication.,gauss elim
13. By trial and error ﬁnd examples of 2 by 2 matrices such that,gauss elim
"(a) A2 = −I, A having only real entries.",gauss elim
"(b) B2 = 0, although B ̸= 0.",gauss elim
"(c) CD = −DC, not allowing the case CD = 0.",gauss elim
"(d) EF = 0, although no entries of E or F are zero.",gauss elim
14. Describe the rows of EA and the columns of AE if,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
"15. Suppose A commutes with every 2 by 2 matrix (AB = BA), and in particular",gauss elim
commutes with B1 =,gauss elim
"Show that a = d and b = c = 0. If AB = BA for all matrices B, then A is a multiple",gauss elim
"16. Let x be the column vector (1,0,...,0). Show that the rule (AB)x = A(Bx) forces the",gauss elim
ﬁrst column of AB to equal A times the ﬁrst column of B.,gauss elim
17. Which of the following matrices are guaranteed to equal (A+B)2?,gauss elim
"18. If A and B are n by n matrices with all entries equal to 1, ﬁnd (AB)ij. Summation",gauss elim
"notation turns the product AB, and the law (AB)C = A(BC), into",gauss elim
c jl = ∑,gauss elim
"Compute both sides if C is also n by n, with every c jl = 2.",gauss elim
19. A fourth way to multiply matrices is columns of A times rows of B:,gauss elim
AB = (column 1)(row 1)+···+(column n)(row n) = sum of simple matrices.,gauss elim
Give a 2 by 2 example of this important rule for matrix multiplication.,gauss elim
20. The matrix that rotates the x-y plane by an angle θ is,gauss elim
Verify that A(θ1)A(θ2) = A(θ1+θ2) from the identities for cos(θ1+θ2) and sin(θ1+,gauss elim
θ2). What is A(θ) times A(−θ)?,gauss elim
"21. Find the powers A2, A3 (A2 times A), and B2, B3, C2, C3. What are Ak, Bk, and Ck?",gauss elim
C = AB =,gauss elim
Problems 22–31 are about elimination matrices.,gauss elim
22. Write down the 3 by 3 matrices that produce these elimination steps:,gauss elim
(a) E21 subtracts 5 times row 1 from row 2.,gauss elim
(b) E32 subtracts −7 times row 2 from row 3.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"(c) P exchanges rows 1 and 2, then rows 2 and 3.",gauss elim
"23. In Problem 22, applying E21 and then E32 to the column b = (1,0,0) gives E32E21b =",gauss elim
. Applying E32 before E21 gives E21E32b =,gauss elim
". When E32 comes ﬁrst, row",gauss elim
feels no effect from row,gauss elim
"24. Which three matrices E21, E31, E32 put A into triangular form U?",gauss elim
Multiply those E’s to get one matrix M that does elimination: MA = U.,gauss elim
"25. Suppose a33 = 7 and the third pivot is 5. If you change a33 to 11, the third pivot is",gauss elim
. If you change a33 to,gauss elim
", there is zero in the pivot position.",gauss elim
"26. If every column of A is a multiple of (1,1,1), then Ax is always a multiple of (1,1,1).",gauss elim
Do a 3 by 3 example. How many pivots are produced by elimination?,gauss elim
"27. What matrix E31 subtracts 7 times row 1 from row 3? To reverse that step, R31 should",gauss elim
. Multiply E31 by R31.,gauss elim
28. (a) E21 subtracts row 1 from row 2 and then P23 exchanges rows 2 and 3. What,gauss elim
matrix M = P23E21 does both steps at once?,gauss elim
(b) P23 exchanges rows 2 and 3 and then E31 subtracts row I from row 3. What,gauss elim
matrix M = E31P23 does both steps at once? Explain why the M’s are the same,gauss elim
but the E’s are different.,gauss elim
29. (a) What 3 by 3 matrix E13 will add row 3 to row 1?,gauss elim
(b) What matrix adds row 1 to row 3 and at the same time adds row 3 to row 1?,gauss elim
(c) What matrix adds row 1 to row 3 and then adds row 3 to row 1?,gauss elim
30. Multiply these matrices:,gauss elim
31. This 4 by 4 matrix needs which elimination matrices E21 and E32 and E43?,gauss elim
Problems 32–44 are about creating and multiplying matrices,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
32. Write these ancient problems in a 2 by 2 matrix form Ax = b and solve them:,gauss elim
"(a) X is twice as old as Y and their ages add to 39,",gauss elim
"(b) (x,y) = (2,5) and (3,7) lie on the line y = mx+c. Find m and c.",gauss elim
"33. The parabola y = a + bx + cx2 goes through the points (x,y) = (1,4) and (2,8) and",gauss elim
"(3,14). Find and solve a matrix equation for the unknowns (a,b,c).",gauss elim
34. Multiply these matrices in the orders EF and FE and E2:,gauss elim
"35. (a) Suppose all columns of B are the same. Then all columns of EB are the same,",gauss elim
because each one is E times,gauss elim
(b) Suppose all rows of B are [1 2 4]. Show by example that all rows of EB are not,gauss elim
[1 2 4]. It is true that those rows are,gauss elim
"36. If E adds row 1 to row 2 and F adds row 2 to row 1, does EF equal FE?",gauss elim
37. The ﬁrst component of Ax is ∑a1 jx j = a11x1 + ··· + a1nxn. Write formulas for the,gauss elim
"third component of Ax and the (1,1) entry of A2.",gauss elim
"38. If AB = I and BC = I, use the associative law to prove A = C.",gauss elim
"39. A is 3 by 5, B is 5 by 3, C is 5 by 1, and D is 3 by 1. All entries are 1. Which of these",gauss elim
"matrix operations are allowed, and what are the results?",gauss elim
40. What rows or columns or matrices do you multiply to ﬁnd,gauss elim
(a) the third column of AB?,gauss elim
(b) the ﬁrst row of AB?,gauss elim
"(c) the entry in row 3, column 4 of AB?",gauss elim
"(d) the entry in row 1, column 1 of CDE?",gauss elim
"41. (3 by 3 matrices) Choose the only B so that for every matrix A,",gauss elim
(a) BA = 4A.,gauss elim
(b) BA = 4B.,gauss elim
(c) BA has rows 1 and 3 of A reversed and row 2 unchanged.,gauss elim
(d) All rows of BA are the same as row 1 of A.,gauss elim
42. True or false?,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
(a) If A2 is deﬁned then A is necessarily square.,gauss elim
(b) If AB and BA are deﬁned then A and B are square.,gauss elim
(c) If AB and BA are deﬁned then AB and BA are square.,gauss elim
(d) If AB = B then A = I.,gauss elim
"43. If A is m by n, how many separate multiplications are involved when",gauss elim
(a) A multiplies a vector x with n components?,gauss elim
(b) A multiplies an n by p matrix B? Then AB is m by p.,gauss elim
(c) A multiplies itself to produce A2? Here m = n.,gauss elim
"44. To prove that (AB)C = A(BC), use the column vectors b1,...,bn of B. First suppose",gauss elim
"that C has only one column c with entries c1,...,cn:",gauss elim
"AB has columns Ab1,...,Abn, and Bc has one column c1b1 +···+cnbn.",gauss elim
"Then (AB)c = c1Ab1 +···+cnAbn, equals A(c1b1 +···+cnbn) = A(Bc).",gauss elim
"Linearity gives equality of those two sums, and (AB)c = A(Bc). The same is true for",gauss elim
of C. Therefore (AB)C = A(BC).,gauss elim
Problems 45–49 use column-row multiplication and block multiplication.,gauss elim
45. Multiply AB using columns times rows:,gauss elim
46. Block multiplication separates matrices into blocks (submatrices). If their shapes,gauss elim
"make block multiplication possible, then it is allowed. Replace these x’s by numbers",gauss elim
and conﬁrm that block multiplication succeeds.,gauss elim
47. Draw the cuts in A and B and AB to show how each of the four multiplication rules,gauss elim
is really a block multiplication to ﬁnd AB:,gauss elim
(a) Matrix A times columns of B.,gauss elim
(b) Rows of A times matrix B.,gauss elim
(c) Rows of A times columns of B.,gauss elim
(d) Columns of A times rows of B.,gauss elim
1.4 Matrix Notation and Matrix Multiplication,gauss elim
48. Block multiplication says that elimination on column 1 produces,gauss elim
"49. Elimination for a 2 by 2 block matrix: When A−1A = I, multiply the ﬁrst block row",gauss elim
"by CA−1 and subtract from the second row, to ﬁnd the “Schur complement” S:",gauss elim
"50. With i2 = −1, the product (A + iB)(x + iy) is Ax + iBx + iAy − By. Use blocks to",gauss elim
separate the real part from the imaginary part that multiplies i:,gauss elim
51. Suppose you solve Ax = b for three special right-hand sides b:,gauss elim
"If the solutions x1, x2, x3 are the columns of a matrix X, what is AX?",gauss elim
"52. If the three solutions in Question 51 are x1 = (1,1,1) and x2 = (0,1,1) and x3 =",gauss elim
"(0,0,1), solve Ax = b when b = (3,5,8). Challenge problem: What is A?",gauss elim
53. Find all matrices,gauss elim
"54. If you multiply a northwest matrix A and a southeast matrix B, what type of matri-",gauss elim
ces are AB and BA? “Northwest” and “southeast” mean zeros below and above the,gauss elim
"antidiagonal going from (1,n) to (n,1).",gauss elim
55. Write 2x+3y+z+5t = 8 as a matrix A (how many rows?) multiplying the column,gauss elim
"vector (x,y,z,t) to produce b. The solutions ﬁll a plane in four-dimensional space.",gauss elim
The plane is three-dimensional with no 4D volume.,gauss elim
"56. What 2 by 2 matrix P1 projects the vector (x,y) onto the x axis to produce (x,0)?",gauss elim
"What matrix P2 projects onto the y axis to produce (0,y)? If you multiply (5,7) by",gauss elim
"P1 and then multiply by P2, you get (",gauss elim
"57. Write the inner product of (1,4,5) and (x,y,z) as a matrix multiplication Ax. A has",gauss elim
one row. The solutions to Ax = 0 lie on a,gauss elim
perpendicular to the vector,gauss elim
columns of A are only in,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"58. In MATLAB notation, write the commands that deﬁne the matrix A and the column",gauss elim
vectors x and b. What command would test whether or not Ax = b?,gauss elim
59. The MATLAB commands A = eye(3) and v = [3:5]’ produce the 3 by 3 identity,gauss elim
"matrix and the column vector (3,4,5). What are the outputs from A ∗ v and v’ ∗ v?",gauss elim
"(Computer not needed!) If you ask for v ∗ A, what happens?",gauss elim
"60. If you multiply the 4 by 4 all-ones matrix A = ones(4,4) and the column v =",gauss elim
"ones(4,1), what is A ∗ v? (Computer not needed.) If you multiply B = eye(4)",gauss elim
"+ ones(4,4) times w = zeros(4,1) + 2 ∗ ones(4,1), what is B ∗ w?",gauss elim
"61. Invent a 3 by 3 magic matrix M with entries 1,2,...,9. All rows and columns and",gauss elim
"diagonals add to 15. The ﬁrst row could be 8, 3, 4. What is M times (1,1,1)? What",gauss elim
is the row vector,gauss elim
Triangular Factors and Row Exchanges,gauss elim
"We want to look again at elimination, to see what it means in terms of matrices. The",gauss elim
starting point was the model system Ax = b:,gauss elim
"Then there were three elimination steps, with multipliers 2, −1, −1:",gauss elim
Step 1. Subtract 2 times the ﬁrst equation from the second;,gauss elim
Step 2. Subtract −1 times the ﬁrst equation from the third;,gauss elim
Step 3. Subtract −1 times the second equation from the third.,gauss elim
"The result was an equivalent system Ux = c, with a new coefﬁcient matrix U:",gauss elim
This matrix U is upper triangular—all entries below the diagonal are zero.,gauss elim
The new right side c was derived from the original vector b by the same steps that,gauss elim
took A into U. Forward elimination amounted to three row operations:,gauss elim
1.5 Triangular Factors and Row Exchanges,gauss elim
Start with A and b;,gauss elim
"Apply steps 1, 2, 3 in that order;",gauss elim
End with U and c.,gauss elim
Ux = c is solved by back-substitution. Here we concentrate on connecting A to U.,gauss elim
"The matrices E for step 1, F for step 2, and G for step 3 were introduced in the",gauss elim
"previous section. They are called elementary matrices, and it is easy to see how they",gauss elim
"work. To subtract a multiple ℓ of equation j from equation i, put the number −ℓ into",gauss elim
"the (i, j) position. Otherwise keep the identity matrix, with 1s on the diagonal and 0s",gauss elim
elsewhere. Then matrix multiplication executes the row operation.,gauss elim
"The result of all three steps is GFEA = U. Note that E is the ﬁrst to multiply A,",gauss elim
"then F, then G. We could multiply GFE together to ﬁnd the single matrix that takes A",gauss elim
to U (and also takes b to c). It is lower triangular (zeros are omitted):,gauss elim
From A to U,gauss elim
"This is good, but the most important question is exactly the opposite: How would we",gauss elim
get from U back to A? How can we undo the steps of Gaussian elimination?,gauss elim
"To undo step 1 is not hard. Instead of subtracting, we add twice the ﬁrst row to the",gauss elim
second. (Not twice the second row to the ﬁrst!) The result of doing both the subtraction,gauss elim
and the addition is to bring back the identity matrix:,gauss elim
"One operation cancels the other. In matrix terms, one matrix is the inverse of the other.",gauss elim
"If the elementary matrix E has the number −ℓ in the (i, j) position, then its inverse E−1",gauss elim
"has +ℓ in that position. Thus E−1E = I, which is equation (4).",gauss elim
"We can invert each step of elimination, by using E−1 and F−1 and G−1. I think it’s",gauss elim
"not bad to see these inverses now, before the next section. The ﬁnal problem is to undo",gauss elim
"the whole process at once, and see what matrix takes U back to A.",gauss elim
"Since step 3 was last in going from A to U, its matrix G must be the ﬁrst to be",gauss elim
inverted in the reverse direction. Inverses come in the opposite order! The second,gauss elim
reverse step is F−1 and the last is E−1:,gauss elim
From U back to A,gauss elim
E−1F−1G−1U = A is LU = A.,gauss elim
"You can substitute GFEA for U, to see how the inverses knock out the original steps.",gauss elim
"Now we recognize the matrix L that takes U back to A. It is called L, because it is",gauss elim
lower triangular. And it has a special property that can be seen only by multiplying the,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
three inverse matrices in the right order:,gauss elim
"The special thing is that the entries below the diagonal are the multipliers ℓ = 2, −1,",gauss elim
"and −1. When matrices are multiplied, there is usually no direct way to read off the",gauss elim
answer. Here the matrices come in just the right order so that their product can be,gauss elim
written down immediately. If the computer stores each multiplier ℓij—the number that,gauss elim
"multiplies the pivot row j when it is subtracted from row i, and produces a zero in the i,",gauss elim
j position—then these multipliers give a complete record of elimination.,gauss elim
The numbers ℓij ﬁt right into the matrix L that takes U back to A.,gauss elim
Triangular factorization A = LU with no exchanges of rows. L is lower,gauss elim
"triangular, with 1s on the diagonal. The multipliers ℓij (taken from elimination)",gauss elim
are below the diagonal. U is the upper triangular matrix which appears after,gauss elim
"forward elimination, The diagonal entries of U are the pivots.",gauss elim
goes to U =,gauss elim
Then LU = A.,gauss elim
Example 2. (which needs a row exchange),gauss elim
cannot be factored into A = LU.,gauss elim
Example 3. (with all pivots and multipliers equal to 1),gauss elim
From A to U there are subtractions of rows. From U to A there are additions of rows.,gauss elim
Example 4. (when U is the identity and L is the same as A),gauss elim
"The elimination steps on this A are easy: (i) E subtracts ℓ21 times row 1 from row 2, (ii)",gauss elim
"F subtracts ℓ31 times row 1 from row 3, and (iii) G subtracts ℓ32 times row 2 from row 3.",gauss elim
"The result is the identity matrix U = I. The inverses of E, F, and G will bring back A:",gauss elim
1.5 Triangular Factors and Row Exchanges,gauss elim
E−1 applied to F−1 applied to G−1 applied to I produces A.,gauss elim
The order is right for the ℓ’s to fall into position. This always happens! Note that,gauss elim
parentheses in E−1F−1G−1 were not necessary because of the associative law.,gauss elim
A = LU: The n by n case,gauss elim
The factorization A = LU is so important that we must say more. It used to be missing,gauss elim
in linear algebra courses when they concentrated on the abstract side. Or maybe it was,gauss elim
thought to be too hard—but you have got it. If the last Example 4 allows any U instead,gauss elim
"of the particular U = I, we can see how the rule works in general. The matrix L, applied",gauss elim
"to U, brings back A:",gauss elim
row 1 of U,gauss elim
row 2 of U,gauss elim
row 3 of U,gauss elim
�� = original A.,gauss elim
The proof is to apply the steps of elimination. On the right-hand side they take A to U.,gauss elim
"On the left-hand side they reduce L to I, as in Example 4. (The ﬁrst step subtracts ℓ21",gauss elim
"times (1,0,0) from the second row, which removes ℓ21.) Both sides of (7) end up equal",gauss elim
"to the same matrix U, and the steps to get there are all reversible. Therefore (7) is correct",gauss elim
and A = LU.,gauss elim
"A = LU is so crucial, and so beautiful, that Problem 8 at the end of this section",gauss elim
"suggests a second approach. We are writing down 3 by 3 matrices, but you can see how",gauss elim
"the arguments apply to larger matrices. Here we give one more example, and then put",gauss elim
A = LU to use.,gauss elim
"Example 5. (A = LU, with zeros in the empty spaces)",gauss elim
That shows how a matrix A with three diagonals has factors L and U with two diagonals.,gauss elim
This example comes from an important problem in differential equations (Section 1.7).,gauss elim
The second difference in A is a backward difference L times a forward difference U.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
One Linear System = Two Triangular Systems,gauss elim
There is a serious practical point about A = LU. It is more than just a record of elimi-,gauss elim
nation steps; L and U are the right matrices to solve Ax = b. In fact A could be thrown,gauss elim
away! We go from b to c by forward elimination (this uses L) and we go from c to x by,gauss elim
back-substitution (that uses U). We can and should do it without A:,gauss elim
Splitting of Ax = b,gauss elim
"Multiply the second equation by L to give LUx = Lc, which is Ax = b. Each triangular",gauss elim
system is quickly solved. That is exactly what a good elimination code will do:,gauss elim
1. Factor (from A ﬁnd its factors L and U).,gauss elim
2. Solve (from L and U and b ﬁnd the solution x).,gauss elim
The separation into Factor and Solve means that a series of b’s can be processed. The,gauss elim
Solve subroutine obeys equation (8): two triangular systems in n2/2 steps each. The,gauss elim
solution for any new right-hand side b can be found in only n2 operations. That is,gauss elim
far below the n3/3 steps needed to factor A on the left-hand side.,gauss elim
"Example 6. This is the previous matrix A with a right-hand side b = (1,1,1,1).",gauss elim
splits into Lc = b and Ux = c.,gauss elim
"For these special “tridiagonal matrices,” the operation count drops from n2 to 2n. You",gauss elim
see how Lc = b is solved forward (c1 comes before c2). This is precisely what happens,gauss elim
during forward elimination. Then Ux = c is solved backward (x4 before x3).,gauss elim
Remark 1. The LU form is “unsymmetric” on the diagonal: L has 1s where U has the,gauss elim
1.5 Triangular Factors and Row Exchanges,gauss elim
pivots. This is easy to correct. Divide out of U a diagonal pivot matrix D:,gauss elim
In the last example all pivots were di = 1. In that case D = I. But that was very excep-,gauss elim
"tional, and normally LU is different from LDU (also written LDV).",gauss elim
"The triangular factorization can be written A = LDU, where L and U have 1s on",gauss elim
the diagonal and D is the diagonal matrix of pivots.,gauss elim
"Whenever you see LDU or LDV, it is understood that U or V has is on the diagonal—",gauss elim
each row was divided by the pivot in D. Then L and U are treated evenly. An example,gauss elim
of LU splitting into LDU is,gauss elim
"That has the 1s on the diagonals of L and U, and the pivots 1 and −2 in D.",gauss elim
"Remark 2. We may have given the impression in describing each elimination step, that",gauss elim
"the calculations must be done in that order. This is wrong. There is some freedom, and",gauss elim
there is a “Crout algorithm” that arranges the calculations in a slightly different way.,gauss elim
"There is no freedom in the ﬁnal L, D, and U. That is our main point:",gauss elim
"If A = L1D1U1 and also A = L2D2U2, where the L’s are lower triangular",gauss elim
"with unit diagonal, the U’s are upper triangular with unit diagonal, and the",gauss elim
"D’s are diagonal matrices with no zeros on the diagonal, then L1 = L2, D1 =",gauss elim
"D2, U1 = U2. The LDU factorization and the LU factorization are uniquely",gauss elim
The proof is a good exercise with inverse matrices in the next section.,gauss elim
Row Exchanges and Permutation Matrices,gauss elim
We now have to face a problem that has so far been avoided: The number we expect to,gauss elim
use as a pivot might be zero. This could occur in the middle of a calculation. It will,gauss elim
happen at the very beginning if a11 = 0. A simple example is,gauss elim
Zero in the pivot position,gauss elim
The difﬁculty is clear; no multiple of the ﬁrst equation will remove the coefﬁcient 3.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"The remedy is equally clear. Exchange the two equations, moving the entry 3 up",gauss elim
into the pivot. In this example the matrix would become upper triangular:,gauss elim
"To express this in matrix terms, we need the permutation matrix P that produces the",gauss elim
row exchange. It comes from exchanging the rows of I:,gauss elim
"P has the same effect on b, exchanging b1 and b2. The new system is PAx = Pb. The",gauss elim
unknowns u and v are not reversed in a row exchange.,gauss elim
A permutation matrix P has the same rows as the identity (in some order). There is,gauss elim
a single “1” in every row and column. The most common permutation matrix is P = I (it,gauss elim
exchanges nothing). The product of two permutation matrices is another permutation—,gauss elim
the rows of I get reordered twice.,gauss elim
"After P = I, the simplest permutations exchange two rows. Other permutations ex-",gauss elim
change more rows. There are n! = (n)(n−1)···(1) permutations of size n. Row 1 has,gauss elim
"n choices, then row 2 has n−1 choices, and ﬁnally the last row has only one choice. We",gauss elim
can display all 3 by 3 permutations (there are 3! = (3)(2)(1) = 6 matrices):,gauss elim
There will be 24 permutation matrices of order n = 4. There are only two permutation,gauss elim
"matrices of order 2, namely",gauss elim
"When we know about inverses and transposes (the next section deﬁnes A−1 and AT),",gauss elim
we discover an important fact: P−1 is always the same as PT.,gauss elim
"A zero in the pivot location raises two possibilities: The trouble may be easy to ﬁx,",gauss elim
or it may be serious. This is decided by looking below the zero. If there is a nonzero,gauss elim
"entry lower down in the same column, then a row exchange is carried out. The nonzero",gauss elim
"entry becomes the needed pivot, and elimination can get going again:",gauss elim
d = 0 =⇒ no ﬁrst pivot,gauss elim
a = 0 =⇒ no second pivot,gauss elim
=⇒ no third pivot.,gauss elim
1.5 Triangular Factors and Row Exchanges,gauss elim
"If d = 0, the problem is incurable and this matrix is singular. There is no hope for a",gauss elim
"unique solution to Ax = b. If d is not zero, an exchange P13 of rows 1 and 3 will move",gauss elim
d into the pivot. However the next pivot position also contains a zero. The number a is,gauss elim
now below it (the e above it is useless). If a is not zero then another row exchange P23 is,gauss elim
One more point: The permutation P23P13 will do both row exchanges at once:,gauss elim
"If we had known, we could have multiplied A by P in the ﬁrst place. With the rows in",gauss elim
"the right order PA, any nonsingular matrix is ready for elimination.",gauss elim
Elimination in a Nutshell: PA = LU,gauss elim
"The main point is this: If elimination can be completed with the help of row exchanges,",gauss elim
then we can imagine that those exchanges are done ﬁrst (by P). The matrix PA will not,gauss elim
"need row exchanges. In other words, PA allows the standard factorization into L times",gauss elim
U. The theory of Gaussian elimination can be summarized in a few lines:,gauss elim
"In the nonsingular case, there is a permutation matrix P that reorders",gauss elim
the rows of A to avoid zeros in the pivot positions. Then Ax = b has a unique,gauss elim
"With the rows reordered in advance, PA can be factored into LU.",gauss elim
"In the singular case, no P can produce a full set of pivots: elimination fails.",gauss elim
"In practice, we also consider a row exchange when the original pivot is near zero—",gauss elim
even if it is not exactly zero. Choosing a larger pivot reduces the roundoff error.,gauss elim
"You have to be careful with L. Suppose elimination subtracts row 1 from row 2,",gauss elim
creating ℓ21 = 1. Then suppose it exchanges rows 2 and 3. If that exchange is done in,gauss elim
"advance, the multiplier will change to ℓ31 = 1 in PA = LU.",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
That row exchange recovers LU—but now ℓ31 = 1 and ℓ21 = 2:,gauss elim
"In MATLAB, A([r k] :) exchanges row k with row r below it (where the kth pivot has",gauss elim
"been found). We update the matrices L and P the same way. At the start, P = I and sign",gauss elim
"A([r k], :) = A([k r], :);",gauss elim
"L([r k], 1:k-1) = L([k r], 1:k-1);",gauss elim
"P([r k], :) = P([k r], :);",gauss elim
The “sign” of P tells whether the number of row exchanges is even (sign = +1) or odd,gauss elim
(sign = −1). A row exchange reverses sign. The ﬁnal value of sign is the determinant,gauss elim
of P and it does not depend on the order of the row exchanges.,gauss elim
To summarize: A good elimination code saves L and U and P. Those matrices carry,gauss elim
the information that originally came in A—and they carry it in a more usable form. Ax =,gauss elim
b reduces to two triangular systems. This is the practical equivalent of the calculation,gauss elim
we do next—to ﬁnd the inverse matrix A−1 and the solution x = A−1b.,gauss elim
1. When is an upper triangular matrix nonsingular (a full set of pivots)?,gauss elim
2. What multiple ℓ32 of row 2 of A will elimination subtract from row 3 of A? Use the,gauss elim
What will be the pivots? Will a row exchange be required?,gauss elim
3. Multiply the matrix L = E−1F−1G−1 in equation (6) by GFE in equation (3):,gauss elim
Multiply also in the opposite order. Why are the answers what they are?,gauss elim
1.5 Triangular Factors and Row Exchanges,gauss elim
4. Apply elimination to produce the factors L and U for,gauss elim
"5. Factor A into LU, and write down the upper triangular system Ux = c which appears",gauss elim
6. Find E2 and E8 and E−1 if,gauss elim
7. Find the products FGH and HGF if (with upper triangular zeros omitted),gauss elim
0 0 0 1,gauss elim
0 0 0 1,gauss elim
0 0 2 1,gauss elim
8. (Second proof of A = LU) The third row of U comes from the third row of A by,gauss elim
subtracting multiples of rows 1 and 2 (of U!):,gauss elim
row 3 of U = row 3 of A−ℓ31(row 1 of U)−ℓ32(row 2 of U).,gauss elim
(a) Why are rows of U subtracted off and not rows of A? Answer: Because by the,gauss elim
"time a pivot row is used,",gauss elim
(b) The equation above is the same as,gauss elim
row 3 of A = ℓ31(row 1 of U)+ℓ32(row 2 of U)+1(row 3 of U).,gauss elim
Which rule for matrix multiplication makes this row 3 of L times U?,gauss elim
The other rows of LU agree similarly with the rows of A.,gauss elim
9. (a) Under what conditions is the following product nonsingular?,gauss elim
(b) Solve the system Ax = b starting with Lc = b:,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
10. (a) Why does it take approximately n2/2 multiplication-subtraction steps to solve,gauss elim
each of Lc = b and Ux = c?,gauss elim
(b) How many steps does elimination use in solving 10 systems with the same 60 by,gauss elim
60 coefﬁcient matrix A?,gauss elim
"11. Solve as two triangular systems, without multiplying LU to ﬁnd A:",gauss elim
"12. How could you factor A into a product UL, upper triangular times lower triangular?",gauss elim
Would they be the same factors as in A = LU?,gauss elim
"13. Solve by elimination, exchanging rows when necessary:",gauss elim
+ 4v + 2w = −2,gauss elim
−2u − 8v + 3w =,gauss elim
v + w =,gauss elim
u + v + w = 1.,gauss elim
Which permutation matrices are required?,gauss elim
"14. Write down all six of the 3 by 3 permutation matrices, including P = I. Identify their",gauss elim
"inverses, which are also permutation matrices. The inverses satisfy PP−1 = I and are",gauss elim
on the same list.,gauss elim
15. Find the PA = LDU factorizations (and check them) for,gauss elim
16. Find a 4 by 4 permutation matrix that requires three row exchanges to reach the end,gauss elim
of elimination (which is U = I).,gauss elim
17. The less familiar form A = LPU exchanges rows only at the end:,gauss elim
�� → L−1A =,gauss elim
�� = PU =,gauss elim
"What is L is this case? Comparing with PA = LU in Box 1J, the multipliers now stay",gauss elim
in place (ℓ21 is 1 and ℓ31 is 2 when A = LPU).,gauss elim
"18. Decide whether the following systems are singular or nonsingular, and whether they",gauss elim
"have no solution, one solution, or inﬁnitely many solutions:",gauss elim
v − w = 2,gauss elim
− w = 2,gauss elim
v − w = 0,gauss elim
− w = 0,gauss elim
v + w =,gauss elim
+ w = 1.,gauss elim
1.5 Triangular Factors and Row Exchanges,gauss elim
"19. Which numbers a, b, c lead to row exchanges? Which make the matrix singular?",gauss elim
Problems 20–31 compute the factorization A = LU (and also A = LDU).,gauss elim
20. Forward elimination changes,gauss elim
x = b to a triangular,gauss elim
x + 2y = 7 → x + y = 5,gauss elim
That step subtracted ℓ21 =,gauss elim
times row 1 from row 2. The reverse step adds ℓ21,gauss elim
times row 1 to row 2. The matrix for that reverse step is L =,gauss elim
. Multiply this L,gauss elim
times the triangular system,gauss elim
". In letters, L multiplies",gauss elim
Ux = c to give,gauss elim
21. (Move to 3 by 3) Forward elimination changes Ax = b to a triangular Ux = c:,gauss elim
The equation z = 2 in Ux = c comes from the original x+3y+6z = 11 in Ax = b by,gauss elim
times equation 1 and ℓ32 =,gauss elim
times the ﬁnal equation 2.,gauss elim
Reverse that to recover [1 3 6 11] in [A b] from the ﬁnal [1 1 1 5] and [0 1 2 2],gauss elim
and [0 0 1 2] in [U c]:,gauss elim
= (ℓ31 Row 1 +ℓ32 Row 2+1 Row 3) of,gauss elim
In matrix notation this is multiplication by L. So A = LU and b = Lc.,gauss elim
22. What are the 3 by 3 triangular systems Lc = b and Ux = c from Problem 21? Check,gauss elim
"that c = (5,2,2) solves the ﬁrst one. Which x solves the second one?",gauss elim
23. What two elimination matrices E21 and E32 put A into upper triangular form E32E21A =,gauss elim
U? Multiply by E−1,gauss elim
21 to factor A into LU = E−1,gauss elim
"24. What three elimination matrices E21, E31, E32 put A into upper triangular form",gauss elim
E32E31E21A = U? Multiply by E−1,gauss elim
21 to factor A into LU where L =,gauss elim
32 . Find L and U:,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"25. When zero appears in a pivot position, A = LU is not possible! (We need nonzero",gauss elim
"pivots d, f, i in U.) Show directly why these are both impossible:",gauss elim
26. Which number c leads to zero in the second pivot position? A row exchange is,gauss elim
needed and A = LU is not possible. Which c produces zero in the third pivot position?,gauss elim
Then a row exchange can’t help and elimination fails:,gauss elim
27. What are L and D for this matrix A? What is U in A = LU and what is the new U in,gauss elim
28. A and B are symmetric across the diagonal (because 4 = 4). Find their triple factor-,gauss elim
izations LDU and say how U is related to L for these symmetric matrices:,gauss elim
29. (Recommended) Compute L and U for the symmetric matrix,gauss elim
a a a a,gauss elim
a b b b,gauss elim
a b c d,gauss elim
"Find four conditions on a, b, c, d to get A = LU with four pivots.",gauss elim
30. Find L and U for the nonsymmetric matrix,gauss elim
a b c d,gauss elim
"Find the four conditions on a, b, c, d, r, s, t to get A = LU with four pivots.",gauss elim
1.5 Triangular Factors and Row Exchanges,gauss elim
31. Tridiagonal matrices have zero entries except on the main diagonal and the two,gauss elim
adjacent diagonals. Factor these into A = LU and A = LDV:,gauss elim
32. Solve the triangular system Lc = b to ﬁnd c. Then solve Ux = c to ﬁnd x:,gauss elim
For safety ﬁnd A = LU and solve Ax = b as usual. Circle c when you see it.,gauss elim
33. Solve Lc = b to ﬁnd c. Then solve Ux = c to ﬁnd x. What was A?,gauss elim
"34. If A and B have nonzeros in the positions marked by x, which zeros are still zero in",gauss elim
their factors L and U?,gauss elim
0 x x x,gauss elim
0 0 x x,gauss elim
"35. (Important) If A has pivots 2, 7, 6 with no row exchanges, what are the pivots for the",gauss elim
upper left 2 by 2 submatrix B (without row 3 and column 3)? Explain why.,gauss elim
"36. Starting from a 3 by 3 matrix A with pivots 2, 7, 6, add a fourth row and column to",gauss elim
"produce M. What are the ﬁrst three pivots for M, and why? What fourth row and",gauss elim
column are sure to produce 9 as the fourth pivot?,gauss elim
37. Use chol(pascal(5)) to ﬁnd the triangular factors of MATLAB’s pascal(5). Row,gauss elim
"exchanges in [L, U] = lu(pascal(5)) spoil Pascal’s pattern!",gauss elim
38. (Review) For which numbers c is A = LU impossible—with three pivots?,gauss elim
39. Estimate the time difference for each new right-hand side b when n = 800. Create A,gauss elim
"= rand(800) and b = rand(800,1) and B = rand(800,9). Compare the times from",gauss elim
tic; A\b; toc and tic; A\B; toc (which solves for 9 right sides).,gauss elim
Problems 40–48 are about permutation matrices.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"40. There are 12 “even” permutations of (1,2,3,4), with an even number of exchanges.",gauss elim
"Two of them are (1,2,3,4) with no exchanges and (4,3,2,1) with two exchanges.",gauss elim
"List the other ten. Instead of writing each 4 by 4 matrix, use the numbers 4, 3, 2, 1",gauss elim
to give the position of the 1 in each row.,gauss elim
"41. How many exchanges will permute (5,4,3,2,1) back to (1,2,3,4,5)? How many",gauss elim
"exchanges to change (6,5,4,3,2,1) to (1,2,3,4,5,6)? One is even and the other is",gauss elim
"odd. For (n,...,1) to (1,...,n), show that n = 100 and 101 are even, n = 102 and",gauss elim
"42. If P1 and P2 are permutation matrices, so is P1P2. This still has the rows of I in some",gauss elim
order. Give examples with P1P2 ̸= P2P1 and P3P4 = P4P3.,gauss elim
43. (Try this question.) Which permutation makes PA upper triangular? Which permu-,gauss elim
tations make P1AP2 lower triangular? Multiplying A on the right by P2 exchanges,gauss elim
44. Find a 3 by 3 permutation matrix with P3 = I (but not P = I). Find a 4 by 4 permu-,gauss elim
tation �P with �P4 ̸= I.,gauss elim
"45. If you take powers of a permutation, why is some Pk eventually equal to I? Find a 5",gauss elim
by 5 permutation P so that the smallest power to equal I is P6. (This is a challenge,gauss elim
question. Combine a 2 by 2 block with a 3 by 3 block.),gauss elim
"46. The matrix P that multiplies (x,y,z) to give (z,x,y) is also a rotation matrix. Find P",gauss elim
"and P3. The rotation axis a = (1,1,1) doesn’t move, it equals Pa. What is the angle",gauss elim
"of rotation from v = (2,3,−5) to Pv = (−5,2,3)?",gauss elim
"47. If P is any permutation matrix, ﬁnd a nonzero vector x so that (I − P)x = 0. (This",gauss elim
"will mean that I −P has no inverse, and has determinant zero.)",gauss elim
"48. If P has 1s on the antidiagonal from (1,n) to (n,1), describe PAP.",gauss elim
The inverse of an n by n matrix is another n by n matrix. The inverse of A is written A−1,gauss elim
(and pronounced “A inverse”). The fundamental property is simple: If you multiply by A,gauss elim
"and then multiply by A−1, you are back where you started:",gauss elim
1.6 Inverses and Transposes,gauss elim
Thus A−1Ax = x. The matrix A−1 times A is the identity matrix. Not all matrices have,gauss elim
inverses. An inverse is impossible when Ax is zero and x is nonzero. Then A−1 would,gauss elim
have to get back from Ax = 0 to x. No matrix can multiply that zero vector Ax and,gauss elim
produce a nonzero vector x.,gauss elim
"Our goals are to deﬁne the inverse matrix and compute it and use it, when A−1",gauss elim
exists—and then to understand which matrices don’t have inverses.,gauss elim
The inverse of A is a matrix B such that BA = I and AB = I. There is at,gauss elim
"most one such B, and it is denoted by A−1:",gauss elim
Note 1. The inverse exists if and only if elimination produces n pivots (row exchanges,gauss elim
allowed). Elimination solves Ax = b without explicitly ﬁnding A−1.,gauss elim
"Note 2. The matrix A cannot have two different inverses, Suppose BA = I and also",gauss elim
"AC = I. Then B = C, according to this “proof by parentheses”:",gauss elim
This shows that a left-inverse B (multiplying from the left) and a right-inverse C (multi-,gauss elim
plying A from the right to give AC = I) must be the same matrix.,gauss elim
"Note 3. If A is invertible, the one and only solution to Ax = b is x = A−1b:",gauss elim
x = A−1Ax = A−1b.,gauss elim
Note 4. (Important) Suppose there is a nonzero vector x such that Ax = 0. Then A,gauss elim
cannot have an inverse. To repeat: No matrix can bring 0 back to x.,gauss elim
"If A is invertible, then Ax = 0 can only have the zero solution x = 0.",gauss elim
Note 5. A 2 by 2 matrix is invertible if and only if ad −bc is not zero:,gauss elim
2 by 2 inverse,gauss elim
This number ad − bc is the determinant of A. A matrix is invertible if its determinant,gauss elim
"is not zero (Chapter 4). In MATLAB, the invertibility test is to ﬁnd n nonzero pivots.",gauss elim
Elimination produces those pivots before the determinant appears.,gauss elim
Note 6. A diagonal matrix has an inverse provided no diagonal entries are zero:,gauss elim
"When two matrices are involved, not much can be done about the inverse of A + B.",gauss elim
"The sum might or might not be invertible. Instead, it is the inverse of their product",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
AB which is the key formula in matrix computations. Ordinary numbers are the same:,gauss elim
"(a+b)−1 is hard to simplify, while 1/ab splits into 1/a times 1/b. But for matrices the",gauss elim
order of multiplication must be correct—if ABx = y then Bx = A−1y and x = B−1A−1y.,gauss elim
The inverses come in reverse order.,gauss elim
A product AB of invertible matrices is inverted by B−1A−1:,gauss elim
"Proof. To show that B−1A−1 is the inverse of AB, we multiply them and use the associa-",gauss elim
tive law to remove parentheses. Notice how B sits next to B−1:,gauss elim
(AB)(B−1A−1) = ABB−1A−1 = AIA−1 = AA−1 = I,gauss elim
(B−1A−1)(AB) = B−1A−1AB = B−1IB = B−1B = I.,gauss elim
A similar rule holds with three or more matrices:,gauss elim
"We saw this change of order when the elimination matrices E, F, G were inverted to",gauss elim
"come back from U to A. In the forward direction, GFEA was U. In the backward",gauss elim
"direction, L = E−1F−1G−1 was the product of the inverses. Since G came last, G−1",gauss elim
comes ﬁrst. Please check that A−1 would be U−1GFE.,gauss elim
The Calculation of A−1: The Gauss-Jordan Method,gauss elim
"Consider the equation AA−1 = I. If it is taken a column at a time, that equation de-",gauss elim
"termines each column of A−1. The ﬁrst column of A−1 is multiplied by A, to yield the",gauss elim
ﬁrst column of the identity: Ax1 = e1. Similarly Ax2 = e2 and Ax3 = e3 the e’s are the,gauss elim
"columns of I. In a 3 by 3 example, A times A−1 is I:",gauss elim
Thus we have three systems of equations (or n systems). They all have the same coefﬁ-,gauss elim
"cient matrix A. The right-hand sides e1, e2, e3 are different, but elimination is possible",gauss elim
on all systems simultaneously. This is the Gauss-Jordan method. Instead of stopping,gauss elim
"at U and switching to back-substitution, it continues by subtracting multiples of a row",gauss elim
from the rows above. This produces zeros above the diagonal as well as below. When it,gauss elim
reaches the identity matrix we have found A−1.,gauss elim
"The example keeps all three columns e1, e2, e3, and operates on rows of length six:",gauss elim
1.6 Inverses and Transposes,gauss elim
Example 1. Using the Gauss-Jordan Method to Find A−1,gauss elim
1 1 0 0,gauss elim
−6 0 0 1 0,gauss elim
2 0 0 1,gauss elim
pivot = 2 →,gauss elim
0 −8 −2 −2 1 0,gauss elim
pivot = −8 →,gauss elim
0 −8 −2 −2 1 0,gauss elim
This completes the ﬁrst half—forward elimination. The upper triangular U appears in,gauss elim
the ﬁrst three columns. The other three columns are the same as L−1. (This is the effect,gauss elim
of applying the elementary operations GFE to the identity matrix.) Now the second half,gauss elim
will go from U to I (multiplying by U−1). That takes L−1 to U−1L−1 which is A−1.,gauss elim
"Creating zeros above the pivots, we reach A−1:",gauss elim
0 −8 0 −4,gauss elim
zeros above pivots →,gauss elim
0 −8 0 −4,gauss elim
divide by pivots →,gauss elim
0 0 1 −1,gauss elim
"At the last step, we divided the rows by their pivots 2 and −8 and 1. The coefﬁcient",gauss elim
"matrix in the left-hand half became the identity. Since A went to I, the same operations",gauss elim
on the right-hand half must have carried I into A−1. Therefore we have computed the,gauss elim
A note for the future: You can see the determinant −16 appearing in the denominators,gauss elim
of A−1. The determinant is the product of the pivots (2)(−8)(1). It enters at the end,gauss elim
when the rows are divided by the pivots.,gauss elim
"Remark 1. In spite of this brilliant success in computing A−1, I don’t recommend it, I",gauss elim
admit that A−1 solves Ax = b in one step. Two triangular steps are better:,gauss elim
We could write c = L−1b and then x = U−1c = U−1L−1b. But note that we did not,gauss elim
"explicitly form, and in actual computation should not form, these matrices L−1 and U−1.",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"It would be a waste of time, since we only need back-substitution for x (and forward",gauss elim
A similar remark applies to A−1; the multiplication A−1b would still take n2 steps. It,gauss elim
"is the solution that we want, and not all the entries in the inverse.",gauss elim
"Remark 2. Purely out of curiosity, we might count the number of operations required",gauss elim
"to ﬁnd A−1. The normal count for each new right-hand side is n2, half in the forward",gauss elim
"direction and half in back-substitution. With n right-hand sides e1,...,en this makes n3.",gauss elim
"After including the n3/3 operations on A itself, the total seems to be 4n3/3.",gauss elim
This result is a little too high because of the zeros in the e j. Forward elimination,gauss elim
"changes only the zeros below the 1. This part has only n − j components, so the count",gauss elim
"for e j is effectively changed to (n − j)2/2. Summing over all j, the total for forward",gauss elim
elimination is n3/6. This is to be combined with the usual n3/3 operations that are,gauss elim
"applied to A, and the n(n2/2) back-substitution steps that ﬁnally produce the columns x j",gauss elim
of A−1. The ﬁnal count of multiplications for computing A−1 is n3:,gauss elim
"This count is remarkably low. Since matrix multiplication already takes n3 steps, it",gauss elim
requires as many operations to compute A2 as it does to compute A−1! That fact seems,gauss elim
"almost unbelievable (and computing A3 requires twice as many, as far as we can see).",gauss elim
"Nevertheless, if A−1 is not needed, it should not be computed.",gauss elim
"Remark 3. In the Gauss-Jordan calculation we went all the way forward to U, before",gauss elim
"starting backward to produce zeros above the pivots. That is like Gaussian elimination,",gauss elim
but other orders are possible. We could have used the second pivot when we were there,gauss elim
"earlier, to create a zero above it as well as below it. This is not smart. At that time",gauss elim
"the second row is virtually full, whereas near the end it has zeros from the upward row",gauss elim
operations that have already taken place.,gauss elim
Invertible = Nonsingular (n pivots),gauss elim
Ultimately we want to know which matrices are invertible and which are not. This,gauss elim
question is so important that it has many answers. See the last page of the book!,gauss elim
Each of the ﬁrst ﬁve chapters will give a different (but equivalent) test for invertibility.,gauss elim
Sometimes the tests extend to rectangular matrices and one-sided inverses: Chapter 2,gauss elim
"looks for independent rows and independent columns, Chapter 3 inverts AAT or ATA.",gauss elim
The other chapters look for nonzero determinants or nonzero eigenvalues or nonzero,gauss elim
pivots. This last test is the one we meet through Gaussian elimination. We want to show,gauss elim
(in a few theoretical paragraphs) that the pivot test succeeds.,gauss elim
Suppose A has a full set of n pivots. AA−1 = I gives n separate systems Axi = ei,gauss elim
for the columns of A−1. They can be solved by elimination or by Gauss-Jordan. Row,gauss elim
"exchanges may be needed, but the columns of A−1 are determined.",gauss elim
1.6 Inverses and Transposes,gauss elim
"Strictly speaking, we have to show that the matrix A−1 with those columns is also",gauss elim
"a left-inverse. Solving AA−1 = I has at the same time solved A−1A = I, but why? A",gauss elim
"1-sided inverse of a square matrix is automatically a 2-sided inverse. To see why,",gauss elim
notice that every Gauss-Jordan step is a multiplication on the left by an elementary,gauss elim
matrix. We are allowing three types of elementary matrices:,gauss elim
1. Eij to subtract a multiple ℓ of row j from row i,gauss elim
2. Pij to exchange rows i and j,gauss elim
3. D (or D−1) to divide all rows by their pivots.,gauss elim
The Gauss-Jordan process is really a giant sequence of matrix multiplications:,gauss elim
(D−1···E ···P···E)A = I.,gauss elim
"That matrix in parentheses, to the left of A, is evidently a left-inverse! It exists, it equals",gauss elim
"the right-inverse by Note 2, so every nonsingular matrix is invertible.",gauss elim
"The converse is also true: If A is invertible, it has n pivots. In an extreme case that",gauss elim
is clear: A cannot have a whole column of zeros. The inverse could never multiply a,gauss elim
"column of zeros to produce a column of I. In a less extreme case, suppose elimination",gauss elim
starts on an invertible matrix A but breaks down at column 3:,gauss elim
No pivot in column 3,gauss elim
"This matrix cannot have an inverse, no matter what the x’s are. One proof is to use",gauss elim
column operations (for the ﬁrst time?) to make the whole third column zero. By sub-,gauss elim
"tracting multiples of column 2 and then of column 1, we reach a matrix that is certainly",gauss elim
not invertible. Therefore the original A was not invertible. Elimination gives a complete,gauss elim
test: An n by n matrix is invertible if and only if it has n pivots.,gauss elim
"We need one more matrix, and fortunately it is much simpler than the inverse. The",gauss elim
transpose of A is denoted by AT. Its columns are taken directly from the rows of A—the,gauss elim
ith row of A becomes the ith column of AT:,gauss elim
"At the same time the columns of A become the rows of AT, If A is an m by n matrix, then",gauss elim
"AT is n by m. The ﬁnal effect is to ﬂip the matrix across its main diagonal, and the entry",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"in row i, column j of AT comes from row j, column i of A:",gauss elim
(AT)ij = A ji.,gauss elim
The transpose of a lower triangular matrix is upper triangular. The transpose of AT brings,gauss elim
us back to A.,gauss elim
"If we add two matrices and then transpose, the result is the same as ﬁrst transposing",gauss elim
and then adding: (A+B)T is the same as AT +BT. But what is the transpose of a product,gauss elim
AB or an inverse A−1? Those are the essential formulas of this section:,gauss elim
"(i) The transpose of AB is (AB)T = BTAT,",gauss elim
(ii) The transpose of A−1 is (A−1)T = (AT)−1.,gauss elim
Notice how the formula for (AB)T resembles the one for (AB)−1. In both cases we,gauss elim
"reverse the order, giving BTAT and B−1A−1. The proof for the inverse was easy, but this",gauss elim
one requires an unnatural patience with matrix multiplication. The ﬁrst row of (AB)T is,gauss elim
the ﬁrst column of AB. So the columns of A are weighted by the ﬁrst column of B. This,gauss elim
amounts to the rows of AT weighted by the ﬁrst row of BT. That is exactly the ﬁrst row,gauss elim
of BTAT. The other rows of (AB)T and BTAT also agree.,gauss elim
"To establish the formula for (A−1)T, start from AA−1 = I and A−1A = I and take trans-",gauss elim
"poses. On one side, IT = I. On the other side, we know from part (i) the transpose of a",gauss elim
"product. You see how (A−1)T is the inverse of AT, proving (ii):",gauss elim
Inverse of AT = Transpose of A−1,gauss elim
"With these rules established, we can introduce a special class of matrices, probably",gauss elim
the most important class of all. A symmetric matrix is a matrix that equals its own,gauss elim
transpose: AT = A. The matrix is necessarily square. Each entry on one side of the,gauss elim
diagonal equals its “mirror image” on the other side: aij = a ji. Two simple examples are,gauss elim
A and D (and also A−1):,gauss elim
1.6 Inverses and Transposes,gauss elim
A symmetric matrix need not be invertible; it could even be a matrix of zeros. But if,gauss elim
"A−1 exists it is also symmetric. From formula (ii) above, the transpose of A−1 always",gauss elim
equals (AT)−1; for a symmetric matrix this is just A−1. A−1 equals its own transpose; it,gauss elim
is symmetric whenever A is. Now we show that multiplying any matrix R by RT gives a,gauss elim
"Symmetric Products RTR, RRT, and LDLT",gauss elim
"Choose any matrix R, probably rectangular. Multiply RT times R. Then the product RTR",gauss elim
is automatically a square symmetric matrix:,gauss elim
"That is a quick proof of symmetry for RTR. Its i, j entry is the inner product of row i",gauss elim
"of RT (column i of R) with column j of R. The (j,i) entry is the same inner product,",gauss elim
column j with column i. So RTR is symmetric.,gauss elim
"RRT is also symmetric, but it is different from RTR. In my experience, most scientiﬁc",gauss elim
problems that start with a rectangular matrix R end up with RTR or RRT or both.,gauss elim
Example 2. R = [1 2] and RT = [1,gauss elim
2] produce RTR =,gauss elim
and RRT = [5].,gauss elim
"The product RTR is n by n. In the opposite order, RRT is m by m. Even if m = n, it is not",gauss elim
"very likely that RTR = RRT. Equality can happen, but it’s not normal.",gauss elim
Symmetric matrices appear in every subject whose laws are fair. “Each action has an,gauss elim
equal and opposite reaction.” The entry aij that gives the action of i onto j is matched,gauss elim
"by a ji. We will see this symmetry in the next section, for differential equations. Here,",gauss elim
LU misses the symmetry but LDLT captures it perfectly.,gauss elim
Suppose A = AT can be factored into A = LDU without row exchanges.,gauss elim
ThenU is the transpose of L. The symmetric factorization becomes A = LDLT.,gauss elim
"The transpose of A = LDU gives AT = UTDTLT. Since A = AT, we now have two",gauss elim
factorizations of A into lower triangular times diagonal times upper triangular. (LT is,gauss elim
"upper triangular with ones on the diagonal, exactly like U.) Since the factorization is",gauss elim
"unique (see Problem 17), LT must be identical to U.",gauss elim
LT = U and A = LDLT,gauss elim
"When elimination is applied to a symmetric matrix, AT = A is an advantage. The smaller",gauss elim
"matrices stay symmetric as elimination proceeds, and we can work with half the matrix!",gauss elim
The lower right-hand corner remains symmetric:,gauss elim
0 d − b2,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
The work of elimination is reduced from n3/3 to n3/6. There is no need to store entries,gauss elim
"from both sides of the diagonal, or to store both L and U.",gauss elim
1. Find the inverses (no special system required) of,gauss elim
2. (a) Find the inverses of the permutation matrices,gauss elim
(b) Explain for permutations why P−1 is always the same as PT. Show that the 1s,gauss elim
are in the right places to give PPT = I.,gauss elim
3. From AB = C ﬁnd a formula for A−1. Also ﬁnd A−1 from PA = LU.,gauss elim
"4. (a) If A is invertible and AB = AC, prove quickly that B = C.",gauss elim
(b) If A = [1 0,gauss elim
"0 0], ﬁnd an example with AB = AC but B ̸= C.",gauss elim
"5. If the inverse of A2 is B, show that the inverse of A is AB. (Thus A is invertible",gauss elim
whenever A2 is invertible.),gauss elim
6. Use the Gauss-Jordan method to invert,gauss elim
"7. Find three 2 by 2 matrices, other than A = I and A = −I, that are their own inverses:",gauss elim
8. Show that A = [1 1,gauss elim
"3 3] has no inverse by solving Ax = 0, and by failing to solve",gauss elim
9. Suppose elimination fails because there is no pivot in column 3:,gauss elim
2 1 4 6,gauss elim
0 3 8 5,gauss elim
0 0 0 7,gauss elim
0 0 0 9,gauss elim
1.6 Inverses and Transposes,gauss elim
"Show that A cannot be invertible. The third row of A−1, multiplying A, should give",gauss elim
the third row [0 0 1 0] of A−1A = I. Why is this impossible?,gauss elim
10. Find the inverses (in any legal way) of,gauss elim
0 0 0 1,gauss elim
0 0 2 0,gauss elim
0 3 0 0,gauss elim
4 0 0 0,gauss elim
a b 0 0,gauss elim
0 0 a b,gauss elim
11. Give examples of A and B such that,gauss elim
(a) A+B is not invertible although A and B are invertible.,gauss elim
(b) A+B is invertible although A and B are not invertible.,gauss elim
"(c) all of A, B, and A+B are invertible.",gauss elim
(d) In the last case use A−1(A+B)B−1 = B−1 +A−1 to show that C = B−1 +A−1 is,gauss elim
also invertible—and ﬁnd a formula for C−1.,gauss elim
"12. If A is invertible, which properties of A remain true for A−1?",gauss elim
(a) A is triangular. (b) A is symmetric. (c) A is tridiagonal. (d) All entries are whole,gauss elim
numbers. (e) All entries are fractions (including numbers like 3,gauss elim
13. If A = [3,gauss elim
1] and B = [2,gauss elim
"2], compute ATB, BTA, ABT, and BAT.",gauss elim
"14. If B is square, show that A = B+BT is always symmetric and K = B−BT is always",gauss elim
skew-symmetric—which means that KT = −K. Find these matrices A and K when,gauss elim
B = [1 3,gauss elim
"1 1], and write B as the sum of a symmetric matrix and a skew-symmetric",gauss elim
15. (a) How many entries can be chosen independently in a symmetric matrix of order,gauss elim
(b) How many entries can be chosen independently in a skew-symmetric matrix,gauss elim
(KT = −K) of order n? The diagonal of K is zero!,gauss elim
"16. (a) If A = LDU, with 1s on the diagonals of L and U, what is the corresponding",gauss elim
factorization of AT? Note that A and AT (square matrices with no row exchanges),gauss elim
share the same pivots.,gauss elim
(b) What triangular systems will give the solution to ATy = b?,gauss elim
"17. If A = L1D1U1 and A = L2D2U2, prove that L1 = L2, D1 = D2, and U1 = U2. If A is",gauss elim
"invertible, the factorization is unique.",gauss elim
(a) Derive the equation L−1,gauss elim
1 L2D2 = D1U1U−1,gauss elim
"2 , and explain why one side is lower",gauss elim
triangular and the other side is upper triangular.,gauss elim
(b) Compare the main diagonals and then compare the off-diagonals.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
18. Under what conditions on their entries are A and B invertible?,gauss elim
19. Compute the symmetric LDLT factorization of,gauss elim
20. Find the inverse of,gauss elim
"21. (Remarkable) If A and B are square matrices, show that I −BA is invertible if I −AB",gauss elim
is invertible. Start from B(I −AB) = (1−BA)B.,gauss elim
"22. Find the inverses (directly or from the 2 by 2 formula) of A, B, C:",gauss elim
23. Solve for the columns of A−1 =,gauss elim
24. Show that [1 2,gauss elim
"3 6] has no inverse by trying to solve for the column (x,y):",gauss elim
"25. (Important) If A has row 1 + row 2 = row 3, show that A is not invertible:",gauss elim
"(a) Explain why Ax = (1,0,0) cannot have a solution.",gauss elim
"(b) Which right-hand sides (b1,b2,b3) might allow a solution to Ax = b?",gauss elim
(c) What happens to row 3 in elimination?,gauss elim
"26. If A has column 1 + column 2 = column 3, show that A is not invertible:",gauss elim
1.6 Inverses and Transposes,gauss elim
(a) Find a nonzero solution x to Ax = 0. The matrix is 3 by 3.,gauss elim
(b) Elimination keeps column 1 + column 2 = column 3. Explain why there is no,gauss elim
27. Suppose A is invertible and you exchange its ﬁrst two rows to reach B. Is the new,gauss elim
matrix B invertible? How would you ﬁnd B−1 from A−1?,gauss elim
"28. If the product M = ABC of three square matrices is invertible, then A, B, C are",gauss elim
invertible. Find a formula for B−1 that involves M−1 and A and C.,gauss elim
29. Prove that a matrix with a column of zeros cannot have an inverse.,gauss elim
30. Multiply [a b,gauss elim
c d] times [ d −b,gauss elim
−c a ]. What is the inverse of each matrix if ad ̸= bc?,gauss elim
31. (a) What matrix E has the same effect as these three steps? Subtract row 1 from row,gauss elim
"2, subtract row 1 from row 3, then subtract row 2 from row 3.",gauss elim
(b) What single matrix L has the same effect as these three reverse steps? Add row,gauss elim
"2 to row 3, add row 1 to row 3, then add row 1 to row 2.",gauss elim
"32. Find the numbers a and b that give the inverse of 5 ∗ eye(4) − ones(4,4):",gauss elim
a b b b,gauss elim
b a b b,gauss elim
b b a b,gauss elim
b b b a,gauss elim
"What are a and b in the inverse of 6 ∗ eye(5) − ones(5,5)?",gauss elim
"33. Show that A = 4 ∗ eye(4) − ones(4,4) is not invertible: Multiply A ∗ ones(4,1).",gauss elim
34. There are sixteen 2 by 2 matrices whose entries are 1s and 0s. How many of them,gauss elim
Problems 35–39 are about the Gauss-Jordan method for calculating A−1.,gauss elim
35. Change I into A−1 as you reduce A to I (by row operations):,gauss elim
1 3 1 0,gauss elim
2 7 0 1,gauss elim
1 4 1 0,gauss elim
3 9 0 1,gauss elim
36. Follow the 3 by 3 text example but with plus signs in A. Eliminate above and below,gauss elim
the pivots to reduce [A I] to [I A−1]:,gauss elim
2 1 0 1 0 0,gauss elim
1 2 1 0 1 0,gauss elim
0 1 2 0 0 1,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
37. Use Gauss-Jordan elimination on [A I] to solve AA−1 = I:,gauss elim
38. Invert these matrices A by the Gauss-Jordan method starting with [A I]:,gauss elim
39. Exchange rows and continue with Gauss-Jordan to ﬁnd A−1:,gauss elim
0 2 1 0,gauss elim
2 2 0 1,gauss elim
40. True or false (with a counterexample if false and a reason if true):,gauss elim
(a) A 4 by 4 matrix with a row of zeros is not invertible.,gauss elim
(b) A matrix with Is down the main diagonal is invertible.,gauss elim
(c) If A is invertible then A−1 is invertible.,gauss elim
(d) If AT is invertible then A is invertible.,gauss elim
"41. For which three numbers c is this matrix not invertible, and why not?",gauss elim
42. Prove that A is invertible if a ̸= 0 and a ̸= b (ﬁnd the pivots and A−1):,gauss elim
43. This matrix has a remarkable inverse. Find A−1 by elimination on [A I]. Extend to a,gauss elim
5 by 5 “alternating matrix” and guess its inverse:,gauss elim
1.6 Inverses and Transposes,gauss elim
"44. If B has the columns of A in reverse order, solve (A−B)x = 0 to show that A−B is",gauss elim
not invertible. An example will lead you to x.,gauss elim
45. Find and check the inverses (assuming they exist) of these block matrices:,gauss elim
46. Use inv(S) to invert MATLAB’s 4 by 4 symmetric matrix S = pascal(4). Create,gauss elim
"Pascal’s lower triangular A = abs(pascal(4,1)) and test inv(S) = inv(A’) ∗ inv(A).",gauss elim
"47. If A = ones(4,4) and b = rand(4,1), how does MATLAB tell you that Ax = b has",gauss elim
"no solution? If b = ones(4,1), which solution to Ax = b is found by A\b?",gauss elim
48. M−1 shows the change in A−1 (useful to know) when a matrix is subtracted from A.,gauss elim
Check part 3 by carefully multiplying MM−1 to get I:,gauss elim
M = I −uvT,gauss elim
M−1 = I +uvT/(1−vTu).,gauss elim
M−1 = A−1 +A−1uvTA−1/(1−vTA−1u).,gauss elim
M = I −UV,gauss elim
M−1 = In +U(Im −VU)−1V.,gauss elim
M = A−UW −1V,gauss elim
M−1 = A−1 +A−1U(W −VA−1U)−1VA−1.,gauss elim
"The four identities come from the 1, 1 block when inverting these matrices:",gauss elim
Problems 49–55 are about the rules for transpose matrices.,gauss elim
49. Find AT and A−1 and (A−1)T and (AT)−1 for,gauss elim
50. Verify that (AB)T equals BTAT but those are different from ATBT:,gauss elim
"In case AB = BA (not generally true!), how do you prove that BTAT = ATBT?",gauss elim
51. (a) The matrix,gauss elim
(AB)−1�T comes from (A−1)T and (B−1)T. In what order?,gauss elim
(b) If U is upper triangular then (U−1)T is,gauss elim
52. Show that A2 = 0 is possible but ATA = 0 is not possible (unless A = zero matrix).,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
53. (a) The row vector xT times A times the column y produces what number?,gauss elim
(b) This is the row xTA =,gauss elim
"times the column y = (0,1,0).",gauss elim
(c) This is the row xT = [0 1] times the column Ay =,gauss elim
54. When you transpose a block matrix M = [ A B,gauss elim
C D] the result is MT =,gauss elim
"Under what conditions on A, B, C, D is the block matrix symmetric?",gauss elim
55. Explain why the inner product of x and y equals the inner product of Px and Py.,gauss elim
"Then (Px)T(Py) = xTy says that PTP = I for any permutation. With x = (1,2,3) and",gauss elim
"y = (1,4,2), choose P to show that (Px)Ty is not always equal to xT(PTy).",gauss elim
Problems 56–60 are about symmetric matrices and their factorizations.,gauss elim
"56. If A = AT and B = BT, which of these matrices are certainly symmetric?",gauss elim
"57. If A = AT needs a row exchange, then it also needs a column exchange to stay sym-",gauss elim
"metric. In matrix language, PA loses the symmetry of A but",gauss elim
"58. (a) How many entries of A can be chosen independently, if A = AT is 5 by 5?",gauss elim
(b) How do L and D (5 by 5) give the same number of choices in LDLT?,gauss elim
59. Suppose R is rectangular (m by n) and A is symmetric (m by m).,gauss elim
(a) Transpose RTAR to show its symmetry. What shape is this matrix?,gauss elim
(b) Show why RTR has no negative numbers on its diagonal.,gauss elim
60. Factor these symmetric matrices into A = LDLT. The matrix D is diagonal:,gauss elim
The next three problems are about applications of (Ax)Ty = xT(ATy).,gauss elim
"61. Wires go between Boston, Chicago, and Seattle. Those cities are at voltages xB, xC,",gauss elim
"xS. With unit resistances between cities, the three currents are in y:",gauss elim
1.6 Inverses and Transposes,gauss elim
(a) Find the total currents ATy out of the three cities.,gauss elim
(b) Verify that (Ax)Ty agrees with xT(ATy)—six terms in both.,gauss elim
"62. Producing x1 trucks and x2 planes requires x1 + 50x2 tons of steel, 40x1 + 1000x2",gauss elim
"pounds of rubber, and 2x1 +50x2 months of labor. If the unit costs y1, y2, y3 are $700",gauss elim
"per ton, $3 per pound, and $3000 per month, what are the values of one truck and",gauss elim
one plane? Those are the components of ATy.,gauss elim
"63. Ax gives the amounts of steel, rubber, and labor to produce x in Problem 62. Find A.",gauss elim
Then (Ax)Ty is the,gauss elim
of inputs while xT(ATy) is the value of,gauss elim
64. Here is a new factorization of A into triangular times symmetric:,gauss elim
Start from A = LDU. Then A equals L(UT)−1 times UTDU.,gauss elim
Why is L(UT)−1 triangular? Its diagonal is all 1s. Why is UTDU symmetric?,gauss elim
65. A group of matrices includes AB and A−1 if it includes A and B. “Products and,gauss elim
inverses stay in the group.” Which of these sets are groups? Lower triangularmatri-,gauss elim
"ces L with is on the diagonal, symmetric matrices S, positive matrices M, diagonal",gauss elim
"invertible matrices D, permutation matrices P. Invent two more matrix groups.",gauss elim
"66. If every row of a 4 by 4 matrix contains the numbers 0, 1, 2, 3 in some order, can the",gauss elim
matrix be symmetric? Can it be invertible?,gauss elim
67. Prove that no reordering of rows and reordering of columns can transpose a typical,gauss elim
"68. A square northwest matrix B is zero in the southeast corner, below the antidiagonal",gauss elim
"that connects (1,n) to (n,1). Will BT and B2 be northwest matrices? Will B−1 be",gauss elim
northwest or southeast? What is the shape of BC = northwest times southeast?,gauss elim
You are allowed to combine permutations with the usual L and U (southwest and,gauss elim
69. Compare tic; inv(A); toc for A = rand(500) and A = rand(1000). The n3 count,gauss elim
says that computing time (measured by tic; toc) should multiply by 8 when n is,gauss elim
doubled. Do you expect these random A to be invertible?,gauss elim
70. I = eye(1000); A = rand(1000); B = triu(A); produces a random triangular matrix,gauss elim
B. Compare the times for inv(B) and B\I. Backslash is engineered to use the zeros,gauss elim
"in B, while inv uses the zeros in I when reducing [B I] by Gauss-Jordan. (Compare",gauss elim
also with inv(A) and A\I for the full matrix A.),gauss elim
"71. Show that L−1 has entries j/i for i ≤ j (the −1, 2, −1 matrix has this L):",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
"Test this pattern for L = eye(5) − diag(1:5)\diag(1:4,−1) and inv(L).",gauss elim
Special Matrices and Applications,gauss elim
This section has two goals. The ﬁrst is to explain one way in which large linear systems,gauss elim
Ax = b can arise in practice. The truth is that a large and completely realistic problem in,gauss elim
engineering or economics would lead us far aﬁeld. But there is one natural and important,gauss elim
application that does not require a lot of preparation.,gauss elim
"The other goal is to illustrate, by this same application, the special properties that co-",gauss elim
efﬁcient matrices frequently have. Large matrices almost always have a clear pattern—,gauss elim
"frequently a pattern of symmetry, and very many zero entries. Since a sparse matrix",gauss elim
"contains far fewer than n2 pieces of information, the computations ought to be fast. We",gauss elim
"look at band matrices, to see how concentration near the diagonal speeds up elimination.",gauss elim
In fact we look at one special tridiagonal matrix.,gauss elim
The matrix itself can be seen in equation (6). It comes from changing a differential,gauss elim
"equation to a matrix equation. The continuous problem asks for u(x) at every x, and a",gauss elim
computer cannot solve it exactly. It has to be approximated by a discrete problem—the,gauss elim
"more unknowns we keep, the better will be the accuracy and the greater the expense.",gauss elim
"As a simple but still very typical continuous problem, our choice falls on the differential",gauss elim
0 ≤ x ≤ 1.,gauss elim
This is a linear equation for the unknown function u(x). Any combination C + Dx,gauss elim
"could be added to any solution, since the second derivative ofC+Dx contributes nothing.",gauss elim
The uncertainty left by these two arbitrary constants C and D is removed by a “boundary,gauss elim
condition” at each end of the interval:,gauss elim
"The result is a two-point boundary-value problem, describing not a transient but a steady-",gauss elim
"state phenomenon—the temperature distribution in a rod, for example, with ends ﬁxed",gauss elim
at 0℃ and with a heat source f(x).,gauss elim
"Remember that our goal is to produce a discrete problem—in other words, a problem",gauss elim
in linear algebra. For that reason we can only accept a ﬁnite amount of information about,gauss elim
"f(x), say its values at n equally spaced points x = h,x = 2h,...,x = nh. We compute",gauss elim
"approximate values u1,...,un for the true solution u at these same points. At the ends",gauss elim
"x = 0 and x = 1 = (n+1)h, the boundary values are u0 = 0 and un+1 = 0.",gauss elim
The ﬁrst question is: How do we replace the derivative d2u/dx2? The ﬁrst derivative,gauss elim
"can be approximated by stopping ∆u/∆x at a ﬁnite stepsize, and not permitting h (or ∆x)",gauss elim
1.7 Special Matrices and Applications,gauss elim
"to approach zero. The difference ∆u can be forward, backward, or centered:",gauss elim
The last is symmetric about x and it is the most accurate. For the second derivative there,gauss elim
is just one combination that uses only the values at x and x±h:,gauss elim
"This also has the merit of being symmetric about x. To repeat, the right-hand side ap-",gauss elim
"proaches the true value of d2u/dx2 as h → 0, but we have to stop at a positive h.",gauss elim
"At each meshpoint x = jh, the equation −d2u/dx2 = f(x) is replaced by its discrete",gauss elim
analogue (5). We multiplied through by h2 to reach n equations Au = b:,gauss elim
−u j+1 +2u j −u j−1 = h2 f(jh),gauss elim
"for j = 1,...,n.",gauss elim
"The ﬁrst and last equations (j = 1 and j = n) include u0 = 0 and un+1 = 0, which are",gauss elim
known from the boundary conditions. These values would be shifted to the right-hand,gauss elim
side of the equation if they were not zero. The structure of these n equations (5) can be,gauss elim
better visualized in matrix form. We choose h = 1,gauss elim
"6, to get a 5 by 5 matrix A:",gauss elim
"From now on, we will work with equation (6). It has a very regular coefﬁcient matrix,",gauss elim
"whose order n can be very large. The matrix A possesses many special properties, and",gauss elim
three of those properties are fundamental:,gauss elim
1. The matrix A is tridiagonal. All nonzero entries lie on the main diagonal and the,gauss elim
two adjacent diagonals. Outside this band all entries are aij = 0. These zeros will,gauss elim
bring a tremendous simpliﬁcation to Gaussian elimination.,gauss elim
"2. The matrix is symmetric. Each entry aij equals its mirror image a ji, so that AT = A.",gauss elim
"The upper triangular U will be the transpose of the lower triangular L, and A =",gauss elim
LDLT. This symmetry of A reﬂects the symmetry of d2u/dx2. An odd derivative,gauss elim
like du/dx or d3u/dx3 would destroy the symmetry.,gauss elim
3. The matrix is positive deﬁnite. This extra property says that the pivots are positive.,gauss elim
Row exchanges are unnecessary in theory and in practice. This is in contrast to the,gauss elim
"matrix B at the end of this section, which is not positive deﬁnite. Without a row",gauss elim
exchange it is totally vulnerable to roundoff.,gauss elim
Positive deﬁniteness brings this whole course together (in Chapter 6)!,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
We return to the fact that A is tridiagonal. What effect does this have on elimination?,gauss elim
The ﬁrst stage of the elimination process produces zeros below the ﬁrst pivot:,gauss elim
on A: Step 1,gauss elim
"Compared with a general 5 by 5 matrix, that step displays two major simpliﬁcations:",gauss elim
1. There was only one nonzero entry below the pivot.,gauss elim
2. The pivot row was very short.,gauss elim
The multiplier ℓ21 = −1,gauss elim
2 came from one division. The new pivot 3,gauss elim
2 came from a single,gauss elim
"multiplication-subtraction. Furthermore, the tridiagonal pattern is preserved: Every",gauss elim
stage of elimination admits the simpliﬁcations (a) and (b).,gauss elim
The ﬁnal result is the LDU = LDLT factorization of A. Notice the pivots!,gauss elim
The L and U factors of a tridiagonal matrix are bidiagonal. The three factors together,gauss elim
have the same band structure of three essential diagonals (3n−2 parameters) as A. Note,gauss elim
"too that L and U are transposes of one another, as expected from the symmetry. The",gauss elim
"pivots 2/1, 3/2, 4/3, 5/4, 6/5 are all positive. Their product is the determinant of A:",gauss elim
"detA = 6. The pivots are obviously converging to 1, as n gets large. Such matrices make",gauss elim
a computer very happy.,gauss elim
These sparse factors L and U completely change the usual operation count. Elimina-,gauss elim
"tion on each column needs only two operations, as above, and there are n columns. In",gauss elim
place of n3/3 operations we need only 2n. Tridiagonal systems Ax = b can be solved,gauss elim
almost instantly. The cost of solving a tridiagonal system is proportional to n.,gauss elim
A band matrix has aij = 0 except in the band |i − j| < w (Figure 1.8). The “half,gauss elim
"bandwidth” is w = 1 for a diagonal matrix, w = 2 for a tridiagonal matrix, and w = n",gauss elim
"for a full matrix. For each column, elimination requires w(w − 1) operations: a row",gauss elim
of length w acts on w − 1 rows below. Elimination on the n columns of a band matrix,gauss elim
requires about w2n operations.,gauss elim
"As w approaches n, the matrix becomes full, and the count is roughly n3. For an exact",gauss elim
"count, the lower right-hand corner has no room for bandwidth w. The precise number of",gauss elim
"divisions and multiplication-subtractions that produce L, D, and U (without assuming a",gauss elim
1.7 Special Matrices and Applications,gauss elim
Figure 1.8: A band matrix A and its factors L and U.,gauss elim
symmetric A) is P = 1,gauss elim
"3w(w−1)(3n−2w+1). For a full matrix with w = n, we recover",gauss elim
"3n(n−1)(n+1). This is a whole number, since n−1, n, and n+1 are consecutive",gauss elim
"integers, and one of them is divisible by 3.",gauss elim
"That is our last operation count, and we emphasize the main point. A ﬁnite-difference",gauss elim
"matrix like A has a full inverse. In solving Ax = b, we are actually much worse off",gauss elim
"knowing A−1 than knowing L and U. Multiplying A−1 by b takes n2 steps, whereas 4n",gauss elim
are sufﬁcient for the forward elimination and back-substitution that produce x =U−1c =,gauss elim
We hope this example reinforced the reader’s understanding of elimination (which,gauss elim
we now assume to be perfectly understood!). It is a genuine example of the large linear,gauss elim
systems that are actually met in practice. The next chapter turns to the existence and the,gauss elim
"uniqueness of x, for m equations in n unknowns.",gauss elim
In theory the nonsingular case is completed. There is a full set of pivots (with row ex-,gauss elim
"changes). In practice, more row exchanges may be equally necessary—or the computed",gauss elim
solution can easily become worthless. We will devote two pages (entirely optional in,gauss elim
class) to making elimination more stable—why it is needed and how it is done.,gauss elim
"For a system of moderate size, say 100 by 100, elimination involves a third of a mil-",gauss elim
"3n3). With each operation we must expect a roundoff error. Normally,",gauss elim
"we keep a ﬁxed number of signiﬁcant digits (say three, for an extremely weak computer).",gauss elim
Then adding two numbers of different sizes gives an error:,gauss elim
loses the digits 2 and 3.,gauss elim
How do all these individual errors contribute to the ﬁnal error in Ax = b?,gauss elim
"This is not an easy problem. It was attacked by John von Neumann, who was the",gauss elim
leading mathematician at the time when computers suddenly made a million operations,gauss elim
possible. In fact the combination of Gauss and von Neumann gives the simple elimina-,gauss elim
"tion algorithm a remarkably distinguished history, although even von Neumann overes-",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
timated the ﬁnal roundoff error. It was Wilkinson who found the right way to answer the,gauss elim
"question, and his books are now classics.",gauss elim
Two simple examples will illustrate three important points about roundoff error. The,gauss elim
A is nearly singular whereas B is far from singular. If we slightly change the last entry,gauss elim
"of A to a22 = 1, it is singular. Consider two very close right-hand sides b:",gauss elim
u + 1.0001v = 2,gauss elim
u + 1.0001v = 2.0001,gauss elim
"The solution to the ﬁrst is u = 2, v = 0. The solution to the second is u = v = 1. A",gauss elim
change in the ﬁfth digit of b was ampliﬁed to a change in the ﬁrst digit of the solution. No,gauss elim
numerical method can avoid this sensitivity to small perturbations. The ill-conditioning,gauss elim
"can be shifted from one place to another, but it cannot be removed. The true solution is",gauss elim
"very sensitive, and the computed solution cannot be less so.",gauss elim
The second point is as follows.,gauss elim
1O Even a well-conditioned matrix like B can be ruined by a poor algorithm.,gauss elim
"We regret to say that for the matrix B, direct Gaussian elimination is a poor algorithm.",gauss elim
"Suppose .0001 is accepted as the ﬁrst pivot. Then 10,000 times the ﬁrst row is subtracted",gauss elim
"from the second. The lower right entry becomes −9999, but roundoff to three places",gauss elim
"would give −10,000. Every trace of the entry 1 would disappear:",gauss elim
u+v = 2 −→ .0001u+v = 1,gauss elim
"Roundoff will produce −10,000v = −10,000, or v = 1. This is correct to three decimal",gauss elim
places. Back-substitution with the right v = .9999 would leave u = 1:,gauss elim
"Instead, accepting v = 1, which is wrong only in the fourth place, we obtain u = 0:",gauss elim
The computed u is completely mistaken. B is well-conditioned but elimination is vio-,gauss elim
"lently unstable. L, D, and U are completely out of scale with B:",gauss elim
"The small pivot .0001 brought instability, and the remedy is clear—exchange rows.",gauss elim
1.7 Special Matrices and Applications,gauss elim
A small pivot forces a practical change in elimination. Normally we,gauss elim
compare each pivot with all possible pivots in the same column. Exchanging,gauss elim
rows to obtain the largest possible pivot is called partial pivoting.,gauss elim
"For B, the pivot .0001 would be compared with the possible pivot I below it. A row",gauss elim
"exchange would take place immediately. In matrix terms, this is multiplication by a",gauss elim
permutation matrix P = [0 1,gauss elim
1 0]. The new matrix C = PB has good factors:,gauss elim
"The pivots for C are 1 and .9999, much better than .0001 and −9999 for B.",gauss elim
The strategy of complete pivoting looks also in all later columns for the largest pos-,gauss elim
Not only a row but also a column exchange may be needed.,gauss elim
postmultiplication by a permutation matrix.) The difﬁculty with being so conservative,gauss elim
"is the expense, and partial pivoting is quite adequate.",gauss elim
We have ﬁnally arrived at the fundamental algorithm of numerical linear algebra:,gauss elim
"elimination with partial pivoting. Some further reﬁnements, such as watching to see",gauss elim
"whether a whole row or column needs to be resealed, are still possible. But essentially",gauss elim
the reader now knows what a computer does with a system of linear equations. Com-,gauss elim
"pared with the “theoretical” description—ﬁnd A−1, and multiply A−1b—our description",gauss elim
has consumed a lot of the reader’s time (and patience). I wish there were an easier way,gauss elim
"to explain how x is actually found, but I do not think there is.",gauss elim
1. Write out the LDU = LDLT factors of A in equation (6) when n = 4. Find the deter-,gauss elim
minant as the product of the pivots in D.,gauss elim
"2. Modify a11 in equation (6) from a11 = 2 to a11 = 1, and ﬁnd the LDU factors of this",gauss elim
3. Find the 5 by 5 matrix A0 (h = 1,gauss elim
replacing these boundary conditions by u0 = u1 and u6 = u5. Check that your A0,gauss elim
"times the constant vector (C,C,C,C,C), yields zero; A0 is singular. Analogously, if",gauss elim
"u(x) is a solution of the continuous problem, then so is u(x)+C.",gauss elim
4. Write down the 3 by 3 ﬁnite-difference matrix equation (h = 1,gauss elim
"dx2 +u = x,",gauss elim
u(0) = u(1) = 0.,gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
5. With h = 1,gauss elim
"4 and f(x) = 4π2sin2πx, the difference equation (5) is",gauss elim
"Solve for u1, u2, u3 and ﬁnd their error in comparison with the true solution u =",gauss elim
sin2πx at x = 1,gauss elim
"4, x = 1",gauss elim
"2, and x = 3",gauss elim
"6. What 5 by 5 system replaces (6) if the boundary conditions are changed to u(0) = 1,",gauss elim
Problems 7–11 are about roundoff error and row exchanges.,gauss elim
7. Compute H−1 in two ways for the 3 by 3 Hilbert matrix,gauss elim
ﬁrst by exact computation and second by rounding off each number to three ﬁgures.,gauss elim
This matrix H is ill-conditioned and row exchanges don’t help.,gauss elim
"8. For the same matrix H, compare the right-hand sides of Hx = b when the solutions",gauss elim
"are x = (1,1,1) and x = (0,6,−3.6).",gauss elim
"9. Solve Hx = b = (1,0,...,0) for the 10 by 10 Hilbert matrix with hij = 1/(i+ j −1),",gauss elim
using any computer code for linear equations. Then change an entry of b by .0001,gauss elim
and compare the solutions.,gauss elim
10. Compare the pivots in direct elimination to those with partial pivoting for,gauss elim
(This is actually an example that needs rescaling before elimination.),gauss elim
11. Explain why partial pivoting produces multipliers ℓij in L that satisfy |ℓij| ≤ 1. Can,gauss elim
you construct a 3 by 3 example with all |aij| ≤ 1 whose last pivot is 4? This is the,gauss elim
"worst possible, since each entry is at most doubled when |ℓij| ≤ 1.",gauss elim
1.1 (a) Write down the 3 by 3 matrices with entries,gauss elim
aij = i− j,gauss elim
1.7 Special Matrices and Applications,gauss elim
(b) Compute the products AB and BA and A2.,gauss elim
1.2 For the matrices,gauss elim
compute AB and BA and A−1 and B−1 and (AB)−1.,gauss elim
1.3 Find examp1es of 2 by 2 matrices with a12 = 1,gauss elim
2 for which (a) A2 = I.,gauss elim
(c) A2 = A.,gauss elim
1.4 Solve by elimination and back-substitution:,gauss elim
+ w = 4,gauss elim
u + v + w = 6,gauss elim
v + w =,gauss elim
1.5 Factor the preceding matrices into A = LU or PA = LU.,gauss elim
1.6 (a) There are sixteen 2 by 2 matrices whose entries are 1s and 0s. How many are,gauss elim
(b) (Much harder!) If you put 1s and 0s at random into the entries of a 10 by 10,gauss elim
"matrix, is it more likely to be invertible or singular?",gauss elim
1.7 There are sixteen 2 by 2 matrices whose entries are 1s and −1s. How many are,gauss elim
1.8 How are the rows of EA related to the rows of A in the following cases?,gauss elim
1.9 Write down a 2 by 2 system with inﬁnitely many solutions.,gauss elim
"1.10 Find inverses if they exist, by inspection or by Gauss-Jordan:",gauss elim
"1.11 If E is 2 by 2 and it adds the ﬁrst equation to the second, what are E2 and E8 and",gauss elim
"1.12 True or false, with reason if true or counterexample if false:",gauss elim
"(1) If A is invertible and its rows are in reverse order in B, then B is invertible.",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
(2) If A and B are symmetric then AB is symmetric.,gauss elim
(3) If A and B are invertible then BA is invertible.,gauss elim
(4) Every nonsingular matrix can be factored into the product A = LU of a lower,gauss elim
triangular L and an upper triangular U.,gauss elim
1.13 Solve Ax = b by solving the triangular systems Lc = b and Ux = c:,gauss elim
A = LU =,gauss elim
"What part of A−1 have you found, with this particular b?",gauss elim
"1.14 If possible, ﬁnd 3 by 3 matrices B such that",gauss elim
(1) BA = 2A for every A.,gauss elim
(2) BA = 2B for every A.,gauss elim
(3) BA has the ﬁrst and last rows of A reversed.,gauss elim
(4) BA has the ﬁrst and last columns of A reversed.,gauss elim
1.15 Find the value for c in the following n by n inverse:,gauss elim
1 1 1 c,gauss elim
1.16 For which values of k does,gauss elim
x + ky = 1,gauss elim
"have no solution, one solution, or inﬁnitely many solutions?",gauss elim
1.17 Find the symmetric factorization A = LDLT of,gauss elim
1.18 Suppose A is the 4 by 4 identity matrix except for a vector v in column 2:,gauss elim
1.7 Special Matrices and Applications,gauss elim
"(a) Factor A into LU, assuming v2 ̸= 0.",gauss elim
"(b) Find A−1, which has the same form as A.",gauss elim
"1.19 Solve by elimination, or show that there is no solution:",gauss elim
u + 2v + 3w = 0,gauss elim
3u + 5v + 7w = 1,gauss elim
u + 3w =,gauss elim
3u + 5v + 7w = 1.,gauss elim
1.20 The n by n permutation matrices are an important example of a “group.” If you,gauss elim
multiply them you stay inside the group; they have inverses in the group; the identity,gauss elim
is in the group; and the law P1(P2P3) = (P1P2)P3 is true—because it is true for all,gauss elim
(a) How many members belong to the groups of 4 by 4 and n by n permutation,gauss elim
(b) Find a power k so that all 3 by 3 permutation matrices satisfy Pk = I.,gauss elim
1.21 Describe the rows of DA and the columns of AD if D = [2 0,gauss elim
1.22 (a) If A is invertible what is the inverse of AT?,gauss elim
(b) If A is also symmetric what is the transpose of A−1?,gauss elim
(c) Illustrate both formulas when A = [2 1,gauss elim
"1.23 By experiment with n = 2 and n = 3, ﬁnd",gauss elim
"1.24 Starting with a ﬁrst plane u+2v−w = 6, ﬁnd the equation for",gauss elim
(a) the parallel plane through the origin.,gauss elim
"(b) a second plane that also contains the points (6,0,0) and (2,2,0).",gauss elim
"(c) a third plane that meets the ﬁrst and second in the point (4,1,0).",gauss elim
1.25 What multiple of row 2 is subtracted from row 3 in forward elimination of A?,gauss elim
"How do you know (without multiplying those factors) that A is invertible, symmet-",gauss elim
"ric, and tridiagonal? What are its pivots?",gauss elim
"1.26 (a) What vector x will make Ax = column 1 of A + 2(column 3), for a 3 by 3 matrix",gauss elim
Chapter 1 Matrices and Gaussian Elimination,gauss elim
(b) Construct a matrix that has column 1 + 2(column 3) = 0. Check that A is,gauss elim
singular (fewer than 3 pivots) and explain why that must be the case.,gauss elim
"1.27 True or false, with reason if true and counterexample if false:",gauss elim
"(1) If L1U1 = L2U2 (upper triangular U’s with nonzero diagonal, lower triangular",gauss elim
"L’s with unit diagonal), then L1 = L2 and U1 = U2. The LU factorization is",gauss elim
(2) If A2 +A = I then A−1 = A+I.,gauss elim
"(3) If all diagonal entries of A are zero, then A is singular.",gauss elim
1.28 By experiment or the Gauss-Jordan method compute,gauss elim
1.29 Write down the 2 by 2 matrices that,gauss elim
(a) reverse the direction of every vector.,gauss elim
(b) project every vector onto the x2 axis.,gauss elim
(c) turn every vector counterclockwise through 90°.,gauss elim
(d) reﬂect every vector through the 45° line x1 = x2.,gauss elim
Vector Spaces and Subspaces,vector spaces
"Elimination can simplify, one entry at a time, the linear system Ax = b. Fortunately it",vector spaces
also simpliﬁes the theory. The basic questions of existence and uniqueness—Is there,vector spaces
"one solution, or no solution, or an inﬁnity of solutions?—are much easier to answer",vector spaces
"after elimination, We need to devote one more section to those questions, to ﬁnd every",vector spaces
solution for an m by n system. Then that circle of ideas will be complete.,vector spaces
But elimination produces only one kind of understanding of Ax = b. Our chief object,vector spaces
is to achieve a different and deeper understanding. This chapter may be more difﬁcult,vector spaces
than the ﬁrst one. It goes to the heart of linear algebra.,vector spaces
"For the concept of a vector space, we start immediately with the most important",vector spaces
"spaces. They are denoted by R1,R2,R3,...; the space Rn consists of all column vectors",vector spaces
with n components. (We write R because the components are real numbers.) R2 is,vector spaces
represented by the usual x-y plane; the two components of the vector become the x and,vector spaces
y coordinates of the corresponding point. The three components of a vector in R3 give a,vector spaces
point in three-dimensional space. The one-dimensional space R1 is a line.,vector spaces
The valuable thing for linear algebra is that the extension to n dimensions is so,vector spaces
"straightforward. For a vector in R7 we just need the seven components, even if the",vector spaces
"geometry is hard to visualize. Within all vector spaces, two operations are possible:",vector spaces
"We can add any two vectors, and we can multiply all vectors by scalars.",vector spaces
"In other words, we can take linear combinations.",vector spaces
Addition obeys the commutative law x + y = y + x; there is a “zero vector” satisfying,vector spaces
0+x = x; and there is a vector “−x” satisfying −x+x = 0. Eight properties (including,vector spaces
those three) are fundamental; the full list is given in Problem 5 at the end of this section.,vector spaces
A real vector space is a set of vectors together with rules for vector addition and mul-,vector spaces
tiplication by real numbers. Addition and multiplication must produce vectors in the,vector spaces
"space, and they must satisfy the eight conditions.",vector spaces
Chapter 2 Vector Spaces,vector spaces
Normally our vectors belong to one of the spaces Rn; they are ordinary column vec-,vector spaces
"tors. If x = (1,0,0,3), then 2x (and also x + x) has components 2, 0, 0, 6. The formal",vector spaces
deﬁnition allows other things to be “vectors”-provided that addition and scalar multipli-,vector spaces
cation are all right. We give three examples:,vector spaces
"1. The inﬁnite-dimensional space R∞. Its vectors have inﬁnitely many components, as",vector spaces
"in x = (1,2,1,2,...). The laws for x+y and cx stay unchanged.",vector spaces
2. The space of 3 by 2 matrices. In this case the “vectors” are matrices! We can add,vector spaces
"two matrices, and A+B = B+A, and there is a zero matrix, and so on. This space",vector spaces
is almost the same as R6. (The six components are arranged in a rectangle instead,vector spaces
"of a column.) Any choice of m and n would give, as a similar example, the vector",vector spaces
space of all m by n matrices.,vector spaces
3. The space of functions f(x). Here we admit all functions f that are deﬁned on,vector spaces
"a ﬁxed interval, say 0 ≤ x ≤ 1. The space includes f(x) = x2, g(x) = sinx, their",vector spaces
"sum (f +g)(x) = x2 +sinx, and all multiples like 3x2 and −sinx. The vectors are",vector spaces
"functions, and the dimension is somehow a larger inﬁnity than for R∞.",vector spaces
"Other examples are given in the exercises, but the vector spaces we need most are",vector spaces
somewhere else—they are inside the standard spaces Rn. We want to describe them,vector spaces
"and explain why they are important. Geometrically, think of the usual three-dimensional",vector spaces
R3 and choose any plane through the origin. That plane is a vector space in its own,vector spaces
"right. If we multiply a vector in the plane by 3, or −3, or any other scalar, we get a",vector spaces
"vector in the same plane. If we add two vectors in the plane, their sum stays in the",vector spaces
"plane. This plane through (0,0,0) illustrates one of the most fundamental ideas in linear",vector spaces
algebra; it is a subspace of the original space R3.,vector spaces
Deﬁnition. A subspace of a vector space is a nonempty subset that satisﬁes the require-,vector spaces
ments for a vector space: Linear combinations stay in the subspace.,vector spaces
"(i) If we add any vectors x and y in the subspace, x+y is in the subspace.",vector spaces
"(ii) If we multiply any vector x in the subspace by any scalar c, cx is in the subspace.",vector spaces
Notice our emphasis on the word space. A subspace is a subset that is “closed” under,vector spaces
"addition and scalar multiplication. Those operations follow the rules of the host space,",vector spaces
keeping us inside the subspace. The eight required properties are satisﬁed in the larger,vector spaces
space and will automatically be satisﬁed in every subspace. Notice in particular that the,vector spaces
zero vector will belong to every subspace. That comes from rule (ii): Choose the scalar,vector spaces
to be c = 0.,vector spaces
"The smallest subspace Z contains only one vector, the zero vector. It is a “zero-",vector spaces
"dimensional space,” containing only the point at the origin. Rules (i) and (ii) are satisﬁed,",vector spaces
2.1 Vector Spaces and Subspaces,vector spaces
"since the sum 0 + 0 is in this one-point space, and so are all multiples c0. This is the",vector spaces
smallest possible vector space: the empty set is not allowed. At the other extreme. the,vector spaces
"largest subspace is the whole of the original space. If the original space is R3, then the",vector spaces
"possible subspaces are easy to describe: R3 itself, any plane through the origin, any line",vector spaces
"through the origin, or the origin (the zero vector) alone.",vector spaces
The distinction between a subset and a subspace is made clear by examples. In each,vector spaces
"case, can you add vectors and multiply by scalars, without leaving the space?",vector spaces
Example 1. Consider all vectors in R2 whose components are positive or zero. This,vector spaces
subset is the ﬁrst quadrant of the x-y plane; the coordinates satisfy x ≥ 0 and y ≥ 0. It,vector spaces
"is not a subspace, even though it contains zero and addition does leave us within the",vector spaces
"subset. Rule (ii) is violated, since if the scalar is −1 and the vector is [1 1], the multiple",vector spaces
cx = [−1 −1] is in the third quadrant instead of the ﬁrst.,vector spaces
"If we include the third quadrant along with the ﬁrst, scalar multiplication is all right.",vector spaces
"Every multiple cx will stay in this subset. However, rule (i) is now violated, since adding",vector spaces
"[1 2]+ [−2 − 1] gives [−1 1], which is not in either quadrant. The smallest subspace",vector spaces
containing the ﬁrst quadrant is the whole space R2.,vector spaces
Example 2. Start from the vector space of 3 by 3 matrices. One possible subspace is,vector spaces
the set of lower triangular matrices. Another is the set of symmetric matrices. A + B,vector spaces
"and cA are lower triangular if A and B are lower triangular, and they are symmetric if A",vector spaces
"and B are symmetric. Of course, the zero matrix is in both subspaces.",vector spaces
The Column Space of A,vector spaces
"We now come to the key examples, the column space and the nullspace of a matrix",vector spaces
A. The column space contains all linear combinations of the columns of A. It is a,vector spaces
subspace of Rm. We illustrate by a system of m = 3 equations in n = 2 unknowns:,vector spaces
Combination of columns equals b,vector spaces
With m > n we have more equations than unknowns—and usually there will be no solu-,vector spaces
tion. The system will be solvable only for a very “thin” subset of all possible b’s. One,vector spaces
way of describing this thin subset is so simple that it is easy to overlook.,vector spaces
2A The system Ax = b is solvable if and only if the vector b can be expressed,vector spaces
as a combination of the columns of A. Then b is in the column space.,vector spaces
"This description involves nothing more than a restatement of Ax = b, by columns:",vector spaces
Chapter 2 Vector Spaces,vector spaces
"Figure 2.1: The column space C(A), a plane in three-dimensional space.",vector spaces
These are the same three equations in two unknowns. Now the problem is: Find numbers,vector spaces
u and v that multiply the ﬁrst and second columns to produce b. The system is solvable,vector spaces
"exactly when such coefﬁcients exist, and the vector (u,v) is the solution x.",vector spaces
We are saying that the attainable right-hand sides b are all combinations of the columns,vector spaces
of A. One possible right-hand side is the ﬁrst column itself; the weights are u = 1 and,vector spaces
v = 0. Another possibility is the second column: u = 0 and v = 1. A third is the right-,vector spaces
"hand side b = 0. With u = 0 and v = 0, the vector b = 0 will always be attainable.",vector spaces
We can describe all combinations of the two columns geometrically: Ax = b can be,vector spaces
solved if and only if b lies in the plane that is spanned by the two column vectors (Figure,vector spaces
"2.1). This is the thin set of attainable b. If b lies off the plane, then it is not a combination",vector spaces
of the two columns. In that case Ax = b has no solution.,vector spaces
What is important is that this plane is not just a subset of R3 it is a subspace. It is,vector spaces
"the column space of A, consisting of all combinations of the columns. It is denoted by",vector spaces
C(A). Requirements (i) and (ii) for a subspace of Rm are easy to check:,vector spaces
"(i) Suppose b and b′ lie in the column space, so that Ax = b for some x and Ax′ = b′",vector spaces
"for some x′. Then A(x + x′) = b + b′, so that b + b′ is also a combination of the",vector spaces
columns. The column space of all attainable vectors b is closed under addition.,vector spaces
"(ii) If b is in the column space C(A), so is any multiple cb. If some combination",vector spaces
"of columns produces b (say Ax = b), then multiplying that combination by c will",vector spaces
"produce cb. In other words, A(cx) = cb.",vector spaces
"For another matrix A, the dimensions in Figure 2.1 may be very different. The small-",vector spaces
est possible column space (one vector only) comes from the zero matrix A = 0. The,vector spaces
2.1 Vector Spaces and Subspaces,vector spaces
"only combination of the columns is b = 0. At the other extreme, suppose A is the 5 by",vector spaces
5 identity matrix. Then C(I) is the whole of R5; the ﬁve columns of I can combine to,vector spaces
produce any ﬁve-dimensional vector b. This is not at all special to the identity matrix.,vector spaces
Any 5 by 5 matrix that is nonsingular will have the whole of R5 as its column space.,vector spaces
For such a matrix we can solve Ax = b by Gaussian elimination; there are ﬁve pivots.,vector spaces
Therefore every b is in C(A) for a nonsingular matrix.,vector spaces
You can see how Chapter 1 is contained in this chapter. There we studied n by n,vector spaces
"matrices whose column space is Rn. Now we allow singular matrices, and rectangu-",vector spaces
lar matrices of any shape. Then C(A) can be somewhere between the zero space and,vector spaces
"the whole space Rm. Together with its perpendicular space, it gives one of our two",vector spaces
approaches to understanding Ax = b.,vector spaces
The Nullspace of A,vector spaces
The second approach to Ax = b is “dual” to the ﬁrst. We are concerned not only with,vector spaces
"attainable right-hand sides b, but also with the solutions x that attain them. The right-",vector spaces
"hand side b = 0 always allows the solution x = 0, but there may be inﬁnitely many other",vector spaces
"solutions. (There always are, if there are more unknowns than equations, n > m.) The",vector spaces
solutions to Ax = 0 form a vector space—the nullspace of A.,vector spaces
The nullspace of a matrix consists of all vectors x such that Ax = 0. It is,vector spaces
"denoted by N(A). It is a subspace of Rn, just as the column space was a",vector spaces
"Requirement (i) holds: If Ax = 0 and Ax′ = 0, then A(x+x′) = 0. Requirement (ii) also",vector spaces
holds: If Ax = 0 then A(cx) = 0. Both requirements fail if the right-hand side is not zero!,vector spaces
Only the solutions to a homogeneous equation (b = 0) form a subspace. The nullspace,vector spaces
is easy to ﬁnd for the example given above; it is as small as possible:,vector spaces
"The ﬁrst equation gives u = 0, and the second equation then forces v = 0. The nullspace",vector spaces
"contains only the vector (0,0). This matrix has “independent columns”—a key idea that",vector spaces
The situation is changed when a third column is a combination of the ﬁrst two:,vector spaces
B has the same column space as A. The new column lies in the plane of Figure 2.1; it is,vector spaces
the sum of the two column vectors we started with. But the nullspace of B contains the,vector spaces
Chapter 2 Vector Spaces,vector spaces
"vector (1,1,−1) and automatically contains any multiple (c,c,−c):",vector spaces
Nullspace is a line,vector spaces
"The nullspace of B is the line of all points x = c, y = c, z = −c. (The line goes through",vector spaces
"the origin, as any subspace must.) We want to be able, for any system Ax = b, to ﬁnd",vector spaces
C(A) and N(A): all attainable right-hand sides b and all solutions to Ax = 0.,vector spaces
The vectors b are in the column space and the vectors x are in the nullspace. We shall,vector spaces
compute the dimensions of those subspaces and a convenient set of vectors to generate,vector spaces
them. We hope to end up by understanding all four of the subspaces that are intimately,vector spaces
"related to each other and to A—the column space of A, the nullspace of A, and their two",vector spaces
1. Construct a subset of the x-y plane R2 that is,vector spaces
"(a) closed under vector addition and subtraction, but not scalar multiplication.",vector spaces
(b) closed under scalar multiplication but not under vector addition.,vector spaces
"Hint: Starting with u and v, add and subtract for (a). Try cu and cv for (b).",vector spaces
2. Which of the following subsets of R3 are actually subspaces?,vector spaces
"(a) The plane of vectors (b1,b2,b3) with ﬁrst component b1 = 0.",vector spaces
(b) The plane of vectors b with b1 = 1.,vector spaces
"(c) The vectors b with b2b3 = 0 (this is the union of two subspaces, the plane b2 = 0",vector spaces
and the plane b3 = 0).,vector spaces
"(d) All combinations of two given vectors (1,1,0) and (2,0,1).",vector spaces
"(e) The plane of vectors (b1,b2,b3) that satisfy b3 −b2 +3b1 = 0.",vector spaces
3. Describe the column space and the nullspace of the matrices,vector spaces
4. What is the smallest subspace of 3 by 3 matrices that contains all symmetric matrices,vector spaces
and all lower triangular matrices? What is the largest subspace that is contained in,vector spaces
both of those subspaces?,vector spaces
5. Addition and scalar multiplication are required to satisfy these eight rules:,vector spaces
2.1 Vector Spaces and Subspaces,vector spaces
1. x+y = y+x.,vector spaces
2. x+(y+z) = (x+y)+z.,vector spaces
3. There is a unique “zero vector” such that x+0 = x for all x.,vector spaces
4. For each x there is a unique vector −x such that x+(−x) = 0.,vector spaces
5. 1x = x.,vector spaces
6. (c1c2)x = c1(c2x).,vector spaces
7. c(x+y) = cx+cy.,vector spaces
8. (c1 +c2)x = c1x+c2x.,vector spaces
"(a) Suppose addition in R2 adds an extra 1 to each component, so that (3,1)+(5,0)",vector spaces
"equals (9,2) instead of (8,1). With scalar multiplication unchanged, which rules",vector spaces
"(b) Show that the set of all positive real numbers, with x+y and cx redeﬁned to equal",vector spaces
"the usual xy and xc, is a vector space. What is the “zero vector”?",vector spaces
"(c) Suppose (x1,x2) + (y1,y2) is deﬁned to be (x1 + y2,x2 + y1). With the usual",vector spaces
"cx = (cx1,cx2), which of the eight conditions are not satisﬁed?",vector spaces
6. Let P be the plane in 3-space with equation x +2y+z = 6. What is the equation of,vector spaces
the plane P0 through the origin parallel to P? Are P and P0 subspaces of R3?,vector spaces
7. Which of the following are subspaces of R∞?,vector spaces
"(a) All sequences like (1,0,1,0,...) that include inﬁnitely many zeros.",vector spaces
"(b) All sequences (x1,x2,...) with x j = 0 from some point onward.",vector spaces
(c) All decreasing sequences: x j+1 ≤ x j for each j.,vector spaces
(d) All convergent sequences: the x j have a limit as j → ∞.,vector spaces
(e) All arithmetic progressions: x j+1 −x j is the same for all j.,vector spaces
"(f) All geometric progressions (x1,kx1,k2x1,...) allowing all k and x1.",vector spaces
8. Which of the following descriptions are correct? The solutions x of,vector spaces
Chapter 2 Vector Spaces,vector spaces
(e) the nullspace of A.,vector spaces
(f) the column space of A.,vector spaces
9. Show that the set of nonsingular 2 by 2 matrices is not a vector space. Show also that,vector spaces
the set of singular 2 by 2 matrices is not a vector space.,vector spaces
10. The matrix A =,vector spaces
is a “vector” in the space M of all 2 by 2 matrices. Write the,vector spaces
"zero vector in this space, the vector 1",vector spaces
"2A, and the vector −A. What matrices are in the",vector spaces
smallest subspace containing A?,vector spaces
11. (a) Describe a subspace of M that contains A =,vector spaces
but not B =,vector spaces
"(b) If a subspace of M contains A and B, must it contain I?",vector spaces
(c) Describe a subspace of M that contains no nonzero diagonal matrices.,vector spaces
12. The functions f(x) = x2 and g(x) = 5x are “vectors” in the vector space F of all real,vector spaces
functions. The combination 3 f(x)−4g(x) is the function h(x) =,vector spaces
is broken if multiplying f(x) by c gives the function f(cx)?,vector spaces
"13. If the sum of the “vectors” f(x) and g(x) in F is deﬁned to be f(g(x)), then the “zero",vector spaces
"vector” is g(x) = x. Keep the usual scalar multiplication cf(x), and ﬁnd two rules",vector spaces
14. Describe the smallest subspace of the 2 by 2 matrix space M that contains,vector spaces
"15. Let P be the plane in R3 with equation x + y − 2z = 4. The origin (0,0,0) is not in",vector spaces
P! Find two vectors in P and check that their sum is not in P.,vector spaces
"16. P0 is the plane through (0,0,0) parallel to the plane P in Problem 15. What is the",vector spaces
equation for P0? Find two vectors in P0 and check that their sum is in P0.,vector spaces
"17. The four types of subspaces of R3 are planes, lines, R3 itself, or Z containing only",vector spaces
(a) Describe the three types of subspaces of R2.,vector spaces
(b) Describe the ﬁve types of subspaces of R4.,vector spaces
"18. (a) The intersection of two planes through (0,0,0) is probably a",vector spaces
. It can’t be the zero vector Z!,vector spaces
"(b) The intersection of a plane through (0,0,0) with a line through (0,0,0) is prob-",vector spaces
but it could be a,vector spaces
2.1 Vector Spaces and Subspaces,vector spaces
"(c) If S and T are subspaces of R5, their intersection S ∩ T (vectors in both sub-",vector spaces
spaces) is a subspace of R5. Check the requirements on x+y and cx.,vector spaces
"19. Suppose P is a plane through (0,0,0) and L is a line through (0,0,0). The smallest",vector spaces
vector space containing both P and L is either,vector spaces
20. True or false for M = all 3 by 3 matrices (check addition using an example)?,vector spaces
(a) The skew-symmetric matrices in M (with AT = −A) form a subspace.,vector spaces
(b) The unsymmetric matrices in M (with AT ̸= A) form a subspace.,vector spaces
"(c) The matrices that have (1,1,1) in their nullspace form a subspace.",vector spaces
Problems 21–30 are about column spaces C(A) and the equation Ax = b.,vector spaces
21. Describe the column spaces (lines or planes) of these particular matrices:,vector spaces
"22. For which right-hand sides (ﬁnd a condition on b1, b2, b3) are these systems solvable?",vector spaces
23. Adding row 1 of A to row 2 produces B. Adding column 1 to column 2 produces C.,vector spaces
A combination of the columns of,vector spaces
is also a combination of the columns of A.,vector spaces
Which two matrices have the same column,vector spaces
"24. For which vectors (b1,b2,b3) do these systems have a solution?",vector spaces
"25. (Recommended) If we add an extra column b to a matrix A, then the column space",vector spaces
. Give an example in which the column space gets larger and,vector spaces
an example in which it doesn’t. Why is Ax = b solvable exactly when the column,vector spaces
space doesn’t get larger by including b?,vector spaces
26. The columns of AB are combinations of the columns of A. This means: The column,vector spaces
space of AB is contained in (possibly equal to) the column space of A. Give an,vector spaces
example where the column spaces of A and AB are not equal.,vector spaces
Chapter 2 Vector Spaces,vector spaces
"27. If A is any 8 by 8 invertible matrix, then its column space is",vector spaces
28. True or false (with a counterexample if false)?,vector spaces
(a) The vectors b that are not in the column space C(A) form a subspace.,vector spaces
"(b) If C(A) contains only the zero vector, then A is the zero matrix.",vector spaces
(c) The column space of 2A equals the column space of A.,vector spaces
(d) The column space of A−I equals the column space of A.,vector spaces
"29. Construct a 3 by 3 matrix whose column space contains (1,1,0) and (1,0,1) but not",vector spaces
"(1,1,1). Construct a 3 by 3 matrix whose column space is only a line.",vector spaces
"30. If the 9 by 12 system Ax = b is solvable for every b, then C(A) =",vector spaces
31. Why isn’t R2 a subspace of R3?,vector spaces
Solving Ax = 0 and Ax = b,vector spaces
Chapter 1 concentrated on square invertible matrices. There was one solution to Ax = b,vector spaces
and it was x = −A−1b. That solution was found by elimination (not by computing A−1).,vector spaces
A rectangular matrix brings new possibilities—U may not have a full set of pivots. This,vector spaces
section goes onward from U to a reduced form R—the simplest matrix that elimina-,vector spaces
tion can give. R reveals all solutions immediately.,vector spaces
"For an invertible matrix, the nullspace contains only x = 0 (multiply Ax = 0 by A−1).",vector spaces
The column space is the whole space (Ax = b has a solution for every b). The new ques-,vector spaces
tions appear when the nullspace contains more than the zero vector and/or the column,vector spaces
space contains less than all vectors:,vector spaces
1. Any vector xn in the nullspace can be added to a particular solution xp. The solutions,vector spaces
"to all linear equations have this form, x = xp +xn:",vector spaces
A(xp +xn) = b.,vector spaces
"2. When the column space doesn’t contain every b in Rm, we need the conditions on",vector spaces
b that make Ax = b solvable.,vector spaces
A 3 by 4 example will be a good size. We will write down all solutions to Ax = 0. We,vector spaces
will ﬁnd the conditions for b to lie in the column space (so that Ax = b is solvable). The,vector spaces
"1 by 1 system 0x = b, one equation and one unknown, shows two possibilities:",vector spaces
0x = b has no solution unless b = 0. The column space of the 1 by 1 zero,vector spaces
matrix contains only b = 0.,vector spaces
0x = 0 has inﬁnitely many solutions. The nullspace contains all x. A particular,vector spaces
"solution is xp = 0, and the complete solution is x = xp +xn = 0+(any x).",vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
"Simple, I admit. If you move up to 2 by 2, it’s more interesting. The matrix",vector spaces
invertible: y+z = b1 and 2y+2z = b2 usually have no solution.,vector spaces
There is no solution unless b2 = 2b1. The column space of A contains only,vector spaces
"those b’s, the multiples of (1,2).",vector spaces
When b2 = 2b1 there are inﬁnitely many solutions. A particular solution to,vector spaces
"y + z = 2 and 2y + 2z = 4 is xp = (1,1). The nullspace of A in Figure 2.2",vector spaces
"contains (−1,1) and all its multiples xn = (−c,c):",vector spaces
2y + 2z = 4,vector spaces
= shortest particular solution xp,vector spaces
= MATLAB’s particular solution A\b,vector spaces
line of all solutions x = xp + xn,vector spaces
nullspace Axn = 0,vector spaces
Figure 2.2: The parallel lines of solutions to Axn = 0 and,vector spaces
Echelon Form U and Row Reduced Form R,vector spaces
"We start by simplifying this 3 by 4 matrix, ﬁrst to U and then further to R:",vector spaces
−1 −3 3 4,vector spaces
The pivot a11 = 1 is nonzero. The usual elementary operations will produce zeros in the,vector spaces
ﬁrst column below this pivot. The bad news appears in column 2:,vector spaces
No pivot in column 2,vector spaces
1 3 3 2,vector spaces
0 0 3 3,vector spaces
0 0 6 6,vector spaces
The candidate for the second pivot has become zero: unacceptable. We look below that,vector spaces
zero for a nonzero entry—intending to carry out a row exchange. In this case the entry,vector spaces
"below it is also zero. If A were square, this would signal that the matrix was singular.",vector spaces
"With a rectangular matrix, we must expect trouble anyway, and there is no reason to stop.",vector spaces
Chapter 2 Vector Spaces,vector spaces
"All we can do is to go on to the next column, where the pivot entry is 3. Subtracting twice",vector spaces
"the second row from the third, we arrive at U:",vector spaces
1 3 3 2,vector spaces
0 0 3 3,vector spaces
0 0 0 0,vector spaces
"Strictly speaking, we proceed to the fourth column. A zero is in the third pivot position,",vector spaces
"and nothing can be done. U is upper triangular, but its pivots are not on the main diago-",vector spaces
"nal. The nonzero entries of U have a “staircase pattern,” or echelon form. For the 5 by",vector spaces
"8 case in Figure 2.3, the starred entries may or may not be zero.",vector spaces
Figure 2.3: The entries of a 5 by 8 echelon matrix U and its reduced form R.,vector spaces
"We can always reach this echelon form U, with zeros below the pivots:",vector spaces
1. The pivots are the ﬁrst nonzero entries in their rows.,vector spaces
"2. Below each pivot is a column of zeros, obtained by elimination.",vector spaces
3. Each pivot lies to the right of the pivot in the row above. This produces the staircase,vector spaces
"pattern, and zero rows come last.",vector spaces
"Since we started with A and ended with U, the reader is certain to ask: Do we have",vector spaces
"A = LU as before? There is no reason why not, since the elimination steps have not",vector spaces
changed. Each step still subtracts a multiple of one row from a row beneath it. The,vector spaces
inverse of each step adds back the multiple that was subtracted. These inverses come in,vector spaces
the right order to put the multipliers directly into L:,vector spaces
Note that L is square. It has the same number of rows as A and U.,vector spaces
"The only operation not required by our example, but needed in general, is row ex-",vector spaces
change by a permutation matrix P. Since we keep going to the next column when no,vector spaces
"pivots are available, there is no need to assume that A is nonsingular. Here is PA = LU",vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
"For any m by n matrix A there is a permutation P, a lower triangular L",vector spaces
"with unit diagonal, and an m by n echelon matrix U, such that PA = LU.",vector spaces
"Now comes R. We can go further than U, to make the matrix even simpler. Divide",vector spaces
"the second row by its pivot 3, so that all pivots are 1. Then use the pivot row to produce",vector spaces
zero above the pivot. This time we subtract a row from a higher row. The ﬁnal result,vector spaces
(the best form we can get) is the reduced row echelon form R:,vector spaces
1 3 3 2,vector spaces
0 0 3 3,vector spaces
0 0 0 0,vector spaces
1 3 3 2,vector spaces
0 0 1 1,vector spaces
0 0 0 0,vector spaces
1 3 0 -1,vector spaces
This matrix R is the ﬁnal result of elimination on A. MATLAB would use the command,vector spaces
R = rref(A). Of course rref(R) would give R again!,vector spaces
What is the row reduced form of a square invertible matrix? In that case R is the,vector spaces
"identity matrix. There is a full set of pivots, all equal to 1, with zeros above and below.",vector spaces
"So rref(A) = I, when A is invertible.",vector spaces
"For a 5 by 8 matrix with four pivots, Figure 2.3 shows the reduced form R. It still",vector spaces
"contains an identity matrix, in the four pivot rows and four pivot columns. From R",vector spaces
we will quickly ﬁnd the nullspace of A. Rx = 0 has the same solutions as Ux = 0 and,vector spaces
Pivot Variables and Free Variables,vector spaces
Our goal is to read off all the solutions to Rx = 0. The pivots are crucial:,vector spaces
1 3 0 −1,vector spaces
"The unknowns u, v, w, y go into two groups. One group contains the pivot variables,",vector spaces
those that correspond to columns with pivots. The ﬁrst and third columns contain the,vector spaces
"pivots, so u and w are the pivot variables. The other group is made up of the free",vector spaces
"variables, corresponding to columns without pivots. These are the second and fourth",vector spaces
"columns, so v and y are free variables.",vector spaces
"To ﬁnd the most general solution to Rx = 0 (or, equivalently, to Ax = 0) we may assign",vector spaces
arbitrary values to the free variables. Suppose we call these values simply v and y. The,vector spaces
pivot variables are completely determined in terms of v and y:,vector spaces
w +y = 0,vector spaces
Chapter 2 Vector Spaces,vector spaces
"There is a “double inﬁnity” of solutions, with v and y free and independent. The com-",vector spaces
plete solution is a combination of two special solutions:,vector spaces
Please look again at this complete solution to Rx = 0 and Ax = 0. The special solution,vector spaces
"(−3,1,0,0) has free variables v = 1, y = 0. The other special solution (1,0,−1,1) has",vector spaces
v = 0 and y = 1. All solutions are linear combinations of these two. The best way to ﬁnd,vector spaces
all solutions to Ax = 0 is from the special solutions:,vector spaces
"1. After reaching Rx = 0, identify the pivot variables and free variables.",vector spaces
"2. Give one free variable the value 1, set the other free variables to 0, and solve Rx = 0",vector spaces
for the pivot variables. This x is a special solution.,vector spaces
3. Every free variable produces its own “special solution” by step 2. The combinations,vector spaces
of special solutions form the nullspace—all solutions to Ax = 0.,vector spaces
"Within the four-dimensional space of all possible vectors x, the solutions to Ax = 0",vector spaces
"form a two-dimensional subspace—the nullspace of A, In the example, N(A) is gener-",vector spaces
"ated by the special vectors (−3,1,0,0) and (1,0,−1,1). The combinations of these two",vector spaces
vectors produce the whole nullspace.,vector spaces
Here is a little trick. The special solutions are especially easy from R. The numbers 3,vector spaces
and 0 and −1 and 1 lie in the “nonpivot columns” of R. Reverse their signs to ﬁnd the,vector spaces
pivot variables (not free) in the special solutions. I will put the two special solutions,vector spaces
"from equation (2) into a nullspace matrix N, so you see this neat pattern:",vector spaces
The free variables have values 1 and 0. When the free columns moved to the right-,vector spaces
"hand side of equation (2), their coefﬁcients 3 and 0 and −1 and 1 switched sign. That",vector spaces
determined the pivot variables in the special solutions (the columns of N).,vector spaces
This is the place to recognize one extremely important theorem. Suppose a matrix has,vector spaces
"more columns than rows, n > m. Since m rows can hold at most m pivots, there must be",vector spaces
at least n−m free variables. There will be even more free variables if some rows of R,vector spaces
"reduce to zero; but no matter what, at least one variable must be free. This free variable",vector spaces
"can be assigned any value, leading to the following conclusion:",vector spaces
"If Ax = 0 has more unknowns than equations (n > m), it has at least one",vector spaces
special solution: There are more solutions than the trivial x = 0.,vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
"There must be inﬁnitely many solutions, since any multiple cx will also satisfy A(cx) =",vector spaces
"0. The nullspace contains the line through x. And if there are additional free variables,",vector spaces
the nullspace becomes more than just a line in n-dimensional space. The nullspace has,vector spaces
the same “dimension” as the number of free variables and special solutions.,vector spaces
This central idea—the dimension of a subspace—is made precise in the next section.,vector spaces
We count the free variables for the nullspace. We count the pivot variables for the column,vector spaces
"Solving Ax = b, Ux = c, and Rx = d",vector spaces
The case b ̸= 0 is quite different from b = 0. The row operations on A must act also,vector spaces
"on the right-hand side (on b). We begin with letters (b1,b2,b3) to ﬁnd the solvability",vector spaces
"condition—for b to lie in the column space. Then we choose b = (1,5,5) and ﬁnd all",vector spaces
"For the original example Ax = b = (b1,b2,b3), apply to both sides the operations that",vector spaces
led from A to U. The result is an upper triangular system Ux = c:,vector spaces
1 3 3 2,vector spaces
0 0 3 3,vector spaces
0 0 0 0,vector spaces
"The vector c on the right-hand side, which appeared after the forward elimination steps,",vector spaces
is just L−1b as in the previous chapter. Start now with Ux = c.,vector spaces
It is not clear that these equations have a solution. The third equation is very much,vector spaces
"in doubt, because its left-hand side is zero. The equations are inconsistent unless b3 −",vector spaces
"2b2 +5b1 = 0. Even though there are more unknowns than equations, there may be no",vector spaces
solution. We know another way of answering the same question: Ax = b can be solved if,vector spaces
and only if b lies in the column space of A. This subspace comes from the four columns,vector spaces
of A (not of U!):,vector spaces
"Even though there are four vectors, their combinations only ﬁll out a plane in three-",vector spaces
dimensional space. Column 2 is three times column 1. The fourth column equals the,vector spaces
"third minus the ﬁrst. These dependent columns, the second and fourth, are exactly the",vector spaces
"The column space C(A) can be described in two different ways. On the one hand,",vector spaces
"it is the plane generated by columns 1 and 3. The other columns lie in that plane,",vector spaces
"and contribute nothing new. Equivalently, it is the plane of all vectors b that satisfy",vector spaces
b3 −2b2 +5b1 = 0; this is the constraint if the system is to be solvable. Every column,vector spaces
Chapter 2 Vector Spaces,vector spaces
"satisﬁes this constraint, so it is forced on b! Geometrically, we shall see that the vector",vector spaces
"(5,−2,1) is perpendicular to each column.",vector spaces
"If b belongs to the column space, the solutions of Ax = b are easy to ﬁnd. The last",vector spaces
"equation in Ux = c is 0 = 0. To the free variables v and y, we may assign any values,",vector spaces
as before. The pivot variables u and w are still determined by back-substitution. For a,vector spaces
"speciﬁc example with b3 −2b2 +5b1 = 0, choose b = (1,5,5):",vector spaces
−1 −3 3 4,vector spaces
Forward elimination produces U on the left and c on the right:,vector spaces
1 3 3 2,vector spaces
0 0 3 3,vector spaces
0 0 0 0,vector spaces
"The last equation is 0 = 0, as expected. Back-substitution gives",vector spaces
"Again there is a double inﬁnity of solutions: v and y are free, u and w are not:",vector spaces
x = xp +xn,vector spaces
"This has all solutions to Ax = 0, plus the new xp = (−2,0,1,0). That xp is a particular",vector spaces
solution to Ax = b. The last two terms with v and y yield more solutions (because they,vector spaces
satisfy Ax = 0). Every solution to Ax = b is the sum of one particular solution and a,vector spaces
solution to Ax = 0:,vector spaces
xcomplete = xparticular +xnullspace,vector spaces
The particular solution in equation (4) comes from solving the equation with all free,vector spaces
"variables set to zero. That is the only new part, since the nullspace is already computed.",vector spaces
"When you multiply the highlighted equation by A, you get Axcomplete = b+0.",vector spaces
"Geometrically, the solutions again ﬁll a two-dimensional surface—but it is not a sub-",vector spaces
"space. It does not contain x = 0. It is parallel to the nullspace we had before, shifted",vector spaces
by the particular solution xp as in Figure 2.2. Equation (4) is a good way to write the,vector spaces
1. Reduce Ax = b to Ux = c.,vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
"2. With free variables = 0, ﬁnd a particular solution to Axp = b and Uxp = c.",vector spaces
"3. Find the special solutions to Ax = 0 (or Ux = 0 or Rx = 0). Each free variable, in",vector spaces
"turn, is 1. Then x = xp +(any combination xn of special solutions).",vector spaces
"When the equation was Ax = 0, the particular solution was the zero vector! It ﬁts the pat-",vector spaces
"tern, but xparticular = 0 was not written in equation (2). Now xp is added to the nullspace",vector spaces
"solutions, as in equation (4).",vector spaces
Question: How does the reduced form R make this solution even clearer? You will,vector spaces
"see it in our example. Subtract equation 2 from equation 1, and then divide equation 2",vector spaces
"by its pivot. On the left-hand side, this produces R, as before. On the right-hand side,",vector spaces
"these operations change c = (1,3,0) to a new vector d = (−2,1,0):",vector spaces
1 3 0 −1,vector spaces
"Our particular solution xp, (one choice out of many) has free variables v = y = 0.",vector spaces
"Columns 2 and 4 can be ignored. Then we immediately have u = −2 and w = 1, exactly",vector spaces
as in equation (4). The entries of d go directly into xp. This is because the identity,vector spaces
matrix is sitting in the pivot columns of R!,vector spaces
"Let me summarize this section, before working a new example. Elimination reveals",vector spaces
"the pivot variables and free variables. If there are r pivots, there are r pivot variables",vector spaces
and n−r free variables. That important number r will be given a name—it is the rank,vector spaces
"Suppose elimination reduces Ax = b to Ux = c and Rx = d, with r pivot",vector spaces
rows and r pivot columns. The rank of those matrices is r. The last m − r,vector spaces
"rows of U and R are zero, so there is a solution only if the last m−r entries of",vector spaces
c and d are also zero.,vector spaces
The complete solution is x = xp + xn. One particular solution xp has all free,vector spaces
"variables zero. Its pivot variables are the ﬁrst r entries of d, so Rxp = d.",vector spaces
"The nullspace solutions xn are combinations of n − r special solutions, with",vector spaces
one free variable equal to 1. The pivot variables in that special solution can be,vector spaces
found in the corresponding column of R (with sign reversed).,vector spaces
You see how the rank r is crucial. It counts the pivot rows in the “row space” and the,vector spaces
pivot columns in the column space. There are n − r special solutions in the nullspace.,vector spaces
There are m−r solvability conditions on b or c or d.,vector spaces
Chapter 2 Vector Spaces,vector spaces
"The full picture uses elimination and pivot columns to ﬁnd the column space, nullspace,",vector spaces
and rank. The 3 by 4 matrix A has rank 2:,vector spaces
"1. Reduce [A b] to [U c], to reach a triangular system Ux = c.",vector spaces
"2. Find the condition on b1, b2, b3 to have a solution.",vector spaces
3. Describe the column space of A: Which plane in R3?,vector spaces
4. Describe the nullspace of A: Which special solutions in R4?,vector spaces
"5. Find a particular solution to Ax = (0,6,−6) and the complete xp +xn.",vector spaces
6. Reduce [U c] to [R d]: Special solutions from R and xp from d.,vector spaces
Solution. (Notice how the right-hand side is included as an extra column!),vector spaces
"1. The multipliers in elimination are 2 and 3 and −1, taking [A b] to [U c].",vector spaces
2 4 8 12,vector spaces
3 6 7 13,vector spaces
0 0 −2 −2,vector spaces
1 2 3 5,vector spaces
0 0 2 2,vector spaces
0 0 0 0,vector spaces
b3 + b2 − 5b1,vector spaces
2. The last equation shows the solvability condition b3 +b2 −5b1 = 0. Then 0 = 0.,vector spaces
3. The column space of A is the plane containing all combinations of the pivot columns,vector spaces
Second description: The column space contains all vectors with b3+b2−5b1 = 0.,vector spaces
"That makes Ax = b solvable, so b is in the column space. All columns of A pass this",vector spaces
test b3 +b2 −5b1 = 0. This is the equation for the plane (in the ﬁrst description of,vector spaces
"4. The special solutions in N have free variables x2 = 1, x4 = 0 and x2 = 0, x4 = 1:",vector spaces
Special solutions to Ax = 0,vector spaces
Back-substitution in Ux = 0,vector spaces
Just switch signs in Rx = 0,vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
"5. Choose b = (0,6,−6), which has b3 + b2 − 5b1 = 0. Elimination takes Ax = b to",vector spaces
"Ux = c = (0,6,0). Back-substitute with free variables = 0:",vector spaces
"Particular solution to Axp = (0,6,−6)",vector spaces
"The complete solution to Ax = (0,6,−6) is (this xp) + (all xn).",vector spaces
"6. In the reduced R, the third column changes from (3,2,0) to (0,1,0). The right-hand",vector spaces
"side c = (0,6,0) becomes d = (−9,3,0). Then −9 and 3 go into xp:",vector spaces
1 2 3 5 0,vector spaces
0 0 2 2 6,vector spaces
0 0 0 0 0,vector spaces
1 2 0 2 −9,vector spaces
0 0 1 1,vector spaces
0 0 0 0,vector spaces
That ﬁnal matrix [R d] is rref([A b]) = rref([U c]). The numbers 2 and 0 and 2,vector spaces
and 1 in the free columns of R have opposite sign in the special solutions (the nullspace,vector spaces
matrix N). Everything is revealed by Rx = d.,vector spaces
"1. Construct a system with more unknowns than equations, but no solution. Change the",vector spaces
right-hand side to zero and ﬁnd all solutions xn.,vector spaces
"2. Reduce A and B to echelon form, to ﬁnd their ranks. Which variables are free?",vector spaces
1 2 0 1,vector spaces
0 1 1 0,vector spaces
1 2 0 1,vector spaces
Find the special solutions to Ax = 0 and Bx = 0. Find all solutions.,vector spaces
"3. Find the echelon form U, the free variables, and the special solutions:",vector spaces
0 1 0 3,vector spaces
0 2 0 6,vector spaces
Ax = b is consistent (has a solution) when b satisﬁes b2 =,vector spaces
. Find the complete,vector spaces
solution in the same form as equation (4).,vector spaces
Chapter 2 Vector Spaces,vector spaces
4. Carry out the same steps as in the previous problem to ﬁnd the complete solution of,vector spaces
"5. Write the complete solutions x = xp +xn to these systems, as in equation (4):",vector spaces
6. Describe the set of attainable right-hand sides b (in the column space) for,vector spaces
by ﬁnding the constraints on b that turn the third equation into 0 = 0 (after elimina-,vector spaces
"tion). What is the rank, and a particular solution?",vector spaces
"7. Find the value of c that makes it possible to solve Ax = b, and solve it:",vector spaces
v + 2w =,vector spaces
2u + 3v −,vector spaces
3u + 4v +,vector spaces
8. Under what conditions on b1 and b2 (if any) does Ax = b have a solution?,vector spaces
1 2 0 3,vector spaces
2 4 0 7,vector spaces
"Find two vectors in the nullspace of A, and the complete solution to Ax = b.",vector spaces
9. (a) Find the special solutions to Ux = 0. Reduce U to R and repeat:,vector spaces
1 2 3 4,vector spaces
0 0 1 2,vector spaces
0 0 0 0,vector spaces
"(b) If the right-hand side is changed from (0,0,0) to (a,b,0), what are all solutions?",vector spaces
10. Find a 2 by 3 system Ax = b whose complete solution is,vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
Find a 3 by 3 system with these solutions exactly when b1 +b2 = b3.,vector spaces
11. Write a 2 by 2 system Ax = b with many solutions xn but no solution xp. (Therefore,vector spaces
the system has no solution.) Which b’s allow an xp?,vector spaces
12. Which of these rules give a correct deﬁnition of the rank of A?,vector spaces
(a) The number of nonzero rows in R.,vector spaces
(b) The number of columns minus the total number of rows.,vector spaces
(c) The number of columns minus the number of free columns.,vector spaces
(d) The number of 1s in R.,vector spaces
13. Find the reduced row echelon forms R and the rank of these matrices:,vector spaces
(a) The 3 by 4 matrix of all 1s.,vector spaces
(b) The 4 by 4 matrix with aij = (−1)ij.,vector spaces
(c) The 3 by 4 matrix with aij = (−1)j.,vector spaces
"14. Find R for each of these (block) matrices, and the special solutions:",vector spaces
"15. If the r pivot variables come ﬁrst, the reduced R must look like",vector spaces
I is r by r,vector spaces
F is r by n−r,vector spaces
What is the nullspace matrix N containing the special solutions?,vector spaces
16. Suppose all r pivot variables come last. Describe the four blocks in the m by n,vector spaces
reduced echelon form (the block B should be r by r):,vector spaces
What is the nullspace matrix N of special solutions? What is its shape?,vector spaces
17. (Silly problem) Describe all 2 by 3 matrices A1 and A2 with row echelon forms R1,vector spaces
"and R2, such that R1 +R2 is the row echelon form of A1 +A2. Is it true that R1 = A1",vector spaces
and R2 = A2 in this case?,vector spaces
"18. If A has r pivot columns, then AT has r pivot columns. Give a 3 by 3 example for",vector spaces
which the column numbers are different for A and AT.,vector spaces
Chapter 2 Vector Spaces,vector spaces
19. What are the special solutions to Rx = 0 and RTy = 0 for these R?,vector spaces
1 0 2 3,vector spaces
0 1 4 5,vector spaces
0 0 0 0,vector spaces
"20. If A has rank r, then it has an r by r submatrix S that is invertible. Find that submatrix",vector spaces
S from the pivot rows and pivot columns of each A:,vector spaces
21. Explain why the pivot rows and pivot columns of A (not R) always give an r by r,vector spaces
invertible submatrix of A.,vector spaces
22. Find the ranks of AB and AM (rank 1 matrix times rank 1 matrix):,vector spaces
23. Multiplying the rank 1 matrices A = uvT and B = wzT gives uzT times the number,vector spaces
. AB has rank 1 unless,vector spaces
24. Every column of AB is a combination of the columns of A. Then the dimensions of,vector spaces
the column spaces give rank(AB) ≤ rank(A).,vector spaces
Problem: Prove also that rank(AB) ≤ rank(B).,vector spaces
"25. (Important) Suppose A and B are n by n matrices, and AB = I. Prove from rank(AB) ≤",vector spaces
rank(A) that the rank of A is n. So A is invertible and B must be its two-sided inverse.,vector spaces
Therefore BA = I (which is not so obvious!).,vector spaces
"26. If A is 2 by 3 and C is 3 by 2, show from its rank that CA ̸= I. Give an example in",vector spaces
"which AC = I. For m < n, a right inverse is not a left inverse.",vector spaces
27. Suppose A and B have the same reduced-row echelon form R. Explain how to change,vector spaces
A to B by elementary row operations. So B equals an,vector spaces
28. Every m by n matrix of rank r reduces to (m by r) times (r by n):,vector spaces
A = (pivot columns of A)(ﬁrst r rows of R) = (COL)(ROW).,vector spaces
Write the 3 by 4 matrix A at the start of this section as the product of the 3 by 2,vector spaces
matrix from the pivot columns and the 2 by 4 matrix from R:,vector spaces
−1 −3 3 4,vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
29. Suppose A is an m by n matrix of rank r. Its reduced echelon form is R. Describe,vector spaces
exactly the reduced row echelon form of RT (not AT).,vector spaces
30. (Recommended) Execute the six steps following equation (6) to ﬁnd the column,vector spaces
space and nullspace of A and the solution to Ax = b:,vector spaces
2 4 6 4,vector spaces
2 5 7 6,vector spaces
2 3 5 2,vector spaces
"31. For every c, ﬁnd R and the special solutions to Ax = 0:",vector spaces
1 1 2 2,vector spaces
2 2 4 4,vector spaces
1 c 2 2,vector spaces
"32. What is the nullspace matrix N (of special solutions) for A, B, C?",vector spaces
Problems 33–36 are about the solution of Ax = b. Follow the steps in the text to,vector spaces
xp and xn. Reduce the augmented matrix [A b].,vector spaces
33. Find the complete solutions of,vector spaces
1 3 1 2,vector spaces
2 6 4 8,vector spaces
0 0 2 4,vector spaces
"34. Under what condition on b1, b2, b3 is the following system solvable? Include b as a",vector spaces
fourth column in [A b]. Find all solutions when that condition holds:,vector spaces
"35. What conditions on b1, b2, b3, b4 make each system solvable? Solve for x:",vector spaces
Chapter 2 Vector Spaces,vector spaces
"36. Which vectors (b1,b2,b3) are in the column space of A? Which combinations of the",vector spaces
rows of A give zero?,vector spaces
"37. Why can’t a 1 by 3 system have xp = (2,4,0) and xn = any multiple of (1,1,1)?",vector spaces
"38. (a) If Ax = b has two solutions x1 and x2, ﬁnd two solutions to Ax = 0.",vector spaces
(b) Then ﬁnd another solution to Ax = b.,vector spaces
39. Explain why all these statements are false:,vector spaces
(a) The complete solution is any linear combination of xp and xn.,vector spaces
(b) A system Ax = b has at most one particular solution.,vector spaces
(c) The solution xp with all free variables zero is the shortest solution (minimum,vector spaces
length ∥x∥). (Find a 2 by 2 counterexample.),vector spaces
(d) If A is invertible there is no solution xn in the nullspace.,vector spaces
40. Suppose column 5 of U has no pivot. Then x5 is a,vector spaces
variable. The zero vector,vector spaces
"(is) (is not) the only solution to Ax = 0. If Ax = b has a solution, then it has",vector spaces
"41. If you know xp (free variables = 0) and all special solutions for Ax = b, ﬁnd xp and",vector spaces
all special solutions for these systems:,vector spaces
"42. If Ax = b has inﬁnitely many solutions, why is it impossible for Ax = B (new right-",vector spaces
hand side) to have only one solution? Could Ax = B have no solution?,vector spaces
"43. Choose the number q so that (if possible) the ranks are (a) 1, (b) 2, (c) 3:",vector spaces
44. Give examples of matrices A for which the number of solutions to Ax = b is,vector spaces
"(a) 0 or 1, depending on b.",vector spaces
"(b) ∞, regardless of b.",vector spaces
"(c) 0 or ∞, depending on b.",vector spaces
"(d) 1, regardless of b.",vector spaces
2.2 Solving Ax = 0 and Ax = b,vector spaces
45. Write all known relations between r and m and n if Ax = b has,vector spaces
(a) no solution for some b.,vector spaces
(b) inﬁnitely many solutions for every b.,vector spaces
"(c) exactly one solution for some b, no solution for other b.",vector spaces
(d) exactly one solution for every b.,vector spaces
46. Apply Gauss-Jordan elimination (right-hand side becomes extra column) to Ux = 0,vector spaces
and Ux = c. Reach Rx = 0 and Rx = d:,vector spaces
1 2 3 0,vector spaces
0 0 4 0,vector spaces
1 2 3 5,vector spaces
0 0 4 8,vector spaces
Solve Rx = 0 to ﬁnd xn (its free variable is x2 = 1). Solve Rx = d to ﬁnd xp (its free,vector spaces
variable is x2 = 0).,vector spaces
47. Apply elimination with the extra column to reach Rx = 0 and Rx = d:,vector spaces
3 0 6 0,vector spaces
0 0 2 0,vector spaces
0 0 0 0,vector spaces
3 0 6 9,vector spaces
0 0 2 4,vector spaces
0 0 0 5,vector spaces
Solve Rx = 0 (free variable = 1). What are the solutions to Rx = d?,vector spaces
48. Reduce to Ux = c (Gaussian elimination) and then Rx = d:,vector spaces
1 0 2 3,vector spaces
1 3 2 0,vector spaces
2 0 4 9,vector spaces
Find a particular solution xp and all nullspace solutions xn.,vector spaces
49. Find A and B with the given property or explain why you can’t.,vector spaces
(a) The only solution to Ax =,vector spaces
(b) The only solution to Bx =,vector spaces
50. The complete solution to Ax =,vector spaces
"51. The nullspace of a 3 by 4 matrix A is the line through (2,3,1,0).",vector spaces
(a) What is the rank of A and the complete solution to Ax = 0?,vector spaces
(b) What is the exact row reduced echelon form R of A?,vector spaces
Chapter 2 Vector Spaces,vector spaces
52. Reduce these matrices A and B to their ordinary echelon forms U:,vector spaces
1 2 2 4 6,vector spaces
1 2 3 6 9,vector spaces
0 0 1 2 3,vector spaces
Find a special solution for each free variable and describe every solution to Ax = 0,vector spaces
"and Bx = 0. Reduce the echelon forms U to R, and draw a box around the identity",vector spaces
matrix in the pivot rows and pivot columns.,vector spaces
"53. True or False? (Give reason if true, or counterexample to show it is false.)",vector spaces
(a) A square matrix has no free variables.,vector spaces
(b) An invertible matrix has no free variables.,vector spaces
(c) An m by n matrix has no more than n pivot variables.,vector spaces
(d) An m by n matrix has no more than m pivot variables.,vector spaces
54. Is there a 3 by 3 matrix with no zero entries for which U = R = I?,vector spaces
55. Put as many 1s as possible in a 4 by 7 echelon matrix U and in a reduced form R,vector spaces
"whose pivot columns are 2, 4, 5.",vector spaces
56. Suppose column 4 of a 3 by 5 matrix is all 0s. Then x4 is certainly a,vector spaces
The special solution for this variable is the vector x =,vector spaces
57. Suppose the ﬁrst and last columns of a 3 by 5 matrix are the same (nonzero). Then,vector spaces
is a free variable. Find the special solution for this variable.,vector spaces
58. The equation x − 3y − z = 0 determines a plane in R3. What is the matrix A in this,vector spaces
"equation? Which are the free variables? The special solutions are (3,1,0) and",vector spaces
"The parallel plane x−3y−z = 12 contains the particular point (12,0,0). All points",vector spaces
on this plane have the following form (ﬁll in the ﬁrst components):,vector spaces
59. Suppose column 1 + column 3 + column 5 = 0 in a 4 by 5 matrix with four pivots.,vector spaces
Which column is sure to have no pivot (and which variable is free)? What is the,vector spaces
special solution? What is the nullspace?,vector spaces
Problems 60–66 ask for matrices (if possible) with speciﬁc properties.,vector spaces
"60. Construct a matrix whose nullspace consists of all combinations of (2,2,1,0) and",vector spaces
"61. Construct a matrix whose nullspace consists of all multiples of (4,3,2,1).",vector spaces
"2.3 Linear Independence, Basis, and Dimension",vector spaces
"62. Construct a matrix whose column space contains (1,1,5) and (0,3.1) and whose",vector spaces
"63. Construct a matrix whose column space contains (1,1,0) and (0,1,1) and whose",vector spaces
"nullspace contains (1,0,1) and (0,0,1).",vector spaces
"64. Construct a matrix whose column space contains (1,1,1) and whose nullspace is the",vector spaces
"line of multiples of (1,1,1,1).",vector spaces
65. Construct a 2 by 2 matrix whose nullspace equals its column space.,vector spaces
66. Why does no 3 by 3 matrix have a nullspace that equals its column space?,vector spaces
67. The reduced form R of a 3 by 3 matrix with randomly chosen entries is almost sure,vector spaces
. What R is virtually certain if the random A is 4 by 3?,vector spaces
68. Show by example that these three statements are generally false:,vector spaces
(a) A and AT have the same nullspace.,vector spaces
(b) A and AT have the same free variables.,vector spaces
(c) If R is the reduced form rref(A) then RT is rref(AT).,vector spaces
"69. If the special solutions to Rx = 0 are in the columns of these N, go backward to ﬁnd",vector spaces
the nonzero rows of the reduced matrices R:,vector spaces
(empty 3 by 1).,vector spaces
70. Explain why A and −A always have the same reduced echelon form R.,vector spaces
"Linear Independence, Basis, and Dimension",vector spaces
"By themselves, the numbers m and n give an incomplete picture of the true size of a",vector spaces
"linear system. The matrix in our example had three rows and four columns, but the third",vector spaces
"row was only a combination of the ﬁrst two. After elimination it became a zero row, It",vector spaces
had no effect on the homogeneous problem Ax = 0. The four columns also failed to be,vector spaces
"independent, and the column space degenerated into a two-dimensional plane.",vector spaces
The important number that is beginning to emerge (the true size) is the rank r. The,vector spaces
"rank was introduced as the number of pivots in the elimination process. Equivalently,",vector spaces
the ﬁnal matrix U has r nonzero rows. This deﬁnition could be given to a computer. But,vector spaces
it would be wrong to leave it there because the rank has a simple and intuitive meaning:,vector spaces
The rank counts the number of genuinely independent rows in the matrix A. We want,vector spaces
deﬁnitions that are mathematical rather than computational.,vector spaces
The goal of this section is to explain and use four ideas:,vector spaces
Chapter 2 Vector Spaces,vector spaces
1. Linear independence or dependence.,vector spaces
2. Spanning a subspace.,vector spaces
3. Basis for a subspace (a set of vectors).,vector spaces
4. Dimension of a subspace (a number).,vector spaces
"The ﬁrst step is to deﬁne linear independence. Given a set of vectors v1,...,vk, we",vector spaces
"look at their combinations c1v1 + c2v2 + ··· + ckvk. The trivial combination, with all",vector spaces
"weights ci = 0, obviously produces the zero vector: 0v1 +···+0vk = 0. The question is",vector spaces
"whether this is the only way to produce zero. If so, the vectors are independent.",vector spaces
"If any other combination of the vectors gives zero, they are dependent.",vector spaces
2E Suppose c1v1 +···+ckvk = 0 only happens when c1 = ··· = ck = 0. Then,vector spaces
"the vectors v1,...,vk are linearly independent. If any c’s are nonzero, the v’s",vector spaces
are linearly dependent. One vector is a combination of the others.,vector spaces
"Linear dependence is easy to visualize in three-dimensional space, when all vectors",vector spaces
go out from the origin. Two vectors are dependent if they lie on the same line. Three,vector spaces
"vectors are dependent if they lie in the same plane. A random choice of three vectors,",vector spaces
"without any special accident, should produce linear independence (not in a plane). Four",vector spaces
vectors are always linearly dependent in R3.,vector spaces
"Example 1. If v1 = zero vector, then the set is linearly dependent. We may choose",vector spaces
c1 = 3 and all other ci = 0; this is a nontrivial combination that produces zero.,vector spaces
Example 2. The columns of the matrix,vector spaces
−1 −3 3 0,vector spaces
"are linearly dependent, since the second column is three times the ﬁrst. The combination",vector spaces
"of columns with weights −3, 1, 0, 0 gives a column of zeros.",vector spaces
The rows are also linearly dependent; row 3 is two times row 2 minus ﬁve times row,vector spaces
"1. (This is the same as the combination of b1, b2, b3, that had to vanish on the right-hand",vector spaces
"side in order for Ax = b to be consistent. Unless b3 −2b2 +5b1 = 0, the third equation",vector spaces
would not become 0 = 0.),vector spaces
Example 3. The columns of this triangular matrix are linearly independent:,vector spaces
No zeros on the diagonal,vector spaces
"2.3 Linear Independence, Basis, and Dimension",vector spaces
Look for a combination of the columns that makes zero:,vector spaces
Solve Ac = 0,vector spaces
"We have to show that c1, c2, c3 are all forced to be zero. The last equation gives",vector spaces
"c3 = 0. Then the next equation gives c2 = 0, and substituting into the ﬁrst equation forces",vector spaces
c1 = 0. The only combination to produce the zero vector is the trivial combination. The,vector spaces
nullspace of A contains only the zero vector c1 = c2 = c3 = 0.,vector spaces
The columns of A are independent exactly when N(A) = {zero vector}.,vector spaces
"A similar reasoning applies to the rows of A, which are also independent. Suppose",vector spaces
From the ﬁrst components we ﬁnd 3c1 = 0 or c1 = 0. Then the second components give,vector spaces
"c2 = 0, and ﬁnally c3 = 0.",vector spaces
"The nonzero rows of any echelon matrix U must be independent. Furthermore, if we",vector spaces
"pick out the columns that contain the pivots, they also are linearly independent. In our",vector spaces
1 3 3 2,vector spaces
0 0 3 1,vector spaces
0 0 0 0,vector spaces
"the pivot columns 1 and 3 are independent. No set of three columns is independent, and",vector spaces
"certainly not all four. It is true that columns 1 and 4 are also independent, but if that",vector spaces
last 1 were changed to 0 they would be dependent. It is the columns with pivots that are,vector spaces
guaranteed to be independent. The general rule is this:,vector spaces
The r nonzero rows of an echelon matrix U and a reduced matrix R are,vector spaces
linearly independent. So are the r columns that contain pivots.,vector spaces
Example 4. The columns of the n by n identity matrix are independent:,vector spaces
0 0 0 1,vector spaces
"These columns e1,...,en represent unit vectors in the coordinate directions; in R4,",vector spaces
Chapter 2 Vector Spaces,vector spaces
Most sets of four vectors in R4 are independent. Those e’s might be the safest.,vector spaces
"To check any set of vectors v1,...,vn for independence, put them in the columns of A.",vector spaces
Then solve the system Ac = 0; the vectors are dependent if there is a solution other than,vector spaces
"c = 0. With no free variables (rank n), there is no nullspace except c = 0; the vectors are",vector spaces
"independent. If the rank is less than n, at least one free variable can be nonzero and the",vector spaces
"One case has special importance. Let the n vectors have m components, so that A is an",vector spaces
m by n matrix. Suppose now that n > m. There are too many columns to be independents,vector spaces
"There cannot be n pivots, since there are not enough rows to hold them. The rank will",vector spaces
be less than n. Every system Ac = 0 with more unknowns than equations has solutions,vector spaces
A set of n vectors in Rm must be linearly dependent if n > m.,vector spaces
The reader will recognize this as a disguised form of 2C: Every m by n system Ax = 0,vector spaces
has nonzero solutions if n > m.,vector spaces
Example 5. These three columns in R2 cannot be independent:,vector spaces
To ﬁnd the combination of the columns producing zero we solve Ac = 0:,vector spaces
A → U =,vector spaces
"If we give the value 1 to the free variable c3, then back-substitution in Uc = 0 gives",vector spaces
"c2 = −1, c1 = 1. With these three weights, the ﬁrst column minus the second plus the",vector spaces
third equals zero: Dependence.,vector spaces
Now we deﬁne what it means for a set of vectors to span a space. The column space of,vector spaces
A is spanned by the columns. Their combinations produce the whole space:,vector spaces
"If a vector space V consists of all linear combinations of w1,...,wℓ, then",vector spaces
these vectors span the space. Every vector v in V is some combination of the,vector spaces
Every v comes from w’s,vector spaces
v = c1w1 +···+cℓwℓ,vector spaces
for some coefﬁcients ci.,vector spaces
It is permitted that a different combination of w’s could give the same vector v. The,vector spaces
"c’s need not be unique, because the spanning set might be excessively large—it could",vector spaces
"include the zero vector, or even all vectors.",vector spaces
"2.3 Linear Independence, Basis, and Dimension",vector spaces
"Example 6. The vectors w1 = (1,0,0), w2 = (0,1,0), and w3 = (−2,0,0) span a plane",vector spaces
"(the x-y plane) in R3. The ﬁrst two vectors also span this plane, whereas w1 and w3 span",vector spaces
Example 7. The column space of A is exactly the space that is spanned by its columns.,vector spaces
The row space is spanned by the rows. The deﬁnition is made to order. Multiplying A,vector spaces
by any x gives a combination of the columns; it is a vector Ax in the column space.,vector spaces
"The coordinate vectors e1,...,en coming from the identity matrix span Rn. Every",vector spaces
"vector b = (b1,...,bn) is a combination of those columns. In this example the weights",vector spaces
are the components bi themselves: b = b1e1 + ··· + bnen. But the columns of other,vector spaces
matrices also span Rn!,vector spaces
Basis for a Vector Space,vector spaces
"To decide if b is a combination of the columns, we try to solve Ax = b. To decide if",vector spaces
"the columns are independent, we solve Ax = 0. Spanning involves the column space,",vector spaces
"and independence involves the nullspace. The coordinate vectors e1,...,en span Rn",vector spaces
"and they are linearly independent. Roughly speaking, no vectors in that set are wasted.",vector spaces
This leads to the crucial idea of a basis.,vector spaces
A basis for V is a sequence of vectors having two properties at once:,vector spaces
1. The vectors are linearly independent (not too many vectors).,vector spaces
2. They span the space V (not too few vectors).,vector spaces
This combination of properties is absolutely fundamental to linear algebra. It means,vector spaces
"that every vector in the space is a combination of the basis vectors, because they span.",vector spaces
It also means that the combination is unique: If v = a1v1 + ··· + akvk and also v =,vector spaces
"b1v1 + ··· + bkvk, then subtraction gives 0 = ∑(ai − bi)vi. Now independence plays its",vector spaces
part; every coefﬁcient ai − bi must be zero. Therefore ai = bi. There is one and only,vector spaces
one way to write v as a combination of the basis vectors.,vector spaces
"We had better say at once that the coordinate vectors e1,...,en are not the only basis",vector spaces
"for Rn. Some things in linear algebra are unique, but not this. A vector space has",vector spaces
"inﬁnitely many different bases. Whenever a square matrix is invertible, its columns are",vector spaces
independent—and they are a basis for Rn. The two columns of this nonsingular matrix,vector spaces
are a basis for R2:,vector spaces
Every two-dimensional vector is a combination of those (independent!) columns.,vector spaces
Example 8. The x-y plane in Figure 2.4 is just R2. The vector v1 by itself is linearly,vector spaces
"independent, but it fails to span R2. The three vectors v1, v2, v3 certainly span R2, but",vector spaces
"are not independent. Any two of these vectors, say v1 and v2, have both properties—they",vector spaces
Chapter 2 Vector Spaces,vector spaces
"span, and they are independent. So they form a basis. Notice again that a vector space",vector spaces
does not have a unique basis.,vector spaces
"Figure 2.4: A spanning set v1, v2, v3. Bases v1, v2 and v1, v3 and v2, v3.",vector spaces
"Example 9. These four columns span the column space of U, but they are not indepen-",vector spaces
1 3 3 2,vector spaces
0 0 3 1,vector spaces
0 0 0 0,vector spaces
"There are many possibilities for a basis, but we propose a speciﬁc choice: The columns",vector spaces
"that contain pivots (in this case the ﬁrst and third, which correspond to the basic vari-",vector spaces
"ables) are a basis for the column space. These columns are independent, and it is easy",vector spaces
"to see that they span the space. In fact, the column space of U is just the x-y plane",vector spaces
within R3. C(U) is not the same as the column space C(A) before elimination—but the,vector spaces
number of independent columns didn’t change.,vector spaces
To summarize: The columns of any matrix span its column space. If they are indepen-,vector spaces
"dent, they are a basis for the column space—whether the matrix is square or rectangular.",vector spaces
"If we are asking the columns to be a basis for the whole space Rn, then the matrix must",vector spaces
be square and invertible.,vector spaces
Dimension of a Vector Space,vector spaces
"A space has inﬁnitely many different bases, but there is something common to all of",vector spaces
these choices. The number of basis vectors is a property of the space itself:,vector spaces
Any two bases for a vector space V contain the same number of vec-,vector spaces
"tors. This number, which is shared by all bases and expresses the number of",vector spaces
"“degrees of freedom” of the space, is the dimension of V.",vector spaces
We have to prove this fact: All possible bases contain the same number of vectors.,vector spaces
The x-y plane in Figure 2.4 has two vectors in every basis; its dimension is 2. In three,vector spaces
"2.3 Linear Independence, Basis, and Dimension",vector spaces
"dimensions we need three vectors, along the x-y-z axes or in three other (linearly in-",vector spaces
dependent!) directions. The dimension of the space Rn is n. The column space of U,vector spaces
in Example 9 had dimension 2; it was a “two-dimensional subspace of R3.” The zero,vector spaces
"matrix is rather exceptional, because its column space contains only the zero vector. By",vector spaces
"convention, the empty set is a basis for that space, and its dimension is zero.",vector spaces
Here is our ﬁrst big theorem in linear algebra:,vector spaces
"2K If v1,...,vm and w1,...,wn are both bases for the same vector space, then",vector spaces
m = n. The number of vectors is the same.,vector spaces
Proof. Suppose there are more w’s than v’s (n > m). We will arrive at a contradiction.,vector spaces
"Since the v’s form a basis, they must span the space. Every w j can be written as a",vector spaces
"combination of the v’s: If w1 = a11v1 + ··· + am1vm, this is the ﬁrst column of a matrix",vector spaces
"We don’t know each ai j, but we know the shape of A (it is m by n). The second vector",vector spaces
w2 is also a combination of the v’s. The coefﬁcients in that combination ﬁll the second,vector spaces
column of A. The key is that A has a row for every v and a column for every w. A is a,vector spaces
"short, wide matrix, since n > m. There is a nonzero solution to Ax = 0. Then VAx = 0",vector spaces
which is Wx = 0. A combination of the w’s gives zero! The w’s could not be a basis—so,vector spaces
we cannot have n > m.,vector spaces
If m > n we exchange the v’s and w’s and repeat the same steps. The only way to,vector spaces
avoid a contradiction is to have m = n. This completes the proof that m = n. To repeat:,vector spaces
The dimension of a space is the number of vectors in every basis.,vector spaces
This proof was used earlier to show that every set of m+1 vectors in Rm must be de-,vector spaces
pendent. The v’s and w’s need not be column vectors—the proof was all about the matrix,vector spaces
"A of coefﬁcients. In fact we can see this general result: In a subspace of dimension k,",vector spaces
"no set of more than k vectors can be independent, and no set of more than k vectors can.",vector spaces
"There are other “dual” theorems, of which we mention only one. We can start with a",vector spaces
"set of vectors that is too small or too big, and end up with a basis:",vector spaces
"Any linearly independent set in V can be extended to a basis, by adding",vector spaces
more vectors if necessary.,vector spaces
"Any spanning set in V can be reduced to a basis, by discarding vectors if",vector spaces
The point is that a basis is a maximal independent set. It cannot be made larger without,vector spaces
losing independence. A basis is also a minimal spanning set. It cannot be made smaller,vector spaces
and still span the space.,vector spaces
Chapter 2 Vector Spaces,vector spaces
You must notice that the word “dimensional” is used in two different ways. We speak,vector spaces
"about a four-dimensional vector, meaning a vector in R4. Now we have deﬁned a four-",vector spaces
dimensional subspace; an example is the set of vectors in R6 whose ﬁrst and last com-,vector spaces
ponents are zero. The members of this four-dimensional subspace are six-dimensional,vector spaces
One ﬁnal note about the language of linear algebra. We never use the terms “basis of a,vector spaces
matrix” or “rank of a space” or “dimension of a basis.” These phrases have no meaning.,vector spaces
"It is the dimension of the column space that equals the rank of the matrix, as we prove",vector spaces
in the coming section.,vector spaces
Problems 1–10 are about linear independence and linear dependence.,vector spaces
"1. Show that v1, v2, v3 are independent but v1, v2, v3, v4 are dependent:",vector spaces
Solve c1v1 +···+c4v4 = 0 or Ac = 0. The v’s go in the columns of A.,vector spaces
2. Find the largest possible number of independent vectors among,vector spaces
This number is the,vector spaces
of the space spanned by the v’s.,vector spaces
"3. Prove that if a = 0, d = 0, or f = 0 (3 cases), the columns of U are dependent:",vector spaces
"4. If a, d, f in Problem 3 are all nonzero, show that the only solution to Ux = 0 is x = 0.",vector spaces
Then U has independent columns.,vector spaces
5. Decide the dependence or independence of,vector spaces
"(a) the vectors (1,3,2), (2,1,3), and (3.2,1).",vector spaces
"(b) the vectors (1,−3,2), (2,1,−3), and (−3,2,1).",vector spaces
"2.3 Linear Independence, Basis, and Dimension",vector spaces
6. Choose three independent columns of U. Then make two other choices. Do the same,vector spaces
for A. You have found bases for which spaces?,vector spaces
2 3 4 1,vector spaces
0 6 7 0,vector spaces
0 0 0 9,vector spaces
0 0 0 0,vector spaces
2 3 4 1,vector spaces
0 6 7 0,vector spaces
0 0 0 9,vector spaces
4 6 8 2,vector spaces
"7. If w1, w2, w3 are independent vectors, show that the differences v1 = w2 −w3, v2 =",vector spaces
"w1 − w3, and v3 = w1 − w2 are dependent. Find a combination of the v’s that gives",vector spaces
"8. If w1, w2, w3 are independent vectors, show that the sums v1 = w2+w3, v2 = w1+w3,",vector spaces
and v3 = w1 +w2 are independent. (Write c1v1 +c2v2 +c3v3 = 0 in terms of the w’s.,vector spaces
Find and solve equations for the c’s.),vector spaces
"9. Suppose v1, v2, v3, v4 are vectors in R3.",vector spaces
(a) These four vectors are dependent because,vector spaces
(b) The two vectors v1 and v2 will be dependent if,vector spaces
"(c) The vectors v1 and (0,0,0) are dependent because",vector spaces
10. Find two independent vectors on the plane x+2y−3z−t = 0 in R4. Then ﬁnd three,vector spaces
independent vectors. Why not four? This plane is the nullspace of what matrix?,vector spaces
Problems 11–18 are about the space spanned by a set of vectors. Take all linear,vector spaces
combinations of the vectors,vector spaces
11. Describe the subspace of R3 (is it a line or a plane or R3?) spanned by,vector spaces
"(a) the two vectors (1,1,−1) and (−1,−1,1).",vector spaces
"(b) the three vectors (0,1,1) and (1,1,0) and (0,0,0).",vector spaces
(c) the columns of a 3 by 5 echelon matrix with 2 pivots.,vector spaces
(d) all vectors with positive components.,vector spaces
12. The vector b is in the subspace spanned by the columns of A when there is a solution,vector spaces
. The vector c is in the row space of A when there is a solution to,vector spaces
"or false: If the zero vector is in the row space, the rows are dependent.",vector spaces
"13. Find the dimensions of (a) the column space of A, (b) the column space of U, (c) the",vector spaces
"row space of A, (d) the row space of U. Which two of the spaces are the same?",vector spaces
Chapter 2 Vector Spaces,vector spaces
"14. Choose x = (x1,x2,x3,x4) in R4. It has 24 rearrangements like (x2,x1,x3,x4) and",vector spaces
"(x4,x3,x1,x2). Those 24 vectors, including x itself, span a subspace S. Find speciﬁc",vector spaces
"vectors x so that the dimension of S is: (a) 0, (b) 1, (c) 3, (d) 4.",vector spaces
15. v+w and v−w are combinations of v and w. Write v and w as combinations of v+w,vector spaces
and v−w. The two pairs of vectors,vector spaces
the same space. When are they a basis for,vector spaces
"16. Decide whether or not the following vectors are linearly independent, by solving",vector spaces
c1v1 +c2v2 +c3v3 +c4v4 = 0:,vector spaces
"Decide also if they span R4, by trying to solve c1v1 +···+c4v4 = (0,0,0,1).",vector spaces
17. Suppose the vectors to be tested for independence are placed into the rows instead,vector spaces
"of the columns of A, How does the elimination process from A to U decide for or",vector spaces
"18. To decide whether b is in the sub space spanned by w1,...,wn, let the vectors w be",vector spaces
the columns of A and try to solve Ax = b. What is the result for,vector spaces
"(a) w1 = (1,1,0), w2 = (2,2,1), w3 = (0,0,2), b = (3,4,5)?",vector spaces
"(b) w1 = (1,2,0), w2 = (2,5,0), w3 = (0,0,2), w4 = (0,0,0), and any b?",vector spaces
Problems 19–37 are about the requirements for a basis.,vector spaces
"19. If v1,...,vn are linearly independent, the space they span has dimension",vector spaces
"for that space. If the vectors are the columns of an m by n matrix,",vector spaces
20. Find a basis for each of these subspaces of R4:,vector spaces
(a) All vectors whose components are equal.,vector spaces
(b) All vectors whose components add to zero.,vector spaces
"(c) All vectors that are perpendicular to (1,1,0,0) and (1,0,1,1).",vector spaces
(d) The column space (in R2) and nullspace (in R5) of U =,vector spaces
�1 0 1 0 1,vector spaces
0 1 0 1 0,vector spaces
21. Find three different bases for the column space of U above. Then ﬁnd two different,vector spaces
bases for the row space of U.,vector spaces
"22. Suppose v1,v2,...,v6 are six vectors in R4.",vector spaces
(a) Those vectors (do)(do not)(might not) span R4.,vector spaces
"2.3 Linear Independence, Basis, and Dimension",vector spaces
(b) Those vectors (are)(are not)(might be) linearly independent.,vector spaces
(c) Any four of those vectors (are)(are not)(might be) a basis for R4.,vector spaces
"(d) If those vectors are the columns of A, then Ax = b (has) (does not have) (might",vector spaces
not have) a solution.,vector spaces
"23. The columns of A are n vectors from Rm. If they are linearly independent, what is",vector spaces
"the rank of A? If they span Rm, what is the rank? If they are a basis for Rm, what",vector spaces
24. Find a basis for the plane x−2y+3z = 0 in R3. Then ﬁnd a basis for the intersection,vector spaces
of that plane with the xy-plane. Then ﬁnd a basis for all vectors perpendicular to the,vector spaces
25. Suppose the columns of a 5 by 5 matrix A are a basis for R5.,vector spaces
(a) The equation Ax = 0 has only the solution x = 0 because,vector spaces
(b) If b is in R5 then Ax = b is solvable because,vector spaces
Conclusion: A is invertible. Its rank is 5.,vector spaces
26. Suppose S is a ﬁve-dimensional subspace of R6. True or false?,vector spaces
(a) Every basis for S can be extended to a basis for R6 by adding one more vector.,vector spaces
(b) Every basis for R6 can be reduced to a basis for S by removing one vector.,vector spaces
27. U comes from A by subtracting row 1 from row 3:,vector spaces
Find bases for the two column spaces. Find bases for the two row spaces. Find bases,vector spaces
for the two nullspace.,vector spaces
28. True or false (give a good reason)?,vector spaces
"(a) If the columns of a matrix are dependent, so are the rows.",vector spaces
(b) The column space of a 2 by 2 matrix is the same as its row space.,vector spaces
(c) The column space of a 2 by 2 matrix has the same dimension as its row space.,vector spaces
(d) The columns of a matrix are a basis for the column space.,vector spaces
29. For which numbers c and d do these matrices have rank 2?,vector spaces
1 2 5 0 5,vector spaces
0 0 0 d,vector spaces
Chapter 2 Vector Spaces,vector spaces
"30. By locating the pivots, ﬁnd a basis for the column space of",vector spaces
0 5 4 3,vector spaces
0 0 2 1,vector spaces
0 0 0 0,vector spaces
0 0 0 0,vector spaces
"Express each column that is not in the basis as a combination of the basic columns,",vector spaces
"Find also a matrix A with this echelon form U, but a different column space.",vector spaces
"31. Find a counterexample to the following statement: If v1, v2, v3, v4 is a basis for the",vector spaces
"vector space R4, and if W is a subspace, then some subset of the v’s is a basis for W.",vector spaces
32. Find the dimensions of these vector spaces:,vector spaces
(a) The space of all vectors in R4 whose components add to zero.,vector spaces
(b) The nullspace of the 4 by 4 identity matrix.,vector spaces
(c) The space of all 4 by 4 matrices.,vector spaces
33. Suppose V is known to have dimension k. Prove that,vector spaces
(a) any k independent vectors in V form a basis;,vector spaces
(b) any k vectors that span V form a basis.,vector spaces
"In other words, if the number of vectors is known to be correct, either of the two",vector spaces
properties of a basis implies the other.,vector spaces
"34. Prove that if V and W are three-dimensional subspaces of R5, then V and W must",vector spaces
"have a nonzero vector in common. Hint: Start with bases for the two subspaces,",vector spaces
making six vectors in all.,vector spaces
35. True or false?,vector spaces
"(a) If the columns of A are linearly independent, then Ax = b has exactly one solution",vector spaces
"(b) A 5 by 7 matrix never has linearly independent columns,",vector spaces
"36. If A is a 64 by 17 matrix of rank 11, how many independent vectors satisfy Ax = 0?",vector spaces
How many independent vectors satisfy ATy = 0?,vector spaces
37. Find a basis for each of these subspaces of 3 by 3 matrices:,vector spaces
(a) All diagonal matrices.,vector spaces
(b) All symmetric matrices (AT = A).,vector spaces
(c) All skew-symmetric matrices (AT = −A).,vector spaces
Problems 38–42 are about spaces in which the “vectors” are functions.,vector spaces
2.4 The Four Fundamental Subspaces,vector spaces
38. (a) Find all functions that satisfy dy,vector spaces
(b) Choose a particular function that satisﬁes dy,vector spaces
(c) Find all functions that satisfy dy,vector spaces
39. The cosine space F3 contains all combinations y(x) = Acosx + Bcos2x +Ccos3x.,vector spaces
Find a basis for the subspace that has y(0) = 0.,vector spaces
40. Find a basis for the space of functions that satisfy,vector spaces
dx −2y = 0.,vector spaces
"41. Suppose y1(x), y2(x), y3(x) are three different functions of x. The vector space they",vector spaces
"span could have dimension 1, 2, or 3. Give an example of y1, y2, y3 to show each",vector spaces
42. Find a basis for the space of polynomials p(x) of degree ≤ 3. Find a basis for the,vector spaces
subspace with p(1) = 0.,vector spaces
43. Write the 3 by 3 identity matrix as a combination of the other ﬁve permutation matri-,vector spaces
ces! Then show that those ﬁve matrices are linearly independent. (Assume a combi,vector spaces
"nation gives zero, and check entries to prove each term is zero.) The ﬁve permuta-",vector spaces
tions are a basis for the subspace of 3 by 3 matrices with row and column sums all,vector spaces
44. Review: Which of the following are bases for R3?,vector spaces
"(a) (1,2,0) and (0,1,−1).",vector spaces
"(b) (1,1,−1), (2,3,4), (4,1,−1), (0,1,−1).",vector spaces
"(c) (1,2,2), (−1,2,1), (0,8,0).",vector spaces
"(d) (1,2,2), (−1,2,1), (0,8,6).",vector spaces
45. Review: Suppose A is 5 by 4 with rank 4. Show that Ax = b has no solution when the,vector spaces
5 by 5 matrix [A b] is invertible. Show that Ax = b is solvable when [A b] is singular.,vector spaces
The Four Fundamental Subspaces,vector spaces
The previous section dealt with deﬁnitions rather than constructions. We know what a,vector spaces
"basis is, but not how to ﬁnd one. Now, starting from an explicit description of a subspace,",vector spaces
we would like to compute an explicit basis.,vector spaces
"Subspaces can be described in two ways. First, we may be given a set of vectors that",vector spaces
"span the space. (Example: The columns span the column space.) Second, we may be",vector spaces
Chapter 2 Vector Spaces,vector spaces
told which conditions the vectors in the space must satisfy. (Example: The nullspace,vector spaces
consists of all vectors that satisfy Ax = 0.),vector spaces
The ﬁrst description may include useless vectors (dependent columns). The second,vector spaces
description may include repeated conditions (dependent rows). We can’t write a basis,vector spaces
"by inspection, and a systematic procedure is necessary.",vector spaces
The reader can guess what that procedure will be. When elimination on A produces,vector spaces
"an echelon matrix U or a reduced R, we will ﬁnd a basis for each of the subspaces",vector spaces
associated with A. Then we have to look at the extreme case of full rank:,vector spaces
"When the rank is as large as possible, r = n or r = m or r = m = n, the matrix has a",vector spaces
left-inverse B or a right-inverse C or a two-sided A−1.,vector spaces
"To organize the whole discussion, we take each of the four subspaces in turn. Two of",vector spaces
them are familiar and two are new.,vector spaces
1. The column space of A is denoted by C(A). Its dimension is the rank r.,vector spaces
2. The nullspace of A is denoted by N(A). Its dimension is n−r.,vector spaces
"3. The row space of A is the column space of AT. It is C(AT), and it is spanned by the",vector spaces
rows of A. Its dimension is also r.,vector spaces
4. The left nullspace of A is the nullspace of AT. It contains all vectors y such that,vector spaces
"ATy = 0, and it is written N(AT). Its dimension is",vector spaces
The point about the last two subspaces is that they come from AT. If A is an m by n,vector spaces
"matrix, you can see which “host” spaces contain the four subspaces by looking at the",vector spaces
The nullspace N(A) and row space C(AT) are subspaces of Rn.,vector spaces
The left nullspace N(AT) and column space C(A) are subspaces of Rm.,vector spaces
The rows have n components and the columns have m. For a simple matrix like,vector spaces
A = U = R =,vector spaces
the column space is the line through [1,vector spaces
0]. The row space is the line through [1 0 0]T. It,vector spaces
is in R3. The nullspace is a plane in R3 and the left nullspace is a line in R2:,vector spaces
2.4 The Four Fundamental Subspaces,vector spaces
"Note that all vectors are column vectors. Even the rows are transposed, and the row",vector spaces
"space of A is the column space of AT, Our problem will be to connect the four spaces for",vector spaces
U (after elimination) to the four spaces for A:,vector spaces
1 3 3 2,vector spaces
0 0 3 3,vector spaces
0 0 0 0,vector spaces
−1 −3 3 4,vector spaces
"For novelty, we take the four subspaces in a more interesting order.",vector spaces
"3. The row space of A For an echelon matrix like U, the row space is clear. It contains",vector spaces
"all combinations of the rows, as every row space does—but here the third row contributes",vector spaces
nothing. The ﬁrst two rows are a basis for the row space. A similar rule applies to every,vector spaces
"echelon matrix U or R, with r pivots and r nonzero rows: The nonzero rows are a basis,",vector spaces
and the row space has dimension r. That makes it easy to deal with the original matrix,vector spaces
"The row space of A has the same dimension r as the row space of U,",vector spaces
"and it has the same bases, because the row spaces of A and U (and R) are the",vector spaces
The reason is that each elementary operation leaves the row space unchanged. The rows,vector spaces
in U are combinations of the original rows in A. Therefore the row space of U contains,vector spaces
"nothing new. At the same time, because every step can be reversed, nothing is lost; the",vector spaces
"rows of A can be recovered from U. It is true that A and U have different rows, but the",vector spaces
combinations of the rows are identical: same space!,vector spaces
"Note that we did not start with the m rows of A, which span the row space, and discard",vector spaces
"m − r of them to end up with a basis. According to 2L, we could have done so. But it",vector spaces
"might be hard to decide which rows to keep and which to discard, so it was easier just to",vector spaces
take the nonzero rows of U.,vector spaces
2. The nullspace of A Elimination simpliﬁes a system of linear equations without,vector spaces
"changing the solutions. The system Ax = 0 is reduced to Ux = 0, and this process is",vector spaces
reversible. The nullspace of A is the same as the nullspace of U and R. Only r of,vector spaces
the equations Ax = 0 are independent. Choosing the n−r “special solutions” to Ax = 0,vector spaces
provides a deﬁnite basis for the nullspace:,vector spaces
The nullspace N(A) has dimension n − r. The “special solutions” are a,vector spaces
"basis—each free variable is given the value 1, while the other free variables",vector spaces
are 0. Then Ax = 0 or Ux = 0 or Rx = 0 gives the pivot variables by back-,vector spaces
This is exactly the way we have been solving Ux = 0. The basic example above has,vector spaces
pivots in columns 1 and 3. Therefore its free variables are the second and fourth v and y.,vector spaces
Chapter 2 Vector Spaces,vector spaces
The basis for the nullspace is,vector spaces
"Any combination c1x1 +c2x2 has c1 as its v component, and c2 as its y component. The",vector spaces
"only way to have c1x1+c2x2 = 0 is to have c1 = c2 = 0, so these vectors are independent.",vector spaces
They also span the nullspace; the complete solution is vx1 +yx2. Thus the n−r = 4−2,vector spaces
vectors are a basis.,vector spaces
"The nullspace is also called the kernel of A, and its dimension n−r is the nullity.",vector spaces
1. The column space of A The column space is sometimes called the range. This,vector spaces
"is consistent with the usual idea of the range, as the set of all possible values f(x); x is",vector spaces
in the domain and f(x) is in the range. In our case the function is f(x) = Ax. Its domain,vector spaces
"consists of all x in Rn; its range is all possible vectors Ax, which is the column space.",vector spaces
(In an earlier edition of this book we called it R(A).),vector spaces
Our problem is to ﬁnd bases for the column spaces of U and A. Those spaces are,vector spaces
different (just look at the matrices!) but their dimensions are the same.,vector spaces
The ﬁrst and third columns of U are a basis for its column space. They are the,vector spaces
"columns with pivots. Every other column is a combination of those two. Furthermore,",vector spaces
the same is true of the original A—even though its columns are different. The pivot,vector spaces
columns of A are a basis for its column space. The second column is three times,vector spaces
"the ﬁrst, just as in U. The fourth column equals (column 3) − (column 1). The same",vector spaces
nullspace is telling us those dependencies.,vector spaces
The reason is this: Ax = 0 exactly when Ux = 0. The two systems are equivalent and,vector spaces
have the same solutions. The fourth column of U was also (column 3) − (column 1).,vector spaces
Every linear dependence Ax = 0 among the columns of A is matched by a dependence,vector spaces
"Ux = 0 among the columns of U, with exactly the same coefﬁcients. If a set of columns",vector spaces
"of A is independent, then so are the corresponding columns of U, and vice versa.",vector spaces
"To ﬁnd a basis for the column space C(A), we use what is already done for U. The",vector spaces
r columns containing pivots are a basis for the column space of U. We will pick those,vector spaces
same r columns in A:,vector spaces
"The dimension of the column space C(A) equals the rank r, which also",vector spaces
equals the dimension of the row space: The number of independent columns,vector spaces
equals the number of independent rows. A basis for C(A) is formed by the r,vector spaces
"columns of A that correspond, in U, to the columns containing pivots.",vector spaces
The row space and the column space have the same dimension r! This is one of,vector spaces
the most important theorems in linear algebra. It is often abbreviated as “row rank =,vector spaces
"column rank.” It expresses a result that, for a random 10 by 12 matrix, is not at all",vector spaces
2.4 The Four Fundamental Subspaces,vector spaces
obvious. It also says something about square matrices: If the rows of a square matrix,vector spaces
"are linearly independent, then so are the columns (and vice versa). Again, that does not",vector spaces
"seem self-evident (at least, not to the author).",vector spaces
"To see once more that both the row and column spaces of U have dimension r, con-",vector spaces
sider a typical situation with rank r = 3. The echelon matrix U certainly has three,vector spaces
"We claim that U also has three independent columns, and no more, The columns have",vector spaces
"only three nonzero components. If we can show that the pivot columns—the ﬁrst, fourth,",vector spaces
"and sixth—are linearly independent, they must be a basis (for the column space of U,",vector spaces
not A!). Suppose a combination of these pivot columns produced zero:,vector spaces
"Working upward in the usual way, c3 must be zero because the pivot d3 ̸= 0, then c2 must",vector spaces
"be zero because d2 ̸= 0, and ﬁnally c1 = 0. This establishes independence and completes",vector spaces
"the proof. Since Ax = 0 if and only if Ux = 0, the ﬁrst, fourth, and sixth columns of A—",vector spaces
"whatever the original matrix A was, which we do not even know in this example—are a",vector spaces
The row space and column space both became clear after elimination on A. Now,vector spaces
"comes the fourth fundamental subspace, which has been keeping quietly out of sight.",vector spaces
"Since the ﬁrst three spaces were C(A), N(A), and C(AT), the fourth space must be",vector spaces
"N(AT), It is the nullspace of the transpose, or the left nullspace of A. ATy = 0 means",vector spaces
"yTA = 0, and the vector appears on the left-hand side of A.",vector spaces
"4. The left nullspace of A (= the nullspace of AT) If A is an m by n matrix, then",vector spaces
AT is n by m. Its nullspace is a subspace of Rm; the vector y has m components. Written,vector spaces
"as yTA = 0, those components multiply the rows of A to produce the zero row:",vector spaces
"The dimension of this nullspace N(AT) is easy to ﬁnd, For any matrix, the number",vector spaces
of pivot variables plus the number of free variables must match the total number of,vector spaces
"columns. For A, that was r +(n−r) = n. In other words, rank plus nullity equals n:",vector spaces
dimension of C(A)+dimension of N(A) = number of columns.,vector spaces
Chapter 2 Vector Spaces,vector spaces
"This law applies equally to AT, which has m columns. AT is just as good a matrix as A.",vector spaces
"But the dimension of its column space is also r, so",vector spaces
The left nullspace N(AT) has dimension m−r.,vector spaces
The m−r solutions to yTA = 0 are hiding somewhere in elimination. The rows of A,vector spaces
"combine to produce the m−r zero rows of U. Start from PA = LU, or L−1PA = U. The",vector spaces
last m−r rows of the invertible matrix L−1P must be a basis of y’s in the left nullspace—,vector spaces
because they multiply A to give the zero rows in U.,vector spaces
"In our 3 by 4 example, the zero row was row 3 − 2(row 2) + 5(row 1). Therefore",vector spaces
"the components of y are 5, −2, 1. This is the same combination as in b3 −2b2 +5b1 on",vector spaces
"the right-hand side, leading to 0 = 0 as the ﬁnal equation. That vector y is a basis for",vector spaces
"the left nullspace, which has dimension m − r = 3 − 2 = 1. It is the last row of L−1P,",vector spaces
and produces the zero row in U—and we can often see it without computing L−1. When,vector spaces
"desperate, it is always possible just to solve ATy = 0.",vector spaces
I realize that so far in this book we have given no reason to care about N(AT). It is,vector spaces
correct but not convincing if I write in italics that the left nullspace is also important. The,vector spaces
next section does better by ﬁnding a physical meaning for y from Kirchhoff’s Current,vector spaces
"Now we know the dimensions of the four spaces. We can summarize them in a table,",vector spaces
and it even seems fair to advertise them as the,vector spaces
"Fundamental Theorem of Linear Algebra, Part I",vector spaces
1. C(A) = column space of A; dimension r.,vector spaces
2. N(A) = nullspace of A; dimension n−r.,vector spaces
3. C(AT) = row space of A; dimension r.,vector spaces
4. N(AT) = left nullspace of A; dimension m−r.,vector spaces
Example 1. A =,vector spaces
"has m = n = 2, and rank r = 1.",vector spaces
1. The column space contains all multiples of,vector spaces
. The second column is in the same,vector spaces
direction and contributes nothing new.,vector spaces
2. The nullspace contains all multiples of,vector spaces
. This vector satisﬁes Ax = 0.,vector spaces
3. The row space contains all multiples of,vector spaces
". I write it as a column vector, since",vector spaces
strictly speaking it is in the column space of AT.,vector spaces
4. The left nullspace contains all multiples of y =,vector spaces
. The rows of A with coefﬁ-,vector spaces
"cients −3 and 1 add to zero, so ATy = 0.",vector spaces
2.4 The Four Fundamental Subspaces,vector spaces
"In this example all four subspaces are lines. That is an accident, coming from r = 1 and",vector spaces
n−r = 1 and m−r = 1. Figure 2.5 shows that two pairs of lines are perpendicular. That,vector spaces
Figure 2.5: The four fundamental subspaces (lines) for the singular matrix A.,vector spaces
"If you change the last entry of A from 6 to 7, all the dimensions are different. The",vector spaces
column space and row space have dimension r = 2. The nullspace and left nullspace,vector spaces
contain only the vectors x = 0 and y = 0. The matrix is invertible.,vector spaces
"We know that if A has a left-inverse (BA = I) and a right-inverse (AC = I), then the two",vector spaces
"inverses are equal: B = B(AC)(BA)C = C. Now, from the rank of a matrix, it is easy to",vector spaces
"decide which matrices actually have these inverses. Roughly speaking, an inverse exists",vector spaces
only when the rank is as large as possible.,vector spaces
The rank always satisﬁes r ≤ m and also r ≤ n. An m by n matrix cannot have more,vector spaces
than m independent rows or n independent columns. There is not space for more than m,vector spaces
"pivots, or more than n. We want to prove that when r = m there is a right-inverse, and",vector spaces
"Ax = b always has a solution. When r = n there is a left-inverse, and the solution (if it",vector spaces
"Only a square matrix can have both r = m and r = n, and therefore only a square",vector spaces
matrix can achieve both existence and uniqueness. Only a square matrix has a two-sided,vector spaces
EXISTENCE: Full row rank r = m. Ax = b has at least one solution x,vector spaces
for every b if and only if the columns span Rm. Then A has a right-inverse C,vector spaces
such that AC = Im (m by m). This is possible only if m ≤ n.,vector spaces
UNIQUENESS: Full column rank r = n. Ax = b has at most one solution x,vector spaces
for every b if and only if the columns are linearly independent. Then A has an,vector spaces
n by m left-inverse B such that BA = In. This is possible only if m ≥ n.,vector spaces
Chapter 2 Vector Spaces,vector spaces
"In the existence case, one possible solution is x = Cb, since then Ax = ACb = b. But",vector spaces
there will be other solutions if there are other right-inverses. The number of solutions,vector spaces
when the columns span Rm is 1 or ∞.,vector spaces
"In the uniqueness case, if there is a solution to Ax = b, it has to be x = BAx = Bb. But",vector spaces
there may be no solution. The number of solutions is 0 or 1.,vector spaces
"There are simple formulas for the best left and right inverses, if they exist:",vector spaces
Certainly BA = I and AC = I. What is not so certain is that ATA and AAT are actually,vector spaces
"invertible. We show in Chapter 3 that ATA does have an inverse if the rank is n, and AAT",vector spaces
has an inverse when the rank is m. Thus the formulas make sense exactly when the rank,vector spaces
"is as large as possible, and the one-sided inverses are found.",vector spaces
Example 2. Consider a simple 2 by 3 matrix of rank 2:,vector spaces
"Since r = m = 2, the theorem guarantees a right-inverse C:",vector spaces
There are many right-inverses because the last row of C is completely arbitrary. This is,vector spaces
a case of existence but not uniqueness. The matrix A has no left-inverse because the last,vector spaces
column of BA is certain to be zero. The speciﬁc right-inverse C = AT(AAT)−1 chooses,vector spaces
c31 and c32 to be zero:,vector spaces
This is the pseudoinverse—a way of choosing the best C in Section 6.3. The transpose,vector spaces
of A yields an example with inﬁnitely many left-inverses:,vector spaces
Now it is the last column of B that is completely arbitrary. The best left-inverse (also the,vector spaces
"pseudoinverse) has b13 = b23 = 0. This is a “uniqueness case,” when the rank is r = n.",vector spaces
"There are no free variables, since n−r = 0. If there is a solution it will be the only one.",vector spaces
You can see when this example has one solution or no solution:,vector spaces
is solvable exactly when,vector spaces
2.4 The Four Fundamental Subspaces,vector spaces
A rectangular matrix cannot have both existence and uniqueness. If m is different from,vector spaces
"n, we cannot have r = m and r = n.",vector spaces
"A square matrix is the opposite. If m = n, we cannot have one property without the",vector spaces
other. A square matrix has a left-inverse if and only if it has a right-inverse. There is,vector spaces
"only one inverse, namely B = C = A−1. Existence implies uniqueness and uniqueness",vector spaces
"implies existence, when the matrix is square. The condition for invertibility is full rank:",vector spaces
r = m = n. Each of these conditions is a necessary and sufﬁcient test:,vector spaces
"1. The columns span Rn, so Ax = b has at least one solution for every b.",vector spaces
"2. The columns are independent, so Ax = 0 has only the solution x = 0.",vector spaces
"This list can be made much longer, especially if we look ahead to later chapters. Every",vector spaces
"condition is equivalent to every other, and ensures that A is invertible.",vector spaces
3. The rows of A span Rn.,vector spaces
4. The rows are linearly independent.,vector spaces
"5. Elimination can be completed: PA = LDU, with all n pivots.",vector spaces
6. The determinant of A is not zero.,vector spaces
7. Zero is not an eigenvalue of A.,vector spaces
8. ATA is positive deﬁnite.,vector spaces
Here is a typical application to polynomials P(t) of degree n − 1. The only such,vector spaces
"polynomial that vanishes at t1,...,tn is P(t) ≡ 0. No other polynomial of degree n−1 can",vector spaces
"have n roots. This is uniqueness, and it implies existence: Given any values b1,...,bn,",vector spaces
there exists a polynomial of degree n − 1 interpolating these values: P(ti) = bi. The,vector spaces
point is that we are dealing with a square matrix; the number n of coefﬁcients in P(t) =,vector spaces
x1 +x2t +···+xntn−1 matches the number of equations:,vector spaces
That Vandermonde matrix is n by n and full rank. Ax = b always has a solution—a,vector spaces
polynomial can be passed through any bi at distinct points ti. Later we shall actually ﬁnd,vector spaces
the determinant of A; it is not zero.,vector spaces
Matrices of Rank 1,vector spaces
"Finally comes the easiest case, when the rank is as small as possible (except for the zero",vector spaces
"matrix with rank 0), One basic theme of mathematics is, given something complicated,",vector spaces
Chapter 2 Vector Spaces,vector spaces
"to show how it can be broken into simple pieces. For linear algebra, the simple pieces",vector spaces
are matrices of rank 1:,vector spaces
"Every row is a multiple of the ﬁrst row, so the row space is one-dimensional. In fact, we",vector spaces
can write the whole matrix as the product of a column vector and a row vector:,vector spaces
The product of a 4 by 1 matrix and a 1 by 3 matrix is a 4 by 3 matrix. This product has,vector spaces
"rank 1. At the same time, the columns are all multiples of the same column vector; the",vector spaces
column space shares the dimension r = 1 and reduces to a line.,vector spaces
Every matrix of rank 1 has the simple form A = uvT = column times row.,vector spaces
"The rows are all multiples of the same vector vT, and the columns are all multiples of u.",vector spaces
The row space and column space are lines—the easiest case.,vector spaces
"1. True or false: If m = n, then the row space of A equals the column space. If m < n,",vector spaces
then the nullspace has a larger dimension than,vector spaces
2. Find the dimension and construct a basis for the four subspaces associated with each,vector spaces
0 1 4 0,vector spaces
0 2 8 0,vector spaces
0 1 4 0,vector spaces
0 0 0 0,vector spaces
3. Find the dimension and a basis for the four fundamental subspaces for,vector spaces
1 2 0 1,vector spaces
0 1 1 0,vector spaces
1 2 0 1,vector spaces
1 2 0 1,vector spaces
0 1 1 0,vector spaces
0 0 0 0,vector spaces
2.4 The Four Fundamental Subspaces,vector spaces
4. Describe the four subspaces in three-dimensional space associated with,vector spaces
"5. If the product AB is the zero matrix, AB = 0, show that the column space of B is",vector spaces
contained in the nullspace of A. (Also the row space of A is in the left nullspace of,vector spaces
"B, since each row of A multiplies B to give a zero row.)",vector spaces
6. Suppose A is an m by n matrix of rank r. Under what conditions on those numbers,vector spaces
(a) A have a two-sided inverse: AA−1 = A−1A = I?,vector spaces
(b) Ax = b have inﬁnitely many solutions for every b?,vector spaces
"7. Why is there no matrix whose row space and nullspace both contain (1,1,1)?",vector spaces
8. Suppose the only solution to Ax = 0 (m equations in n unknowns) is x = 0. What is,vector spaces
the rank and why? The columns of A are linearly,vector spaces
9. Find a 1 by 3 matrix whose nullspace consists of all vectors in R3 such that x1 +,vector spaces
2x2 +4x3 = 0. Find a 3 by 3 matrix with that same nullspace.,vector spaces
"10. If Ax = b always has at least one solution, show that the only solution to ATy = 0 is",vector spaces
y = 0. Hint: What is the rank?,vector spaces
"11. If Ax = 0 has a nonzero solution, show that ATy = f fails to be solvable for some",vector spaces
right-hand sides f. Construct an example of A and f.,vector spaces
12. Find the rank of A and write the matrix as A = uvT:,vector spaces
1 0 0 3,vector spaces
0 0 0 0,vector spaces
2 0 0 6,vector spaces
"13. If a, b, c are given with a ̸= 0, choose d so that",vector spaces
has rank 1. What are the pivots?,vector spaces
14. Find a left-inverse and/or a right-inverse (when they exist) for,vector spaces
Chapter 2 Vector Spaces,vector spaces
"15. If the columns of A are linearly independent (A is m by n), then the rank is",vector spaces
", the row space is",vector spaces
", and there exists a",vector spaces
16. (A paradox) Suppose A has a right-inverse B. Then AB = I leads to ATAB = AT or,vector spaces
B(ATA)−1AT. But that satisﬁes BA = I; it is a left-inverse. Which step is not justiﬁed?,vector spaces
"17. Find a matrix A that has V as its row space, and a matrix B that has V as its nullspace,",vector spaces
if V is the subspace spanned by,vector spaces
18. Find a basis for each of the four subspaces of,vector spaces
0 1 2 3 4,vector spaces
0 1 2 4 6,vector spaces
0 0 0 1 2,vector spaces
0 1 2 3 4,vector spaces
0 0 0 1 2,vector spaces
0 0 0 0 0,vector spaces
"19. If A has the same four fundamental subspaces as B, does A = cB?",vector spaces
"20. (a) If a 7 by 9 matrix has rank 5, what are the dimensions of the four subspaces?",vector spaces
What is the sum of all four dimensions?,vector spaces
"(b) If a 3 by 4 matrix has rank 3, what are its column space and left nullspace?",vector spaces
"21. Construct a matrix with the required property, or explain why you can’t.",vector spaces
(a) Column space contains,vector spaces
", row space contains",vector spaces
(b) Column space has basis,vector spaces
", nullspace has basis",vector spaces
(c) Dimension of nullspace = 1+ dimension of left nullspace.,vector spaces
(d) Left nullspace contains,vector spaces
", row space contains",vector spaces
"(e) Row space = column space, nullspace ̸= left nullspace.",vector spaces
"22. Without elimination, ﬁnd dimensions and bases for the four subspaces for",vector spaces
0 3 3 3,vector spaces
0 0 0 0,vector spaces
0 1 0 1,vector spaces
"23. Suppose the 3 by 3 matrix A is invertible. Write bases for the four subspaces for A,",vector spaces
and also for the 3 by 6 matrix B = [A A].,vector spaces
"24. What are the dimensions of the four subspaces for A, B, and C, if I is the 3 by 3",vector spaces
identity matrix and 0 is the 3 by 2 zero matrix?,vector spaces
2.4 The Four Fundamental Subspaces,vector spaces
25. Which subspaces are the same for these matrices of different sizes?,vector spaces
Prove that all three matrices have the same rank r.,vector spaces
"26. If the entries of a 3 by 3 matrix are chosen randomly between 0 and 1, what are the",vector spaces
most likely dimensions of the four subspaces? What if the matrix is 3 by 5?,vector spaces
27. (Important) A is an m by n matrix of rank r. Suppose there are right-hand sides b for,vector spaces
which Ax = b has no solution.,vector spaces
"(a) What inequalities (< or ≤) must be true between m, n, and r?",vector spaces
(b) How do you know that ATy = 0 has a nonzero solution?,vector spaces
"28. Construct a matrix with (1,0,1) and (1,2,0) as a basis for its row space and its",vector spaces
column space. Why can’t this be a basis for the row space and nullspace?,vector spaces
"29. Without computing A, ﬁnd bases for the four fundamental subspaces:",vector spaces
1 2 3 4,vector spaces
0 1 2 3,vector spaces
0 0 1 2,vector spaces
"30. If you exchange the ﬁrst two rows of a matrix A, which of the four subspaces stay",vector spaces
"the same? If y = (1,2,3,4) is in the left nullspace of A, write down a vector in the",vector spaces
left nullspace of the new matrix.,vector spaces
"31. Explain why v = (1,0,−1) cannot be a row of A and also be in the nullspace.",vector spaces
32. Describe the four subspaces of R3 associated with,vector spaces
33. (Left nullspace) Add the extra column b and reduce A to echelon form:,vector spaces
1 2 3 b1,vector spaces
4 5 6 b2,vector spaces
7 8 9 b3,vector spaces
A combination of the rows of A has produced the zero row. What combination is it?,vector spaces
(Look at b3 −2b2 +b1 on the right-hand side.) Which vectors are in the nullspace of,vector spaces
AT and which are in the nullspace of A?,vector spaces
Chapter 2 Vector Spaces,vector spaces
"34. Following the method of Problem 33, reduce A to echelon form and look at zero",vector spaces
rows. The b column tells which combinations you have taken of the rows:,vector spaces
"From the b column after elimination, read off m−r basis vectors in the left nullspace",vector spaces
of A (combinations of rows that give zero).,vector spaces
35. Suppose A is the sum of two matrices of rank one: A = uvT +wzT.,vector spaces
(a) Which vectors span the column space of A?,vector spaces
(b) Which vectors span the row space of A?,vector spaces
(c) The rank is less than 2 if,vector spaces
"(d) Compute A and its rank if u = z = (1,0,0) and v = w = (0,0,1).",vector spaces
"36. Without multiplying matrices, ﬁnd bases for the row and column spaces of A:",vector spaces
How do you know from these shapes that A is not invertible?,vector spaces
37. True or false (with a reason or a counterexample)?,vector spaces
(a) A and AT have the same number of pivots.,vector spaces
(b) A and AT have the same left nullspace.,vector spaces
(c) If the row space equals the column space then AT = A.,vector spaces
(d) If AT = −A then the row space of A equals the column space.,vector spaces
"38. If AB = 0, the columns of B are in the nullspace of A. If those vectors are in Rn,",vector spaces
prove that rank(A)+rank(B) ≤ n.,vector spaces
39. Can tic-tac-toe be completed (5 ones and 4 zeros in A) so that rank(A) = 2 but neither,vector spaces
side passed up a winning move?,vector spaces
40. Construct any 2 by 3 matrix of rank 1. Copy Figure 2.5 and put one vector in each,vector spaces
subspace (two in the nullspace). Which vectors are orthogonal?,vector spaces
41. Redraw Figure 2.5 for a 3 by 2 matrix of rank r = 2. Which subspace is Z (zero,vector spaces
vector only)? The nullspace part of any vector x in R2 is xn =,vector spaces
2.5 Graphs and Networks,vector spaces
I am not entirely happy with the 3 by 4 matrix in the previous section. From a theoretical,vector spaces
point of view it was very satisfactory; the four subspaces were computable and their,vector spaces
"dimensions r, n − r, r, m − r were nonzero. But the example was not produced by a",vector spaces
genuine application. It did not show how fundamental those subspaces really are.,vector spaces
This section introduces a class of rectangular matrices with two advantages. They,vector spaces
"are simple, and they are important. They are incidence matrices of graphs, and every",vector spaces
"entry is 1, −1, or 0. What is remarkable is that the same is true of L and U and basis",vector spaces
vectors for all four subspaces. Those subspaces play a central role in network theory.,vector spaces
We emphasize that the word “graph” does not refer to the graph of a function (like a,vector spaces
"parabola for y = x2). There is a second meaning, completely different, which is closer to",vector spaces
"computer science than to calculus—and it is easy to explain. This section is optional, but",vector spaces
it gives a chance to see rectangular matrices in action—and how the square symmetric,vector spaces
matrix ATA turns up in the end.,vector spaces
"A graph consists of a set of vertices or nodes, and a set of edges that connect them.",vector spaces
The graph in Figure 2.6 has 4 nodes and 5 edges. It does not have an edge between,vector spaces
"nodes 1 and 4 (and edges from a node to itself are forbidden). This graph is directed,",vector spaces
because of the arrow in each edge.,vector spaces
"The edgenode incidence matrix is 5 by 4, with a row for every edge. If the edge goes",vector spaces
"from node j to node k, then that row has −1 in column j and +1 in column k. The",vector spaces
incidence matrix A is shown next to the graph (and you could recover the graph if you,vector spaces
only had A). Row 1 shows the edge from node 1 to node 2. Row 5 comes from the ﬁfth,vector spaces
"edge, from node 3 to node 4.",vector spaces
"Figure 2.6: A directed graph (5 edges, 4 nodes, 2 loops) and its incidence matrix A.",vector spaces
Notice the columns of A. Column 3 gives information about node 3—it tells which,vector spaces
"edges enter and leave. Edges 2 and 3 go in, edge 5 goes out (with the minus sign). A is",vector spaces
"sometimes called the connectivity matrix, or the topology matrix. When the graph has m",vector spaces
"edges and n nodes, A is m by n (and normally m > n). Its transpose is the “node-edge”",vector spaces
Each of the four fundamental subspaces has a meaning in terms of the graph. We can,vector spaces
"do linear algebra, or write about voltages and currents. We do both!",vector spaces
Chapter 2 Vector Spaces,vector spaces
Nullspace of A: Is there a combination of the columns that gives Ax = 0? Normally,vector spaces
"the answer comes from elimination, but here it comes at a glance. The columns add up",vector spaces
"to the zero column. The nullspace contains x = (1,1,1,1), since Ax = 0. The equation",vector spaces
Ax = b does not have a unique solution (if it has a solution at all). Any “constant vector”,vector spaces
"x = (c,c,c,c) can be added to any particular solution of Ax = b. The complete solution",vector spaces
has this arbitrary constant c (like the +C when we integrate in calculus).,vector spaces
"This has a meaning if we think of x1, x2, x3, x4 as the potentials (the voltages) at the",vector spaces
nodes. The ﬁve components of Ax give the differences in potential across the ﬁve edges.,vector spaces
"The difference across edge 1 is x2 −x1, from the ±1 in the ﬁrst row.",vector spaces
"The equation Ax = b asks: Given the differences b1,...,b5, ﬁnd the actual potentials",vector spaces
"x1,...,x4. But that is impossible to do! We can raise or lower all the potentials by the",vector spaces
"same constant c, and the differences will not change—conﬁrming that x = (c,c,c,c) is in",vector spaces
"the nullspace of A. Those are the only vectors in the nullspace, since Ax = 0 means equal",vector spaces
potentials across every edge. The nullspace of this incidence matrix is one-dimensional.,vector spaces
The rank is 4− 1 = 3.,vector spaces
"For which differences b1,...,b5 can we solve Ax = b? To ﬁnd a",vector spaces
"direct test, look back at the matrix. Row 1 plus row 3 equals row 2. On the right-hand",vector spaces
"side we need b1 +b3 = b2, or no solution is possible. Similarly, row 3 plus row 5 is row",vector spaces
"4. The right-hand side must satisfy b3 + b5 = b4, for elimination to arrive at 0 = 0. To",vector spaces
"repeat, if b is in the column space, then",vector spaces
b1 −b2 +b3 = 0,vector spaces
b3 −b4 +b5 = 0.,vector spaces
"Continuing the search, we also ﬁnd that rows 1+4 equal rows 2+5. But this is nothing",vector spaces
new; subtracting the equations in (1) already produces b1 + b4 = b2 + b5. There are,vector spaces
"two conditions on the ﬁve components, because the column space has dimension 5−2.",vector spaces
"Those conditions would come from elimination, but here they have a meaning on the",vector spaces
Loops: Kirchhoff’s Voltage Law says that potential differences around a loop must,vector spaces
"add to zero, Around the upper loop in Figure 2.6, the differences satisfy (x2−x1)+(x3−",vector spaces
x2) = (x3 −x1). Those differences are b1 +b3 = b2. To circle the lower loop and arrive,vector spaces
"back at the same potential, we need b3 +b5 = b4.",vector spaces
The test for b to be in the column space is Kirchhoff’s Voltage Law:,vector spaces
The sum of potential differences around a loop must be zero.,vector spaces
"Left Nullspace: To solve ATy = 0, we ﬁnd its meaning on the graph. The vector y has",vector spaces
"ﬁve components, one for each edge. These numbers represent currents ﬂowing along",vector spaces
"the ﬁve edges. Since AT is 4 by 5, the equations ATy = 0 give four conditions on those",vector spaces
2.5 Graphs and Networks,vector spaces
ﬁve currents. They are conditions of “conservation” at each node: Flow in equals ﬂow,vector spaces
out at every node:,vector spaces
y1 −y3 −y4 = 0,vector spaces
y2 +y3 −y5 = 0,vector spaces
y4 +y5 = 0,vector spaces
Total current to node 1 is zero,vector spaces
The beauty of network theory is that both A and AT have important roles.,vector spaces
Solving ATy = 0 means ﬁnding a set of currents that do not “pile up” at any node. The,vector spaces
"trafﬁc keeps circulating, and the simplest solutions are currents around small loops.",vector spaces
"Our graph has two loops, and we send 1 amp of current around each loop:",vector spaces
1 −1 1 0 0,vector spaces
0 0 1 −1 1,vector spaces
Each loop produces a vector y in the left nullspace. The component +1 or −1 indicates,vector spaces
whether the current goes with or against the arrow. The combinations of y1 and y2 ﬁll,vector spaces
"the left nullspace, so y1 and y2 are a basis (the dimension had to be m−r = 5−3 = 2).",vector spaces
"In fact y1 −y2 = (1,−1,0,1,−1) gives the big loop around the outside of the graph.",vector spaces
The column space and left nullspace are closely related. The left nullspace contains,vector spaces
"y1 = (1,1,1,0,0), and the vectors in the column space satisfy b1 − b2 + b3 = 0. Then",vector spaces
yTb = 0: Vectors in the column space and left nullspace are perpendicular! That is soon,vector spaces
to become Part Two of the “Fundamental Theorem of Linear Algebra.”,vector spaces
"The row space of A contains vectors in R4, but not all vectors. Its",vector spaces
"dimension is the rank r = 3. Elimination will ﬁnd three independent rows, and we can",vector spaces
"also look to the graph. The ﬁrst three rows are dependent (row 1 + row 3 = row 2, and",vector spaces
"those edges form a loop). Rows 1,2,4 are independent because edges 1,2,4 contain no",vector spaces
"Rows 1, 2, 4 are a basis for the row space. In each row the entries add to zero. Every",vector spaces
"combination (f1, f2, f3, f4) in the row space will have that same property:",vector spaces
f in row space,vector spaces
f1 + f2 + f3 + f4 = 0,vector spaces
Again this illustrates the Fundamental Theorem: The row space is perpendicular to the,vector spaces
nullspace. If f is in the row space and x is in the nullspace then f Tx = 0.,vector spaces
"For AT, the basic law of network theory is Kirchhoff’s Current Law. The total ﬂow",vector spaces
"into every node is zero. The numbers f1, f2, f3, f4 are current sources into the nodes. The",vector spaces
"source f1 must balance −y1−y2, which is the ﬂow leaving node 1 (along edges 1 and 2).",vector spaces
That is the ﬁrst equation in ATy = f. Similarly at the other three nodes—conservation,vector spaces
of charge requires ﬂow in = ﬂow out. The beautiful thing is that AT is exactly the right,vector spaces
matrix for the Current Law.,vector spaces
The equations ATy = f at the nodes express Kirchhoff’s Current Law:,vector spaces
Chapter 2 Vector Spaces,vector spaces
The net current into every node is zero. Flow in = Flow out.,vector spaces
This law can only be satisﬁed if the total current from outside is f1 + f2 + f3 +,vector spaces
"f4 = 0. With f = 0, the law ATy = 0 is satisﬁed by a current that goes around",vector spaces
Spanning Trees and Independent Rows,vector spaces
Every component of y1 and y2 in the left nullspace is 1 or −1 or 0 (from loop ﬂows).,vector spaces
"The same is true of x = (1,1,1,1) in the nullspace, and all the entries in PA = LDU! The",vector spaces
key point is that every elimination step has a meaning for the graph.,vector spaces
You can see it in the ﬁrst step for our matrix A: subtract row 1 from row 2. This,vector spaces
replaces edge 2 by a new edge “1 minus 2”: That elimination step destroys an edge and,vector spaces
creates a new edge. Here the new edge “1 − 2” is just the old edge 3 in the opposite,vector spaces
direction. The next elimination step will produce zeros in row 3 of the matrix. This,vector spaces
"shows that rows 1, 2, 3 are dependent. Rows are dependent if the corresponding edges",vector spaces
At the end of elimination we have a full set of r independent rows. Those r edges,vector spaces
"form a tree—a graph with no loops. Our graph has r = 3, and edges 1, 2, 4 form one",vector spaces
possible tree. The full name is spanning tree because the tree “spans” all nodes of the,vector spaces
"graph. A spanning tree has n−1 edges if the graph is connected, and including one more",vector spaces
edge will produce a loop.,vector spaces
"In the language of linear algebra, n−1 is the rank of the incidence matrix A. The row",vector spaces
space has dimension n−1. The spanning tree from elimination gives a basis for that row,vector spaces
space—each edge in the tree corresponds to a row in the basis.,vector spaces
The fundamental theorem of linear algebra connects the dimensions of the subspaces:,vector spaces
"Nullspace: dimension 1, contains x = (1,...,1).",vector spaces
"Column space: dimension r = n−1, any n−1 columns are independent.",vector spaces
"Row space: dimension r = n−1, independent rows from any spanning tree.",vector spaces
"Left nullspace: dimension m−r = m−n+1, contains y’s from the loops.",vector spaces
"Those four lines give Euler’s formula, which in some way is the ﬁrst theorem in topol-",vector spaces
ogy. It counts zero-dimensional nodes minus one-dimensional edges plus two-dimensional,vector spaces
2.5 Graphs and Networks,vector spaces
loops. Now it has a linear algebra proof for any connected graph:,vector spaces
(# of nodes)−(# of edges)+(# of loops) = (n)−(m)+(m−n+1) = 1.,vector spaces
"For a single loop of 10 nodes and 10 edges, the Euler number is 10−10+1. If those 10",vector spaces
"nodes are each connected to an eleventh node in the center, then 11−20+10 is still 1.",vector spaces
Every vector f in the row space has xT f = f1+···+ fn = 0—the currents from outside,vector spaces
add to zero. Every vector b in the column space has yTb = 0—the potential differences,vector spaces
add to zero around all loops. In a moment we link x to y by a third law (Ohm’s law for,vector spaces
each resistor). First we stay with the matrix A for an application that seems frivolous,vector spaces
The Ranking of Football Teams,vector spaces
"At the end of the season, the polls rank college football teams. The ranking is mostly an",vector spaces
"average of opinions, and it sometimes becomes vague after the top dozen colleges. We",vector spaces
want to rank all teams on a more mathematical basis.,vector spaces
"The ﬁrst step is to recognize the graph. If team j played team k, there is an edge",vector spaces
"between them. The teams are the nodes, and the games are the edges. There are a few",vector spaces
hundred nodes and a few thousand edges—which will be given a direction by an arrow,vector spaces
"from the visiting team to the home team. Figure 2.7 shows part of the Ivy League,",vector spaces
"and some serious teams, and also a college that is not famous for big time football.",vector spaces
Fortunately for that college (from which I am writing these words) the graph is not,vector spaces
"connected. Mathematically speaking, we cannot prove that MIT is not number 1 (unless",vector spaces
it happens to play a game against somebody).,vector spaces
Figure 2.7: Part of the graph for college football.,vector spaces
"If football were perfectly consistent, we could assign a “potential” x j to every team.",vector spaces
"Then if visiting team v played home team h, the one with higher potential would win. In",vector spaces
"the ideal case, the difference b in the score would exactly equal the difference xh −xv in",vector spaces
their potentials. They wouldn’t even have to play the game! There would be complete,vector spaces
agreement that the team with highest potential is the best.,vector spaces
This method has two difﬁculties (at least). We are trying to ﬁnd a number x for every,vector spaces
"team, and we want xh −xv = bi, for every game. That means a few thousand equations",vector spaces
Chapter 2 Vector Spaces,vector spaces
and only a few hundred unknowns. The equations xh − xv = bi go into a linear system,vector spaces
"Ax = b, in which A is an incidence matrix. Every game has a row, with +1 in column h",vector spaces
and −1 in column v—to indicate which teams are in that game.,vector spaces
First difﬁculty: If b is not in the column space there is no solution. The scores must,vector spaces
ﬁt perfectly or exact potentials cannot be found. Second difﬁculty: If A has nonzero,vector spaces
"vectors in its nullspace, the potentials x are not well determined. In the ﬁrst case x does",vector spaces
not exist; in the second case x is not unique. Probably both difﬁculties are present.,vector spaces
"The nullspace always contains the vector of 1s, since A looks only at the differences",vector spaces
xh −xv. To determine the potentials we can arbitrarily assign zero potential to Harvard.,vector spaces
"(I am speaking mathematically, not meanly.) But if the graph is not connected, every",vector spaces
separate piece of the graph contributes a vector to the nullspace. There is even the vector,vector spaces
with xMIT = 1 and all other x j = 0. We have to ground not only Harvard but one team,vector spaces
in each piece. (There is nothing unfair in assigning zero potential; if all other potentials,vector spaces
are below zero then the grounded team ranks ﬁrst.) The dimension of the nullspace is,vector spaces
the number of pieces of the graph—and there will be no way to rank one piece against,vector spaces
"another, since they play no games.",vector spaces
The column space looks harder to describe. Which scores ﬁt perfectly with a set of,vector spaces
"potentials? Certainly Ax = b is unsolvable if Harvard beats Yale, Yale beats Princeton,",vector spaces
"and Princeton beats Harvard. More than that, the score differences in that loop of games",vector spaces
have to add to zero:,vector spaces
Kirchhoff’s law for score differences,vector spaces
bHY +bYP +bPH = 0.,vector spaces
This is also a law of linear algebra. Ax = b can be solved when b satisﬁes the same linear,vector spaces
dependencies as the rows of A. Then elimination leads to 0 = 0.,vector spaces
"In reality, b is almost certainly not in the column space. Football scores are not that",vector spaces
consistent. To obtain a ranking we can use least squares: Make Ax as close as possible,vector spaces
"to b. That is in Chapter 3, and we mention only one adjustment. The winner gets a bonus",vector spaces
of 50 or even 100 points on top of the score difference. Otherwise winning by 1 is too,vector spaces
"close to losing by 1. This brings the computed rankings very close to the polls, and Dr.",vector spaces
Leake (Notre Dame) gave a full analysis in Management Science in Sports (1976).,vector spaces
"After writing that subsection, I found the following in the New York Times:",vector spaces
"In its ﬁnal rankings for 1985, the computer placed Miami (10-2) in the sev-",vector spaces
"enth spot above Tennessee (9-1-2). A few days after publication, packages",vector spaces
containing oranges and angry letters from disgruntled Tennessee fans began,vector spaces
arriving at the Times sports department. The irritation stems from the fact that,vector spaces
Tennessee thumped Miami 35-7 in the Sugar Bowl. Final AP and UPI polls,vector spaces
"ranked Tennessee fourth, with Miami signiﬁcantly lower.",vector spaces
Yesterday morning nine cartons of oranges arrived at the loading dock. They,vector spaces
were sent to Bellevue Hospital with a warning that the quality and contents of,vector spaces
the oranges were uncertain.,vector spaces
2.5 Graphs and Networks,vector spaces
So much for that application of linear algebra.,vector spaces
Networks and Discrete Applied Mathematics,vector spaces
"A graph becomes a network when numbers c1,...,cm are assigned to the edges. The",vector spaces
"number ci can be the length of edge i, or its capacity, or its stiffness (if it contains a",vector spaces
"spring), or its conductance (if it contains a resistor). Those numbers go into a diagonal",vector spaces
"matrix C, which is m by m. C reﬂects “material properties,” in contrast to the incidence",vector spaces
matrix A—which gives information about the connections.,vector spaces
"Our description will be in electrical terms. On edge i, the conductance is ci and the",vector spaces
resistance is 1/ci. Ohm’s Law says that the current yi through the resistor is proportional,vector spaces
to the voltage drop ei:,vector spaces
(current) = (conductance)(voltage drop).,vector spaces
"This is also written E = IR, voltage drop equals current times resistance. As a vector",vector spaces
"equation on all edges at once, Ohm’s Law is y = Ce.",vector spaces
We need Kirchhoff’s Voltage Law and Current Law to complete the framework:,vector spaces
KVL: The voltage drops around each loop add to zero.,vector spaces
KCL: The currents yi (and fi) into each node add to zero.,vector spaces
"The voltage law allows us to assign potentials x1,...,xn to the nodes. Then the dif-",vector spaces
"ferences around a loop give a sum like (x2 − x1) + (x3 − x2) + (x1 − x3) = 0, in which",vector spaces
everything cancels. The current law asks us to add the currents into each node by the,vector spaces
"multiplication ATy. If there are no external sources of current, Kirchhoff’s Current Law",vector spaces
is ATy = 0.,vector spaces
"The other equation is Ohm’s Law, but we need to ﬁnd the voltage drop e across",vector spaces
the resistor. The multiplication Ax gave the potential difference between the nodes.,vector spaces
"Reversing the signs, −Ax gives the drop in potential. Part of that drop may be due to a",vector spaces
battery in the edge of strength bi. The rest of the drop is e = b−Ax across the resistor:,vector spaces
The fundamental equations of equilibrium combine Ohm and Kirchhoff into a cen-,vector spaces
tral problem of applied mathematics. These equations appear everywhere:,vector spaces
C−1y + Ax =,vector spaces
"That is a linear symmetric system, from which e has disappeared. The unknowns are the",vector spaces
currents y and the potentials x. You see the symmetric block matrix:,vector spaces
Chapter 2 Vector Spaces,vector spaces
"For block elimination the pivot is C−1, the multiplier is ATC, and subtraction knocks out",vector spaces
AT below the pivot. The result is,vector spaces
"The equation for x alone is in the bottom row, with the symmetric matrix ATCA:",vector spaces
Then back-substitution in the ﬁrst equation produces y. Nothing mysterious—substitute,vector spaces
y = C(b−Ax) into ATy = f to reach (7).,vector spaces
Important Remark One potential must be ﬁxed in advance: xn = 0. The nth node,vector spaces
"is grounded, and the nth column of the original incidence matrix is removed. The re-",vector spaces
sulting matrix is what we now mean by A: its n−1 columns are independent. The square,vector spaces
"matrix ATCA, which is the key to solving equation (7) for x, is an invertible matrix of",vector spaces
Example 1. Suppose a battery b3 and a current source f2 (and ﬁve resistors) connect,vector spaces
four nodes. Node 4 is grounded and the potential x4 = 0 is ﬁxed. The ﬁrst thing is the,vector spaces
"current law ATy = f at nodes 1, 2, 3:",vector spaces
"No equation is written for node 4, where the current law is y4+y5+ f2 = 0. This follows",vector spaces
from adding the other three equations.,vector spaces
The other equation is C−1y + Ax = b. The potentials x are connected to the currents,vector spaces
y by Ohm’s Law. The diagonal matrix C contains the ﬁve conductances ci = 1/Ri. The,vector spaces
2.5 Graphs and Networks,vector spaces
right-hand side accounts for the battery of strength b3 in edge 3. The block form has,vector spaces
C−1y+Ax = b above ATy = f:,vector spaces
"The system is 8 by 8, with ﬁve currents and three potentials. Elimination of y’s reduces",vector spaces
to the 3 by 3 system ATCAx = ATCb − f. The matrix ATCA contains the reciprocals,vector spaces
ci = 1/Ri (because in elimination you divide by the pivots). We also show the fourth,vector spaces
"row and column, from the grounded node, outside the 3 by 3 matrix:",vector spaces
"The ﬁrst entry is 1 + 1 + 1, or c1 + c3 + c5 when C is included, because edges 1, 3, 5",vector spaces
"touch node 1. The next diagonal entry is 1+1 or c1 +c2, from the edges touching node",vector spaces
2. Off the diagonal the c’s appear with minus signs. The edges to the grounded node,vector spaces
"4 belong in the fourth row and column, which are deleted when column 4 is removed",vector spaces
from A (making ATCA invertible). The 4 by 4 matrix would have all rows and columns,vector spaces
"adding to zero, and (1,1,1,1) would be in its nullspace.",vector spaces
Notice that ATCA is symmetric. It has positive pivots and it comes from the basic,vector spaces
framework of applied mathematics illustrated in Figure 2.8.,vector spaces
e = b − Ax,vector spaces
"Figure 2.8: The framework for equilibrium: sources b and f, three steps to ATCA.",vector spaces
"In mechanics, x and y become displacements and stresses. In ﬂuids, the unknowns",vector spaces
"are pressure and ﬂow rate. In statistics, e is the error and x is the best least-squares ﬁt to",vector spaces
Chapter 2 Vector Spaces,vector spaces
the data. These matrix equations and the corresponding differential equations are in our,vector spaces
"textbook Introduction to Applied Mathematics, and the new Applied Mathematics and",vector spaces
Scientiﬁc Computing. (See www.wellesleycambridge.com.),vector spaces
We end this chapter at that high point—the formulation of a fundamental problem in,vector spaces
applied mathematics. Often that requires more insight than the solution of the problem.,vector spaces
"We solved linear equations in Chapter 1, as the ﬁrst step in linear algebra. To set up the",vector spaces
"equations has required the deeper insight of Chapter 2. The contribution of mathematics,",vector spaces
"and of people, is not computation but intelligence.",vector spaces
"1. For the 3-node triangular graph in the ﬁgure following, write the 3 by 3 incidence",vector spaces
matrix A. Find a solution to Ax = 0 and describe all other vectors in the nullspace of,vector spaces
A. Find a solution to ATy = 0 and describe all other vectors in the left nullspace of,vector spaces
"2. For the same 3 by 3 matrix, show directly from the columns that every vector b in",vector spaces
the column space will satisfy b1 +b2 −b3 = 0. Derive the same thing from the three,vector spaces
rows—the equations in the system Ax = b. What does that mean about potential,vector spaces
differences around a loop?,vector spaces
3. Show directly from the rows that every vector f in the row space will satisfy f1 +,vector spaces
f2 + f3 = 0. Derive the same thing from the three equations ATy = f. What does that,vector spaces
mean when the f’s are currents into the nodes?,vector spaces
"4. Compute the 3 by 3 matrix ATA, and show that it is symmetric but singular—what",vector spaces
vectors are in its nullspace? Removing the last column of A (and last row of AT),vector spaces
leaves the 2 by 2 matrix in the upper left corner; show that it is not singular.,vector spaces
"5. Put the diagonal matrix C with entries c1, c2, c3 in the middle and compute ATCA.",vector spaces
Show again that the 2 by 2 matrix in the upper left corner is invertible.,vector spaces
6. Write the 6 by 4 incidence matrix A for the second graph in the ﬁgure. The vector,vector spaces
"(1,1,1,1) is in the nullspace of A, but now there will be m−n+1 = 3 independent",vector spaces
vectors that satisfy ATy = 0. Find three vectors y and connect them to the loops in,vector spaces
2.5 Graphs and Networks,vector spaces
"7. If that second graph represents six games between four teams, and the score dif-",vector spaces
"ferences are b1,...,b6, when is it possible to assign potentials x1,...,x4 so that the",vector spaces
potential differences agree with the b’s? You are ﬁnding (from Kirchhoff or from,vector spaces
elimination) the conditions that make Ax = b solvable.,vector spaces
8. Write down the dimensions of the four fundamental subspaces for this 6 by 4 inci-,vector spaces
"dence matrix, and a basis for each subspace.",vector spaces
"9. Compute ATA and ATCA, where the 6 by 6 diagonal matrix C has entries c1,...,c6.",vector spaces
How can you tell from the graph where the c’s will appear on the main diagonal of,vector spaces
10. Draw a graph with numbered and directed edges (and numbered nodes) whose inci-,vector spaces
Is this graph a tree? (Are the rows of A independent?) Show that removing the last,vector spaces
edge produces a spanning tree. Then the remaining rows are a basis for,vector spaces
"11. With the last column removed from the preceding A, and with the numbers 1. 2, 2, 1",vector spaces
"on the diagonal of C, write out the 7 by 7 system",vector spaces
C−1y + Ax =,vector spaces
"Eliminating y1, y2, y3, y4 leaves three equations ATCAx = −f for x1, x2, x3. Solve",vector spaces
"the equations when f = (1,1,6). With those currents entering nodes 1, 2, 3 of the",vector spaces
"network, what are the potentials at the nodes and currents on the edges?",vector spaces
"12. If A is a 12 by 7 incidence matrix from a connected graph, what is its rank? How",vector spaces
many free variables are there in the solution to Ax = b? How many free variables,vector spaces
are there in the solution to ATy = f? How many edges must be removed to leave a,vector spaces
"13. In the graph above with 4 nodes and 6 edges, ﬁnd all 16 spanning trees.",vector spaces
"14. If MIT beats Harvard 35-0, Yale ties Harvard, and Princeton beats Yale 7-6, what",vector spaces
"score differences in the other 3 games (H-P MIT-P, MIT-Y) will allow potential dif-",vector spaces
ferences that agree with the score differences? If the score differences are known for,vector spaces
"the games in a spanning tree, they are known for all games.",vector spaces
"15. In our method for football rankings, should the strength of the opposition be consid-",vector spaces
ered — or is that already built in?,vector spaces
Chapter 2 Vector Spaces,vector spaces
"16. If there is an edge between every pair of nodes (a complete graph), how many edges",vector spaces
"are there? The graph has n nodes, and edges from a node to itself are not allowed.",vector spaces
"17. For both graphs drawn below, verify Euler’s formula:",vector spaces
(# of nodes) − (# of edges) + (# of loops) = 1.,vector spaces
"18. Multiply matrices to ﬁnd ATA, and guess how its entries come from the graph:",vector spaces
(a) The diagonal of ATA tells how many,vector spaces
(b) The off-diagonals −1 or 0 tell which pairs of nodes are,vector spaces
"19. Why does the nullspace of ATA contain (1,1,1,1)? What is its rank?",vector spaces
20. Why does a complete graph with n = 6 nodes have m = 15 edges? A spanning tree,vector spaces
connecting all six nodes has,vector spaces
edges. There are nn−2 = 64 spanning trees!,vector spaces
21. The adjacency matrix of a graph has Mij = 1 if nodes i and j are connected by an,vector spaces
"edge (otherwise Mi j = 0). For the graph in Problem 6 with 6 nodes and 4 edges,",vector spaces
write down M and also M2. Why does (M2)ij count the number of 2-step paths from,vector spaces
node i to node j?,vector spaces
We know how a matrix moves subspaces around when we multiply by A. The nullspace,vector spaces
"goes into the zero vector. All vectors go into the column space, since Ax is always a",vector spaces
combination of the columns. You will soon see something beautiful—that A takes its,vector spaces
"row space into its column space, and on those spaces of dimension r it is 100 percent in-",vector spaces
"vertible. That is the real action of A. It is partly hidden by nullspaces and left nullspaces,",vector spaces
which lie at right angles and go their own way (toward zero).,vector spaces
What matters now is what happens inside the space—which means inside n-dimensional,vector spaces
"space, if A is n by n. That demands a closer look.",vector spaces
"Suppose x is an n-dimensional vector. When A multiplies x, it transforms that vector",vector spaces
into a new vector Ax. This happens at every point x of the n-dimensional space Rn.,vector spaces
"The whole space is transformed, or “mapped into itself,” by the matrix A. Figure 2.9",vector spaces
illustrates four transformations that come from matrices:,vector spaces
"1. A multiple of the identity matrix, A = cI, stretches every vector",vector spaces
by the same factor c. The whole space expands or contracts (or,vector spaces
"somehow goes through the origin and out the opposite side, when",vector spaces
2. A rotation matrix turns the whole space around the origin. This,vector spaces
"example turns all vectors through 90°, transforming every point",vector spaces
3. A reﬂection matrix transforms every vector into its image on,vector spaces
the opposite side of a mirror. In this example the mirror is the,vector spaces
"45° line y = x, and a point like (2,2) is unchanged. A point like",vector spaces
"(2,−2) is reversed to (−2,2). On a combination like v = (2,2)+",vector spaces
"(2,−2) = (4,0), the matrix leaves one part and reverses the other",vector spaces
"part. The output is Av = (2,2)+(−2,2) = (0,4)",vector spaces
That reﬂection matrix is also a permutation matrix! It is alge-,vector spaces
"braically so simple, sending (x,y) to (y,x), that the geometric pic-",vector spaces
A projection matrix takes the whole space onto a lower-,vector spaces
dimensional subspace (not invertible). The example transforms,vector spaces
"each vector (x,y) in the plane to the nearest point (x,0) on the hor-",vector spaces
izontal axis. That axis is the column space of A. The y-axis that,vector spaces
"projects to (0,0) is the nullspace.",vector spaces
Figure 2.9: Transformations of the plane by four matrices.,vector spaces
Those examples could be lifted into three dimensions. There are matrices to stretch,vector spaces
the earth or spin it or reﬂect it across the plane of the equator (forth pole transforming to,vector spaces
south pole). There is a matrix that projects everything onto that plane (both poles to the,vector spaces
"center). It is also important to recognize that matrices cannot do everything, and some",vector spaces
transformations T(x) are not possible with Ax:,vector spaces
"(i) It is impossible to move the origin, since A0 = 0 for every matrix.",vector spaces
"(ii) If the vector x goes to x′, then 2x must go to 2x′. in general cx must go to cx′, since",vector spaces
"(iii) If the vectors x and y go to x′ and y′, then their sum x+y must go to x′ +y′—since",vector spaces
Matrix multiplication imposes those rules on the transformation. The second rule con-,vector spaces
"tains the ﬁrst (take c = 0 to get A0 = 0). We saw rule (iii) in action when (4,0) was",vector spaces
Chapter 2 Vector Spaces,vector spaces
"reﬂected across the 45° line. It was split into (2,2) + (2,−2) and the two parts were",vector spaces
"reﬂected separately. The same could be done for projections: split, project separately,",vector spaces
and add the projections. These rules apply to any transformation that comes from a,vector spaces
Their importance has earned them a name: Transformations that obey rules (i)–(iii),vector spaces
are called linear transformations. The rules can be combined into one requirement:,vector spaces
"For all numbers c and d and all vectors x and y, matrix multiplication",vector spaces
satisﬁes the rule of linearity:,vector spaces
Every transformation T(x) that meets this requirement is a linear transforma-,vector spaces
Any matrix leads immediately to a linear transformation. The more interesting question,vector spaces
is in the opposite direction: Does every linear transformation lead to a matrix? The,vector spaces
"object of this section is to ﬁnd the answer, yes. This is the foundation of an approach",vector spaces
to linear algebra—starting with property (1) and developing its consequences—that is,vector spaces
much more abstract than the main approach in this book. We preferred to begin directly,vector spaces
"with matrices, and now we see how they represent linear transformations.",vector spaces
A transformation need not go from Rn to the same space Rn. It is absolutely permitted,vector spaces
to transform vectors in Rn to vectors in a different space Rm. That is exactly what is done,vector spaces
"by an m by n matrix! The original vector x has n components, and the transformed vector",vector spaces
"Ax has m components. The rule of linearity is equally satisﬁed by rectangular matrices,",vector spaces
so they also produce linear transformations.,vector spaces
"Having gone that far, there is no reason to stop. The operations in the linearity con-",vector spaces
"dition (1) are addition and scalar multiplication, but x and y need not be column vectors",vector spaces
"in Rn. Those are not the only spaces. By deﬁnition, any vector space allows the com-",vector spaces
"binations cx + dy—the “vectors” are x and y, but they may actually be polynomials or",vector spaces
"matrices or functions x(t) and y(t). As long as the transformation satisﬁes equation (1),",vector spaces
"We take as examples the spaces Pn, in which the vectors are polynomials p(t) of",vector spaces
"degree n. They look like p = a0 +a1t +···+antn, and the dimension of the vector space",vector spaces
"is n+1 (because with the constant term, there are n+1 coefﬁcients).",vector spaces
"Example 1. The operation of differentiation, A = d/dt, is linear:",vector spaces
dt (a0 +a1t +···+antn) = a1 +···+nantn−1.,vector spaces
The nullspace of this A is the one-dimensional space of constants: da0/dt = 0. The,vector spaces
column space is the n-dimensional space Pn−1; the right-hand side of equation (2) is,vector spaces
always in that space. The sum of nullity (= 1) and rank (= n) is the dimension of the,vector spaces
Example 2. Integration from 0 to t is also linear (it takes Pn to Pn+1):,vector spaces
0 (a0 +···+antn)dt = a0t +···+,vector spaces
"This time there is no nullspace (except for the zero vector, as always!) but integration",vector spaces
does not produce all polynomials in Pn+1. The right side of equation (3) has no constant,vector spaces
term. Probably the constant polynomials will be the left nullspace.,vector spaces
Example 3. Multiplication by a ﬁxed polynomial like 2+3t is linear:,vector spaces
Ap(t) = (2+3t)(a0 +···+antn) = 2a0 +···+3antn+1.,vector spaces
"Again this transforms Pn to Pn+1, with no nullspace except p = 0.",vector spaces
"In these examples (and in almost all examples), linearity is not difﬁcult to verify. It",vector spaces
"hardly even seems interesting. If it is there, it is practically impossible to miss. Nev-",vector spaces
"ertheless, it is the most important property a transformation can have1. Of course most",vector spaces
"transformations are not linear—for example, to square the polynomial (Ap = p2), or to",vector spaces
"add 1 (Ap = p+1), or to keep the positive coefﬁcients (A(t −t2) = t). It will be linear",vector spaces
"transformations, and only those, that lead us back to matrices.",vector spaces
Transformations Represented by Matrices,vector spaces
"Linearity has a crucial consequence: If we know Ax for each vector in a basis, then we",vector spaces
know Ax for each vector in the entire space. Suppose the basis consists of the n vectors,vector spaces
"x1,...,xn. Every other vector x is a combination of those particular vectors (they span",vector spaces
the space). Then linearity determines Ax:,vector spaces
x = c1x1 +···+cnxn,vector spaces
"The transformation T(x) = Ax has no freedom left, after it has decided what to do with",vector spaces
the basis vectors. The rest is determined by linearity. The requirement (1) for two vectors,vector spaces
"x and y leads to condition (4) for n vectors x1,...,xn. The transformation does have a",vector spaces
"free hand with the vectors in the basis (they are independent). When those are settled,",vector spaces
the transformation of every vector is settled.,vector spaces
Example 4. What linear transformation takes x1 and x2 to Ax1 and Ax2?,vector spaces
It must be multiplication T(x) = Ax by the matrix,vector spaces
1Invertibility is perhaps in second place as an important property.,vector spaces
Chapter 2 Vector Spaces,vector spaces
"Starting with a different basis (1,1) and (2,−1), this same A is also the only linear",vector spaces
Next we ﬁnd matrices that represent differentiation and integration. First we must,vector spaces
decide on a basis. For the polynomials of degree 3 there is a natural choice for the four,vector spaces
"That basis is not unique (it never is), but some choice is necessary and this is the most",vector spaces
"convenient. The derivatives of those four basis vectors are 0, 1, 2t, 3t2:",vector spaces
"“d/dt” is acting exactly like a matrix, but which matrix? Suppose we were in the usual",vector spaces
"four-dimensional space with the usual basis—the coordinate vectors p1 = (1,0,0,0),",vector spaces
"p2 = (0,1,0,0), p3 = (0,0,1,0), p4 = (0,0,0,1). The matrix is decided by equation (5):",vector spaces
0 1 0 0,vector spaces
0 0 2 0,vector spaces
0 0 0 3,vector spaces
0 0 0 0,vector spaces
"Ap1 is its ﬁrst column, which is zero. Ap2 is the second column, which is p1. Ap3 is",vector spaces
2p2 and Ap4 is 3p3. The nullspace contains p1 (the derivative of a constant is zero).,vector spaces
"The column space contains p1, p2, p3 (the derivative of a cubic is a quadratic). The",vector spaces
"derivative of a combination like p = 2 +t −t2 −t3 is decided by linearity, and there is",vector spaces
nothing new about that—it is the way we all differentiate. It would be crazy to memorize,vector spaces
the derivative of every polynomial.,vector spaces
"The matrix can differentiate that p(t), because matrices build in linearity!",vector spaces
dt = Ap −→,vector spaces
0 1 0 0,vector spaces
0 0 2 0,vector spaces
0 0 0 3,vector spaces
0 0 0 0,vector spaces
���� −→ 1−2t −3t2.,vector spaces
"In short, the matrix carries all the essential information. If the basis is known, and the",vector spaces
"matrix is known, then the transformation of every vector is known.",vector spaces
"The coding of the information is simple. To transform a space to itself, one basis is",vector spaces
enough. A transformation from one space to another requires a basis for each.,vector spaces
"Suppose the vectors x1,...,xn are a basis for the space V, and vectors",vector spaces
"y1,...,ym are a basis for W. Each linear transformation T from V to W is",vector spaces
represented by a matrix A. The jth column is found by applying T to the jth,vector spaces
"basis vector x j, and writing T(x j) as a combination of the y’s:",vector spaces
Column j of A,vector spaces
T(x j) = Ax j = a1 jy1 +a2 jy2 +···+amjym.,vector spaces
"For the differentiation matrix, column 1 came from the ﬁrst basis vector p1 = 1. Its",vector spaces
"derivative is zero, so column 1 is zero. The last column came from (d/dt)t3 = 3t2. Since",vector spaces
"3t2 = 0p1+0p2+3p3+0p4, the last column contained 0, 0, 3. 0. The rule (6) constructs",vector spaces
"the matrix, a column at a time.",vector spaces
"We do the same for integration. That goes from cubics to quartics, transforming",vector spaces
"V = P3 into W = P4, so we need a basis for W. The natural choice is y1 = 1, y2 = t,",vector spaces
"y3 = t2, y4 = t3, y5 = t4, spanning the polynomials of degree 4. The matrix A will be m",vector spaces
"by n, or 5 by 4. It comes from applying integration to each basis vector of V:",vector spaces
0 1dt = t,vector spaces
0 t3dt = 1,vector spaces
Differentiation and integration are inverse operations. Or at least integration followed,vector spaces
"by differentiation brings back the original function. To make that happen for matrices,",vector spaces
"we need the differentiation matrix from quartics down to cubics, which is 4 by 5:",vector spaces
0 1 0 0 0,vector spaces
0 0 2 0 0,vector spaces
0 0 0 3 0,vector spaces
0 0 0 0 4,vector spaces
Differentiation is a left-inverse of integration. Rectangular matrices cannot have two-,vector spaces
"sided inverses! In the opposite order, AintAdiff = I cannot be true. The 5 by 5 product has",vector spaces
zeros in column 1. The derivative of a constant is zero. In the other columns AintAdiff is,vector spaces
the identity and the integral of the derivative of tn is tn.,vector spaces
Chapter 2 Vector Spaces,vector spaces
"Rotations Q, Projections P, and Reﬂections H",vector spaces
"This section began with 90° rotations, projections onto the x-axis, and reﬂections through",vector spaces
the 45° line. Their matrices were especially simple:,vector spaces
The underlying linear transformations of the x-y plane are also simple. But rotations,vector spaces
"through other angles, projections onto other lines, and reﬂections in other mirrors are",vector spaces
"almost as easy to visualize, They are still linear transformations, provided that the origin",vector spaces
is ﬁxed: A0 = 0. They must be represented by matrices. Using the natural basis,vector spaces
", we want to discover those matrices.",vector spaces
1. Rotation Figure 2.10 shows rotation through an angle θ. It also shows the effect on,vector spaces
"the two basis vectors. The ﬁrst one goes to (cosθ,sinθ), whose length is still 1; it",vector spaces
"lies on the “θ-line.” The second basis vector (0,1) rotates into (−sinθ,cosθ). By",vector spaces
"rule (6), those numbers go into the columns of the matrix (we use c and s for cosθ",vector spaces
and sinθ). This family of rotations Qθ is a perfect chance to test the correspondence,vector spaces
between transformations and matrices:,vector spaces
Does the inverse of Qθ equal Q−θ (rotation backward through θ)? Yes.,vector spaces
Does the square of Qθ equal Q2θ (rotation through a double angle)? Yes.,vector spaces
Does the product of Qθ and Qϕ equal Qθ+ϕ (rotation through θ then ϕ)?,vector spaces
cosθ cosϕ −sinθ sinϕ,vector spaces
sinθ cosϕ +cosθ sinϕ,vector spaces
"The last case contains the ﬁrst two. The inverse appears when ϕ is −θ, and the",vector spaces
square appears when ϕ is +θ. All three questions were decided by trigonometric,vector spaces
identities (and they give a new way to remember those identities). It was no accident,vector spaces
that all the answers were yes. Matrix multiplication is deﬁned exactly so that the,vector spaces
product of the matrices corresponds to the product of the transformations.,vector spaces
Suppose A and B are linear transformations from V to W and from U,vector spaces
"to V. Their product AB starts with a vector u in U, goes to Bu in V, and",vector spaces
s [ cs ],vector spaces
c [ cs ],vector spaces
Figure 2.10: Rotation through θ (left). Projection onto the θ-line (right).,vector spaces
ﬁnishes with ABu in W. This “composition” AB is again a linear transfor-,vector spaces
mation (from U to W). Its matrix is the product of the individual matrices,vector spaces
representing A and B.,vector spaces
"For AdiffAint, the composite transformation was the identity (and AintAdiff annihilated",vector spaces
"all constants). For rotations, the order of multiplication does not matter. Then U =",vector spaces
"V = W is the x-y plane, and QθQϕ is the same as QϕQθ. For a rotation and a",vector spaces
"reﬂection, the order makes a difference.",vector spaces
"Technical note: To construct the matrices, we need bases for V and W, and then for",vector spaces
"U and V. By keeping the same basis for V, the product matrix goes correctly from",vector spaces
the basis in U to the basis in W. If we distinguish the transformation A from its,vector spaces
"matrix (call that [A]), then the product rule 2V becomes extremely concise: [AB] =",vector spaces
[A][B]. The rule for multiplying matrices in Chapter 1 was totally determined by this,vector spaces
requirement—it must match the product of linear transformations.,vector spaces
"2. Projection Figure 2.10 also shows the projection of (1,0) onto the θ-line. The",vector spaces
"length of the projection is c = cosθ. Notice that the point of projection is not (c,s), as",vector spaces
"I mistakenly thought; that vector has length 1 (it is the rotation), so we must multiply",vector spaces
"by c. Similarly the projection of (0,1) has length s, and falls at s(c,s) = (cs,s2), That",vector spaces
gives the second column of the projection matrix P:,vector spaces
"This matrix has no inverse, because the transformation has no inverse. Points on the",vector spaces
perpendicular line are projected onto the origin; that line is the nullspace of P. Points,vector spaces
on the θ-line are projected to themselves! Projecting twice is the same as projecting,vector spaces
"once, and P2 = P:",vector spaces
c2(c2 +s2) cs(c2 +s2),vector spaces
cs(c2 +s2) s2(c2 +s2),vector spaces
Chapter 2 Vector Spaces,vector spaces
H = 2P − I =,vector spaces
Image + original = 2 × projection,vector spaces
Hx + x = 2Px,vector spaces
Figure 2.11: Reﬂection through the θ-line: the geometry and the matrix.,vector spaces
Of course c2 +s2 = cos2θ +sin2 θ = 1. A projection matrix equals its own square.,vector spaces
"3. Reﬂection Figure 2.11 shows the reﬂection of (1,0) in the θ-line. The length of the",vector spaces
"reﬂection equals the length of the original, as it did after rotation—but here the θ-",vector spaces
line stays where it is. The perpendicular line reverses direction; all points go straight,vector spaces
"through the mirror, Linearity decides the rest.",vector spaces
This matrix H has the remarkable property H2 = I. Two reﬂections bring back,vector spaces
"the original. A reﬂection is its own inverse, H = H−1, which is clear from the",vector spaces
geometry but less clear from the matrix. One approach is through the relationship of,vector spaces
reﬂections to projections: H = 2P − I. This means that Hx + x = 2Px—the image,vector spaces
plus the original equals twice the projection. It also conﬁrms that H2 = I:,vector spaces
"H2 = (2P−I)2 = 4P2 −4P+I = I,",vector spaces
Other transformations Ax can increase the length of x; stretching and shearing are in,vector spaces
the exercises. Each example has a matrix to represent it—which is the main point of this,vector spaces
"section. But there is also the question of choosing a basis, and we emphasize that the",vector spaces
matrix depends on the choice of basis. Suppose the ﬁrst basis vector is on the θ-line and,vector spaces
the second basis vector is perpendicular:,vector spaces
(i) The projection matrix is back to P =,vector spaces
. This matrix is constructed as always:,vector spaces
its ﬁrst column comes from the ﬁrst basis vector (projected to itself). The second,vector spaces
column comes from the basis vector that is projected to zero.,vector spaces
"(ii) For reﬂections, that same basis gives H =",vector spaces
. The second basis vector is re-,vector spaces
"ﬂected onto its negative, to produce this second column. The matrix H is still 2P−I",vector spaces
when the same basis is used for H and P.,vector spaces
"(iii) For rotations, the matrix is not changed. Those lines are still rotated through θ, and",vector spaces
Q = [c −s,vector spaces
s c ] as before.,vector spaces
"The whole question of choosing the best basis is absolutely central, and we come back",vector spaces
"to it in Chapter 5. The goal is to make the matrix diagonal, as achieved for P and H. To",vector spaces
"make Q diagonal requires complex vectors, since all real vectors are rotated.",vector spaces
"We mention here the effect on the matrix of a change of basis, while the linear trans-",vector spaces
formation stays the same. The matrix A (or Q or P or H) is altered to S−1AS. Thus a,vector spaces
"single transformation is represented by different matrices (via different bases, accounted",vector spaces
"for by S). The theory of eigenvectors will lead to this formula S−1AS, and to the best",vector spaces
1. What matrix has the effect of rotating every vector through 90° and then projecting,vector spaces
the result onto the x-axis? What matrix represents projection onto the x-axis followed,vector spaces
by projection onto the y-axis?,vector spaces
2. Does the product of 5 reﬂections and 8 rotations of the x-y plane produce a rotation,vector spaces
3. The matrix A =,vector spaces
produces a stretching in the x-direction. Draw the circle x2 +,vector spaces
"y2 = 1 and sketch around it the points (2x,y) that result from multiplication by A.",vector spaces
What shape is that curve?,vector spaces
4. Every straight line remains straight after a linear transformation. If z is halfway,vector spaces
"between x and y, show that Az is halfway between Ax and Ay.",vector spaces
5. The matrix A =,vector spaces
"yields a shearing transformation, which leaves the y-axis un-",vector spaces
"changed. Sketch its effect on the x-axis, by indicating what happens to (1,0) and",vector spaces
"(2,0) and (−1,0)—and how the whole axis is transformed.",vector spaces
6. What 3 by 3 matrices represent the transformations that,vector spaces
(a) project every vector onto the x-y plane?,vector spaces
(b) reﬂect every vector through the x-y plane?,vector spaces
"(c) rotate the x-y plane through 90°, leaving the z-axis alone?",vector spaces
"(d) rotate the x-y plane, then x-z, then y-z, through 90°?",vector spaces
"(e) carry out the same three rotations, but each one through 180°?",vector spaces
Chapter 2 Vector Spaces,vector spaces
"7. On the space P3 of cubic polynomials, what matrix represents d2/dt2? Construct",vector spaces
"the 4 by 4 matrix from the standard basis 1, t, t2, t3. Find its nullspace and column",vector spaces
space. What do they mean in terms of polynomials?,vector spaces
"8. From the cubics P3 to the fourth-degree polynomials P4, what matrix represents",vector spaces
multiplication by 2 + 3t? The columns of the 5 by 4 matrix A come from applying,vector spaces
"the transformation to 1, t, t2, t3.",vector spaces
9. The solutions to the linear differential equation d2u/dt2 = u form a vector space,vector spaces
"(since combinations of solutions are still solutions). Find two independent solutions,",vector spaces
to give a basis for that solution space.,vector spaces
"10. With initial values u = x and du/dt = y at t = 0, what combination of basis vectors",vector spaces
in Problem 9 solves u′′ = u? This transformation from initial values to solution is,vector spaces
"linear. What is its 2 by 2 matrix (using x = 1, y = 0 and x = 0, y = 1 as basis for V,",vector spaces
and your basis for W)?,vector spaces
11. Verify directly from c2 +s2 = 1 that reﬂection matrices satisfy H2 = 1.,vector spaces
12. Suppose A is a linear transformation from the x-y plane to itself. Why does A−1(x+,vector spaces
"y) = A−1x + A−1y? If A is represented by the matrix M, explain why A−1 is repre-",vector spaces
13. The product (AB)C of linear transformations starts with a vector x and produces,vector spaces
u = Cx. Then rule 2V applies AB to u and reaches (AB)Cx.,vector spaces
(a) Is this result the same as separately applying C then B then A?,vector spaces
(b) Is the result the same as applying BC followed by A? Parentheses are unnecessary,vector spaces
and the associative law (AB)C = A(BC) holds for linear transformations. This is,vector spaces
the best proof of the same law for matrices.,vector spaces
14. Prove that T 2 is a linear transformation if T is linear (from R3 to R3).,vector spaces
15. The space of all 2 by 2 matrices has the four basis “vectors”,vector spaces
"For the linear transformation of transposing, ﬁnd its matrix A with respect to this",vector spaces
basis. Why is A2 = I?,vector spaces
"16. Find the 4 by 4 cyclic permutation matrix: (x1,x2,x3,x4) is transformed to Ax =",vector spaces
"(x2,x3,x4,x1). What is the effect of A2? Show that A3 = A−1.",vector spaces
"17. Find the 4 by 3 matrix A that represents a right shift: (x1,x2,x3) is transformed to",vector spaces
"(0,x1,x2,x3). Find also the left shift matrix B from R4 back to R3, transforming",vector spaces
"(x1,x2,x3,x4) to (x2,x3,x4). What are the products AB and BA?",vector spaces
"18. In the vector space P3 of all p(x) = a0 + a1x + a2x2 + a3x3, let S be the subset of",vector spaces
0 p(x)dx = 0. Verify that S is a subspace and ﬁnd a basis.,vector spaces
19. A nonlinear transformation is invertible if T(x) = b has exactly one solution for,vector spaces
every b. The example T(x) = x2 is not invertible because x2 = b has two solutions,vector spaces
for positive b and no solution for negative b. Which of the following transformations,vector spaces
"(from the real numbers R1 to the real numbers R1) are invertible? None are linear,",vector spaces
"20. What is the axis and the rotation angle for the transformation that takes (x1,x2,x3)",vector spaces
21. A linear transformation must leave the zero vector ﬁxed: T(0) = 0. Prove this from,vector spaces
T(v+w) = T(v)+T(w) by choosing w =,vector spaces
. Prove it also from the requirement,vector spaces
T(cv) = cT(v) by choosing c =,vector spaces
"22. Which of these transformations is not linear? The input is v = (v1,v2).",vector spaces
"23. If S and T are linear with S(v) = T(v) = v, then S(T(v)) = v or v2?",vector spaces
"24. Suppose T(v) = v, except that T(0,v2) = (0,0). Show that this transformation satis-",vector spaces
ﬁes T(cv) = cT(v) but not T(v+w) = T(v)+T(w).,vector spaces
"25. Which of these transformations satisfy T(v + w) = T(v) + T(w), and which satisfy",vector spaces
T(v) = v1 +v2 +v3.,vector spaces
T(v) = largest component of v.,vector spaces
"26. For these transformations of V = R2 to W = R2, ﬁnd T(T(v)).",vector spaces
(a) T(v) = −v.,vector spaces
"(b) T(v) = v+(1,1).",vector spaces
"(c) T(v) = 90° rotation = (−v2,v1).",vector spaces
(d) T(v) = projection =,vector spaces
"27. The “cyclic” transformation T is deﬁned by T(v1,v2,v3) = (v2,v3,v1).",vector spaces
T(T(T(v)))? What is T 100(v)?,vector spaces
Chapter 2 Vector Spaces,vector spaces
28. Find the range and kernel (those are new words for the column space and nullspace),vector spaces
29. A linear transformation from V to W has an inverse from W to V when the range,vector spaces
is all of W and the kernel contains only v = 0. Why are these transformations not,vector spaces
"(a) T(v1,v2) = (v2,v2)",vector spaces
"(b) T(v1,v2) = (v1,v2,v1 +v2)",vector spaces
"(c) T(v1,v2) = v1",vector spaces
"30. Suppose a linear T transforms (1,1) to (2,2) and (2,0) to (0,0). Find T(v) when",vector spaces
"(a) v = (2,2).",vector spaces
"(b) v = (3,1).",vector spaces
"(c) v = (−1,1).",vector spaces
"(d) v = (a,b).",vector spaces
Problems 31–35 may be harder. The input space V contains all 2 by 2 matrices,vector spaces
31. M is any 2 by 2 matrix and A =,vector spaces
. The linear transformation T is deﬁned by,vector spaces
T(M) = AM. What rules of matrix multiplication show that T is linear?,vector spaces
32. Suppose A =,vector spaces
. Show that the identity matrix I is not in the range of T. Find a,vector spaces
nonzero matrix M such that T(M) = AM is zero.,vector spaces
33. Suppose T transposes every matrix M. Try to ﬁnd a matrix A that gives AM =,vector spaces
MT for every M. Show that no matrix A will do it. To professors: Is this a linear,vector spaces
transformation that doesn’t come from a matrix?,vector spaces
34. The transformation T that transposes every matrix is deﬁnitely linear. Which of these,vector spaces
extra properties are true?,vector spaces
(a) T 2 = identity transformation.,vector spaces
(b) The kernel of T is the zero matrix.,vector spaces
(c) Every matrix is in the range of T.,vector spaces
(d) T(M) = −M is impossible.,vector spaces
35. Suppose T(M) =,vector spaces
. Find a matrix with T(M) ̸= 0. Describe all ma-,vector spaces
trices with T(M) = 0 (the kernel of T) and all output matrices T(M) (the range of,vector spaces
Problems 36–40 are about changing the basis,vector spaces
"36. (a) What matrix transforms (1,0) into (2,5) and transforms (0,1) to (1,3)?",vector spaces
"(b) What matrix transforms (2,5) to (1,0) and (1,3) to (0,1)?",vector spaces
"(c) Why does no matrix transform (2,6) to (1,0) and (1,3) to (0,1)?",vector spaces
"37. (a) What matrix M transforms (1,0) and (0,1) to (r,t) and (s,u)?",vector spaces
"(b) What matrix N transforms (a,c) and (b,d) to (1,0) and (0,1)?",vector spaces
"(c) What condition on a, b, c, d will make part (b) impossible?",vector spaces
"38. (a) How do M and N in Problem 37 yield the matrix that transforms (a,c) to (r,t)",vector spaces
"and (b,d) to (s,u)?",vector spaces
"(b) What matrix transforms (2,5) to (1,1) and (1,3) to (0,2)?",vector spaces
"39. If you keep the same basis vectors but put them in a different order, the change-of-",vector spaces
basis matrix M is a,vector spaces
matrix. If you keep the basis vectors in order but change,vector spaces
"their lengths, M is a",vector spaces
"40. The matrix that transforms (1,0) and (0,1) to (1,4) and (1,5) is M =",vector spaces
"combination a(1,4) + b(1,5) that equals (1,0) has (a,b) = (",vector spaces
"those new coordinates of (1,0) related to M or M−1?",vector spaces
"41. What are the three equations for A, B, C if the parabola Y = A + Bx +Cx2 equals 4",vector spaces
"at x = a, 5 at x = b, and 6 at x = c? Find the determinant of the 3 by 3 matrix. For",vector spaces
"which numbers a, b, c will it be impossible to ﬁnd this parabola Y?",vector spaces
"42. Suppose v1, v2, v3 are eigenvectors for T. This means T(vi) = λivi for i = 1,2,3.",vector spaces
What is the matrix for T when the input and output bases are the v’s?,vector spaces
43. Every invertible linear transformation can have I as its matrix. For the output basis,vector spaces
just choose wi = T(vi). Why must T be invertible?,vector spaces
44. Suppose T is reﬂection across the x-axis and S is reﬂection across the y-axis. The,vector spaces
"domain V is the x-y plane. If v = (x,y) what is S(T(v))? Find a simpler description",vector spaces
of the product ST.,vector spaces
"45. Suppose T is reﬂection across the 45° line, and S is reﬂection across the y-axis, If",vector spaces
"v = (2,1) then T(v) = (1,2). Find S(T(v)) and T(S(v)). This shows that generally",vector spaces
46. Show that the product ST of two reﬂections is a rotation. Multiply these reﬂection,vector spaces
matrices to ﬁnd the rotation angle:,vector spaces
47. The 4 by 4 Hadamard matrix is entirely +1 and −1:,vector spaces
Chapter 2 Vector Spaces,vector spaces
"Find H−1 and write v = (7,5,3,1) as a combination of the columns of H.",vector spaces
"48. Suppose we have two bases v1,...,vn and w1,...,wn for Rn. If a vector has coefﬁ-",vector spaces
"cients bi in one basis and ci in the other basis, what is the change-of-basis matrix in",vector spaces
b = Mc? Start from,vector spaces
b1v1 +···+bnvn = Vb = c1w1 +···+cnwn = Wc.,vector spaces
Your answer represents T(v) = v with input basis of v’s and output basis of w’s.,vector spaces
"Because of different bases, the matrix is not I.",vector spaces
"49. True or false: If we know T(v) for n different nonzero vectors in R2, then we know",vector spaces
T(v) for every vector in Rn.,vector spaces
"50. (Recommended) Suppose all vectors x in the unit square 0 ≤ x1 ≤ 1, 0 ≤ x2 ≤ 1 are",vector spaces
transformed to Ax (A is 2 by 2).,vector spaces
(a) What is the shape of the transformed region (all Ax)?,vector spaces
(b) For which matrices A is that region a square?,vector spaces
(c) For which A is it a line?,vector spaces
(d) For which A is the new area still 1?,vector spaces
1.1 Find a basis for the following subspaces of R4:,vector spaces
(a) The vectors for which x1 = 2x4.,vector spaces
(b) The vectors for which x1 +x2 +x3 = 0 and x3 +x4 = 0.,vector spaces
"(c) The subspace spanned by (1,1,1,1), (1,2,3,4), and (2,3,4,5).",vector spaces
"1.2 By giving a basis, describe a two-dimensional subspace of R3 that contains none of",vector spaces
"the coordinate vectors (1,0,0), (0,1,0), (0,0,1).",vector spaces
"1.3 True or false, with counterexample if false:",vector spaces
"(a) If the vectors x1,...,xm span a subspace S, then dimS = m.",vector spaces
(b) The intersection of two subspaces of a vector space cannot be empty.,vector spaces
"(c) If Ax = Ay, then x = y.",vector spaces
(d) The row space of A has a unique basis that can be computed by reducing A to,vector spaces
"(e) If a square matrix A has independent columns, so does A2.",vector spaces
1.4 What is the echelon form U of A?,vector spaces
What are the dimensions of its four fundamental subspaces?,vector spaces
1.5 Find the rank and the nullspace of,vector spaces
0 0 1 2,vector spaces
0 0 1 2,vector spaces
1 1 1 0,vector spaces
1.6 Find bases for the four fundamental subspaces associated with,vector spaces
1 1 0 0,vector spaces
0 1 0 1,vector spaces
"1.7 What is the most general solution to u+v+w = 1, u−w = 2?",vector spaces
"1.8 (a) Construct a matrix whose nullspace contains the vector x = (1,1,2).",vector spaces
"(b) Construct a matrix whose left nullspace contains y = (1,5).",vector spaces
"(c) Construct a matrix whose column space is spanned by (1,1,2) and whose row",vector spaces
"space is spanned by (1,5).",vector spaces
"(d) If you are given any three vectors in R6 and any three vectors in R5, is there a",vector spaces
6 by 5 matrix whose column space is spanned by the ﬁrst three and whose row,vector spaces
space is spanned by the second three?,vector spaces
"1.9 In the vector space of 2 by 2 matrices,",vector spaces
(a) is the set of rank 1 matrices a subspace?,vector spaces
(b) what subspace is spanned by the permutation matrices?,vector spaces
(c) what subspace is spanned by the positive matrices (all aij > 0)?,vector spaces
(d) what subspace is spanned by the invertible matrices?,vector spaces
1.10 Invent a vector space that contains all linear transformations from Rn to Rn. You,vector spaces
have to decide on a rule for addition. What is its dimension?,vector spaces
"1.11 (a) Find the rank of A, and give a basis for its nullspace.",vector spaces
A = LU =,vector spaces
3 2 4 1,vector spaces
1 2 0 1 2 1,vector spaces
0 0 2 2 0 0,vector spaces
0 0 0 0 0 1,vector spaces
0 0 0 0 0 0,vector spaces
Chapter 2 Vector Spaces,vector spaces
(b) The ﬁrst 3 rows of U are a basis for the row space of A—true or false?,vector spaces
"Columns 1, 3, 6 of U are a basis for the column space of A—true or false?",vector spaces
The four rows of A are a basis for the row space of A—true or false?,vector spaces
(c) Find as many linearly independent vectors b as possible for which Ax = b has a,vector spaces
"(d) In elimination on A, what multiple of the third row is subtracted to knock out",vector spaces
"1.12 If A is an n by n − 1 matrix, and its rank is n − 2, what is the dimension of its",vector spaces
"1.13 Use elimination to ﬁnd the triangular factors in A = LU, if",vector spaces
a a a a,vector spaces
a b b b,vector spaces
a b c d,vector spaces
"Under what conditions on the numbers a, b, c, d are the columns linearly indepen-",vector spaces
"1.14 Do the vectors (1,1,3), (2,3,6), and (1,4,3) form a basis for R3?",vector spaces
1.15 What do you know about C(A) when the number of solutions to Ax = b is,vector spaces
"(a) 0 or 1, depending on b.",vector spaces
"(b) ∞, independent of b.",vector spaces
"(c) 0 or ∞, depending on b.",vector spaces
"(d) 1, regardless of b.",vector spaces
"1.16 In the previous exercise, how is r related to m and n in each example?",vector spaces
"1.17 If x is a vector in Rn, and xTy = 0 for every y, prove that x = 0.",vector spaces
"1.18 If A is an n by n matrix such that A2 = A and rankA = n, prove that A = I.",vector spaces
"1.19 What subspace of 3 by 3 matrices is spanned by the elementary matrices Eij, with",vector spaces
1s on the diagonal and at most one nonzero entry below?,vector spaces
1.20 How many 5 by 5 permutation matrices are there? Are they linearly independent?,vector spaces
Do they span the space of all 5 by 5 matrices? No need to write them all down.,vector spaces
1.21 What is the rank of the n by n matrix with every entry equal to 1? How about the,vector spaces
"“checkerboard matrix,” with aij = 0 when i+ j is even, aij = 1 when i+ j is odd?",vector spaces
"1.22 (a) Ax = b has a solution under what conditions on b, for the following A and b?",vector spaces
1 2 0 3,vector spaces
0 0 0 0,vector spaces
2 4 0 1,vector spaces
(b) Find a basis for the nullspace of A.,vector spaces
"(c) Find the general solution to Ax = b, when a solution exists.",vector spaces
(d) Find a basis for the column space of A.,vector spaces
(e) What is the rank of AT?,vector spaces
"1.23 How can you construct a matrix that transforms the coordinate vectors e1,e2,e3 into",vector spaces
"three given vectors v1,v2,v3? When will that matrix be invertible?",vector spaces
"1.24 If e1,e2,e3 are in the column space of a 3 by 5 matrix, does it have a left-inverse?",vector spaces
Does it have a right-inverse?,vector spaces
"1.25 Suppose T is the linear transformation on R3 that takes each point (u,v,w) to (u+",vector spaces
"v+w,u+v,u), Describe what T −1 does to the point (x,y,z).",vector spaces
1.26 True or false?,vector spaces
(a) Every subspace of R4 is the nullspace of some matrix.,vector spaces
"(b) If A has the same nullspace as AT, the matrix must be square.",vector spaces
(c) The transformation that takes x to mx+b is linear (from R1 to R1).,vector spaces
1.27 Find bases for the four fundamental subspaces of,vector spaces
1 2 0 3,vector spaces
0 2 2 2,vector spaces
0 0 0 0,vector spaces
0 0 0 4,vector spaces
1.28 (a) If the rows of A are linearly independent (A is m by n) then the rank is,vector spaces
", and the left nullspace is",vector spaces
"(b) If A is 8 by 10 with a two-dimensional nullspace, show that Ax = b can be solved",vector spaces
1.29 Describe the linear transformations of the x-y plane that are represented with stan-,vector spaces
"dard basis (1,0) and (0,1) by the matrices",vector spaces
"1.30 (a) If A is square, show that the nullspace of A2 contains the nullspace of A.",vector spaces
Chapter 2 Vector Spaces,vector spaces
(b) Show also that the column space of A2 is contained in the column space of A.,vector spaces
1.31 When does the rank-1 matrix A = uvT have A2 = 0?,vector spaces
1.32 (a) Find a basis for the space of all vectors in R6 with x1 +x2 = x3 +x4 = x5 +x6.,vector spaces
(b) Find a matrix with that subspace as its nullspace.,vector spaces
(c) Find a matrix with that subspace as its column space.,vector spaces
1.33 Suppose the matrices in PA = LU are,vector spaces
0 1 0 0,vector spaces
1 0 0 0,vector spaces
0 0 0 1,vector spaces
0 0 1 0,vector spaces
2 −1 5 −1 5,vector spaces
1 0 0 0,vector spaces
0 1 0 0,vector spaces
1 1 1 0,vector spaces
2 1 0 1,vector spaces
(a) What is the rank of A?,vector spaces
(b) What is a basis for the row space of A?,vector spaces
"(c) True or false: Rows 1, 2, 3 of A are linearly independent.",vector spaces
(d) What is a basis for the column space of A?,vector spaces
(e) What is the dimension of the left nullspace of A?,vector spaces
(f) What is the general solution to Ax = 0?,vector spaces
Orthogonal Vectors and Subspaces,orthogonality
"A basis is a set of independent vectors that span a space. Geometrically, it is a set of",orthogonality
"coordinate axes. A vector space is deﬁned without those axes, but every time I think of",orthogonality
"the x-y plane or three-dimensional space or Rn, the axes are there. They are usually per-",orthogonality
pendicular! The coordinate axes that the imagination constructs are practically always,orthogonality
"orthogonal. In choosing a basis, we tend to choose an orthogonal basis.",orthogonality
The idea of an orthogonal basis is one of the foundations of linear algebra. We need,orthogonality
"a basis to convert geometric constructions into algebraic calculations, and we need an",orthogonality
orthogonal basis to make those calculations simple. A further specialization makes the,orthogonality
basis just about optimal: The vectors should have length 1. For an orthonormal basis,orthogonality
"(orthogonal unit vectors), we will ﬁnd",orthogonality
1. the length ∥x∥ of a vector;,orthogonality
2. the test xTy = 0 for perpendicular vectors; and,orthogonality
3. how to create perpendicular vectors from linearly independent vectors.,orthogonality
"More than just vectors, subspaces can also be perpendicular. We will discover, so",orthogonality
"beautifully and simply that it will be a delight to see, that the fundamental subspaces",orthogonality
"meet at right angles. Those four subspaces are perpendicular in pairs, two in Rm and",orthogonality
two in Rn. That will complete the fundamental theorem of linear algebra.,orthogonality
"The ﬁrst step is to ﬁnd the length of a vector. It is denoted by ∥x∥, and in two",orthogonality
dimensions it comes from the hypotenuse of a right triangle (Figure 3.1a). The square,orthogonality
of the length was given a long time ago by Pythagoras: ∥x∥2 = x2,orthogonality
"In three-dimensional space, x = (x1,x2,x3) is the diagonal of a box (Figure 3.1b). Its",orthogonality
length comes from two applications of the Pythagorean formula. The two-dimensional,orthogonality
"case takes care of (x1,x2,0) = (1,2,0) across the base. This forms a right angle with the",orthogonality
"vertical side (0,0,x3) = (0,0,3). The hypotenuse of the bold triangle (Pythagoras again)",orthogonality
5 = 12 + 22,orthogonality
14 = 12 + 22 + 32,orthogonality
"(1, 2, 3) has length",orthogonality
"(1, 2, 0) has length",orthogonality
"Figure 3.1: The length of vectors (x1,x2) and (x1,x2,x3).",orthogonality
is the length ∥x∥ we want:,orthogonality
∥x∥2 = 12 +22 +32,orthogonality
"The extension to x = (x1,...,xn) in n dimensions is immediate. By Pythagoras n−1",orthogonality
"times, the length ∥x∥ in Rn is the positive square root of xTx:",orthogonality
"The sum of squares matches xTx—and the length of x = (1,2,−3) is",orthogonality
�� = 12 +22 +(−3)2 = 14.,orthogonality
How can we decide whether two vectors x and y are perpendicular? What is the test,orthogonality
"for orthogonality in Figure 3.2? In the plane spanned by x and y, those vectors are",orthogonality
orthogonal provided they form a right triangle. We go back to a2 +b2 = c2:,orthogonality
Sides of a right triangle,orthogonality
∥x∥2 +∥y∥2 = ∥x−y∥2.,orthogonality
"Applying the length formula (1), this test for orthogonality in Rn becomes",orthogonality
= (x1 −y1)2 +···+(xn −yn)2.,orthogonality
The right-hand side has an extra −2xiyi from each (xi −yi)2:,orthogonality
3.1 Orthogonal Vectors and Subspaces,orthogonality
"Figure 3.2: A right triangle with 5+20 = 25. Dotted angle 100°, dashed angle 30°.",orthogonality
We have a right triangle when that sum of cross-product terms xiyi is zero:,orthogonality
xTy = x1y1 +···+xnyn = 0.,orthogonality
"This sum is xTy = ∑xiyi = yTx, the row vector xT times the column vector y:",orthogonality
�� = x1y1 +···+xnyn.,orthogonality
"This number is sometimes called the scalar product or dot product, and denoted by (x,y)",orthogonality
or x·y. We will use the name inner product and keep the notation xTy.,orthogonality
3A The inner product xTy is zero if and only if x and y are orthogonal vectors.,orthogonality
"If xTy > 0, their angle is less than 90°. If xTy < 0, their angle is greater than",orthogonality
The length squared is the inner product of x with itself: xTx = x2,orthogonality
n = ∥x∥2. The,orthogonality
only vector with length zero—the only vector orthogonal to itself—is the zero vector.,orthogonality
This vector x = 0 is orthogonal to every vector in Rn.,orthogonality
"Example 1. (2,2,−1) is orthogonal to (−1,2,2). Both have length √4+4+1 = 3.",orthogonality
"Useful fact: If nonzero vectors v1,...,vk are mutually orthogonal (every vector is",orthogonality
"perpendicular to every other), then those vectors are linearly independent.",orthogonality
"Proof. Suppose c1v1 + ··· + ckvk = 0. To show that c1 must be zero, take the inner",orthogonality
product of both sides with v1. Orthogonality of the v’s leaves only one term:,orthogonality
1(c1v1 +···+ckvk) = c1vT,orthogonality
"The vectors are nonzero, so vT",orthogonality
1v1 ̸= 0 and therefore c1 = 0. The same is true of every ci.,orthogonality
The only combination of the v’s producing zero has all ci = 0: independence!,orthogonality
"The coordinate vectors e1,...,en in Rn are the most important orthogonal vectors.",orthogonality
"Those are the columns of the identity matrix. They form the simplest basis for Rn, and",orthogonality
they are unit vectors—each has length ∥ei∥ = 1. They point along the coordinate axes. If,orthogonality
"these axes are rotated, the result is a new orthonormal basis: a new system of mutually",orthogonality
orthogonal unit vectors. In R2 we have cos2 θ +sin2θ = 1:,orthogonality
Orthonormal vectors in R2,orthogonality
We come to the orthogonality of two subspaces. Every vector in one subspace must be,orthogonality
orthogonal to every vector in the other subspace. Subspaces of R3 can have dimension,orthogonality
"0, 1, 2, or 3. The subspaces are represented by lines or planes through the origin—",orthogonality
"and in the extreme cases, by the origin alone or the whole space. The subspace {0}",orthogonality
"is orthogonal to all subspaces. A line can be orthogonal to another line, or it can be",orthogonality
"orthogonal to a plane, but a plane cannot be orthogonal to a plane.",orthogonality
I have to admit that the front wall and side wall of a room look like perpendicular,orthogonality
"planes in R3. But by our deﬁnition, that is not so! There are lines v and w in the front",orthogonality
and side walls that do not meet at a right angle. The line along the corner is in both,orthogonality
"walls, and it is certainly not orthogonal to itself.",orthogonality
Two subspaces V and W of the same space Rn are orthogonal if every,orthogonality
vector v in V is orthogonal to every vector w in W: vTw = 0 for all v and w.,orthogonality
"Example 2. Suppose V is the plane spanned by v1 = (1,0,0,0) and v2 = (1,1,0,0). If",orthogonality
"W is the line spanned by w = (0,0,4,5), then w is orthogonal to both v’s. The line W",orthogonality
will be orthogonal to the whole plane V.,orthogonality
"In this case, with subspaces of dimension 2 and 1 in R4, there is room for a third",orthogonality
"subspace. The line L through z = (0,0,5,−4) is perpendicular to V and W. Then the",orthogonality
"dimensions add to 2+1+1 = 4. What space is perpendicular to all of V, W, and L?",orthogonality
"The important orthogonal subspaces don’t come by accident, and they come two at",orthogonality
a time. In fact orthogonal subspaces are unavoidable: They are the fundamental sub-,orthogonality
spaces! The ﬁrst pair is the nullspace and row space. Those are subspaces of Rn—the,orthogonality
"rows have n components and so does the vector x in Ax = 0. We have to show, using",orthogonality
"Ax = 0, that the rows of A are orthogonal to the nullspace vector x.",orthogonality
Fundamental theorem of orthogonality The row space is orthogonal,orthogonality
to the nullspace (in Rn). The column space is orthogonal to the left nullspace,orthogonality
"First Proof. Suppose x is a vector in the nullspace. Then Ax = 0, and this system of m",orthogonality
3.1 Orthogonal Vectors and Subspaces,orthogonality
equations can be written out as rows of A multiplying x:,orthogonality
The main point is already in the ﬁrst equation: row 1 is orthogonal to x. Their inner,orthogonality
"product is zero; that is equation 1. Every right-hand side is zero, so x is orthogonal to",orthogonality
every row. Therefore x is orthogonal to every combination of the rows. Each x in the,orthogonality
"nullspace is orthogonal to each vector in the row space, so N(A)⊥C(AT).",orthogonality
"The other pair of orthogonal subspaces comes from ATy = 0, or yTA = 0:",orthogonality
"The vector y is orthogonal to every column. The equation says so, from the zeros on",orthogonality
the right-hand side. Therefore y is orthogonal to every combination of the columns.,orthogonality
"It is orthogonal to the column space, and it is a typical vector in the left nullspace:",orthogonality
"N(AT)⊥C(A). This is the same as the ﬁrst half of the theorem, with A replaced by",orthogonality
Second Proof. The contrast with this “coordinate-free proof” should be useful to the,orthogonality
reader. It shows a more “abstract” method of reasoning. I wish I knew which proof is,orthogonality
"clearer, and more permanently understood.",orthogonality
"If x is in the nullspace then Ax = 0. If v is in the row space, it is a combination of the",orthogonality
"rows: v = ATz for some vector z. Now, in one line:",orthogonality
Nullspace ⊥ Row space,orthogonality
vTx = (ATz)Tx = zTAx = zT0 = 0.,orthogonality
"Example 3. Suppose A has rank 1, so its row space and column space are lines:",orthogonality
"The rows are multiples of (1,3). The nullspace contains x = (−3,1), which is orthogonal",orthogonality
to all the rows. The nullspace and row space are perpendicular lines in R2:,orthogonality
"In contrast, the other two subspaces are in R3. The column space is the line through",orthogonality
"(1,2,3). The left nullspace must be the perpendicular plane y1 + 2y2 + 3y3 = 0. That",orthogonality
equation is exactly the content of yTA = 0.,orthogonality
The ﬁrst two subspaces (the two lines) had dimensions 1+1 = 2 in the space R2. The,orthogonality
"second pair (line and plane) had dimensions 1 + 2 = 3 in the space R3. In general, the",orthogonality
row space and nullspace have dimensions that add to r + (n − r) = n. The other pair,orthogonality
"adds to r +(m−r) = m. Something more than orthogonality is occurring, and I have to",orthogonality
ask your patience about that one further point: the dimensions.,orthogonality
It is certainly true that the null space is perpendicular to the row space—but it is not,orthogonality
the whole truth. N(A) contains every vector orthogonal to the row space. The nullspace,orthogonality
was formed from all solutions to Ax = 0.,orthogonality
"Deﬁnition. Given a subspace V of Rn, the space of all vectors orthogonal to V is called",orthogonality
the orthogonal complement of V. It is denoted by V⊥ = “V perp.”,orthogonality
"Using this terminology, the nullspace is the orthogonal complement of the row space:",orthogonality
"N(A) = (C(AT))⊥. At the same time, the row space contains all vectors that are orthog-",orthogonality
onal to the nullspace. A vector z can’t be orthogonal to the nullspace but outside the row,orthogonality
"space. Adding z as an extra row of A would enlarge the row space, but we know that",orthogonality
there is a ﬁxed formula r +(n−r) = n:,orthogonality
dim(row space)+dim(nullspace) = number of columns.,orthogonality
Every vector orthogonal to the nullspace is in the row space: C(AT) = (N(A))⊥.,orthogonality
The same reasoning applied to AT produces the dual result: The left nullspace N(AT),orthogonality
and the column space C(A) are orthogonal complements. Their dimensions add up to,orthogonality
"(m − r) + r = m, This completes the second half of the fundamental theorem of linear",orthogonality
algebra. The ﬁrst half gave the dimensions of the four subspaces. including the fact that,orthogonality
row rank = column rank. Now we know that those subspaces are perpendicular. More,orthogonality
"than that, the subspaces are orthogonal complements.",orthogonality
"Fundamental Theorem of Linear Algebra, Part II",orthogonality
The nullspace is the orthogonal complement of the row space in Rn.,orthogonality
The left nullspace is the orthogonal complement of the column space in Rm.,orthogonality
"To repeat, the row space contains everything orthogonal to the nullspace. The column",orthogonality
"space contains everything orthogonal to the left nullspace. That is just a sentence, hidden",orthogonality
"in the middle of the book, but it decides exactly which equations can be solved! Looked",orthogonality
"at directly, Ax = b requires b to be in the column space. Looked at indirectly. Ax = b",orthogonality
requires b to be perpendicular to the left nullspace.,orthogonality
Ax = b is solvable if and only if yTb = 0 whenever yTA = 0.,orthogonality
3.1 Orthogonal Vectors and Subspaces,orthogonality
The direct approach was “b must be a combination of the columns.” The indirect ap-,orthogonality
proach is “b must be orthogonal to every vector that is orthogonal to the columns.” That,orthogonality
doesn’t sound like an improvement (to put it mildly). But if only one or two vectors,orthogonality
are orthogonal to the columns. it is much easier to check those one or two conditions,orthogonality
yTb = 0. A good example is Kirchhoff’s Voltage Law in Section 2.5. Testing for zero,orthogonality
around loops is much easier than recognizing combinations of the columns.,orthogonality
"When the left-hand sides of Ax = b add to zero, the right-hand sides must, too:",orthogonality
x1 −x2 = b1,orthogonality
x2 −x3 = b2,orthogonality
x3 −x1 = b3,orthogonality
is solvable if and only if b1 +b2 +b3 = 0.,orthogonality
"This test b1 + b2 + b3 = 0 makes b orthogonal to y = (1,1,1) in the left nullspace. By",orthogonality
"the Fundamental Theorem, b is a combination of the columns!",orthogonality
The Matrix and the Subspaces,orthogonality
We emphasize that V and W can be orthogonal without being complements. Their,orthogonality
"dimensions can be too small. The line V spanned by (0,1,0) is orthogonal to the line",orthogonality
"W spanned by (0,0,1), but V is not W⊥. The orthogonal complement of W is a two-",orthogonality
"dimensional plane, and the line is only part of W⊥. When the dimensions are right,",orthogonality
orthogonal subspaces are necessarily orthogonal complements:,orthogonality
"In other words V⊥⊥ = V. The dimensions of V and W are right, and the whole space",orthogonality
Rn is being decomposed into two perpendicular parts (Figure 3.3).,orthogonality
Two orthogonal axes in R3,orthogonality
Line W perpendicular to plane V,orthogonality
Orthogonal complements V = W⊥,orthogonality
Figure 3.3: Orthogonal complements in R3: a plane and a line (not two lines).,orthogonality
Splitting Rn into orthogonal parts will split every vector into x = v+w. The vector v,orthogonality
is the projection onto the subspace V. The orthogonal component w is the projection of,orthogonality
x onto W. The next sections show how to ﬁnd those projections of x. They lead to what,orthogonality
is probably the most important ﬁgure in the book (Figure 3.4).,orthogonality
Figure 3.4 summarizes the fundamental theorem of linear algebra. It illustrates the,orthogonality
true effect of a matrix—what is happening inside the multiplication Ax. The nullspace,orthogonality
Figure 3.4: The true action Ax = A(xrow +xnull) of any m by n matrix.,orthogonality
is carried to the zero vector. Every Ax is in the column space. Nothing is carried to the,orthogonality
"left nullspace. The real action is between the row space and column space, and you see",orthogonality
it by looking at a typical vector x. It has a “row space component” and a “nullspace,orthogonality
"component,” with x = xr +xn. When multiplied by A, this is Ax = Axr +Axn:",orthogonality
The nullspace component goes to zero: Axn = 0.,orthogonality
The row space component goes to the column space: Axr = Ax.,orthogonality
Of course everything goes to the column space—the matrix cannot do anything else. I,orthogonality
"tried to make the row and column spaces the same size, with equal dimension r.",orthogonality
"From the row space to the column space, A is actually invertible. Every",orthogonality
vector b in the column space comes from exactly one vector xr in the row,orthogonality
"Proof. Every b in the column space is a combination Ax of the columns. In fact, b is",orthogonality
"Axr, with xr in the row space, since the nullspace component gives Axn = 0, If another",orthogonality
r in the row space gives Ax′,orthogonality
"r = b, then A(xr −x′",orthogonality
r) = b−b = 0. This puts xr −x′,orthogonality
"in the nullspace and the row space, which makes it orthogonal to itself. Therefore it is",orthogonality
"zero, and xr −x′",orthogonality
r. Exactly one vector in the row space is carried to b.,orthogonality
Every matrix transforms its row space onto its column space.,orthogonality
On those r-dimensional spaces A is invertible. On its nullspace A is zero. When A is,orthogonality
"diagonal, you see the invertible submatrix holding the r nonzeros.",orthogonality
"AT goes in the opposite direction, from Rm to Rn and from C(A) back to C(AT).",orthogonality
"Of course the transpose is not the inverse! AT moves the spaces correctly, but not the",orthogonality
3.1 Orthogonal Vectors and Subspaces,orthogonality
individual vectors. That honor belongs to A−1 if it exists—and it only exists if r = m = n.,orthogonality
We cannot ask A−1 to bring back a whole nullspace out of the zero vector.,orthogonality
"When A−1 fails to exist, the best substitute is the pseudoinverse A+. This inverts A",orthogonality
"where that is possible: A+Ax = x for x in the row space. On the left nullspace, nothing",orthogonality
"can be done: A+y = 0. Thus A+ inverts A where it is invertible, and has the same rank r.",orthogonality
One formula for A+ depends on the singular value decomposition—for which we ﬁrst,orthogonality
need to know about eigenvalues.,orthogonality
"1. Find the lengths and the inner product of x = (1,4,0,2) and y = (2,−2,1,3).",orthogonality
"2. Give an example in R2 of linearly independent vectors that are not orthogonal. Also,",orthogonality
give an example of orthogonal vectors that are not independent.,orthogonality
3. Two lines in the plane are perpendicular when the product of their slopes is −1.,orthogonality
"Apply this to the vectors x = (x1,x2) and y = (y1,y2), whose slopes are x2/x1 and",orthogonality
"y2/y1, to derive again the orthogonality condition xTy = 0.",orthogonality
4. How do we know that the ith row of an invertible matrix B is orthogonal to the jth,orthogonality
"column of B−1, if i ̸= j?",orthogonality
"5. Which pairs are orthogonal among the vectors v1, v2, v3, v4?",orthogonality
"6. Find all vectors in R3 that are orthogonal to (1,1,1) and (1,−1,0). Produce an",orthogonality
orthonormal basis from these vectors (mutually orthogonal unit vectors).,orthogonality
"7. Find a vector x orthogonal to the row space of A, and a vector y orthogonal to the",orthogonality
"column space, and a vector z orthogonal to the nullspace:",orthogonality
"8. If V and W are orthogonal subspaces, show that the only vector they have in common",orthogonality
is the zero vector: V∩W = {0}.,orthogonality
"9. Find the orthogonal complement of the plane spanned by the vectors (1,1,2) and",orthogonality
"(1,2,3), by taking these to be the rows of A and solving Ax = 0. Remember that the",orthogonality
complement is a whole line.,orthogonality
10. Construct a homogeneous equation in three unknowns whose solutions are the linear,orthogonality
"combinations of the vectors (1,1,2) and (1,2,3). This is the reverse of the previous",orthogonality
"exercise, but the two problems are really the same.",orthogonality
11. The fundamental theorem is often stated in the form of Fredholm’s alternative: For,orthogonality
"any A and b, one and only one of the following systems has a solution:",orthogonality
(i) Ax = b.,orthogonality
"(ii) ATy = 0, yTb ̸= 0.",orthogonality
Either b is in the column space C(A) or there is a y in N(AT) such that yTb ̸= 0.,orthogonality
Show that it is contradictory for (i) and (ii) both to have solutions.,orthogonality
12. Find a basis for the orthogonal complement of the row space of A:,orthogonality
"Split x = (3,3,3) into a row space component xr and a nullspace component xn.",orthogonality
"13. Illustrate the action of AT by a picture corresponding to Figure 3.4, sending C(A)",orthogonality
back to the row space and the left nullspace to zero.,orthogonality
14. Show that x−y is orthogonal to x+y if and only if ∥x∥ = ∥y∥.,orthogonality
"15. Find a matrix whose row space contains (1,2,1) and whose nullspace contains (1,−2,1),",orthogonality
or prove that there is no such matrix.,orthogonality
"16. Find all vectors that are perpendicular to (1,4,4,1) and (2,9,8,2).",orthogonality
"17. If V is the orthogonal complement of W in Rn, is there a matrix with row space V",orthogonality
"and nullspace W? Starting with a basis for V, construct such a matrix.",orthogonality
"18. If S = {0} is the subspace of R4 containing only the zero vector, what is S⊥? If S is",orthogonality
"spanned by (0,0,0,1), what is S⊥? What is (S⊥)⊥?",orthogonality
19. Why are these statements false?,orthogonality
"(a) If V is orthogonal to W, then V⊥ is orthogonal to W⊥.",orthogonality
(b) V orthogonal to W and W orthogonal to Z makes V orthogonal to Z.,orthogonality
20. Let S be a subspace of Rn. Explain what (S⊥)⊥ = S means and why it is true.,orthogonality
21. Let P be the plane in R2 with equation x + 2y − z = 0. Find a vector perpendicular,orthogonality
"to P. What matrix has the plane P as its nullspace, and what matrix has P as its row",orthogonality
22. Let S be the subspace of R4 containing all vectors with x1 +x2 +x3 +x4 = 0. Find a,orthogonality
"basis for the space S⊥, containing all vectors orthogonal to S.",orthogonality
3.1 Orthogonal Vectors and Subspaces,orthogonality
23. Construct an unsymmetric 2 by 2 matrix of rank 1. Copy Figure 3.4 and put one,orthogonality
vector in each subspace. Which vectors are orthogonal?,orthogonality
24. Redraw Figure 3.4 for a 3 by 2 matrix of rank r = 2. Which subspace is Z (zero,orthogonality
vector only)? The nullspace part of any vector x in R2 is xn =,orthogonality
25. Construct a matrix with the required property or say why that is impossible.,orthogonality
(a) Column space contains,orthogonality
(b) Row space contains,orthogonality
has a solution and AT� 1,orthogonality
(d) Every row is orthogonal to every column (A is not the zero matrix).,orthogonality
"(e) The columns add up to a column of 0s, the rows add to a row of 1s.",orthogonality
26. If AB = 0 then the columns of B are in the,orthogonality
of A. The rows of A are in the,orthogonality
of B. Why can’t A and B be 3 by 3 matrices of rank 2?,orthogonality
"27. (a) If Ax = b has a solution and ATy = 0, then y is perpendicular to",orthogonality
"(b) If ATy = c has a solution and Ax = 0, then x is perpendicular to",orthogonality
28. This is a system of equations Ax = b with no solution:,orthogonality
"Find numbers y1, y2, y3 to multiply the equations so they add to 0 = 1. You have",orthogonality
found a vector y in which subspace? The inner product yTb is 1.,orthogonality
"29. In Figure 3.4, how do we know that Axr is equal to Ax? How do we know that this",orthogonality
vector is in the column space? If A =,orthogonality
30. If Ax is in the nullspace of AT then Ax = 0. Reason: Ax is also in the,orthogonality
. Conclusion: ATA has the same nullspace as A.,orthogonality
31. Suppose A is a symmetric matrix (AT = A).,orthogonality
(a) Why is its column space perpendicular to its nullspace?,orthogonality
"(b) If Ax = 0 and Az = 5z, which subspaces contain these “eigenvectors” x and z?",orthogonality
Symmetric matrices have perpendicular eigenvectors (see Section 5.5).,orthogonality
32. (Recommended) Draw Figure 3.4 to show each subspace for,orthogonality
"33. Find the pieces xr and xn, and draw Figure 3.4 properly, if",orthogonality
Problems 34–44 are about orthogonal subspaces.,orthogonality
34. Put bases for the orthogonal subspaces V and W into the columns of matrices V and,orthogonality
W. Why does V TW = zero matrix? This matches vTw = 0 for vectors.,orthogonality
35. The ﬂoor and the wall are not orthogonal subspaces because they share a nonzero,orthogonality
vector (along the line where they meet). Two planes in R3 cannot be orthogonal!,orthogonality
Find a vector in both column spaces C(A) and C(B):,orthogonality
This will be a vector Ax and also B�x. Think 3 by 4 with the matrix [A B].,orthogonality
36. Extend Problem 35 to a p-dimensional subspace V and a q-dimensional subspace W,orthogonality
of Rn. What inequality on p+q guarantees that V intersects W in a nonzero vector?,orthogonality
These subspaces cannot be orthogonal.,orthogonality
"37. Prove that every y in N(AT) is perpendicular to every Ax in the column space, using",orthogonality
the matrix shorthand of equation (8). Start from ATy = 0.,orthogonality
"38. If S is the subspace of R3 containing only the zero vector, what is S⊥? If S is spanned",orthogonality
"by (1,1,1), what is S⊥? If S is spanned by (2,0,0) and (0,0,3), what is S⊥?",orthogonality
"39. Suppose S only contains (1,5,1) and (2,2,2) (not a subspace). Then S⊥ is the",orthogonality
nullspace of the matrix A =,orthogonality
. S⊥ is a subspace even if S is not.,orthogonality
40. Suppose L is a one-dimensional subspace (a line) in R3. Its orthogonal complement,orthogonality
perpendicular to L. Then (L⊥)⊥ is a,orthogonality
perpendicular to L⊥. In,orthogonality
fact (L⊥)⊥ is the same as,orthogonality
41. Suppose V is the whole space R4. Then V⊥ contains only the vector,orthogonality
. So (V⊥)⊥ is the same as,orthogonality
"42. Suppose S is spanned by the vectors (1,2,2,3) and (1,3,3,2). Find two vectors that",orthogonality
span S⊥. This is the same as solving Ax = 0 for which A?,orthogonality
"43. If P is the plane of vectors in R4 satisfying x1 + x2 + x3 + x4 = 0, write a basis for",orthogonality
P⊥. Construct a matrix that has P as its nullspace.,orthogonality
"44. If a subspace S is contained in a subspace V, prove that S⊥ contains V⊥.",orthogonality
Problems 45–50 are about perpendicular columns and rows.,orthogonality
3.2 Cosines and Projections onto Lines,orthogonality
45. Suppose an n by n matrix is invertible: AA−1 = I. Then the ﬁrst column of A−1 is,orthogonality
orthogonal to the space spanned by which rows of A?,orthogonality
"46. Find ATA if the columns of A are unit vectors, all mutually perpendicular.",orthogonality
47. Construct a 3 by 3 matrix A with no zero entries whose columns are mutually per-,orthogonality
pendicular. Compute ATA. Why is it a diagonal matrix?,orthogonality
48. The lines 3x+y = b1 and 6x+2y = b2 are,orthogonality
. They are the same line if,orthogonality
"that case (b1,b2) is perpendicular to the vector",orthogonality
. The nullspace of the matrix is,orthogonality
the line 3x+y =,orthogonality
. One particular vector in that nullspace is,orthogonality
49. Why is each of these statements false?,orthogonality
"(a) (1,1,1) is perpendicular to (1,1,−2), so the planes x+y+z = 0 and x+y−2z =",orthogonality
0 are orthogonal subspaces.,orthogonality
"(b) The subspace spanned by (1,1,0,0,0) and (0,0,0,1,1) is the orthogonal com-",orthogonality
"plement of the subspace spanned by (1,−1,0,0,0) and (2,−2,3,4,−4).",orthogonality
(c) Two subspaces that meet only in the zero vector are orthogonal.,orthogonality
"50. Find a matrix with v = (1,2,3) in the row space and column space. Find another",orthogonality
matrix with v in the nullspace and column space. Which pairs of subspaces can v not,orthogonality
"51. Suppose A is 3 by 4, B is 4 by 5, and AB = 0. Prove rank(A)+rank(B) ≤ 4.",orthogonality
52. The command N = null(A) will produce a basis for the nullspace of A. Then the,orthogonality
command B = null(N’) will produce a basis for the,orthogonality
Cosines and Projections onto Lines,orthogonality
"Vectors with xTy = 0 are orthogonal. Now we allow inner products that are not zero,",orthogonality
"and angles that are not right angles. We want to connect inner products to angles, and",orthogonality
also to transposes. In Chapter 1 the transpose was constructed by ﬂipping over a matrix,orthogonality
as if it were some kind of pancake. We have to do better than that.,orthogonality
One fact is unavoidable: The orthogonal case is the most important. Suppose we,orthogonality
want to ﬁnd the distance from a point b to the line in the direction of the vector a. We,orthogonality
are looking along that line for the point p closest to b. The key is in the geometry: The,orthogonality
line connecting b to p (the dotted line in Figure 3.5) is perpendicular to a. This fact will,orthogonality
"allow us to ﬁnd the projection p. Even though a and b are not orthogonal, the distance",orthogonality
problem automatically brings in orthogonality.,orthogonality
The situation is the same when we are given a plane (or any subspace S) instead of a,orthogonality
line. Again the problem is to ﬁnd the point p on that subspace that is closest to b. This,orthogonality
e = b − p,orthogonality
p = projection of b,orthogonality
onto line through a,orthogonality
Figure 3.5: The projection p is the point (on the line through a) closest to b.,orthogonality
point p is the projection of b onto the subspace. A perpendicular line from b to S meets,orthogonality
"the subspace at p. Geometrically, that gives the distance between points b and subspaces",orthogonality
S. But there are two questions that need to be asked:,orthogonality
1. Does this projection actually arise in practical applications?,orthogonality
"2. If we have a basis for the subspace S, is there a formula for the projection p?",orthogonality
The answers are certainly yes. This is exactly the problem of the least-squares solu-,orthogonality
tion to an overdetermined system. The vector b represents the data from experiments,orthogonality
"or questionnaires, and it contains too many errors to be found in the subspace S. When",orthogonality
"we try to write b as a combination of the basis vectors for S, it cannot be done—the",orthogonality
"equations are inconsistent, and Ax = b has no solution.",orthogonality
The least-squares method selects p as the best choice to replace b. There can be no,orthogonality
"doubt of the importance of this application. In economics and statistics, least squares",orthogonality
"enters regression analysis. In geodesy, the U.S. mapping survey tackled 2.5 million",orthogonality
"equations in 400,000 unknowns.",orthogonality
A formula for p is easy when the subspace is a line. We will project b onto a in several,orthogonality
"different ways, and relate the projection p to inner products and angles. Projection onto a",orthogonality
higher dimensional subspace is by far the most important case; it corresponds to a least-,orthogonality
"squares problem with several parameters, and it is solved in Section 3.3. The formulas",orthogonality
are even simpler when we produce an orthogonal basis for S.,orthogonality
inner products and cosines,orthogonality
We pick up the discussion of inner products and angles. You will soon see that it is not,orthogonality
"the angle, but the cosine of the angle, that is directly related to inner products. We look",orthogonality
back to trigonometry in the two-dimensional case to ﬁnd that relationship. Suppose the,orthogonality
vectors a and b make angles α and β with the x-axis (Figure 3.6). The length ∥a∥ is the,orthogonality
hypotenuse in the triangle OaQ. So the sine and cosine of α are,orthogonality
3.2 Cosines and Projections onto Lines,orthogonality
u · i = cos θ,orthogonality
"b = (b1, b2)",orthogonality
"a = (a1, a2)",orthogonality
Figure 3.6: The cosine of the angle θ = β −α using inner products.,orthogonality
"For the angle β, the sine is b2/∥b∥ and the cosine is b1/∥b∥ . The cosine of θ = β −α",orthogonality
comes from an identity that no one could forget:,orthogonality
cosθ = cosβ cosα +sinβ sinα = a1b1 +a2b2,orthogonality
The numerator in this formula is exactly the inner product of a and b. It gives the,orthogonality
relationship between aTb and cosθ:,orthogonality
The cosine of the angle between any nonzero vectors a and b is,orthogonality
"This formula is dimensionally correct; if we double the length of b, then both numerator",orthogonality
"and denominator are doubled, and the cosine is unchanged. Reversing the sign of b, on",orthogonality
"the other hand, reverses the sign of cosθ—and changes the angle by 180°.",orthogonality
There is another law of trigonometry that leads directly to the same result. It is not so,orthogonality
"unforgettable as the formula in equation (1), but it relates the lengths of the sides of any",orthogonality
∥b−a∥2 = ∥b∥2 +∥a∥2 −2∥b∥∥a∥cosθ.,orthogonality
"When θ is a right angle, we are back to Pythagoras: ∥b − a∥2 = ∥b∥2 + ∥a∥2. For any",orthogonality
"angle θ, the expression ∥b−a∥2 is (b−a)T(b−a), and equation (3) becomes",orthogonality
"Canceling bTb and aTa on both sides of this equation, you recognize formula (2) for the",orthogonality
"cosine: aTb = ∥a∥∥b∥cosθ. In fact, this proves the cosine formula in n dimensions,",orthogonality
since we only have to worry about the plane triangle Oab.,orthogonality
Projection onto a Line,orthogonality
Now we want to ﬁnd the projection point p. This point must be some multiple p = �xa of,orthogonality
the given vector a—every point on the line is a multiple of a. The problem is to compute,orthogonality
e = b − p,orthogonality
p = �xa = aTb,orthogonality
"Figure 3.7: The projection p of b onto a, with cosθ = Op",orthogonality
the coefﬁcient �x. All we need is the geometrical fact that the line from b to the closest,orthogonality
point p = �xa is perpendicular to the vector a:,orthogonality
"aT(b− �a) = 0,",orthogonality
That gives the formula for the number �x and the projection p:,orthogonality
3H The projection of the vector b onto the line in the direction of a is p = �xa:,orthogonality
Projection onto a line,orthogonality
p = �xa = aTb,orthogonality
This allows us to redraw Figure 3.5 with a correct formula for p (Figure 3.7).,orthogonality
"This leads to the Schwarz inequality in equation (6), which is the most important",orthogonality
inequality in mathematics. A special case is the fact that arithmetic means 1,orthogonality
larger than geometric means √xy. (It is also equivalent—see Problem 1 at the end of,orthogonality
this section—to the triangle inequality for vectors.) The Schwarz inequality seems to,orthogonality
come almost accidentally from the statement that ∥e∥2 = ∥b− p∥2 in Figure 3.7 cannot,orthogonality
This tells us that (bTb)(aTa) ≥ (aTb)2—and then we take square roots:,orthogonality
"3I All vectors a and b satisfy the Schwarz inequality, which is |cosθ| ≤ 1 in",orthogonality
"According to formula (2), the ratio between aTb and ∥a∥∥b∥ is exactly |cosθ|. Since",orthogonality
"all cosines lie in the interval −1 ≤ cosθ ≤ 1, this gives another proof of equation (6):",orthogonality
the Schwarz inequality is the same as |cosθ| ≤ 1. In some ways that is a more easily,orthogonality
"understood proof, because cosines are so familiar. Either proof is all right in Rn, but",orthogonality
3.2 Cosines and Projections onto Lines,orthogonality
notice that ours came directly from the calculation of ∥b− p∥2. This stays nonnegative,orthogonality
when we introduce new possibilities for the lengths and inner products. The name of,orthogonality
"Cauchy is also attached to this inequality |aTb| ≤ ∥a∥∥b∥, and the Russians refer to it as",orthogonality
the Cauchy-Schwarz-Buniakowsky inequality! Mathematical historians seem to agree,orthogonality
that Buniakowsky’s claim is genuine.,orthogonality
One ﬁnal observation about |aTb| ≤ ∥a∥∥b∥. Equality holds if and only if b is a,orthogonality
multiple of a. The angle is θ = 0° or θ = 180° and the cosine is 1 or −1. In this case b,orthogonality
"is identical with its projection p, and the distance between b and the line is zero.",orthogonality
"Example 1. Project b = (1,2,3) onto the line through a = (1,1,1) to get �x and p:",orthogonality
"The projection is p = �xa = (2,2,2). The angle between a and b has",orthogonality
The Schwarz inequality |aTb| ≤ ∥a∥∥b∥ is 6 ≤,orthogonality
14. If we write 6 as,orthogonality
"36, that is the",orthogonality
"42. The cosine is less than 1, because b is not parallel to a.",orthogonality
Projection Matrix of Rank 1,orthogonality
The projection of b onto the line through a lies at p = a(aTb/aTa). That is our formula,orthogonality
"p = �xa, but it is written with a slight twist: The vector a is put before the number",orthogonality
�x = aTb/aTa. There is a reason behind that apparently trivial change. Projection onto,orthogonality
"a line is carried out by a projection matrix P, and written in this new order we can see",orthogonality
what it is. P is the matrix that multiplies b and produces p:,orthogonality
so the projection matrix is,orthogonality
That is a column times a row—a square matrix—divided by the number aTa.,orthogonality
"Example 2. The matrix that projects onto the line through a = (1,1,1) is",orthogonality
This matrix has two properties that we will see as typical of projections:,orthogonality
1. P is a symmetric matrix.,orthogonality
2. Its square is itself: P2 = P.,orthogonality
P2b is the projection of Pb—and Pb is already on the line! So P2b = Pb. This matrix P,orthogonality
also gives a great example of the four fundamental subspaces:,orthogonality
"The column space consists of the line through a = (1,1,1).",orthogonality
The nullspace consists of the plane perpendicular to a.,orthogonality
The rank is r = 1.,orthogonality
"Every column is a multiple of a, and so is Pb = �xa. The vectors that project to p = 0",orthogonality
are especially important. They satisfy aTb = 0—they are perpendicular to a and their,orthogonality
component along the line is zero. They lie in the nullspace = perpendicular plane.,orthogonality
Actually that example is too perfect. It has the nullspace orthogonal to the column,orthogonality
"space, which is haywire. The nullspace should be orthogonal to the row space. But",orthogonality
"because P is symmetric, its row and column spaces are the same.",orthogonality
Remark on scaling The projection matrix aaT/aTa is the same if a is doubled:,orthogonality
"The line through a is the same, and that’s all the projection matrix cares about. If a has",orthogonality
"unit length, the denominator is aTa = 1 and the matrix is just P = aaT.",orthogonality
Example 3. Project onto the “θ-direction” in the x-y plane. The line goes through,orthogonality
"a = (cosθ,sinθ) and the matrix is symmetric with P2 = P:",orthogonality
"Here c is cosθ, s is sinθ, and c2 + s2 = 1 in the denominator. This matrix P was dis-",orthogonality
covered in Section 2.6 on linear transformations. Now we know P in any number of,orthogonality
dimensions. We emphasize that it produces the projection p:,orthogonality
"To project b onto a, multiply by the projection matrix P: p = Pb.",orthogonality
Transposes from Inner Products,orthogonality
"Finally we connect inner products to AT. Up to now, AT is simply the reﬂection of A",orthogonality
"across its main diagonal; the rows of A become the columns of AT, and vice versa. The",orthogonality
"entry in row i, column j of AT is the (j,i) entry of A:",orthogonality
"There is a deeper signiﬁcance to AT, Its close connection to inner products gives a new",orthogonality
and much more “abstract” deﬁnition of the transpose:,orthogonality
3.2 Cosines and Projections onto Lines,orthogonality
The transpose AT can be deﬁned by the following property: The inner,orthogonality
"product of Ax with y equals the inner product of x with ATy. Formally, this",orthogonality
(Ax)Ty = xTATy = xT(ATy).,orthogonality
"This deﬁnition gives us another (better) way to verify the formula (AB)T = BTAT, Use",orthogonality
Move A then move B,orthogonality
(ABx)Ty = (Bx)T(ATY) = xT(BTATy).,orthogonality
"The transposes turn up in reverse order on the right side, just as the inverses do in the",orthogonality
formula (AB)−1 = B−1A−1. We mention again that these two formulas meet to give the,orthogonality
remarkable combination (A−1)T = (AT)−1.,orthogonality
"1. (a) Given any two positive numbers x and y, choose the vector b equal to (√x,√y),",orthogonality
"and choose a = (√y,√x). Apply the Schwarz inequality to compare the arith-",orthogonality
2(x+y) with the geometric mean √xy.,orthogonality
"(b) Suppose we start with a vector from the origin to the point x, and then add a",orthogonality
vector of length ∥y∥ connecting x to x + y. The third side of the triangle goes,orthogonality
from the origin to x+y. The triangle inequality asserts that this distance cannot,orthogonality
be greater than the sum of the ﬁrst two:,orthogonality
"After squaring both sides, and expanding (x + y)T(x + y), reduce this to the",orthogonality
"2. Verify that the length of the projection in Figure 3.7 is ∥p∥ = ∥b∥cosθ, using formula",orthogonality
"3. What multiple of a = (1,1,1) is closest to the point b = (2,4,4)? Find also the point",orthogonality
closest to a on the line through b.,orthogonality
4. Explain why the Schwarz inequality becomes an equality in the case that a and b,orthogonality
"lie on the same line through the origin, and only in that case. What if they lie on",orthogonality
opposite sides of the origin?,orthogonality
"5. In n dimensions, what angle does the vector (1,1,...,1) make with the coordinate",orthogonality
axes? What is the projection matrix P onto that vector?,orthogonality
6. The Schwarz inequality has a one-line proof if a and b are normalized ahead of time,orthogonality
to be unit vectors:,orthogonality
�� ≤ ∑|a j||b j| ≤ ∑,orthogonality
|a j|2 +|b j|2,orthogonality
Which previous problem justiﬁes the middle step?,orthogonality
"7. By choosing the correct vector b in the Schwarz inequality, prove that",orthogonality
(a1 +···+an)2 ≤ n(a2,orthogonality
When does equality hold?,orthogonality
8. The methane molecule CH4 is arranged as if the carbon atom were at the center of a,orthogonality
regular tetrahedron with four hydrogen atoms at the vertices. If vertices are placed,orthogonality
"at (0,0,0), (1,1,0), (1,0,1), and (0,1,1)—note that all six edges have length",orthogonality
so the tetrahedron is regular—what is the cosine of the angle between the rays going,orthogonality
from the center (1,orthogonality
"2) to the vertices? (The bond angle itself is about 109.5°, an",orthogonality
old friend of chemists.),orthogonality
"9. Square the matrix P = aaT/aTa, which projects onto a line, and show that P2 = P.",orthogonality
(Note the number aTa in the middle of the matrix aaTaaT!),orthogonality
10. Is the projection matrix P invertible? Why or why not?,orthogonality
11. (a) Find the projection matrix P1 onto the line through a = [1,orthogonality
3] and also the matrix P2,orthogonality
that projects onto the line perpendicular to a.,orthogonality
(b) Compute P1 +P2 and P1P2 and explain.,orthogonality
12. Find the matrix that projects every point in the plane onto the line x+2y = 0.,orthogonality
13. Prove that the trace of P = aaT/aTa—which is the sum of its diagonal entries—,orthogonality
14. What matrix P projects every point in R3 onto the line of intersection of the planes,orthogonality
x+y+t = 0 and x−t = 0?,orthogonality
15. Show that the length of Ax equals the length of ATx if AAT = ATA.,orthogonality
16. Suppose P is the projection matrix onto the line through a.,orthogonality
(a) Why is the inner product of x with Py equal to the inner product of Px with y?,orthogonality
"(b) Are the two angles the same? Find their cosines if a = (1,1,−1), x = (2,0,1),",orthogonality
(c) Why is the inner product of Px with Py again the same? What is the angle,orthogonality
Problems 17–26 ask for projections onto lines. Also errors e = b− p and matri-,orthogonality
17. Project the vector b onto the line through a. Check that e is perpendicular to a:,orthogonality
3.2 Cosines and Projections onto Lines,orthogonality
18. Draw the projection of b onto a and also compute it from p = �xa:,orthogonality
"19. In Problem 17, ﬁnd the projection matrix P = aaT/aTa onto the line through each",orthogonality
vector a. Verify in both cases that P2 = P. Multiply Pb in each case to compute the,orthogonality
20. Construct the projection matrices P1 and P2 onto the lines through the a’s in Problem,orthogonality
18. Is it true that (P1 +P2)2 = P1 +P2? This would be true if P1P2 = 0.,orthogonality
"For Problems 21–26, consult the accompanying ﬁgures.",orthogonality
"21. Compute the projection matrices aaT/aTa onto the lines through a1 = (−1,2,2) and",orthogonality
"a2 = (2,2,−1), Multiply those projection matrices and explain why their product",orthogonality
P1P2 is what it is.,orthogonality
"22. Project b = (1,0,0) onto the lines through a1 and a2 in Problem 21 and also onto",orthogonality
"a3 = (2,−1,2). Add the three projections p1 + p2 + p3.",orthogonality
"23. Continuing Problems 21–22, ﬁnd the projection matrix P3 onto a3 = (2,−1,2). Ver-",orthogonality
"ify that P1 +P2 +P3 = I. The basis a1, a2, a3 is orthogonal!",orthogonality
"24. Project the vector b = (1,1) onto the lines through a1 = (1,0) and a2 = (1,2). Draw",orthogonality
the projections p1 and p2 and add p1 + p2. The projections do not add to b because,orthogonality
the a’s are not orthogonal.,orthogonality
"25. In Problem 24, the projection of b onto the plane of a1 and a2 will equal b. Find",orthogonality
P = A(ATA)−1AT for A = [a1 a2],orthogonality
"26. Project a1 = (1,0) onto a2 = (1,2). Then project the result back onto a1. Draw these",orthogonality
projections and multiply the projection matrices P1P2: Is this a projection?,orthogonality
Projections and Least Squares,orthogonality
"Up to this point, Ax = b either has a solution or not. If b is not in the column space C(A),",orthogonality
the system is inconsistent and Gaussian elimination fails. This failure is almost certain,orthogonality
when there are several equations and only one unknown:,orthogonality
"This is solvable when b1, b2, b3 are in the ratio 2:3:4. The solution x will exist only if b",orthogonality
"is on the same line as the column a = (2,3,4).",orthogonality
"In spite of their unsolvability, inconsistent equations arise all the time in practice.",orthogonality
"They have to be solved! One possibility is to determine x from part of the system, and",orthogonality
ignore the rest; this is hard to justify if all m equations come from the same source.,orthogonality
"Rather than expecting no error in some equations and large errors in the others, it is",orthogonality
much better to choose the x that minimizes an average error E in the m equations.,orthogonality
The most convenient “average” comes from the sum of squares:,orthogonality
E2 = (2x−b1)2 +(3x−b2)2 +(4x−b3)2.,orthogonality
"If there is an exact solution, the minimum error is E = 0. In the more likely case that b",orthogonality
"is not proportional to a, the graph of E2 will be a parabola. The minimum error is at the",orthogonality
"lowest point, where the derivative is zero:",orthogonality
"Solving for x, the least-squares solution of this model system ax = b is denoted by �x:",orthogonality
�x = 2b1 +3b2 +4b3,orthogonality
You recognize aTb in the numerator and aTa in the denominator.,orthogonality
The general case is the same. We “solve” ax = b by minimizing,orthogonality
E2 = ∥ax−b∥2 = (a1x−b1)2 +···+(amx−bm)2.,orthogonality
"The derivative of E2 is zero at the point �x, if",orthogonality
(a1�x−b1)a1 +···+(am�x−bm)am = 0.,orthogonality
"We are minimizing the distance from b to the line through a, and calculus gives the same",orthogonality
"answer, �x = (a1b1 +···+ambm)/(a2",orthogonality
"m), that geometry did earlier:",orthogonality
3.3 Projections and Least Squares,orthogonality
3K The least-squares solution to a problem ax = b in one unknown is �x = aTb,orthogonality
You see that we keep coming back to the geometrical interpretation of a least-squares,orthogonality
"problem—to minimize a distance. By setting the derivative of E2 to zero, calculus con-",orthogonality
ﬁrms the geometry of the previous section. The error vector e connecting b to p must be,orthogonality
Orthogonality of a and e,orthogonality
aT(b− �xa) = aTb− aTb,orthogonality
"As a side remark, notice the degenerate case a = 0. All multiples of a are zero, and",orthogonality
the line is only a point. Therefore p = 0 is the only candidate for the projection. But,orthogonality
"the formula for �x becomes a meaningless 0/0, and correctly reﬂects the fact that �x is",orthogonality
"completely undetermined. All values of x give the same error E = ∥0x − b∥, so E2 is",orthogonality
a horizontal line instead of a parabola. The “pseudoinverse” assigns the deﬁnite value,orthogonality
"�x = 0, which is a more “symmetric” choice than any other number.",orthogonality
Least Squares Problems with Several Variables,orthogonality
"Now we are ready for the serious step, to project b onto a subspace—rather than just",orthogonality
onto a line. This problem arises from Ax = b when A is an m by n matrix. Instead,orthogonality
"of one column and one unknown x, the matrix now has n columns. The number m of",orthogonality
"observations is still larger than the number n of unknowns, so it must be expected that",orthogonality
"Ax = b will be inconsistent. Probably, there will not exist a choice of x that perfectly",orthogonality
"ﬁts the data b. In other words, the vector b probably will not be a combination of the",orthogonality
columns of A; it will be outside the column space.,orthogonality
"Again the problem is to choose �x so as to minimize the error, and again this mini-",orthogonality
"mization will be done in the least-squares sense. The error is E = ∥Ax − b∥, and this",orthogonality
is exactly the distance from b to the point Ax in the column space. Searching for the,orthogonality
"least-squares solution �x, which minimizes E, is the same as locating the point p = A�x",orthogonality
that is closer to b than any other point in the column space.,orthogonality
"We may use geometry or calculus to determine �x. In n dimensions, we prefer the",orthogonality
appeal of geometry; p must be the “projection of b onto the column space.” The error,orthogonality
vector e = b − A�x must be perpendicular to that space (Figure 3.8). Finding �x and the,orthogonality
projection p = A�x is so fundamental that we do it in two ways:,orthogonality
1. All vectors perpendicular to the column space lie in the left nullspace. Thus the,orthogonality
error vector e = b−A�x must be in the nullspace of AT:,orthogonality
Figure 3.8: Projection onto the column space of a 3 by 2 matrix.,orthogonality
"2. The error vector must be perpendicular to each column a1,...,an of A:",orthogonality
"This is again AT(b − A�x) = 0 and ATA�x = ATb, The calculus way is to take partial",orthogonality
derivatives of E2 = (Ax − b)T(Ax − b). That gives the same 2ATAx − 2ATb = 0. The,orthogonality
fastest way is just to multiply the unsolvable equation Ax = b by AT. All these equivalent,orthogonality
methods produce a square coefﬁcient matrix ATA. It is symmetric (its transpose is not,orthogonality
AAT!) and it is the fundamental matrix of this chapter.,orthogonality
The equations ATA�x = ATb are known in statistics as the normal equations.,orthogonality
"When Ax = b is inconsistent, its least-squares solution minimizes ∥Ax −",orthogonality
ATA is invertible exactly when the columns of A are linearly independent!,orthogonality
The projection of b onto the column space is the nearest point A�x:,orthogonality
p = A�x = A(ATA)−1ATb.,orthogonality
We choose an example in which our intuition is as good as the formulas:,orthogonality
gives the best x.,orthogonality
3.3 Projections and Least Squares,orthogonality
"Both columns end with a zero, so C(A) is the x-y plane within three-dimensional space",orthogonality
"The projection of b = (4,5,6) is p = (4,5,0)—the x and y components stay the same",orthogonality
but z = 6 will disappear. That is conﬁrmed by solving the normal equations:,orthogonality
�x = (ATA)−1ATb =,orthogonality
p = A�x =,orthogonality
"In this special case, the best we can do is to solve the ﬁrst two equations of Ax = b. Then",orthogonality
�x1 = 2 and �x2 = 1. The error in the equation 0x1 +0x2 = 6 is sure to be 6.,orthogonality
Remark 4. Suppose b is actually in the column space of A—it is a combination b = Ax,orthogonality
of the columns. Then the projection of b is still b:,orthogonality
b in column space,orthogonality
p = A(ATA)−1ATAx = Ax = b.,orthogonality
The closest point p is just b itself—which is obvious.,orthogonality
"Remark 5. At the other extreme, suppose b is perpendicular to every column, so ATb =",orthogonality
0. In this case b projects to the zero vector:,orthogonality
b in left nullspace,orthogonality
p = A(ATA)−1ATb = A(ATA)−10 = 0.,orthogonality
"Remark 6. When A is square and invertible, the column space is the whole space. Every",orthogonality
"vector projects to itself, p equals b, and �x = x:",orthogonality
If A is invertible,orthogonality
p = A(ATA)−1ATb = AA−1(AT)−1ATb = b.,orthogonality
"This is the only case when we can take apart (ATA)−1, and write it as A−1(AT)−1. When",orthogonality
A is rectangular that is not possible.,orthogonality
"Remark 7. Suppose A has only one column, containing a. Then the matrix ATA is the",orthogonality
number aTa and �x is aTb/aTa. We return to the earlier formula.,orthogonality
The Cross-Product Matrix ATA,orthogonality
"The matrix ATA is certainly symmetric. Its transpose is (ATA)T = ATATT, which is ATA",orthogonality
"again. Its i, j entry (and j, i entry) is the inner product of column i of A with column j",orthogonality
"of A. The key question is the invertibility of ATA, and fortunately",orthogonality
ATA has the same nullspace as A.,orthogonality
Certainly if Ax = 0 then ATAx = 0. Vectors x in the nullspace of A are also in the,orthogonality
"nullspace of ATA. To go in the other direction, start by supposing that ATAx = 0, and",orthogonality
take the inner product with x to show that Ax = 0:,orthogonality
"The two nullspaces are identical. In particular, if A has independent columns (and only",orthogonality
"x = 0 is in its nullspace), then the same is true for ATA:",orthogonality
"3M If A has independent columns, then ATA is square, symmetric, and invert-",orthogonality
We show later that ATA is also positive deﬁnite (all pivots and eigenvalues are positive).,orthogonality
This case is by far the most common and most important. Independence is not so,orthogonality
hard in m-dimensional space if m > n. We assume it in what follows.,orthogonality
We have shown that the closest point to b is p = A(ATA)−1ATb. This formula expresses,orthogonality
in matrix terms the construction of a perpendicular line from b to the column space of,orthogonality
"A. The matrix that gives p is a projection matrix, denoted by P:",orthogonality
"This matrix projects any vector b onto the column space of A.1 In other words, p = Pb",orthogonality
"is the component of b in the column space, and the error e = b − Pb is the component",orthogonality
in the orthogonal complement. (I −P is also a projection matrix! It projects b onto the,orthogonality
"orthogonal complement, and the projection is b−Pb.)",orthogonality
"In short, we have a matrix formula for splitting any b into two perpendicular compo-",orthogonality
"nents. Pb is in the column space C(A), and the other component (I − P)b is in the left",orthogonality
nullspace N(AT)—which is orthogonal to the column space.,orthogonality
These projection matrices can be understood geometrically and algebraically.,orthogonality
The projection matrix P = A(ATA)−1AT has two basic properties:,orthogonality
(i) It equals its square: P2 = P.,orthogonality
(ii) It equals its transpose: PT = P.,orthogonality
"Conversely, any symmetric matrix with P2 = P represents a projection.",orthogonality
"1There may be a risk of confusion with permutation matrices, also denoted by P, but the risk should be small,",orthogonality
and we try never to let both appear on the same page.,orthogonality
3.3 Projections and Least Squares,orthogonality
"Proof. It is easy to see why P2 = P. If we start with any b, then Pb lies in the subspace",orthogonality
we are projecting onto. When we project again nothing is changed. The vector Pb is,orthogonality
"already in the subspace, and P(Pb) is still Pb. In other words P2 = P. Two or three or",orthogonality
ﬁfty projections give the same point p as the ﬁrst projection:,orthogonality
P2 = A(ATA)−1ATA(ATA)−1AT = A(ATA)−1AT = P.,orthogonality
"To prove that P is also symmetric, take its transpose. Multiply the transposes in",orthogonality
"reverse order, and use symmetry of (ATA)−1, to come back to P:",orthogonality
(ATA)−1�TAT = A(ATA)−1AT = P.,orthogonality
"For the converse, we have to deduce from P2 = P and PT = P that Pb is the projection",orthogonality
of b onto the column space of P. The error vector b − Pb is orthogonal to the space.,orthogonality
"For any vector Pc in the space, the inner product is zero:",orthogonality
(b−Pb)TPc = bT(I −P)TPc = bT(P−P2)c = 0.,orthogonality
"Thus b−Pb is orthogonal to the space, and Pb is the projection onto the column space.",orthogonality
"Example 1. Suppose A is actually invertible. If it is 4 by 4, then its four columns are",orthogonality
independent and its column space is all of R4. What is the projection onto the whole,orthogonality
space? It is the identity matrix.,orthogonality
P = A(ATA)−1AT = AA−1(AT)−1AT = I.,orthogonality
"The identity matrix is symmetric, I2 = I, and the error b−Ib is zero.",orthogonality
The point of all other examples is that what happened in equation (5) is not allowed.,orthogonality
To repeat: We cannot invert the separate parts AT and A when those matrices are rectan-,orthogonality
gular. It is the square matrix ATA that is invertible.,orthogonality
Least-Squares Fitting of Data,orthogonality
"Suppose we do a series of experiments, and expect the output b to be a linear function",orthogonality
of the input t. We look for a straight line b = C +Dt. For example:,orthogonality
1. At different times we measure the distance to a satellite on its way to Mars. In this,orthogonality
case t is the time and b is the distance. Unless the motor was left on or gravity is,orthogonality
"strong, the satellite should move with nearly constant velocity v: b = b0 +vt.",orthogonality
"2. We vary the load on a structure, and measure the movement it produces. In this",orthogonality
experiment t is the load and b is the reading from the strain gauge. Unless the load,orthogonality
"is so great that the material becomes plastic, a linear relation b = C +Dt is normal",orthogonality
in the theory of elasticity.,orthogonality
"3. The cost of producing t books like this one is nearly linear, b =C+Dt, with editing",orthogonality
and typesetting in C and then printing and binding in D. C is the set-up cost and D,orthogonality
is the cost for each additional book.,orthogonality
"How to compute C and D? If there is no experimental error, then two measurements",orthogonality
"of b will determine the line b = C + Dt. But if there is error, we must be prepared to",orthogonality
“average” the experiments and ﬁnd an optimal line. That line is not to be confused with,orthogonality
"the line through a on which b was projected in the previous section! In fact, since there",orthogonality
"are two unknowns C and D to be determined, we now project onto a two-dimensional",orthogonality
subspace. A perfect experiment would give a perfect C and D:,orthogonality
"This is an overdetermined system, with m equations and only two unknowns. If errors",orthogonality
"are present, it will have no solution. A has two columns, and x = (C,D):",orthogonality
"The best solution (�C, �D) is the �x that minimizes the squared error E2:",orthogonality
E2 = ∥b−Ax∥2 = (b1 −C −Dt1)2 +···+(bm −C −Dtm)2.,orthogonality
"The vector p = A�x is as close as possible to b. Of all straight lines b = C + Dt, we are",orthogonality
"choosing the one that best ﬁts the data (Figure 3.9). On the graph, the errors are the",orthogonality
vertical distances b−C −Dt to the straight line (not perpendicular distances!). It is the,orthogonality
"vertical distances that are squared, summed, and minimized.",orthogonality
"Example 2. Three measurements b1, b2, b3 are marked on Figure 3.9a:",orthogonality
"Note that the values t = −1,1,2 are not required to be equally spaced. The ﬁrst step is",orthogonality
to write the equations that would hold if a line could go through all three points. Then,orthogonality
every C +Dt would agree exactly with b:,orthogonality
C + 2D = 3,orthogonality
3.3 Projections and Least Squares,orthogonality
Figure 3.9: Straight-line approximation matches the projection p of b.,orthogonality
"If those equations Ax = b could be solved, there would be no errors. They can’t be solved",orthogonality
because the points are not on a line. Therefore they are solved by least squares:,orthogonality
The best solution is �C = 9,orthogonality
"7, �D = 4",orthogonality
7 and the best line is 9,orthogonality
Note the beautiful connections between the two ﬁgures. The problem is the same but,orthogonality
"the art shows it differently. In Figure 3.9b, b is not a combination of the columns (1,1,1)",orthogonality
"and (−1,1,2). In Figure 3.9, the three points are not on a line. Least squares replaces",orthogonality
"points b that are not on a line by points p that are! Unable to solve Ax = b, we solve",orthogonality
7t has heights 5,orthogonality
"7 at the measurement times −1, 1, 2. Those points",orthogonality
do lie on a line. Therefore the vector p = (5,orthogonality
7 ) is in the column space. This vector,orthogonality
is the projection. Figure 3.9b is in three dimensions (or m dimensions if there are m,orthogonality
points) and Figure 3.9a is in two dimensions (or n dimensions if there are n parameters).,orthogonality
"Subtracting p from b, the errors are e = (2",orthogonality
7). Those are the vertical errors in,orthogonality
"Figure 3.9a, and they are the components of the dashed vector in Figure 3.9b. This error",orthogonality
"vector is orthogonal to the ﬁrst column (1,1,1), since −2",orthogonality
7 = 0. It is orthogonal,orthogonality
"to the second column (−1,1,2), because −2",orthogonality
7 = 0. It is orthogonal to the column,orthogonality
"space, and it is in the left nullspace.",orthogonality
Question: If the measurements b = (2,orthogonality
"7) were those errors, what would be the",orthogonality
best line and the best �x? Answer: The zero line—which is the horizontal axis—and,orthogonality
�x = 0. Projection to zero.,orthogonality
We can quickly summarize the equations for ﬁtting by a straight line. The ﬁrst column,orthogonality
"of A contains 1s, and the second column contains the times ti. Therefore ATA contains",orthogonality
the sum of the 1s and the ti and the t2,orthogonality
"The measurements b1,...,bm are given at distinct points t1,...,tm. Then",orthogonality
the straight line �C + �Dt which minimizes E2 comes from least squares:,orthogonality
Remark. The mathematics of least squares is not limited to ﬁtting the data by straight,orthogonality
"lines. In many experiments there is no reason to expect a linear relationship, and it",orthogonality
"would be crazy to look for one. Suppose we are handed some radioactive material, The",orthogonality
output b will be the reading on a Geiger counter at various times t. We may know that,orthogonality
"we are holding a mixture of two chemicals, and we may know their half-lives (or rates",orthogonality
"of decay), but we do not know how much of each is in our hands. If these two unknown",orthogonality
"amounts are C and D, then the Geiger counter readings would behave like the sum of",orthogonality
two exponentials (and not like a straight line):,orthogonality
b = Ce−λt +De−µt.,orthogonality
"In practice, the Geiger counter is not exact. Instead, we make readings b1,...,bm at",orthogonality
"times t1,...,tm, and equation (8) is approximately satisﬁed:",orthogonality
"If there are more than two readings, m > 2, then in all likelihood we cannot solve for",orthogonality
C and D. But the least-squares principle will give optimal values �C and �D.,orthogonality
"The situation would be completely different if we knew the amounts C and D, and",orthogonality
were trying to discover the decay rates λ and µ. This is a problem in nonlinear least,orthogonality
"squares, and it is harder. We would still form E2, the sum of the squares of the errors,",orthogonality
and minimize it. But setting its derivatives to zero will not give linear equations for the,orthogonality
"optimal λ and µ. In the exercises, we stay with linear least squares.",orthogonality
A simple least-squares problem is the estimate �x of a patient’s weight from two obser-,orthogonality
"vations x = b1 and x = b2. Unless b1 = b2, we are faced with an inconsistent system of",orthogonality
two equations in one unknown:,orthogonality
"Up to now, we accepted b1 and b2 as equally reliable. We looked for the value �x that",orthogonality
minimized E2 = (x−b1)2 +(x−b2)2:,orthogonality
�x = b1 +b2,orthogonality
3.3 Projections and Least Squares,orthogonality
The optimal �x is the average. The same conclusion comes from ATA�x = ATb. In fact,orthogonality
"ATA is a 1 by 1 matrix, and the normal equation is 2�x = b1 +b2.",orthogonality
Now suppose the two observations are not trusted to the same degree. The value,orthogonality
"x = b1 may be obtained from a more accurate scale—or, in a statistical problem, from a",orthogonality
"larger sample—than x = b2. Nevertheless, if b2 contains some information, we are not",orthogonality
willing to rely totally on b1. The simplest compromise is to attach different weights w2,orthogonality
"2, and choose the �xW that minimizes the weighted sum of squares:",orthogonality
"If w1 > w2, more importance is attached to b1. The minimizing process (derivative = 0)",orthogonality
tries harder to make (x−b1)2 small:,orthogonality
"Instead of the average of b1 and b2 (for w1 = w2 = 1), �xW is a weighted average of the",orthogonality
data. This average is closer to b1 than to b2.,orthogonality
The ordinary least-squares problem leading to �xW comes from changing Ax = b to,orthogonality
the new system WAx = Wb. This changes the solution from �x to �xW. The matrix W TW,orthogonality
turns up on both sides of the weighted normal equations:,orthogonality
The least squares solution to WAx = Wb is �xW:,orthogonality
(ATW TWA)�xW = ATW TWb.,orthogonality
What happens to the picture of b projected to A�x? The projection A�xW is still the,orthogonality
point in the column space that is closest to b. But the word “closest” has a new meaning,orthogonality
when the length involves W. The weighted length of x equals the ordinary length of Wx.,orthogonality
Perpendicularity no longer means yTx = 0; in the new system the test is (Wy)T(Wx) = 0.,orthogonality
"The matrix W TW appears in the middle. In this new sense, the projection A�xW and the",orthogonality
error b−A�xW are again perpendicular.,orthogonality
That last paragraph describes all inner products: They come from invertible matrices,orthogonality
W. They involve only the symmetric combination C = W TW. The inner product of x,orthogonality
"and y is yTCx. For an orthogonal matrix W = Q, when this combination is C = QTQ = I,",orthogonality
the inner product is not new or different. Rotating the space leaves the inner product,orthogonality
unchanged. Every other W changes the length and inner product.,orthogonality
"For any invertible matrix W, these rules deﬁne a new inner product and length:",orthogonality
"Since W is invertible, no vector is assigned length zero (except the zero vector). All",orthogonality
possible inner products—which depend linearly on x and y and are positive when x =,orthogonality
"y ̸= 0—are found in this way, from some matrix C = W TW.",orthogonality
"In practice, the important question is the choice of C. The best answer comes from",orthogonality
"statisticians, and originally from Gauss. We may know that the average error is zero.",orthogonality
That is the “expected value” of the error in b—although the error is not really expected,orthogonality
to be zero! We may also know the average of the square of the error; that is the variance.,orthogonality
"If the errors in the bi are independent of each other, and their variances are σ2",orthogonality
"i , then the",orthogonality
"right weights are wi = 1/σi. A more accurate measurement, which means a smaller",orthogonality
"variance, gets a heavier weight.",orthogonality
"In addition to unequal reliability, the observations may not be independent. If the",orthogonality
"errors are coupled—the polls for President are not independent of those for Senator,",orthogonality
and certainly not of those for Vice-President—then W has off-diagonal terms. The best,orthogonality
"unbiased matrix C = W TW is the inverse of the covariance matrix—whose i, j entry is",orthogonality
the expected value of (error in bi) times (error in b j). Then the main diagonal of C−1,orthogonality
contains the variances σ2,orthogonality
"i , which are the average of (error in bi)2.",orthogonality
Example 3. Suppose two bridge partners both guess (after the bidding) the total num-,orthogonality
"ber of spades they hold. For each guess, the errors −1, 0, 1 might have equal probability",orthogonality
3. Then the expected error is zero and the variance is 2,orthogonality
"The two guesses are dependent, because they are based on the same bidding—but not",orthogonality
"identical, because they are looking at different hands. Say the chance that they are",orthogonality
"both too high or both too low is zero, but the chance of opposite errors is 1",orthogonality
"3(−1), and the inverse of the covariance matrix is W TW:",orthogonality
= C = W TW.,orthogonality
This matrix goes into the middle of the weighted normal equations.,orthogonality
"1. Find the best least-squares solution �x to 3x = 10, 4x = 5. What error E2 is minimized?",orthogonality
"Check that the error vector (10−3�x,5−4�x) is perpendicular to the column (3,4).",orthogonality
2. Suppose the values b1 = 1 and b2 = 7 at times t1 = 1 and t2 = 2 are ﬁtted by a line,orthogonality
"b = Dt through the origin. Solve D = 1 and 2D = 7 by least squares, and sketch the",orthogonality
3.3 Projections and Least Squares,orthogonality
"3. Solve Ax = b by least squares, and ﬁnd p = A�x if",orthogonality
Verify that the error b− p is perpendicular to the columns of A.,orthogonality
"4. Write out E2 = ∥Ax−b∥2 and set to zero its derivatives with respect to u and v, if",orthogonality
"Compare the resulting equations with ATA�x = ATb, conﬁrming that calculus as well",orthogonality
as geometry gives the normal equations. Find the solution �x and the projection p =,orthogonality
A�x. Why is p = b?,orthogonality
5. The following system has no solution:,orthogonality
Sketch and solve a straight-line ﬁt that leads to the minimization of the quadratic,orthogonality
(C−D−4)2 +(C−5)2 +(C+D−9)2? What is the projection of b onto the column,orthogonality
6. Find the projection of b onto the column space of A:,orthogonality
"Split b into p + q, with p in the column space and q perpendicular to that space.",orthogonality
Which of the four subspaces contains q?,orthogonality
"7. Find the projection matrix P onto the space spanned by a1 = (1,0,1) and a2 =",orthogonality
"8. If P is the projection matrix onto a k-dimensional subspace S of the whole space Rn,",orthogonality
what is the column space of P and what is its rank?,orthogonality
"9. (a) If P = PTP, show that P is a projection matrix.",orthogonality
(b) What subspace does the matrix P = 0 project onto?,orthogonality
"10. If the vectors a1, a2, and b are orthogonal, what are ATA and ATb? What is the",orthogonality
projection of b onto the plane of a1 and a2?,orthogonality
11. Suppose P is the projection matrix onto the subspace S and Q is the projection onto,orthogonality
the orthogonal complement S⊥. What are P+Q and PQ? Show that P−Q is its own,orthogonality
"12. If V is the subspace spanned by (1,1,0,1) and (0,0,1,0), ﬁnd",orthogonality
(a) a basis for the orthogonal complement V⊥.,orthogonality
(b) the projection matrix P onto V.,orthogonality
"(c) the vector in V closest to the vector b = (0,1,0,−1) in V⊥.",orthogonality
13. Find the best straight-line ﬁt (least squares) to the measurements,orthogonality
"Then ﬁnd the projection of b = (4,3,1,0) onto the column space of",orthogonality
"14. The vectors a1 = (1,1,0) and a2 = (1,1,1) span a plane in R3. Find the projection",orthogonality
"matrix P onto the plane, and ﬁnd a nonzero vector b that is projected to zero.",orthogonality
"15. If P is the projection matrix onto a line in the x-y plane, draw a ﬁgure to describe",orthogonality
the effect of the “reﬂection matrix” H = I − 2P. Explain both geometrically and,orthogonality
algebraically why H2 = I.,orthogonality
"16. Show that if u has unit length, then the rank-1 matrix P = uuT is a projection matrix:",orthogonality
"It has properties (i) and (ii) in 3N. By choosing u = a/∥a∥, P becomes the projection",orthogonality
"onto the line through a, and Pb is the point p = �xa. Rank-1 projections correspond",orthogonality
exactly to least-squares problems in one unknown.,orthogonality
17. What 2 by 2 matrix projects the x-y plane onto the −45° line x+y = 0?,orthogonality
18. We want to ﬁt a plane y = C +Dt +Ez to the four points,orthogonality
"t = 1,z = 1",orthogonality
"t = 0,z = 3",orthogonality
"t = 2,z = 1",orthogonality
"t = 0,z = 0.",orthogonality
(a) Find 4 equations in 3 unknowns to pass a plane through the points (if there is,orthogonality
(b) Find 3 equations in 3 unknowns for the best least-squares solution.,orthogonality
"19. If PC = A(ATA)−1AT is the projection onto the column space of A, what is the pro-",orthogonality
jection PR onto the row space? (It is not PT,orthogonality
3.3 Projections and Least Squares,orthogonality
"20. If P is the projection onto the column space of A, what is the projection onto the left",orthogonality
21. Suppose L1 is the line through the origin in the direction of a1 and L2 is the line,orthogonality
through b in the direction of a2. To ﬁnd the closest points x1a1 and b + x2a2 on the,orthogonality
"two lines, write the two equations for the x1 and x2 that minimize ∥x1a1 −x2a2 −b∥.",orthogonality
"Solve for x if a1 = (1,1,0), a2 = (0,1,0), b = (2,1,4).",orthogonality
"22. Find the best line C +Dt to ﬁt b = 4,2,−1,0,0 at times t = −2,−1,0,1,2.",orthogonality
"23. Show that the best least-squares ﬁt to a set of measurements y1,...,ym by a horizontal",orthogonality
line (a constant function y = C) is their average,orthogonality
C = y1 +···+ym,orthogonality
"24. Find the best straight-line ﬁt to the following measurements, and sketch your solu-",orthogonality
"25. Suppose that instead of a straight line, we ﬁt the data in Problem 24 by a parabola:",orthogonality
y = C +Dt +Et2. In the inconsistent system Ax = b that comes from the four mea-,orthogonality
"surements, what are the coefﬁcient matrix A, the unknown vector x, and the data",orthogonality
vector b? You need not compute �x.,orthogonality
"26. A Middle-Aged man was stretched on a rack to lengths L = 5, 6, and 7 feet under",orthogonality
"applied forces of F = 1, 2, and 4 tons. Assuming Hooke’s law L = a+bF, ﬁnd his",orthogonality
normal length a by least squares.,orthogonality
Problems 27–31 introduce basic ideas of statistics—the foundation for least,orthogonality
"27. (Recommended) This problem projects b = (b1,...,bm) onto the line through a =",orthogonality
"(1,...,1). We solve m equations ax = b in 1 unknown (by least squares).",orthogonality
"(a) Solve aTa�x = aTb to show that is the mean (the average) of the b’s,",orthogonality
"(b) Find e = b−a�x, the variance ∥e∥2, and the standard deviation ∥e∥.",orthogonality
"(c) The horizontal line �b = 3 is closest to b = (1,2,6), Check that p = (3,3,3) is",orthogonality
perpendicular to e and ﬁnd the projection matrix P.,orthogonality
28. First assumption behind least squares: Each measurement error has mean zero. Mul-,orthogonality
"tiply the 8 error vectors b − Ax = (±1,±1,±1) by (ATA)−1AT to show that the 8",orthogonality
vectors �x−x also average to zero. The estimate �x is unbiased.,orthogonality
29. Second assumption behind least squares: The m errors ei are independent with,orthogonality
"variance σ2, so the average of (b − Ax)(b − Ax)T is σ2I. Multiply on the left by",orthogonality
(ATA)−1AT and on the right by A(ATA)−1 to show that the average of (�x−x)(�x−x)T,orthogonality
is σ2(ATA)−1. This is the all-important covariance matrix for the error in �x.,orthogonality
"30. A doctor takes four readings of your heart rate. The best solution to x = b1,...,x = b4",orthogonality
"is the average �x of b1,...,b4. The matrix A is a column of 1s. Problem 29 gives the",orthogonality
expected error (�x−x)2 as σ2(ATA)−1 =,orthogonality
". By averaging, the variance drops from",orthogonality
"31. If you know the average �x9 of 9 numbers b1,...,b9, how can you quickly ﬁnd the",orthogonality
average �x10 with one more number b10? The idea of recursive least squares is to,orthogonality
avoid adding 10 numbers. What coefﬁcient of �x9 correctly gives �x10?,orthogonality
"Problems 32–37 use four points b = (0,8,8,20) to bring out more ideas.",orthogonality
"32. With b = 0,8,8,20 at t = 0,1,3,4, set up and solve the normal equations ATA�x = ATb.",orthogonality
"For the best straight line as in Figure 3.9a, ﬁnd its four heights pi and four errors ei.",orthogonality
What is the minimum value E2 = e2,orthogonality
"33. (Line C + Dt does go through p’s) With b = 0,8,8,20 at times t = 0,1,3,4, write",orthogonality
"the four equations Ax = b (unsolvable). Change the measurements to p = 1,5,13,17",orthogonality
and ﬁnd an exact solution to A�x = p.,orthogonality
"34. Check that e = b − p = (−1,3,−5,3) is perpendicular to both columns of A. What",orthogonality
is the shortest distance ∥e∥ from b to the column space of A?,orthogonality
"35. For the closest parabola b = C +Dt +Et2 to the same four points, write the unsolv-",orthogonality
"able equations Ax = b in three unknowns x = (C,D,E). Set up the three normal",orthogonality
equations ATA�x = ATb (solution not required). You are now ﬁtting a parabola to four,orthogonality
points—what is happening in Figure 3.9b?,orthogonality
"36. For the closest cubic b = C +Dt +Et2 +Ft3 to the same four points, write the four",orthogonality
"equations Ax = b. Solve them by elimination, This cubic now goes exactly through",orthogonality
the points. What are p and e?,orthogonality
37. The average of the four times is �t = 1,orthogonality
4(0+1+3+4) = 2. The average of the four b’s,orthogonality
is �b = 1,orthogonality
"(a) Verify that the best line goes through the center point (�t,�b) = (2,9).",orthogonality
(b) Explain why C +D�t = �b comes from the ﬁrst equation in ATA�x = ATb.,orthogonality
38. What happens to the weighted average �xW = (w2,orthogonality
2) if the ﬁrst,orthogonality
weight w1 approaches zero? The measurement b1 is totally unreliable.,orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"39. From m independent measurements b1,...,bm of your pulse rate, weighted by w1,...,wm,",orthogonality
what is the weighted average that replaces equation (9)? It is the best estimate when,orthogonality
the statistical variances are σ2,orthogonality
40. If W =,orthogonality
", ﬁnd the W-inner product of x = (2,3) and y = (1,1), and the W-length",orthogonality
of x. What line of vectors is W-perpendicular to y?,orthogonality
41. Find the weighted least-squares solution �xW to Ax = b:,orthogonality
Check that the projection A�xW is still perpendicular (in the W-inner product!) to the,orthogonality
"42. (a) Suppose you guess your professor’s age, making errors e = −2,−1,5 with prob-",orthogonality
4. Check that the expected error E(e) is zero and ﬁnd the variance,orthogonality
"(b) If the professor guesses too (or tries to remember), making errors −1, 0, 1 with",orthogonality
"8, what weights w1 and w2 give the reliability of your guess",orthogonality
and the professor’s guess?,orthogonality
Orthogonal Bases and Gram-Schmidt,orthogonality
"In an orthogonal basis, every vector is perpendicular to every other vector. The coor-",orthogonality
"dinate axes are mutually orthogonal. That is just about optimal, and the one possible",orthogonality
"improvement is easy: Divide each vector by its length, to make it a unit vector. That",orthogonality
changes an orthogonal basis into an orthonormal basis of q’s:,orthogonality
"The vectors q1,...,qn are orthonormal if",orthogonality
i q j =,orthogonality
"whenever i ̸= j,",orthogonality
"whenever i = j,",orthogonality
A matrix with orthonormal columns will be called Q.,orthogonality
"The most important example is the standard basis. For the x-y plane, the best-known",orthogonality
"axes e1 = (1,0) and e2 = (0,1) are not only perpendicular but horizontal and vertical. Q",orthogonality
"is the 2 by 2 identity matrix. In n dimensions the standard basis e1,...,en again consists",orthogonality
of the columns of Q = I:,orthogonality
That is not the only orthonormal basis! We can rotate the axes without changing the,orthogonality
right angles at which they meet. These rotation matrices will be examples of Q.,orthogonality
"If we have a subspace of Rn, the standard vectors ei might not lie in that subspace.",orthogonality
"But the subspace always has an orthonormal basis, and it can be constructed in a simple",orthogonality
"way out of any basis whatsoever. This construction, which converts a skewed set of axes",orthogonality
"into a perpendicular set, is known as Gram-Schmidt orthogonalization.",orthogonality
"To summarize, the three topics basic to this section are:",orthogonality
1. The deﬁnition and properties of orthogonal matrices Q.,orthogonality
"2. The solution of Qx = b, either n by n or rectangular (least squares).",orthogonality
3. The Gram-Schmidt process and its interpretation as a new factorization A = QR.,orthogonality
"If Q (square or rectangular) has orthonormal columns, then QTQ = I:",orthogonality
1 0 · 0,orthogonality
0 1 · 0,orthogonality
0 0 · 1,orthogonality
An orthogonal matrix is a square matrix with orthonormal columns.2 Then,orthogonality
"QT is Q−1. For square orthogonal matrices, the transpose is the inverse.",orthogonality
"When row i of QT multiplies column j of Q, the result is qT",orthogonality
j q j = 0. On the diagonal,orthogonality
"where i = j, we have qT",orthogonality
i qi = 1. That is the normalization to unit vectors of length 1.,orthogonality
Note that QTQ = I even if Q is rectangular. But then QT is only a left-inverse.,orthogonality
QT = Q−1 =,orthogonality
"2Orthonormal matrix would have been a better name, but it is too late to change. Also, there is no accepted word",orthogonality
"for a rectangular matrix with orthonormal columns. We still write Q, but we won’t call it an “orthogonal matrix”",orthogonality
unless it is square.,orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"Q rotates every vector through the angle θ, and QT rotates it back through −θ. The",orthogonality
"columns are clearly orthogonal, and they are orthonormal because sin2θ + cos2θ = 1.",orthogonality
The matrix QT is just as much an orthogonal matrix as Q.,orthogonality
Example 2. Any permutation matrix P is an orthogonal matrix. The columns are cer-,orthogonality
tainly unit vectors and certainly orthogonal—because the 1 appears in a different place,orthogonality
in each column: The transpose is the inverse.,orthogonality
P−1 = PT =,orthogonality
"An anti-diagonal P, with P13 = P22 = P31 = I, takes the x-y-z axes into the z-y-x axes—",orthogonality
a “right-handed” system into a “left-handed” system. So we were wrong if we suggested,orthogonality
that every orthogonal Q represents a rotation. A reﬂection is also allowed. P =,orthogonality
"reﬂects every point (x,y) into (y,x), its mirror image across the 45° line. Geometrically,",orthogonality
an orthogonal Q is the product of a rotation and a reﬂection.,orthogonality
"There does remain one property that is shared by rotations and reﬂections, and in fact",orthogonality
"by every orthogonal matrix. It is not shared by projections, which are not orthogonal or",orthogonality
"even invertible. Projections reduce the length of a vector, whereas orthogonal matrices",orthogonality
have a property that is the most important and most characteristic of all:,orthogonality
Multiplication by any Q preserves lengths:,orthogonality
for every vector x.,orthogonality
"It also preserves inner products and angles, since (Qx)T(Qy) = xTQTQy = xTy.",orthogonality
The preservation of lengths comes directly from QTQ = I:,orthogonality
(Qx)T(Qx) = xTQTQx = xTx.,orthogonality
"All inner products and lengths are preserved, when the space is rotated or reﬂected.",orthogonality
We come now to the calculation that uses the special property QT = Q−1. If we have a,orthogonality
"basis, then any vector is a combination of the basis vectors. This is exceptionally simple",orthogonality
"for an orthonormal basis, which will be a key idea behind Fourier series. The problem",orthogonality
is to ﬁnd the coefﬁcients of the basis vectors:,orthogonality
Write b as a combination b = x1q1 +x2q2 +···+xnqn.,orthogonality
To compute x1 there is a neat trick. Multiply both sides of the equation by qT,orthogonality
left-hand side is qT,orthogonality
1b. On the right-hand side all terms disappear (because qT,orthogonality
1q j = 0),orthogonality
except the ﬁrst term. We are left with,orthogonality
"1q1 = 1, we have found x1 = qT",orthogonality
1b. Similarly the second coefﬁcient is x2 = qT,orthogonality
that term survives when we multiply by qT,orthogonality
2. The other terms die of orthogonality. Each,orthogonality
"piece of b has a simple formula, and recombining the pieces gives back b:",orthogonality
Every vector b is equal to (qT,orthogonality
I can’t resist putting this orthonormal basis into a square matrix Q. The vector equa-,orthogonality
tion x1q1 +···+xnqn = b is identical to Qx = b. (The columns of Q multiply the compo-,orthogonality
nents of x.) Its solution is x = Q−1b. But since Q−1 = QT—this is where orthonormality,orthogonality
enters—the solution is also x = QTb:,orthogonality
x = QTb =,orthogonality
The components of x are the inner products qT,orthogonality
"i b, as in equation (4).",orthogonality
The matrix form also shows what happens when the columns are not orthonormal.,orthogonality
Expressing b as a combination x1a1 +···+xnan is the same as solving Ax = b. The basis,orthogonality
"vectors go into the columns of A. In that case we need A−1, which takes work. In the",orthogonality
orthonormal case we only need QT.,orthogonality
"Remark 1. The ratio aTb/aTa appeared earlier, when we projected b onto a line. Here a",orthogonality
"is q1, the denominator is 1, and the projection is (qT",orthogonality
1b)q1. Thus we have a new interpre-,orthogonality
tation for formula (4): Every vector b is the sum of its one-dimensional projections onto,orthogonality
the lines through the q’s.,orthogonality
"Since those projections are orthogonal, Pythagoras should still be correct. The square",orthogonality
of the hypotenuse should still be the sum of squares of the components:,orthogonality
"Remark 2. Since QT = Q−1, we also have QQT = I. When Q comes before QT, mul-",orthogonality
tiplication takes the inner products of the rows of Q. (For QTQ it was the columns.),orthogonality
"Since the result is again the identity matrix, we come to a surprising conclusion: The",orthogonality
rows of a square matrix are orthonormal whenever the columns are. The rows point,orthogonality
"in completely different directions from the columns, and I don’t see geometrically why",orthogonality
they are forced to be orthonormal—but they are.,orthogonality
Rectangular Matrices with Orthogonal Columns,orthogonality
"This chapter is about Ax = b, when A is not necessarily square. For Qx = b we now",orthogonality
admit the same possibility—there may be more rows than columns. The n orthonormal,orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
vectors qi in the columns of Q have m > n components. Then Q is an m by n matrix and,orthogonality
we cannot expect to solve Qx = b exactly. We solve it by least squares.,orthogonality
"If there is any justice, orthonormal columns should make the problem simple. It",orthogonality
"worked for square matrices, and now it will work for rectangular matrices. The key is to",orthogonality
notice that we still have QTQ = I. So QT is still the left-inverse of Q.,orthogonality
For least squares that is all we need. The normal equations came from multiplying,orthogonality
"Ax = b by the transpose matrix, to give ATA�x = ATb. Now the normal equations are",orthogonality
"QTQ = QTb. But QTQ is the identity matrix! Therefore �x = QTb, whether Q is square",orthogonality
"and �x is an exact solution, or Q is rectangular and we need least squares.",orthogonality
"If Q has orthonormal columns, the least-squares problem becomes easy:",orthogonality
rectangular system with no solution for most b.,orthogonality
rectangular system with no solution for most b.,orthogonality
normal equation for the best �x—in which QTQ = I.,orthogonality
the projection of b is (qT,orthogonality
the projection matrix is P = QQT.,orthogonality
The last formulas are like p = A�x and P = A(ATA)−1AT. When the columns are orthonor-,orthogonality
"mal, the “cross-product matrix” ATA becomes QTQ = I. The hard part of least squares",orthogonality
"disappears when vectors are orthonormal. The projections onto the axes are uncoupled,",orthogonality
and p is the sum p = (qT,orthogonality
"We emphasize that those projections do not reconstruct b. In the square case m = n,",orthogonality
"they did. In the rectangular case m > n, they don’t. They give the projection p and",orthogonality
not the original vector b—which is all we can expect when there are more equations,orthogonality
"than unknowns, and the q’s are no longer a basis. The projection matrix is usually",orthogonality
"A(ATA)−1AT, and here it simpliﬁes to",orthogonality
"Notice that QTQ is the n by n identity matrix, whereas QQT is an m by m projection P.",orthogonality
"It is the identity matrix on the columns of Q (P leaves them alone), But QQT is the zero",orthogonality
matrix on the orthogonal complement (the nullspace of QT).,orthogonality
Example 3. The following case is simple but typical. Suppose we project a point,orthogonality
"b = (x,y,z) onto the x-y plane. Its projection is p = (x,y,0), and this is the sum of the",orthogonality
separate projections onto the x- and y-axes:,orthogonality
The overall projection matrix is,orthogonality
Projection onto a plane = sum of projections onto orthonormal q1 and q2.,orthogonality
"Example 4. When the measurement times average to zero, ﬁtting a straight line leads to",orthogonality
"orthogonal columns. Take t1 = −3, t2 = 0, and t3 = 3. Then the attempt to ﬁt y = C+Dt",orthogonality
leads to three equations in two unknowns:,orthogonality
"The columns (1,1,1) and (−3,0,3) are orthogonal. We can project y separately onto",orthogonality
"each column, and the best coefﬁcients �C and �D can be found separately:",orthogonality
Notice that �C = (y1 + y2 + y3)/3 is the mean of the data.,orthogonality
�C gives the best ﬁt by a,orthogonality
"horizontal line, whereas �Dt is the best ﬁt by a straight line through the origin. The",orthogonality
"columns are orthogonal, so the sum of these two separate pieces is the best ﬁt by any",orthogonality
"straight line whatsoever. The columns are not unit vectors, so �C and �D have the length",orthogonality
squared in the denominator.,orthogonality
Orthogonal columns are so much better that it is worth changing to that case. if the,orthogonality
average of the observation times is not zero—it is ¯t = (t1 + ··· +tm)/m—then the time,orthogonality
origin can be shifted by ¯t. Instead of y = C+Dt we work with y = c+d(t − ¯t). The best,orthogonality
"line is the same! As in the example, we ﬁnd",orthogonality
(t1 − ¯t) ···,orthogonality
(t1 − ¯t)2 +···+(tm − ¯t)2,orthogonality
= ∑(ti − ¯t)yi,orthogonality
∑(ti − ¯t)2 .,orthogonality
"The best �c is the mean, and we also get a convenient formula for �d. The earlier ATA",orthogonality
"had the off-diagonal entries ∑ti, and shifting the time by ¯t made these entries zero. This",orthogonality
"shift is an example of the Gram-Schmidt process, which orthogonalizes the situation in",orthogonality
"Orthogonal matrices are crucial to numerical linear algebra, because they introduce",orthogonality
"no instability. While lengths stay the same, roundoff is under control. Orthogonalizing",orthogonality
vectors has become an essential technique. Probably it comes second only to elimina-,orthogonality
tion. And it leads to a factorization A = QR that is nearly as famous as A = LU.,orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"Suppose you are given three independent vectors a, b, c. If they are orthonormal, life is",orthogonality
"easy. To project a vector v onto the ﬁrst one, you compute (aTv)a. To project the same",orthogonality
"vector v onto the plane of the ﬁrst two, you just add (aTv)a + (bTv)b. To project onto",orthogonality
"the span of a, b, c, you add three projections. All calculations require only the inner",orthogonality
"products aTv, bTv, and cTv. But to make this true, we are forced to say, “If they are",orthogonality
orthonormal.” Now we propose to ﬁnd a way to make them orthonormal.,orthogonality
"The method is simple. We are given a, b, c and we want q1, q2, q3. There is no",orthogonality
"problem with q1: it can go in the direction of a. We divide by the length, so that q1 =",orthogonality
a/∥a∥ is a unit vector. The real problem begins with q2—which has to be orthogonal,orthogonality
to q1. If the second vector b has any component in the direction of q1 (which is the,orthogonality
"direction of a), that component has to be subtracted:",orthogonality
"B is orthogonal to q1. It is the part of b that goes in a new direction, and not in the a. In",orthogonality
"Figure 3.10, B is perpendicular to q1. It sets the direction for q2.",orthogonality
Figure 3.10: The qi component of b is removed; a and B normalized to q1 and q2.,orthogonality
At this point q1 and q2 are set. The third orthogonal direction starts with c. It will,orthogonality
"not be in the plane of q1 and q2, which is the plane of a and b. However, it may have a",orthogonality
"component in that plane, and that has to be subtracted. (If the result is C = 0, this signals",orthogonality
"that a, b, c were not independent in the ﬁrst place) What is left is the component C we",orthogonality
"want, the part that is in a new direction perpendicular to the plane:",orthogonality
"This is the one idea of the whole Gram-Schmidt process, to subtract from every new",orthogonality
vector its components in the directions that are already settled. That idea is used over,orthogonality
"and over again.3 When there is a fourth vector, we subtract away its components in the",orthogonality
"directions of q1, q2, q3.",orthogonality
"3If Gram thought of it ﬁrst, what was left for Schmidt?",orthogonality
"Example 5. Gram-Schmidt Suppose the independent vectors are a, b, c:",orthogonality
"To ﬁnd q1, make the ﬁrst vector into a unit vector: q1 = a/",orthogonality
"2. To ﬁnd q2, subtract from",orthogonality
the second vector its component in the ﬁrst direction:,orthogonality
"The normalized q2 is B divided by its length, to produce a unit vector:",orthogonality
"To ﬁnd q3, subtract from c its components along q1 and q2:",orthogonality
"This is already a unit vector, so it is q3. I went to desperate lengths to cut down the num-",orthogonality
ber of square roots (the painful part of Gram-Schmidt). The result is a set of orthonormal,orthogonality
"vectors q1, q2, q3, which go into the columns of an orthogonal matrix Q:",orthogonality
"3T The Gram-Schmidt process starts with independent vectors a1,...,an and",orthogonality
"ends with orthonormal vectors q1,...,qn. At step j it subtracts from a j its",orthogonality
"components in the directions q1,...,q j−1 that are already settled:",orthogonality
A j = a j −(qT,orthogonality
Then q j is the unit vector A j/∥Aj∥.,orthogonality
"Remark on the calculations I think it is easier to compute the orthogonal a, B, C,",orthogonality
"without forcing their lengths to equal one. Then square roots enter only at the end, when",orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"dividing by those lengths. The example above would have the same B and C, without",orthogonality
using square roots. Notice the 1,orthogonality
2 from aTb/aTa instead of,orthogonality
The Factorization A = QR,orthogonality
"We started with a matrix A, whose columns were a, b, c. We ended with a matrix Q,",orthogonality
"whose columns are q1, q2, q3. What is the relation between those matrices? The matrices",orthogonality
"A and Q are m by n when the n vectors are in m-dimensional space, and there has to be",orthogonality
a third matrix that connects them.,orthogonality
The idea is to write the a’s as combinations of the q’s. The vector b in Figure 3.10 is,orthogonality
"a combination of the orthonormal q1 and q2, and we know what combination it is:",orthogonality
Every vector in the plane is the sum of its q1 and q2 components. Similarly c is the sum,orthogonality
"of its q1, q2, q3 components: c = (qT",orthogonality
3c)q3. If we express that in,orthogonality
matrix form we have the new factorization A = QR:,orthogonality
Notice the zeros in the last matrix! R is upper triangular because of the way Gram-,orthogonality
"Schmidt was done. The ﬁrst vectors a and q1 fell on the same line. Then q1, q2 were in",orthogonality
"the same plane as a, b. The third vectors c and q3 were not involved until step 3.",orthogonality
"The QR factorization is like A = LU, except that the ﬁrst factor Q has orthonormal",orthogonality
"columns. The second factor is called R, because the nonzeros are to the right of the di-",orthogonality
agonal (and the letter U is already taken). The off-diagonal entries of R are the numbers,orthogonality
"2, found above. The whole factorization is",orthogonality
"You see the lengths of a, B, C on the diagonal of R. The orthonormal vectors q1, q2, q3,",orthogonality
"which are the whole object of orthogonalization, are in the ﬁrst factor Q.",orthogonality
Maybe QR is not as beautiful as LU (because of the square roots). Both factoriza-,orthogonality
"tions are vitally important to the theory of linear algebra, and absolutely central to the",orthogonality
"calculations. If LU is Hertz, then QR is Avis.",orthogonality
The entries rij = qT,orthogonality
"i a j appear in formula (11), when ∥A j∥q j is substituted for Aj:",orthogonality
a j = (qT,orthogonality
j−1a j)q j−1 +∥A j∥q j = Q times column j of R.,orthogonality
Every m by n matrix with independent columns can be factored into,orthogonality
"A = QR. The columns of Q are orthonormal, and R is upper triangular and",orthogonality
"invertible. When m = n and all matrices are square, Q becomes an orthogonal",orthogonality
I must not forget the main point of orthogonalization. It simpliﬁes the least-squares,orthogonality
"problem Ax = b. The normal equations are still correct, but ATA becomes easier:",orthogonality
ATA = RTQTQR = RTR.,orthogonality
The fundamental equation ATA�x = ATb simpliﬁes to a triangular system:,orthogonality
"Instead of solving QRx = b, which can’t be done, we solve R�x = QTb which is just",orthogonality
back-substitution because R is triangular. The real cost is the mn2 operations of Gram-,orthogonality
"Schmidt, which are needed to ﬁnd Q and R in the ﬁrst place.",orthogonality
"The same idea of orthogonality applies to functions, The sines and cosines are or-",orthogonality
"thogonal; the powers 1, x, x2 are not. When f(x) is written as a combination of sines and",orthogonality
"cosines, that is a Fourier series. Each term is a projection onto a line—the line in func-",orthogonality
tion space containing multiples of cosnx or sinnx. It is completely parallel to the vector,orthogonality
"case, and very important. And ﬁnally we have a job for Schmidt: To orthogonalize the",orthogonality
powers of x and produce the Legendre polynomials.,orthogonality
Function Spaces and Fourier Series,orthogonality
"This is a brief and optional section, but it has a number of good intentions:",orthogonality
1. to introduce the most famous inﬁnite-dimensional vector space (Hilbert space);,orthogonality
2. to extend the ideas of length and inner product from vectors v to functions f(x):,orthogonality
3. to recognize the Fourier series as a sum of one-dimensional projections (the orthog-,orthogonality
onal “columns” are the sines and cosines);,orthogonality
"4. to apply Gram-Schmidt orthogonalization to the polynomials 1,x,x2,...; and",orthogonality
5. to ﬁnd the best approximation to f(x) by a straight line.,orthogonality
"We will try to follow this outline, which opens up a range of new applications for",orthogonality
"linear algebra, in a systematic way.",orthogonality
"After studying Rn, it is natural to think of the space R∞. It con-",orthogonality
"tains all vectors v = (v1,v2,v3,...) with an inﬁnite sequence of components. This space",orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
is actually too big when there is no control on the size of components v j. A much better,orthogonality
"idea is to keep the familiar deﬁnition of length, using a sum of squares, and to include",orthogonality
only those vectors that have a ﬁnite length:,orthogonality
"The inﬁnite series must converge to a ﬁnite sum. This leaves (1, 1",orthogonality
"3,...) but not (1,1,1,...).",orthogonality
"Vectors with ﬁnite length can be added (∥v+w∥ ≤ ∥v∥+∥w∥) and multiplied by scalars,",orthogonality
so they form a vector space. It is the celebrated Hilbert space.,orthogonality
"Hilbert space is the natural way to let the number of dimensions become inﬁnite,",orthogonality
and at the same time to keep the geometry of ordinary Euclidean space. Ellipses become,orthogonality
"inﬁnite-dimensional ellipsoids, and perpendicular lines are recognized exactly as before.",orthogonality
The vectors v and w are orthogonal when their inner product is zero:,orthogonality
vTw = v1w1 +v2w2 +v3w3 +··· = 0.,orthogonality
"This sum is guaranteed to converge, and for any two vectors it still obeys the Schwarz",orthogonality
"inequality |vTw| ≤ ∥v∥∥w∥. The cosine, even in Hilbert space, is never larger than 1.",orthogonality
There is another remarkable thing about this space: It is found under a great many,orthogonality
"different disguises. Its “vectors” can turn into functions, which is the second point.",orthogonality
2. Lengths and Inner Products. Suppose f(x) = sinx on the interval 0 ≤ x ≤ 2π. This,orthogonality
"f is like a vector with a whole continuum of components, the values of sinx along the",orthogonality
"whole interval. To ﬁnd the length of such a vector, the usual rule of adding the squares",orthogonality
"of the components becomes impossible. This summation is replaced, in a natural and",orthogonality
"inevitable way, by integration:",orthogonality
Length ∥ f∥ of function,orthogonality
"Our Hilbert space has become a function space. The vectors are functions, we have a",orthogonality
"way to measure their length, and the space contains all those functions that have a ﬁnite",orthogonality
"length—just as in equation (16). It does not contain the function F(x) = 1/x, because",orthogonality
the integral of 1/x2 is inﬁnite.,orthogonality
The same idea of replacing summation by integration produces the inner product of,orthogonality
"two functions: If f(x) = sinx and g(x) = cosx, then their inner product is",orthogonality
This is exactly like the vector inner product f Tg. It is still related to the length by,orthogonality
"(f, f) = ∥ f∥2. The Schwarz inequality is still satisﬁed: |(f,g)| ≤ ∥ f∥∥g∥. Of course,",orthogonality
two functions like sinx and cosx—whose inner product is zero—will be called orthogo-,orthogonality
nal. They are even orthonormal after division by their length √π.,orthogonality
3. The Fourier series of a function is an expansion into sines and cosines:,orthogonality
f(x) = a0 +a1cosx+b1sinx+a2cos2x+b2sin2x+··· .,orthogonality
"To compute a coefﬁcient like b1, multiply both sides by the corresponding function sinx",orthogonality
"and integrate from 0 to 2π. (The function f(x) is given on that interval.) In other words,",orthogonality
take the inner product of both sides with sinx:,orthogonality
"On the right-hand side, every integral is zero except one—the one in which sinx multi-",orthogonality
plies itself. The sines and cosines are mutually orthogonal as in equation (18) Therefore,orthogonality
b1 is the left-hand side divided by that one nonzero integral:,orthogonality
"The Fourier coefﬁcient a1 would have cosx in place of sinx, and a2 would use cos2x.",orthogonality
The whole point is to see the analogy with projections. The component of the vector,orthogonality
b along the line spanned by a is bTa/aTa. A Fourier series is projecting f(x) onto sinx.,orthogonality
Its component p in this direction is exactly b1sinx.,orthogonality
The coefﬁcient b1 is the least squares solution of the inconsistent equation b1sinx =,orthogonality
f(x). This brings b1sinx as close as possible to f(x). All the terms in the series are,orthogonality
"projections onto a sine or cosine. Since the sines and cosines are orthogonal, the Fourier",orthogonality
series gives the coordinates of the “vector” f (x) with respect to a set of (inﬁnitely many),orthogonality
4. Gram-Schmidt for Functions.,orthogonality
There are plenty of useful functions other than,orthogonality
"sines and cosines, and they are not always orthogonal. The simplest are the powers of x,",orthogonality
and unfortunately there is no interval on which even 1 and x2 are perpendicular. (Their,orthogonality
"inner product is always positive, because it is the integral of x2.) Therefore the closest",orthogonality
"parabola to f(x) is not the sum of its projections onto 1, x, and x2. There will be a matrix",orthogonality
"like (ATA)−1, and this coupling is given by the ill-conditioned Hilbert matrix. On the",orthogonality
"interval 0 ≤ x ≤ 1,",orthogonality
"This matrix has a large inverse, because the axes 1, x, x2 are far from perpendicular. The",orthogonality
situation becomes impossible if we add a few more axes. It is virtually hopeless to solve,orthogonality
ATA�x = ATb for the closest polynomial of degree ten.,orthogonality
"More precisely, it is hopeless to solve this by Gaussian elimination; every roundoff",orthogonality
"error would be ampliﬁed by more than 1013. On the other hand, we cannot just give",orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
up; approximation by polynomials has to be possible. The right idea is to switch to,orthogonality
"orthogonal axes (by Gram-Schmidt). We look for combinations of 1, x, and x2 that are",orthogonality
"It is convenient to work with a symmetrically placed interval like −1 ≤ x ≤ 1, because",orthogonality
this makes all the odd powers of x orthogonal to all the even powers:,orthogonality
Therefore the Gram-Schmidt process can begin by accepting v1 = 1 and v2 = x as the,orthogonality
"ﬁrst two perpendicular axes. Since (x,x2) = 0, it only has to correct the angle between 1",orthogonality
"and x2. By formula (10), the third orthogonal polynomial is",orthogonality
"v3 = x2 − (1,x2)",orthogonality
"(x,x) x = x2 −",orthogonality
= x2 − 1,orthogonality
The polynomials constructed in this way are called the Legendre polynomials and they,orthogonality
are orthogonal to each other over the interval −1 ≤ x ≤ 1.,orthogonality
"The closest polynomial of degree ten is now computable, without disaster, by projecting",orthogonality
onto each of the ﬁrst 10 (or 11) Legendre polynomials.,orthogonality
5. Best Straight Line.,orthogonality
Suppose we want to approximate y = x5 by a straight line,orthogonality
"C +Dx between x = 0 and x = 1. There are at least three ways of ﬁnding that line, and",orthogonality
if you compare them the whole chapter might become clear!,orthogonality
1. Solve [1 x],orthogonality
= x5 by least squares. The equation ATA�x = ATb is,orthogonality
2. Minimize E2 =,orthogonality
0 (x5 −C−Dx)2dx = 1,orthogonality
"tives with respect to C and D, after dividing by 2, bring back the normal equations",orthogonality
of method 1 (and the solution is �C = 1,orthogonality
"14, �D = 5",orthogonality
6 +C + 1,orthogonality
"3. Apply Gram-Schmidt to replace x by x − (1,x)/(1,1). That is x − 1",orthogonality
"2, which is or-",orthogonality
thogonal to 1. Now the one-dimensional projections add to the best line:,orthogonality
"C +Dx = (x5,1)",orthogonality
1. (a) Write the four equations for ﬁtting y = C +Dt to the data,orthogonality
Show that the columns are orthogonal.,orthogonality
"(b) Find the optimal straight line, draw its graph, and write E2.",orthogonality
(c) Interpret the zero error in terms of the original system of four equations in two,orthogonality
"unknowns: The right-hand side (−4,−3,−1,0) is in the",orthogonality
"2. Project b = (0,3,0) onto each of the orthonormal vectors a1 = (2",orthogonality
3) and a2 =,orthogonality
"3), and then ﬁnd its projection p onto the plane of a1 and a2.",orthogonality
"3. Find also the projection of b = (0,3,0) onto a3 = (2",orthogonality
"3), and add the three pro-",orthogonality
jections. Why is P = a1aT,orthogonality
3 equal to I?,orthogonality
"4. If Q1 and Q2 are orthogonal matrices, so that QTQ = I, show that Q1Q2 is also",orthogonality
"orthogonal. If Q1 is rotation through θ, and Q2 is rotation through φ, what is Q1Q2?",orthogonality
Can you ﬁnd the trigonometric identities for sin(θ +φ) and cos(θ +φ) in the matrix,orthogonality
"5. If u is a unit vector, show that Q = I − 2uuT is a symmetric orthogonal matrix. (It",orthogonality
"is a reﬂection, also known as a Householder transformation.) Compute Q when",orthogonality
6. Find a third column so that the matrix,orthogonality
is orthogonal. It must be a unit vector that is orthogonal to the other columns; how,orthogonality
much freedom does this leave? Verify that the rows automatically become orthonor-,orthogonality
mal at the same time.,orthogonality
"7. Show, by forming bTb directly, that Pythagoras’s law holds for any combination",orthogonality
b = x1q1 +···+xnqn of orthonormal vectors: ∥b∥2 = x2,orthogonality
"n. In matrix terms,",orthogonality
"b = Qx, so this again proves that lengths are preserved: ∥Qx∥2 = ∥x∥2.",orthogonality
"8. Project the vector b = (1,2) onto two vectors that are not orthogonal, a1 = (1,0)",orthogonality
"and a2 = (1,1). Show that, unlike the orthogonal case, the sum of the two one-",orthogonality
dimensional projections does not equal b.,orthogonality
"9. If the vectors q1, q2, q3 are orthonormal, what combination of q1 and q2 is closest to",orthogonality
3.4 Orthogonal Bases and Gram-Schmidt,orthogonality
"10. If q1 and q2 are the outputs from Gram-Schmidt, what were the possible input vectors",orthogonality
11. Show that an orthogonal matrix that is upper triangular must be diagonal.,orthogonality
12. What multiple of a1 =,orthogonality
should be subtracted from a2 =,orthogonality
to make the result,orthogonality
orthogonal to a1? Factor,orthogonality
into QR with orthonormal vectors in Q.,orthogonality
13. Apply the Gram-Schmidt process to,orthogonality
and write the result in the form A = QR.,orthogonality
"14. From the nonorthogonal a, b, c, ﬁnd orthonormal vectors q1, q2, q3:",orthogonality
"15. Find an orthonormal set q1, q2, q3 for which q1, q2 span the column space of",orthogonality
Which fundamental subspace contains q3? What is the least-squares solution of,orthogonality
Ax = b if b = [1 2 7]T?,orthogonality
"16. Express the Gram-Schmidt orthogonalization of a1, a2 as A = QR:",orthogonality
"Given n vectors ai with m components, what are the shapes of A, Q, and R?",orthogonality
"17. With the same matrix A as in Problem 16, and with b = [1 1 1]T, use A = QR to",orthogonality
solve the least-squares problem Ax = b.,orthogonality
"18. If A = QR, ﬁnd a simple formula for the projection matrix P onto the column space",orthogonality
19. Show that these modiﬁed Gram-Schmidt steps produce the same C as in equation,orthogonality
C = C∗ −(qT,orthogonality
"This is much more stable, to subtract the projections one at a time.",orthogonality
"20. In Hilbert space, ﬁnd the length of the vector v = (1/",orthogonality
length of the function f(x) = ex (over the interval 0 ≤ x ≤ 1). What is the inner,orthogonality
product over this interval of ex and e−x?,orthogonality
21. What is the closest function acosx + bsinx to the function f(x) = sin2x on the in-,orthogonality
terval from −π to π? What is the closest straight line c+dx?,orthogonality
"22. By setting the derivative to zero, ﬁnd the value of b1 that minimizes",orthogonality
Compare with the Fourier coefﬁcient b1.,orthogonality
"23. Find the Fourier coefﬁcients a0, a1, b1 of the step function y(x), which equals 1 on",orthogonality
the interval 0 ≤ x ≤ π and 0 on the remaining interval π < x < 2π:,orthogonality
24. Find the fourth Legendre polynomial. It is a cubic x3+ax2+bx+c that is orthogonal,orthogonality
"to 1, x, and x2 − 1",orthogonality
3 over the interval −1 ≤ x ≤ 1.,orthogonality
25. What is the closest straight line to the parabola y = x2 over −1 ≤ x ≤ 1?,orthogonality
"26. In the Gram-Schmidt formula (10), verify that C is orthogonal to q1 and q2.",orthogonality
"27. Find an orthonormal basis for the subspace spanned by a1 = (1,−1,0,0), a2 =",orthogonality
"(0,1,−1,0), a3 = (0,0,1,−1).",orthogonality
"28. Apply Gram-Schmidt to (1,−1,0), (0,1,−1), and (1,0,−1), to ﬁnd an orthonormal",orthogonality
"basis on the plane x1 +x2 +x3 = 0. What is the dimension of this subspace, and how",orthogonality
many nonzero vectors come out of Gram-Schmidt?,orthogonality
"29. (Recommended) Find orthogonal vectors A, B, C by Gram-Schmidt from a, b, c:",orthogonality
"A, B, C and a, b, c are bases for the vectors perpendicular to d = (1,1,1,1).",orthogonality
30. If A = QR then ATA = RTR =,orthogonality
on A corresponds to elimination on ATA. Compare,orthogonality
"For ATA, the pivots are 2, 3",orthogonality
3 and the multipliers are −1,orthogonality
3.5 The Fast Fourier Transform,orthogonality
"(a) Using those multipliers in A, show that column 1 of A and B = column 2 −",orthogonality
2(column 1) and C = column 3− 2,orthogonality
3(column 2) are orthogonal.,orthogonality
"(b) Check that ∥column 1∥2 = 2, ∥B∥2 = 3",orthogonality
"2, and ∥C∥2 = 4",orthogonality
"3, using the pivots.",orthogonality
31. True or false (give an example in either case):,orthogonality
(a) Q−1 is an orthogonal matrix when Q is an orthogonal matrix.,orthogonality
(b) If Q (3 by 2) has orthonormal columns then ∥Qx∥ always equals ∥x∥.,orthogonality
32. (a) Find a basis for the subspace S in R4 spanned by all solutions of,orthogonality
x1 +x2 +x3 −x4 = 0.,orthogonality
(b) Find a basis for the orthogonal complement S⊥.,orthogonality
"(c) Find b1 in S and b2 in S⊥ so that b1 +b2 = b = (1,1,1,1).",orthogonality
The Fast Fourier Transform,orthogonality
The Fourier series is linear algebra in inﬁnite dimensions. The “vectors” are functions,orthogonality
f(x); they are projected onto the sines and cosines; that produces the Fourier coefﬁcients,orthogonality
"ak and bk. From this inﬁnite sequence of sines and cosines, multiplied by ak and bk, we",orthogonality
"can reconstruct f(x). That is the classical case, which Fourier dreamt about, but in actual",orthogonality
"calculations it is the discrete Fourier transform that we compute. Fourier still lives, but",orthogonality
"This is pure linear algebra, based on orthogonality. The input is a sequence of num-",orthogonality
"bers y0,...,yn−1, instead of a function f(x). The output c0,...,cn−1 has the same length",orthogonality
"n. The relation between y and c is linear, so it must be given by a matrix. This is the",orthogonality
"Fourier matrix F, and the whole technology of digital signal processing depends on it.",orthogonality
The Fourier matrix has remarkable properties.,orthogonality
"Signals are digitized, whether they come from speech or images or sonar or TV (or",orthogonality
"even oil exploration). The signals are transformed by the matrix F, and later they can be",orthogonality
transformed back—to reconstruct. What is crucially important is that F and F−1 can be,orthogonality
F−1 must be simple. The multiplications by F and F−1 must be fast.,orthogonality
"Those are both true. F−1 has been known for years, and it looks just like F. In fact,",orthogonality
"F is symmetric and orthogonal (apart from a factor √n), and it has only one drawback:",orthogonality
"Its entries are complex numbers. That is a small price to pay, and we pay it below. The",orthogonality
difﬁculties are minimized by the fact that all entries of F and F−1 tare powers of a single,orthogonality
number w. That number has wn = 1.,orthogonality
The 4 by 4 discrete Fourier transform uses w = i (and notice i4 = 1). The success of,orthogonality
the whole DFT depends on F times its complex conjugate F:,orthogonality
Immediately FF = 4I tells us that F−1 = F/4. The columns of F are orthogonal (to give,orthogonality
the zero entries in 4I). The n by n matrices will have FF = nI. Then the inverse of F is,orthogonality
just F/n. In a moment we will look at the complex number w = e2πi/n (which equals i,orthogonality
for n = 4).,orthogonality
"It is remarkable that F is so easy to invert. If that were all (and up to 1965 it was all),",orthogonality
the discrete transform would have an important place. Now there is more. The multipli-,orthogonality
cations by F and F−1 can be done in an extremely fast and ingenious way. Instead of,orthogonality
"n2 separate multiplications, coming from the n2 entries in the matrix, the matrix-vector",orthogonality
products Fc and F−1y require only 1,orthogonality
2nlogn steps. This rearrangement of the multiplica-,orthogonality
tion is called the Fast Fourier Transform.,orthogonality
"The section begins with w and its properties, moves on to F−1, and ends with the",orthogonality
"FFT—the fast transform. The great application in signal processing is ﬁltering, and the",orthogonality
"key to its success is the convolution rule. In matrix language, all “circulant matrices”",orthogonality
are diagonalized by F. So they reduce to two FFTs and a diagonal matrix.,orthogonality
Complex Roots of Unity,orthogonality
Real equations can have complex solutions. The equation x2 +1 = 0 led to the invention,orthogonality
"of i (and also to −i!). That was declared to be a solution, and the case was closed. If",orthogonality
"someone asked about x2 − i = 0, there was an answer: The square roots of a complex",orthogonality
"number are again complex numbers. You must allow combinations x + iy, with a real",orthogonality
"part x and an imaginary part y, but no further inventions are necessary. Every real or",orthogonality
complex polynomial of degree n has a full set of n roots (possibly complex and possibly,orthogonality
repeated). That is the fundamental theorem of algebra.,orthogonality
We are interested in equations like x4 = 1. That has four solutions—the fourth roots,orthogonality
of unity. The two square roots of unity are 1 and −1. The fourth roots are the square,orthogonality
"roots of the square roots, 1 and −1, i and −i. The number i will satisfy i4 = 1 because",orthogonality
"it satisﬁes i2 = −1. For the eighth roots of unity we need the square roots of i, and that",orthogonality
brings us to w = (1+i)/,orthogonality
"2. Squaring w produces (1+2i+i2)/2, which is i—because",orthogonality
1+i2 is zero. Then w8 = i4 = 1. There has to be a system here.,orthogonality
The complex numbers cosθ +isinθ in the Fourier matrix are extremely special. The,orthogonality
real part is plotted on the x-axis and the imaginary part on the y-axis (Figure 3.11). Then,orthogonality
the number w lies on the unit circle; its distance from the origin is cos2 θ +sin2θ = 1.,orthogonality
3.5 The Fast Fourier Transform,orthogonality
"It makes an angle θ with the horizontal. The whole plane enters in Chapter 5, where",orthogonality
complex numbers will appear as eigenvalues (even of real matrices). Here we need only,orthogonality
"special points w, all of them on the unit circle, in order to solve wn = 1.",orthogonality
w = e2π/8 = cos 2π,orthogonality
8 + i sin 2π,orthogonality
"Figure 3.11: The eight solutions to z8 = 1 are 1,w,w2,...,w7 with w = (1+i)/",orthogonality
The square of w can be found directly (it just doubles the angle):,orthogonality
w2 = (cosθ +isinθ)2 = cos2θ −sin2 θ +2isinθ cosθ.,orthogonality
"The real part cos2θ − sin2θ is cos2θ, and the imaginary part 2 sinθ cosθ is sin2θ.",orthogonality
(Note that i is not included; the imaginary part is a real number.) Thus w2 = cos2θ +,orthogonality
"isin2θ. The square of w is still on the unit circle, but at the double angle 2θ. That",orthogonality
"makes us suspect that wn lies at the angle nθ, and we are right.",orthogonality
There is a better way to take powers of w. The combination of cosine and sine is a,orthogonality
"complex exponential, with amplitude one and phase angle θ:",orthogonality
cosθ +isinθ = eiθ.,orthogonality
"The rules for multiplying, like (e2)(e3) = e5, continue to hold when the exponents iθ are",orthogonality
imaginary. The powers of w = eiθ stay on the unit circle:,orthogonality
"The nth power is at the angle nθ. When n = −1, the reciprocal 1/w has angle −θ. If",orthogonality
"we multiply cosθ +isinθ by cos(−θ)+isin(−θ), we get the answer 1:",orthogonality
eiθe−iθ = (cosθ +isinθ)(cosθ −isinθ) = cos2θ +sin2θ = 1.,orthogonality
"Note. I remember the day when a letter came to MIT from a prisoner in New York,",orthogonality
asking if Euler’s formula (2) was true. It is really astonishing that three of the key,orthogonality
functions of mathematics should come together in such a graceful way. Our best answer,orthogonality
was to look at the power series for the exponential:,orthogonality
eiθ = 1+iθ + (iθ)2,orthogonality
"The real part 1−θ 2/2+··· is cosθ. The imaginary part θ −θ 3/6+··· is the sine, The",orthogonality
"formula is correct, and I wish we had sent a more beautiful proof.",orthogonality
"With this formula, we can solve wn = 1. It becomes einθ = 1, so that nθ must carry",orthogonality
us around the unit circle and back to the start. The solution is to choose θ = 2π/n: The,orthogonality
“primitive” nth root of unity is,orthogonality
wn = e2πi/n = cos 2π,orthogonality
"Its nth power is e2πi, which equals 1. For n = 8, this root is (1+i)/",orthogonality
w4 = cos π,orthogonality
w8 = cos π,orthogonality
"The fourth root is at θ = 90°, which is 1",orthogonality
4(360°). The other fourth roots are the powers,orthogonality
"i2 = −1, i3 = −i, and i4 = 1. The other eighth roots are the powers w2",orthogonality
"roots are equally spaced around the unit circle, at intervals of 2π/n. Note again that the",orthogonality
"square of w8 is w4, which will be essential in the Fast Fourier Transform. The roots add",orthogonality
"up to zero. First 1+i−1−i = 0, and then",orthogonality
Sum of eighth roots,orthogonality
"One proof is to multiply the left side by w8, which leaves it unchanged. (It yields w8 +",orthogonality
"8 equals 1.) The eight points each move through 45°, but they remain",orthogonality
the same eight points. Since zero is the only number that is unchanged when multiplied,orthogonality
"by w8, the sum must be zero. When n is even the roots cancel in pairs (like 1 + i2 = 0",orthogonality
and i+i3 = 0). But the three cube roots of 1 also add to zero.,orthogonality
The Fourier Matrix and Its Inverse,orthogonality
"In the continuous case, the Fourier series can reproduce f(x) over a whole interval. It",orthogonality
"uses inﬁnitely many sines and cosines (or exponentials). In the discrete case, with only",orthogonality
"n coefﬁcients c0,...,cn−1 to choose, we only ask for equality at n points. That gives n",orthogonality
"equations. We reproduce the four values y = 2,4,6,8 when Fc = y:",orthogonality
3.5 The Fast Fourier Transform,orthogonality
"The input sequence is y = 2,4,6,8. The output sequence is c0,c1,c2,c3. The four equa-",orthogonality
tions (6) look for a four-term Fourier series that matches the inputs at four equally spaced,orthogonality
points x on the interval from 0 to 2π:,orthogonality
c0 +c1eix +c2e2ix +c3e3ix =,orthogonality
Those are the four equations in system (6). At x = 2π the series returns y0 = 2 and,orthogonality
"continues periodically. The Discrete Fourier Series is best written in this complex form,",orthogonality
as a combination of exponentials eikx rather than sinkx and coskx.,orthogonality
"For every n, the matrix connecting y to c can be inverted. It represents n equations,",orthogonality
requiring the ﬁnite series c0 +c1eix +··· (n terms) to agree with y (at n points). The ﬁrst,orthogonality
"agreement is at x = 0, where c0 +···+cn−1 = y0. The remaining points bring powers of",orthogonality
"w, and the full problem is Fc = y:",orthogonality
There stands the Fourier matrix F with entries Fjk = w jk. It is natural to number the,orthogonality
"rows and columns from 0 to n − 1, instead of 1 to n. The ﬁrst row has j = 0, the ﬁrst",orthogonality
"column has k = 0, and all their entries are w0 = 1.",orthogonality
"To ﬁnd the c’s we have to invert F. In the 4 by 4 case, F−1 was built from 1/i = −i.",orthogonality
"That is the general rule, that F−1 comes from the complex number w−1 = w. It lies at",orthogonality
"the angle −2π/n, where w was at the angle +2π/n:",orthogonality
The inverse matrix is built from the powers of w−1 = 1/w = w:,orthogonality
Row j of F times column j of F−1 is always (1+1+···+1)/n = 1. The harder part is,orthogonality
"off the diagonal, to show that row j of F times column k of F−1 gives zero:",orthogonality
1·1+w jw−k +w2jw−2k +···+w(n−1)jw−(n−1)k = 0,orthogonality
The key is to notice that those terms are the powers of W = w jw−k:,orthogonality
1+W +W 2 +···+W n−1 = 0.,orthogonality
This number W is still a root of unity: W n = wnjw−nk is equal to 1 j1−k = 1. Since j,orthogonality
"is different from k, W is different from 1. It is one of the other roots on the unit circle.",orthogonality
Those roots all satisfy 1+W +···+W n−1 = 0. Another proof comes from,orthogonality
1−W n = (1−W)(1+W +W 2 +···+W n−1).,orthogonality
"Since W n = 1, the left side is zero. But W is not 1, so the last factor must be zero. The",orthogonality
columns of F are orthogonal.,orthogonality
The Fast Fourier Transform,orthogonality
"Fourier analysis is a beautiful theory, and it is also very practical. To analyze a waveform",orthogonality
into its frequencies is the best way to take a signal apart. The reverse process brings it,orthogonality
"back. For physical and mathematical reasons the exponentials are special, and we can",orthogonality
"pinpoint one central cause: If you differentiate eikx, or integrate it, or translate x to",orthogonality
"x+h, the result is still a multiple of eikx. Exponentials are exactly suited to differential",orthogonality
"equations, integral equations, and difference equations. Each frequency component goes",orthogonality
"its own way, as an eigenvector, and then they recombine into the solution. The analysis",orthogonality
and synthesis of signals—computing c from y and y from c—is a central part of scientiﬁc,orthogonality
We want to show that Fc and F−1y can be done quickly. The key is in the relation of,orthogonality
"F4 to F2—or rather to two copies of F2, which go into a matrix F∗",orthogonality
"F4 contains the powers of w4 = i, the fourth root of 1. F∗",orthogonality
"2 contains the powers of w2 = −1,",orthogonality
the square root of 1. Note especially that half the entries in F∗,orthogonality
2 are zero. The 2 by 2,orthogonality
"transform, done twice, requires only half as much work as a direct 4 by 4 transform. If",orthogonality
"64 by 64 transform could be replaced by two 32 by 32 transforms, the work would be",orthogonality
"cut in half (plus the cost of reassembling the results). What makes this true, and possible",orthogonality
"in practice, is the simple connection between w64 and w32:",orthogonality
"The 32nd root is twice as far around the circle as the 64th root. If w64 = 1, then (w2)32 =",orthogonality
"1. The mth root is the square of the nth root, if m is half of n:",orthogonality
3.5 The Fast Fourier Transform,orthogonality
"The speed of the FFT, in the standard form presented here, depends on working with",orthogonality
"highly composite numbers like 210 = 1024. Without the fast transform, it takes (1024)2",orthogonality
"multiplications to produce F times c (which we want to do often). By contrast, a fast",orthogonality
"transform can do each multiplication in only 5·1024 steps. It is 200 times faster, because",orthogonality
it replaces one factor of 1024 by 5. In general it replaces n2 multiplications by 1,orthogonality
"when n is 2ℓ. By connecting Fn to two copies of Fn/2, and then to four copies of Fn/4,",orthogonality
"and eventually to a very small F, the usual n2 steps are reduced to 1",orthogonality
We need to see how y = Fnc (a vector with n components) can be recovered from two,orthogonality
"vectors that are only half as long. The ﬁrst step is to divide c itself, by separating its",orthogonality
even-numbered components from its odd-numbered components:,orthogonality
"The coefﬁcients just go alternately into c′ and c′′. From those vectors, the half-size",orthogonality
transform gives y′ = Fmc′ and y′′ = Fmc′′. Those are the two multiplications by the,orthogonality
smaller matrix Fm. The central problem is to recover y from the half-size vectors y′ and,orthogonality
"y′′, and Cooley and Tukey noticed how it could be done:",orthogonality
The ﬁrst m and the last m components of the vector y = Fnc are,orthogonality
y j = y′,orthogonality
y j+m = y′,orthogonality
"Thus the three steps are: split c into c′ and c′′, transform them by Fm into y′",orthogonality
"and y′′, and reconstruct y from equation (13).",orthogonality
We verify in a moment that this gives the correct y. (You may prefer the ﬂow graph,orthogonality
to the algebra.) This idea can be repeated. We go from F1024 to F512 to F256. The ﬁnal,orthogonality
"2nℓ, when starting with the power n = 2ℓ and going all the way to n = 1—where",orthogonality
no multiplication is needed. This number 1,orthogonality
4nℓ satisﬁes the rule given above: twice the,orthogonality
"count for m, plus m extra multiplications, produces the count for n:",orthogonality
Another way to count: There are ℓ steps from n = 2ℓ to n = 1. Each step needs n/2,orthogonality
"multiplications by Dn/2 in equation (13), which is really a factorization of Fn:",orthogonality
The cost is only slightly more than linear. Fourier analysis has been completely trans-,orthogonality
"formed by the FFT. To verify equation (13), split y j into even and odd:",orthogonality
Each sum on the right has m = 1,orthogonality
2n terms. Since w2,orthogonality
"n is wm, the two sums are",orthogonality
"For the second part of equation (13), j +m in place of j produces a sign change:",orthogonality
"Inside the sums, wk(j+1)",orthogonality
m = 1k = 1.,orthogonality
n = e2πim/n = eπi = −1.,orthogonality
The FFT idea is easily modiﬁed to allow other prime factors of n (not only powers of 2).,orthogonality
"If n itself is a prime, a completely different algorithm is used.",orthogonality
Example 1. The steps from n = 4 to m = 2 are,orthogonality
"Combined, the three steps multiply c by F4 to give y. Since each step is linear, it must",orthogonality
"come from a matrix, and the product of those matrices must be F4:",orthogonality
You recognize the two copies of F2 in the center. At the right is the permutation matrix,orthogonality
that separates c into c′ and c′′. At the left is the matrix that multiplies by w j,orthogonality
"started with F8, the middle matrix would contain two copies of F4. Each of those would",orthogonality
be split as above. Thus the FFT amounts to a giant factorization of the Fourier matrix!,orthogonality
The single matrix F with n2 nonzeros is a product of approximately ℓ = log2n matrices,orthogonality
(and a permutation) with a total of only nℓ nonzeros.,orthogonality
The Complete FFT and the Butterﬂy,orthogonality
The ﬁrst step of the FFT changes multiplication by Fn to two multiplications by Fm.,orthogonality
"The even-numbered components (c0,c2) are transformed separately from (c1,c3), Figure",orthogonality
"3.12 gives a ﬂow graph for n = 4. For n = 8, the key idea is to replace each F4 box by",orthogonality
F2 boxes. The new factor w4 = i is the square of the old factor w = w8 = e2πi/8. The,orthogonality
ﬂow graph shows the order that the c’s enter the FFT and the log2n stages that take them,orthogonality
through it—and it also shows the simplicity of the logic.,orthogonality
Every stage needs 1,orthogonality
2n multiplications so the ﬁnal count is 1,orthogonality
2nlogn. There is an amaz-,orthogonality
ing rule for the overall permutation of c’s before entering the FFT: Write the subscripts,orthogonality
3.5 The Fast Fourier Transform,orthogonality
Figure 3.12: Flow graph for the Fast Fourier Transform with n = 4.,orthogonality
"0,...,7 in binary and reverse the order of their bits. The subscripts appear in “bit-",orthogonality
reversed order” on the left side of the graph. Even numbers come before odd (numbers,orthogonality
ending in 0 come before numbers ending in 1).,orthogonality
1. What are F2 and F4 for the 4 by 4 Fourier matrix F?,orthogonality
"2. Find a permutation P of the columns of F that produces FP = F (n by n), Combine",orthogonality
with FF = nI to ﬁnd F2 and F4 for the n by n Fourier matrix.,orthogonality
"3. If you form a 3 by 3 submatrix of the 6 by 6 matrix F6, keeping only the entries in",orthogonality
"its ﬁrst, third, and ﬁfth rows and columns, what is that submatrix?",orthogonality
4. Mark all the sixth roots of 1 in the complex plane. What is the primitive root w6?,orthogonality
(Find its real and imaginary part.) Which power of w6 is equal to 1/w6? What is,orthogonality
1+w+w2 +w3 +w4 +w5?,orthogonality
"5. Find all solutions to the equation eix = −1, and all solutions to eiθ = i.",orthogonality
"6. What are the square and the square root of w128, the primitive 128th root of 1?",orthogonality
"7. Solve the 4 by 4 system (6) if the right-hand sides are y0 = 2, y1 = 0, y2 = 2, y3 = 0.",orthogonality
"In other words, solve F4c = y.",orthogonality
"8. Solve the same system with y = (2,0,−2,0) by knowing F−1",orthogonality
and computing c =,orthogonality
"4 y. Verify that c0 +c1eix +c2e2ix +c3e3ix takes the values 2, 0, −2, 0 at the points",orthogonality
"9. (a) If y = (1,1,1,1), show that c = (1,0,0,0) satisﬁes F4c = y.",orthogonality
"(b) Now suppose y = (1,0,0,0), and ﬁnd c.",orthogonality
"10. For n = 2, write y0 from the ﬁrst line of equation (13) and y1 from the second line.",orthogonality
"For n = 4, use the ﬁrst line to ﬁnd y0 and y1, and the second to ﬁnd y2 and y3, all in",orthogonality
terms of y′ and y′′.,orthogonality
"11. Compute y = F4c by the three steps of the Fast Fourier Transform if c = (1,0,1,0).",orthogonality
"12. Compute y = F8c by the three steps of the Fast Fourier Transform if c = (1,0,1,0,1,0,1,0).",orthogonality
"Repeat the computation with c = (0,1,0,1,0,1,0,1).",orthogonality
"13. For the 4 by 4 matrix, write out the formulas for c0, c1, c2, c3 and verify that if f is",orthogonality
"odd then c is odd. The vector f is odd if fn−j = −f j; for n = 4 that means f0 = 0,",orthogonality
"f3 = −f1, f2 = 0 as in sin0, sinπ/2, sinπ, sin3π/2. This is copied by c and it leads",orthogonality
to a fast sine transform.,orthogonality
14. Multiply the three matrices in equation (16) and compare with F. in which six entries,orthogonality
do you need to know that i2 = −1?,orthogonality
15. Invert the three factors in equation (14) to ﬁnd a fast factorization of F−1.,orthogonality
16. F is symmetric. So transpose equation (14) to ﬁnd a new Fast Fourier Transform!,orthogonality
17. All entries in the factorization of F6 involve powers of w = sixth root of 1:,orthogonality
"Write these factors with 1, w, w2 in D and 1, w2, w4 in F3. Multiply!",orthogonality
"Problems 18–20 introduce the idea of an eigenvector and eigenvalue, when a matrix",orthogonality
times a vector is a multiple of that vector. This is the theme of Chapter 5.,orthogonality
18. The columns of the Fourier matrix F are the eigenvectors of the cyclic permutation,orthogonality
P. Multiply PF to ﬁnd the eigenvalues λ0 to λ3:,orthogonality
0 1 0 0,orthogonality
0 0 1 0,orthogonality
0 0 0 1,orthogonality
1 0 0 0,orthogonality
This is PF = FΛ or P = FΛF−1.,orthogonality
"19. Two eigenvectors of this circulant matrix C are (1,1,1,1) and (1,i,i2,i3). What are",orthogonality
the eigenvalues e0 and e1?,orthogonality
3.5 The Fast Fourier Transform,orthogonality
"20. Find the eigenvalues of the “periodic” −1, 2, −1 matrix C. The −1s in the corners",orthogonality
of C make it periodic (a circulant matrix):,orthogonality
"c0 = 2, c1 = −1, c2 = 0, c3 = −1.",orthogonality
"21. To multiply C times x, when C = FEF−1, we can multiply F(E(F−1x)) instead. The",orthogonality
"direct Cx uses n2 separate multiplications. Knowing E and F, the second way uses",orthogonality
"only nlog2n+n multiplications. How many of those come from E, how many from",orthogonality
"F, and how many from F−1?",orthogonality
"22. How could you quickly compute these four components of Fc starting from c0 +c2,",orthogonality
"c0 −c2, c1 +c3, c1 −c3? You are ﬁnding the Fast Fourier Transform!",orthogonality
c0 +c1 +c2 +c3,orthogonality
c0 +ic1 +i2c2 +i3c3,orthogonality
c0 +i2c1 +i4c2 +i6c3,orthogonality
c0 +i3c1 +i6c2 +i9c3,orthogonality
"3.1 Find the length of a = (2,−2,1), and write two independent vectors that are per-",orthogonality
"3.2 Find all vectors that are perpendicular to (1,3,1) and (2,7,2), by making those the",orthogonality
rows of A and solving Ax = 0.,orthogonality
"3.3 What is the angle between a = (2,−2,1) and b = (1,2,2)?",orthogonality
"3.4 What is the projection p of b = (1,2,2) onto a = (2,−2,1)?",orthogonality
"3.5 Find the cosine of the angle between the vectors (3,4) and (4,3),",orthogonality
"3.6 Where is the projection of b = (1,1,1) onto the plane spanned by (1,0,0) and",orthogonality
3.7 The system Ax = b has a solution if and only if b is orthogonal to which of the four,orthogonality
"3.8 Which straight line gives the best ﬁt to the following data: b = 0 at t = 0, b = 0 at",orthogonality
"t = 1, b = 12 at t = 3?",orthogonality
"3.9 Construct the projection matrix P onto the space spanned by (1,1,1) and (0,1,3).",orthogonality
3.10 Which constant function is closest to y = x4 (in the least-squares sense) over the,orthogonality
interval 0 ≤ x ≤ 1?,orthogonality
"3.11 If Q is orthogonal, is the same true of Q3?",orthogonality
3.12 Find all 3 by 3 orthogonal matrices whose entries are zeros and ones.,orthogonality
"3.13 What multiple of a1 should be subtracted from a2, to make the result orthogonal to",orthogonality
a1? Sketch a ﬁgure.,orthogonality
"into QR, recognizing that the ﬁrst column is already a unit vector.",orthogonality
3.15 If every entry in an orthogonal matrix is either 1,orthogonality
"4, how big is the matrix?",orthogonality
"3.16 Suppose the vectors q1,...,qn are orthonormal. If b = c1q1 + ··· + cnqn, give a",orthogonality
formula for the ﬁrst coefﬁcient c1 in terms of b and the q’s.,orthogonality
"3.17 What words describe the equation ATA�x = ATb, the vector p = A�x = Pb, and the",orthogonality
matrix P = A(ATA)−1AT?,orthogonality
3.18 If the orthonormal vectors q1 = (2,orthogonality
3) and q2 = (−1,orthogonality
3) are the columns of,orthogonality
"Q, what are the matrices QTQ and QQT? Show that QQT is a projection matrix",orthogonality
(onto the plane of q1 and q2).,orthogonality
"3.19 If v1,...,vn is an orthonormal basis for Rn, show that v1vT",orthogonality
"3.20 True or false: If the vectors x and y are orthogonal, and P is a projection, then Px",orthogonality
and Py are orthogonal.,orthogonality
"3.21 Try to ﬁt a line b =C+Dt through the points b = 0, t = 2, and b = 6, t = 2, and show",orthogonality
"that the normal equations break down. Sketch all the optimal lines, minimizing the",orthogonality
sum of squares of the two errors.,orthogonality
"3.22 What point on the plane x+y−z = 0 is closest to b = (2,1,0)?",orthogonality
"3.23 Find an orthonormal basis for R3 starting with the vector (1,1,1).",orthogonality
3.24 CT scanners examine the patient from different directions and produce a matrix,orthogonality
"giving the densities of bone and tissue at each point. Mathematically, the problem",orthogonality
"is to recover a matrix from its projections. in the 2 by 2 case, can you recover the",orthogonality
matrix A if you know the sum along each row and down each column?,orthogonality
"3.25 Can you recover a 3 by 3 matrix if you know its row sums and column sums, and",orthogonality
also the sums down the main diagonal and the four other parallel diagonals?,orthogonality
3.5 The Fast Fourier Transform,orthogonality
"3.26 Find an orthonormal basis for the plane x − y + z = 0, and ﬁnd the matrix P that",orthogonality
projects onto the plane. What is the nullspace of P?,orthogonality
"3.27 Let A = [3 1 1], and let V be the nullspace of A.",orthogonality
(a) Find a basis for V and a basis for V⊥.,orthogonality
"(b) Write an orthonormal basis for V⊥, and ﬁnd the projection matrix P1 that projects",orthogonality
vectors in R3 onto V⊥.,orthogonality
(c) Find the projection matrix P2 that projects vectors in R3 onto V.,orthogonality
"3.28 Use Gram-Schmidt to construct an orthonormal pair q1, q2 from a1 = (4,5,2,2)",orthogonality
"and a2 = (1,2,0,0), Express a1 and a2 as combinations of q1 and q2, and ﬁnd the",orthogonality
triangular R in A = QR.,orthogonality
"3.29 For any A, b, x, and y, show that",orthogonality
"(a) if Ax = b and yTA = 0, then yTb = 0.",orthogonality
"(b) if Ax = 0 and ATy = b, then xTb = 0.",orthogonality
What theorem does this prove about the fundamental subspaces?,orthogonality
"3.30 Is there a matrix whose row space contains (1,1,0) and whose nullspace contains",orthogonality
3.31 The distance from a plane aTx = c (in m-dimensional space) to the origin is |c|/∥a∥.,orthogonality
"How far is the plane x1 + x2 − x3 − x4 = 8 from the origin, and what point on it is",orthogonality
"3.32 In the parallelogram with corners at 0, v, w, and v + w, show that the sum of the",orthogonality
squared lengths of the four sides equals the sum of the squared lengths of the two,orthogonality
3.33 (a) Find an orthonormal basis for the column space of A.,orthogonality
"(b) Write A as QR, where Q has orthonormal columns and R is upper triangular.",orthogonality
"(c) Find the least-squares solution to Ax = b, if b = (−3,7,1,0,4).",orthogonality
3.34 With weighting matrix W =,orthogonality
", what is the W-inner product of (1,0) with (0,1)?",orthogonality
"3.35 To solve a rectangular system Ax = b, we replace A−1 (which doesn’t exist) by",orthogonality
(ATA)−1AT (which exists if A has independent columns). Show that this is a left-,orthogonality
inverse of A but not a right-inverse. On the left of A it gives the identity; on the right,orthogonality
it gives the projection P.,orthogonality
"3.36 Find the straight line C + Dt that best ﬁts the measurements b = 0,1,2,5 at times",orthogonality
3.37 Find the curve y = C + D2t which gives the best least-squares ﬁt to the measure-,orthogonality
"ments y = 6 at t = 0, y = 4 at t = 1, y = 0 at t = 2. Write the three equations that",orthogonality
"are solved if the curve goes through the three points, and ﬁnd the best C and D.",orthogonality
3.38 If the columns of A are orthogonal to each other what can you say about the form,orthogonality
"of ATA? If the columns are orthonormal, what can you say then?",orthogonality
3.39 Under what condition on the columns of A (which may be rectangular) is ATA in-,orthogonality
Determinants are much further from the center of linear algebra than they were a hundred,determinants
"years ago. Mathematics keeps changing direction! After all, a single number can tell",determinants
"only so much about a matrix. Still, it is amazing how much this number can do.",determinants
One viewpoint is this: The determinant provides an explicit “formula” for each entry,determinants
of A−1 and A−1b. This formula will not change the way we compute; even the deter-,determinants
"minant itself is found by elimination. In fact, elimination can be regarded as the most",determinants
efﬁcient way to substitute the entries of an n by n matrix into the formula. What the,determinants
"formula does is to show how A−1 depends on the n2 entries of A, and how it varies when",determinants
We can list four of the main uses of determinants:,determinants
"1. They test for invertibility. If the determinant of A is zero, then A is singular. If",determinants
"detA ̸= 0, then A is invertible (and A−1 involves 1/detA).",determinants
"The most important application, and the reason this chapter is essential to the book,",determinants
is to the family of matrices A − λI. The parameter λ is subtracted all along the main,determinants
"diagonal, and the problem is to ﬁnd the eigenvalues for which A − λI is singular. The",determinants
test is det(A−λI) = 0. This polynomial of degree n in λ has exactly n roots. The matrix,determinants
"has n eigenvalues, This is a fact that follows from the determinant formula, and not from",determinants
2. The determinant of A equals the volume of a box in n-dimensional space. The edges,determinants
of the box come from the rows of A (Figure 4.1). The columns of A would give an,determinants
entirely different box with the same volume.,determinants
"The simplest box is a little cube dV = dxdydz, as in",determinants
"��� f(x,y,z)dV. Suppose we",determinants
"change to cylindrical coordinates by x = rcosθ, y = rsinθ, z = z. Just as a small inter-",determinants
val dx is stretched to (dx/du)du—when u replaces x in a single integral—so the volume,determinants
element becomes J drdθ dz. The Jacobian determinant is the three-dimensional ana-,determinants
Figure 4.1: The box formed from the rows of A: volume = |determinant|.,determinants
logue of the stretching factor dx/du:,determinants
The value of this determinant is J = r. It is the r in the cylindrical volume element,determinants
"r drdθ dz; this element is our little box. (It looks curved if we try to draw it, but proba-",determinants
bly it gets straighter as the edges become inﬁnitesimal.),determinants
"3. The determinant gives a formula for each pivot. Theoretically, we could predict",determinants
"when a pivot entry will be zero, requiring a row exchange. From the formula determi-",determinants
"nant = ± (product of the pivots), it follows that regardless of the order of elimination,",determinants
the product of the pivots remains the same apart from sign.,determinants
"Years ago, this led to the belief that it was useless to escape a very small pivot by",determinants
"exchanging rows, since eventually the small pivot would catch up with us. But what",determinants
"usually happens in practice, if an abnormally small pivot is not avoided, is that it is very",determinants
soon followed by an abnormally large one. This brings the product back to normal but,determinants
it leaves the numerical solution in ruins.,determinants
4. The determinant measures the dependence of A−1b on each element of b. If one,determinants
"parameter is changed in an experiment, or one observation is corrected, the “inﬂuence",determinants
coefﬁcient” in A−1 is a ratio of determinants.,determinants
There is one more problem about the determinant. It is difﬁcult not only to decide,determinants
"on its importance, and its proper place in the theory of linear algebra, but also to choose",determinants
4.2 Properties of the Determinant,determinants
"the best deﬁnition. Obviously, detA will not be some extremely simple function of n2",determinants
variables; otherwise A−1 would be much easier to ﬁnd than it actually is.,determinants
"The simple things about the determinant are not the explicit formulas, but the prop-",determinants
erties it possesses. This suggests the natural place to begin. The determinant can be (and,determinants
"will be) deﬁned by its three most basic properties: detI = 1, the sign is reversed by a",determinants
"row exchange, the determinant is linear in each row separately. The problem is then",determinants
"to show, by systematically using these properties, how the determinant can be computed.",determinants
This will bring us back to the product of the pivots.,determinants
"Section 4.2 explains these three deﬁning properties of the determinant, and their most",determinants
important consequences. Section 4.3 gives two more formulas for the determinant—the,determinants
"“big formula” with n! terms, and a formula “by induction”. In Section 4.4 the determi-",determinants
"nant is applied to ﬁnd A−1. Then we compute x = A−1b by Cramer’s rule. And ﬁnally, in",determinants
"an optional remark on permutations, we show that whatever the order in which the prop-",determinants
"erties are used, the result is always the same—the deﬁning properties are self-consistent.",determinants
Here is a light-hearted question about permutations. How many exchanges does it,determinants
take to change VISA into AVIS? Is this permutation odd or even?,determinants
Properties of the Determinant,determinants
"This will be a pretty long list. Fortunately each rule is easy to understand, and even",determinants
"easier to illustrate, for a 2 by 2 example. Therefore we shall verify that the familiar",determinants
"deﬁnition in the 2 by 2 case,",determinants
"����� = ad −bc,",determinants
possesses every property in the list. (Notice the two accepted notations for the deter-,determinants
"minant, detA and |A|.) Properties 4–10 will be deduced from the previous ones. Every",determinants
property is a consequence of the ﬁrst three. We emphasize that the rules apply to,determinants
square matrices of any size.,determinants
1. The determinant of the identity matrix is 1.,determinants
2. The determinant changes sign when two rows are exchanged.,determinants
����� = cb−ad = −,determinants
"The determinant of every permutation matrix is detP = ±1. By row exchanges, we can",determinants
"turn P into the identity matrix. Each row exchange switches the sign of the determinant,",determinants
until we reach detI = 1. Now come all other matrices!,determinants
"3. The determinant depends linearly on the ﬁrst row. Suppose A, B, C are the same",determinants
from the second row down—and row 1 of A is a linear combination of the ﬁrst rows of,determinants
B and C. Then the rule says: detA is the same combination of detB and detC.,determinants
Linear combinations involve two operations—adding vectors and multiplying by scalars.,determinants
Therefore this rule can be split into two parts:,determinants
Add vectors in row 1,determinants
Multiply by t in row 1,determinants
Notice that the ﬁrst part is not the false statement det(B+C) = detB+detC. You cannot,determinants
add all the rows: only one row is allowed to change. Both sides give the answer ad +,determinants
The second part is not the false statement det(tA) = t detA. The matrix tA has a factor,determinants
"t in every row (and the determinant is multiplied by tn). It is like the volume of a box,",determinants
when all sides are stretched by 4. In n dimensions the volume and determinant go up by,determinants
"4n. If only one side is stretched, the volume and determinant go up by 4; that is rule 3.",determinants
"By rule 2, there is nothing special about the ﬁrst row.",determinants
"The determinant is now settled, but that fact is not at all obvious. Therefore we grad-",determinants
ually use these rules to ﬁnd the determinant of any matrix.,determinants
"4. If two rows of A are equal, then detA = 0.",determinants
����� = ab−ba = 0.,determinants
"This follows from rule 2, since if the equal rows are exchanged, the determinant is sup-",determinants
"posed to change sign. But it also has to stay the same, because the matrix stays the same.",determinants
"The only number which can do that is zero, so detA = 0. (The reasoning fails if 1 = −1,",determinants
which is the case in Boolean algebra. Then rule 4 should replace rule 2 as one of the,determinants
4.2 Properties of the Determinant,determinants
5. Subtracting a multiple of one row from another row leaves the same determinant.,determinants
Rule 3 would say that there is a further term −ℓ,determinants
"��, but that term is zero by rule 4. The",determinants
usual elimination steps do not affect the determinant!,determinants
"6. If A has a row of zeros, then detA = 0.",determinants
"One proof is to add some other row to the zero row. The determinant is unchanged, by",determinants
"rule 5. Because the matrix will now have two identical rows, detA = 0 by rule 4.",determinants
7. If A is triangular then detA is the product a11a22···ann of the diagonal entries. If,determinants
"the triangular A has 1s along the diagonal, then detA = 1.",determinants
Proof. Suppose the diagonal entries are nonzero. Then elimination can remove all the,determinants
"off-diagonal entries, without changing the determinant (by rule 5). If A is lower triangu-",determinants
"lar, the steps are downward as usual. If A is upper triangular, the last column is cleared",determinants
out ﬁrst—using multiples of ann. Either way we reach the diagonal matrix D:,determinants
detD = a11a22···anndetI = a11a22···ann.,determinants
To ﬁnd detD we patiently apply rule 3. Factoring out a11 and then a22 and ﬁnally ann,determinants
leaves the identity matrix. At last we have a use for rule 1: detI = 1.,determinants
If a diagonal entry is zero then elimination will produce a zero row. By rule 5 these,determinants
elimination steps do not change the determinant. By rule 6 the zero row means a zero,determinants
determinant. This means: When a triangular matrix is singular (because of a zero on the,determinants
main diagonal) its determinant is zero.,determinants
This is a key property. All singular matrices have a zero determinant.,determinants
"8. If A is singular, then detA = 0. If A is invertible, then detA ̸= 0.",determinants
is not invertible if and only if,determinants
ad −bc = 0.,determinants
"If A is singular, elimination leads to a zero row in U. Then detA = detU = 0. If A",determinants
"is nonsingular, elimination puts the pivots d1,...,dn on the main diagonal. We have a",determinants
“product of pivots” formula for detA! The sign depends on whether the number of row,determinants
exchanges is even or odd:,determinants
detA = ±detU = ±d1d2···dn.,determinants
The ninth property is the product rule. I would say it is the most surprising.,determinants
9. The determinant of AB is the product of detA times detB.,determinants
Product rule |A||B| = |AB|,determinants
ae+bg a f +bh,determinants
ce+dg c f +dh,determinants
A particular case of this rule gives the determinant of A−1. It must be 1/detA:,determinants
(detA)(detA−1) = detAA−1 = detI = 1.,determinants
"In the 2 by 2 case, the product rule could be patiently checked:",determinants
(ad −bc)(eh− fg) = (ae+bg)(cf +dh)−(a f +bh)(ce+dg).,determinants
In the n by n case we suggest two possible proofs—since this is the least obvious rule.,determinants
"Both proofs assume that A and B are nonsingular; otherwise AB is singular, and the",determinants
"equation detAB = (detA)(detB) is easily veriﬁed. By rule 8, it becomes 0 = 0.",determinants
(i) We prove that the ratio d(A) = detAB/detB has properties 1–3. Then d(A) must,determinants
"equal detA. For example, d(I) = detB/detB = 1; rule 1 is satisﬁed. If two rows",determinants
"of A are exchanged, so are the same two rows of AB, and the sign of d changes",determinants
as required by rule 2. A linear combination in the ﬁrst row of A gives the same,determinants
"linear combination in the ﬁrst row of AB. Then rule 3 for the determinant of AB,",determinants
"divided by the ﬁxed quantity detB, leads to rule 3 for the ratio d(A). Thus d(A) =",determinants
"detAB/detB coincides with detA, which is our product formula.",determinants
"(ii) This second proof is less elegant. For a diagonal matrix, detDB = (detD)(detB)",determinants
follows by factoring each di from its row. Reduce a general matrix A to D by,determinants
"elimination—from A to U as usual, and from U to D by upward elimination. The",determinants
"determinant does not change, except for a sign reversal when rows are exchanged.",determinants
"The same steps reduce AB to DB, with precisely the same effect on the determinant.",determinants
But for DB it is already conﬁrmed that rule 9 is correct.,determinants
10. The transpose of A has the same determinant as A itself: detAT = detA.,determinants
4.2 Properties of the Determinant,determinants
"Again the singular case is separate; A is singular if and only if AT is singular, and we",determinants
"have 0 = 0. If A is nonsingular, then it allows the factorization PA = LDU, and we apply",determinants
rule 9 for the determinant of a product:,determinants
"Transposing PA = LDU gives ATPT = UTDTLT, and again by rule 9,",determinants
"This is simpler than it looks, because L, U, LT, and UT are triangular with unit diagonal.",determinants
"By rule 7, their determinants all equal 1. Also, any diagonal matrix is the same as its",determinants
transpose: D = DT. We only have to show that detP = detPT.,determinants
"Certainly detP is 1 or −1, because P comes from I by row exchanges. Observe also",determinants
"that PPT = I. (The 1 in the ﬁrst row of P matches the 1 in the ﬁrst column of PT, and",determinants
"misses the 1s in the other columns.) Therefore detPdetPT = detI = 1, and P and PT",determinants
must have the same determinant: both 1 or both −1.,determinants
"We conclude that the products (3) and (4) are the same, and detA = detAT. This",determinants
"fact practically doubles our list of properties, because every rule that applied to the rows",determinants
can now be applied to the columns: The determinant changes sign when two columns,determinants
"are exchanged, two equal columns (or a column of zeros) produce a zero determinant,",determinants
and the determinant depends linearly on each individual column. The proof is just to,determinants
transpose the matrix and work with the rows.,determinants
I think it is time to stop and call the list complete. It only remains to ﬁnd a deﬁnite,determinants
"formula for the determinant, and to put that formula to use.",determinants
1. If a 4 by 4 matrix has detA = 1,determinants
"2, ﬁnd det(2A), det(−A), det(A2), and det(A−1).",determinants
"2. If a 3 by 3 matrix has detA = −1, ﬁnd det(1",determinants
"2A), det(−A), det(A2), and det(A−1).",determinants
"3. Row exchange: Add row 1 of A to row 2, then subtract row 2 from row 1. Then",determinants
add row 1 to row 2 and multiply row 1 by −1 to reach B. Which rules show the,determinants
Those rules could replace Rule 2 in the deﬁnition of the determinant.,determinants
"4. By applying row operations to produce an upper triangular U, compute",determinants
Exchange rows 3 and 4 of the second matrix and recompute the pivots and determi-,determinants
Note. Some readers will already know a formula for 3 by 3 determinants. It has six,determinants
"terms (equation (2) of the next section), three going parallel to the main diagonal and",determinants
three others going the opposite way with minus signs. There is a similar formula for,determinants
"4 by 4 determinants, but it contains 4! = 24 terms (not just eight). You cannot even",determinants
"be sure that a minus sign goes with the reverse diagonal, as the next exercises show.",determinants
5. Count row exchanges to ﬁnd these determinants:,determinants
0 0 0 1,determinants
0 0 1 0,determinants
0 1 0 0,determinants
1 0 0 0,determinants
0 1 0 0,determinants
0 0 1 0,determinants
0 0 0 1,determinants
1 0 0 0,determinants
"6. For each n, how many exchanges will put (row n, row n − 1,..., row 1) into the",determinants
"normal order (row 1, ... , row n − 1, row n)? Find detP for the n by n permutation",determinants
with 1s on the reverse diagonal. Problem 5 had n = 4.,determinants
7. Find the determinants of:,determinants
(a) a rank one matrix,determinants
(b) the upper triangular matrix,determinants
4 4 8 8,determinants
0 1 2 2,determinants
0 0 2 6,determinants
0 0 0 2,determinants
(c) the lower triangular matrix UT.,determinants
(d) the inverse matrix U−1.,determinants
4.2 Properties of the Determinant,determinants
"(e) the “reverse-triangular” matrix that results from row exchanges,",determinants
0 0 0 2,determinants
0 0 2 6,determinants
0 1 2 2,determinants
4 4 8 8,determinants
8. Show how rule 6 (det = 0 if a row is zero) comes directly from rules 2 and 3.,determinants
"9. Suppose you do two row operations at once, going from",determinants
"Find the determinant of the new matrix, by rule 3 or by direct calculation.",determinants
"10. If Q is an orthogonal matrix, so that QTQ = I, prove that detQ equals +1 or −1.",determinants
What kind of box is formed from the rows (or columns) of Q?,determinants
11. Prove again that detQ = 1 or −1 using only the Product rule. If |detQ| > 1 then,determinants
detQn blows up. How do you know this can’t happen to Qn?,determinants
12. Use row operations to verify that the 3 by 3 “Vandermonde determinant” is,determinants
"13. (a) A skew-symmetric matrix satisﬁes KT = −K, as in",determinants
"In the 3 by 3 case, why is det(−K) = (−1)3detK? On the other hand detKT =",determinants
detK (always). Deduce that the determinant must be zero.,determinants
(b) Write down a 4 by 4 skew-symmetric matrix with detK not zero.,determinants
"14. True or false, with reason if true and counterexample if false:",determinants
"(a) If A and B are identical except that b11 = 2a11, then detB = 2detA.",determinants
(b) The determinant is the product of the pivots.,determinants
"(c) If A is invertible and B is singular, then A+B is invertible.",determinants
"(d) If A is invertible and B is singular, then AB is singular.",determinants
(e) The determinant of AB−BA is zero.,determinants
"15. If every row of A adds to zero, prove that detA = 0. If every row adds to 1, prove",determinants
that det(A−I) = 0. Show by example that this does not imply detA = 1.,determinants
16. Find these 4 by 4 determinants by Gaussian elimination:,determinants
11 12 13 14,determinants
21 22 23 24,determinants
31 32 33 34,determinants
41 42 43 44,determinants
17. Find the determinants of,determinants
For which values of λ is A−λI a singular matrix?,determinants
18. Evaluate detA by reducing the matrix to triangular form (rules 5 and 7).,determinants
"What are the determinants of B, C, AB, ATA, and CT?",determinants
"19. Suppose that CD = −DC, and ﬁnd the ﬂaw in the following argument: Taking de-",determinants
"terminants gives (detC)(detD) = −(detD)(detC), so either detC = 0 or detD = 0.",determinants
Thus CD = −DC is only possible if C or D is singular.,determinants
"20. Do these matrices have determinant 0, 1, 2, or 3?",determinants
21. The inverse of a 2 by 2 matrix seems to have determinant = 1:,determinants
ad −bc = 1.,determinants
What is wrong with this calculation? What is the correct detA−1?,determinants
Problems 22–28 use the rules to compute speciﬁc determinants.,determinants
22. Reduce A to U and ﬁnd detA = product of the pivots:,determinants
4.2 Properties of the Determinant,determinants
"23. By applying row operations to produce an upper triangular U, compute",determinants
−1 0 0 3,determinants
2 1 1 1,determinants
1 2 1 1,determinants
1 1 2 1,determinants
1 1 1 2,determinants
24. Use row operations to simplify and compute these determinants:,determinants
25. Elimination reduces A to U. Then A = LU:,determinants
"Find the determinants of L, U, A, U−1L−1, and U−1L−1A.",determinants
"26. If aij is i times j, show that detA = 0. (Exception when A = [1].)",determinants
"27. If aij is i+ j, show that detA = 0. (Exception when n = 1 or 2.)",determinants
28. Compute the determinants of these matrices by row operations:,determinants
0 a 0 0,determinants
0 0 b 0,determinants
0 0 0 c,determinants
29. What is wrong with this proof that projection matrices have detP = 1?,determinants
30. (Calculus question) Show that the partial derivatives of ln(detA) give A−1:,determinants
"f(a,b,c,d) = ln(ad −bc)",determinants
∂ f/∂b ∂ f/∂d,determinants
"31. (MATLAB) The Hilbert matrix hilb(n) has i, j entry equal to 1/(i + j − 1). Print",determinants
"ti determinants of hilb(1),hilb(2),...,hilb(10). Hilbert matrices are hard to work",determinants
with! What are the pivots?,determinants
32. (MATLAB) What is a typical determinant (experimentally) of rand(n) and randn(n),determinants
"for n = 50,100,200,400? (And what does “Inf” mean in MATLAB?)",determinants
"33. Using MATLAB, ﬁnd the largest determinant of a 4 by 4 matrix of 0s and 1s.",determinants
"34. If you know that detA = 6, what is the determinant of B?",determinants
"35. Suppose the 4 by 4 matrix M has four equal rows all containing a, b, c, d. We know",determinants
that det(M) = 0. The problem is to ﬁnd det(I +M) by any method:,determinants
Partial credit if you ﬁnd this determinant when a = b = c = d = 1. Sudden death if,determinants
you say that det(I +M) = detI +detM.,determinants
Formulas for the Determinant,determinants
The ﬁrst formula has already appeared. Row operations produce the pivots in D:,determinants
"4A If A is invertible, then PA = LDU and detP = ±1. The product rule gives",determinants
detA = ±detLdetDdetU = ±(product of the pivots).,determinants
The sign ±1 depends on whether the number of row exchanges is even or odd.,determinants
The triangular factors have detL = detU = 1 and detD = d1···dn.,determinants
"In the 2 by 2 case, the standard LDU factorization is",determinants
The product of the pivots is ad − bc. That is the determinant of the diagonal matrix D.,determinants
"If the ﬁrst step is a row exchange, the pivots are c and (−detA)/c.",determinants
"Example 1. The −1,2,−1 second difference matrix has pivots 2/1,3/2,... in D:",determinants
= LDU = L,determinants
4.3 Formulas for the Determinant,determinants
"Its determinant is the product of its pivots. The numbers 2,...,n all cancel:",determinants
MATLAB computes the determinant from the pivots. But concentrating all information,determinants
into the pivots makes it impossible to ﬁgure out how a change in one entry would affect,determinants
the determinant. We want to ﬁnd an explicit expression for the determinant in terms of,determinants
"For n = 2, we will be proving that ad − bc is correct. For n = 3, the determinant",determinants
formula is again pretty well known (it has six terms):,determinants
= +a11a22a33 +a12a23a31 +a13a21a32,determinants
Our goal is to derive these formulas directly from the deﬁning properties 1–3 of detA. If,determinants
"we can handle n = 2 and n = 3 in an organized way, you will see the pattern.",determinants
"To start, each row can be broken down into vectors in the coordinate directions:",determinants
"Then we apply the property of linearity, ﬁrst in row 1 and then in row 2:",determinants
nn = 22 easy,determinants
"Every row splits into n coordinate directions, so this expansion has nn terms. Most of",determinants
those terms (all but n! = n factorial) will be automatically zero. When two rows are in,determinants
"the same coordinate direction, one will be a multiple of the other, and",determinants
We pay attention only when the rows point in different directions. The nonzero terms,determinants
have to come in different columns. Suppose the ﬁrst row has a nonzero term in column,determinants
"α, the second row is nonzero in column β, and ﬁnally the nth row in column v. The",determinants
"column numbers α,β,...,v are all different. They are a reordering, or permutation, of",determinants
"the numbers 1,2,...,n. The 3 by 3 case produces 3! = 6 determinants:",determinants
"All but these n! determinants are zero, because a column is repeated. (There are",determinants
"n choices for the ﬁrst column α, n − 1 remaining choices for β, and ﬁnally only one",determinants
"choice for the last column v. All but one column will be used by that time, when we",determinants
"“snake” down the rows of the matrix). In other words, there are n! ways to permute the",determinants
"numbers 1,2,...,n. The column numbers give the permutations:",determinants
"(α,β,v) = (1,2,3), (2,3,1), (3,1,2), (1,3,2), (2,1,3), (3,2,1).",determinants
"Those are the 3! = 6 permutations of (1,2,3); the ﬁrst one is the identity.",determinants
The determinant of A is now reduced to six separate and much simpler determinants.,determinants
"Factoring out the aij, there is a term for every one of the six permutations:",determinants
"Every term is a product of n = 3 entries aij, with each row and column represented once.",determinants
"If the columns come in the order (α,...,v), that term is the product a1α ···anv times the",determinants
determinant of a permutation matrix P. The determinant of the whole matrix is the sum,determinants
"of these n! terms, and that sum is the explicit formula we are after:",determinants
"For an n by n matrix, this sum is taken over all n! permutations (α,...,v) of the numbers",determinants
"(1,...,n). The permutation gives the column numbers as we go down the matrix. The is",determinants
appear in P at the same places where the a’s appeared in A.,determinants
It remains to ﬁnd the determinant of P. Row exchanges transform it to the identity,determinants
"matrix, and each exchange reverses the sign of the determinant:",determinants
detP = +1 or −1,determinants
for an even or odd number of row exchanges.,determinants
4.3 Formulas for the Determinant,determinants
"(1,3,2) requires one exchange and (3,1,2) requires two exchanges to recover (1,2,3).",determinants
"These are two of the six ± signs. For n = 2, we only have (1,2) and (2,1):",determinants
"No one can claim that the big formula (6) is particularly simple. Nevertheless, it is",determinants
"possible to see why it has properties 1–3. For A = I, every product of the aij will be",determinants
"zero, except for the column sequence (1,2,...,n). This term gives detI = 1. Property 2",determinants
"will be checked in the next section, because here we are most interested in property 3:",determinants
"The determinant should depend linearly on the ﬁrst row a11,a12,...,a1n.",determinants
Look at all the terms a1αa2β ···anv involving a11. The ﬁrst column is α = 1. This,determinants
"leaves some permutation (β,...,v) of the remaining columns (2,...,n). We collect all",determinants
"these terms together as a11C11, where the coefﬁcient of a11 is a smaller determinant—",determinants
with row 1 and column 1 removed:,determinants
C11 = ∑(a2β ···anv)detP = det(submatrix of A).,determinants
"Similarly, the entry a12 is multiplied by some smaller determinant C12. Grouping all the",determinants
"terms that start with the same a1j, formula (6) becomes",determinants
Cofactors along row 1,determinants
detA = a11C11 +a12C12 +···+a1nC1n.,determinants
"This shows that detA depends linearly on the entries a11,...,a1n of the ﬁrst row.",determinants
"Example 2. For a 3 by 3 matrix, this way of collecting terms gives",determinants
detA = a11(a22a33 − a23a32)+a12(a23a31 − a21a33)+a13(a21a32 − a22a31). (9),determinants
"The cofactors C11, C12, C13 are the 2 by 2 determinants in parentheses.",determinants
Expansion of detA in Cofactors,determinants
"We want one more formula for the determinant. If this meant starting again from scratch,",determinants
"it would be too much, But the formula is already discovered—it is (8), and the only point",determinants
is to identify the cofactors C1 j that multiply a1 j.,determinants
"We know that C1 j depends on rows 2,...,n. Row 1 is already accounted for by",determinants
"a1j. Furthermore, a1j also accounts for the jth column, so its cofactor C1 j must depend",determinants
entirely on the other columns. No row or column can be used twice in the same term.,determinants
What we are really doing is splitting the determinant into the following sum:,determinants
"For a determinant of order n, this splitting gives n smaller determinants (minors) of order",determinants
n − 1; you see the three 2 by 2 submatrices. The submatrix M1j is formed by throwing,determinants
away row 1 and column j. Its determinant is multiplied by a1 j—and by a plus or minus,determinants
"sign. These signs alternate as in detM11, −detM12, detM13:",determinants
Cofactors of row 1,determinants
C1 j = (−1)1+j detM1 j.,determinants
"The second cofactor C12 is a23a31 −a21a33, which is detM12 times −1. This same tech-",determinants
nique works on every n by n matrix. The splitting above conﬁrms that C11 is the deter-,determinants
minant of the lower right corner M11.,determinants
"There is a similar expansion on any other row, say row i. It could be proved by",determinants
exchanging row i with row 1. Remember to delete row i and column j of A for Mij:,determinants
The determinant of A is a combination of any row i times its cofactors:,determinants
detA = ai1Ci1 +ai2Ci2 +···+ainCin.,determinants
The cofactor C1j is the determinant of Mij with the correct sign:,determinants
delete row i and column j,determinants
Cij = (−1)i+j detMij.,determinants
These formulas express detA as a combination of determinants of order n − 1. We,determinants
"could have deﬁned the determinant by induction on n. A 1 by 1 matrix has detA = a11,",determinants
"and then equation (10) deﬁnes the determinants of 2 by 2 matrices, 3 by 3 matrices,",determinants
"and n by n matrices. We preferred to deﬁne the determinant by its properties, which",determinants
are much simpler to explain. The explicit formula (6) and the cofactor formula (10),determinants
followed directly from these properties.,determinants
There is one more consequence of detA = detAT. We can expand in cofactors of a,determinants
"column of A, which is a row of AT. Down column j of A,",determinants
detA = a1 jC1 j +a2 jC2 j +···+anjCnj.,determinants
Example 3. The 4 by 4 second difference matrix A4 has only two nonzeros in row 1:,determinants
4.3 Formulas for the Determinant,determinants
"C11 comes from erasing row 1 and column 1, which leaves the −1, 2, −1 pattern:",determinants
C11 = detA3 = det,determinants
"For a12 = −1 it is column 2 that gets removed, and we need its cofactor C12:",determinants
This left us with the 2 by 2 determinant. Altogether row 1 has produced 2C11 −C12:,determinants
detA4 = 2(detA3)−detA2 = 2(4)−3 = 5,determinants
"The same idea applies to A5 and A6, and every An:",determinants
This gives the determinant of increasingly bigger matrices. At every step the determinant,determinants
"of An is n+1, from the previous determinants n and n−1:",determinants
"−1, 2, −1 matrix",determinants
detAn = 2(n)−(n−1) = n+1.,determinants
The answer n+1 agrees with the product of pivots at the start of this section.,determinants
"1. For these matrices, ﬁnd the only nonzero term in the big formula (6):",determinants
0 1 0 0,determinants
1 0 1 0,determinants
0 1 0 1,determinants
0 0 1 0,determinants
0 0 1 2,determinants
0 3 4 5,determinants
6 7 8 9,determinants
0 0 0 1,determinants
There is only one way of choosing four nonzero entries from different rows and,determinants
"different columns. By deciding even or odd, compute detA and detB.",determinants
2. Expand those determinants in cofactors of the ﬁrst row. Find the cofactors (they,determinants
include the signs (−1)i+j) and the determinants of A and B.,determinants
3. True or false?,determinants
(a) The determinant of S−1AS equals the determinant of A.,determinants
(b) If detA = 0 then at least one of the cofactors must be zero.,determinants
"(c) A matrix whose entries are 0s and 1s has determinant 1, 0, or −1.",determinants
"4. (a) Find the LU factorization, the pivots, and the determinant of the 4 by 4 matrix",determinants
whose entries are aij = smaller of i and j. (Write out the matrix.),determinants
"(b) Find the determinant if aij = smaller of ni and n j, where n1 = 2, n2 = 6, n3 = 8,",determinants
n4 = 10. Can you give a general rule for any n1 ≤ n2 ≤ n3 ≤ n4?,determinants
"5. Let Fn be the determinant of the 1, 1, −1 tridiagonal matrix (n by n):",determinants
"By expanding in cofactors along row 1, show that Fn = Fn−1 +Fn−2. This yields the",determinants
"Fibonacci sequence 1,2,3,5,8,13,... for the determinants.",determinants
6. Suppose An is the n by n tridiagonal matrix with is on the three diagonals:,determinants
Let Dn be the determinant of An; we want to ﬁnd it.,determinants
(a) Expand in cofactors along the ﬁrst row to show that Dn = Dn−1 −Dn−2.,determinants
"(b) Starting from D1 = 1 and D2 = 0, ﬁnd D3,D4,...,D8. By noticing how these",determinants
numbers cycle around (with what period?) ﬁnd D1000.,determinants
7. (a) Evaluate this determinant by cofactors of row 1:,determinants
4 4 4 4,determinants
1 2 0 1,determinants
2 0 1 2,determinants
1 1 0 2,determinants
(b) Check by subtracting column 1 from the other columns and recomputing.,determinants
"8. Compute the determinants of A2, A3, A4. Can you predict An?",determinants
0 1 1 1,determinants
1 0 1 1,determinants
1 1 0 1,determinants
1 1 1 0,determinants
"Use row operations to produce zeros, or use cofactors of row 1.",determinants
4.3 Formulas for the Determinant,determinants
9. How many multiplications to ﬁnd an n by n determinant from,determinants
(a) the big formula (6)?,determinants
"(b) the cofactor formula (10), building from the count for n−1?",determinants
(c) the product of pivots formula (including the elimination steps)?,determinants
"10. In a 5 by 5 matrix, does a + sign or − sign go with a15a24a33a42a51 down the reverse",determinants
"diagonal? In other words, is P = (5,4,3,2,1) even or odd? The checkerboard pattern",determinants
of ± signs for cofactors does not give detP.,determinants
"11. If A is m by n and B is n by m, explain why",determinants
Do an example with m < n and an example with m > n. Why does your second,determinants
example automatically have detAB = 0?,determinants
"12. Suppose the matrix A is ﬁxed, except that a11 varies from −∞ to +∞. Give examples",determinants
in which detA is always zero or never zero. Then show from the cofactor expansion,determinants
(8) that otherwise detA = 0 for exactly one value of a11.,determinants
Problems 13–23 use the big formula with n! terms: |A| = ∑±a1αa2β ···anv.,determinants
"13. Compute the determinants of A, B, C from six terms. Independent rows?",determinants
"14. Compute the determinants of A, B, C. Are their columns independent?",determinants
"15. Show that detA = 0, regardless of the ﬁve nonzeros marked by x’s:",determinants
(What is the rank of A?),determinants
16. This problem shows in two ways that detA = 0 (the x’s are any numbers):,determinants
0 0 0 x x,determinants
0 0 0 x x,determinants
0 0 0 x x,determinants
5 by 5 matrix,determinants
3 by 3 zero matrix,determinants
(a) How do you know that the rows are linearly dependent?,determinants
(b) Explain why all 120 terms are zero in the big formula for detA.,determinants
17. Find two ways to choose nonzeros from four different rows and columns:,determinants
1 0 0 1,determinants
0 1 1 1,determinants
1 1 0 1,determinants
1 0 0 1,determinants
1 0 0 2,determinants
0 3 4 5,determinants
5 4 0 3,determinants
2 0 0 1,determinants
(B has the same zeros as A.),determinants
Is detA equal to 1+1 or 1−1 or −1−1? What is detB?,determinants
18. Place the smallest number of zeros in a 4 by 4 matrix that will guarantee detA = 0.,determinants
Place as many zeros as possible while still allowing detA ̸= 0.,determinants
"19. (a) If a11 = a22 = a33 = 0, how many of the six terms in detA will be zero?",determinants
"(b) If a11 = a22 = a33 = a44 = 0, how many of the 24 products a1ja2ka3ℓa4m are sure",determinants
20. How many 5 by 5 permutation matrices have detP = +1? Those are even permuta-,determinants
tions. Find one that needs four exchanges to reach the identity matrix.,determinants
"21. If detA ̸= 0, at least one of the n! terms in the big formula (6) is not zero. Deduce",determinants
that some ordering of the rows of A leaves no zeros on the diagonal. (Don’t use P,determinants
from elimination; that PA can have zeros on the diagonal.),determinants
22. Prove that 4 is the largest determinant for a 3 by 3 matrix of 1s and −1s.,determinants
"23. How many permutations of (1,2,3,4) are even and what are they? Extra credit:",determinants
What are all the possible 4 by 4 determinants of I +Peven?,determinants
"Problems 24–33 use cofactors Cij = (−1)i+j detMij. Delete row i, column j.",determinants
24. Find cofactors and then transpose. Multiply CT,determinants
B by A and B!,determinants
25. Find the cofactor matrix C and compare ACT with A−1:,determinants
4.3 Formulas for the Determinant,determinants
"26. The matrix Bn is the −1, 2, −1 matrix An except that b11 = 1 instead of a11 = 2.",determinants
"Using cofactors of the last row of B4, show that |B4| = 2|B3|−|B2| = 1:",determinants
The recursion |Bn| = 2|Bn−1|−|Bn−2| is the same as for the A’s. The difference is in,determinants
"the starting values 1, 1, 1 for n = 1,2,3. What are the pivots?",determinants
"27. Bn is still the same as An except for b11 = 1. So use linearity in the ﬁrst row, where",determinants
[1 −1 0] equals [2 −1 0] minus [1 0 0]:,determinants
Linearity in row 1 gives |Bn| = |An|−|An−1| =,determinants
28. The n by n determinant Cn has 1s above and below the main diagonal:,determinants
0 1 0 0,determinants
1 0 1 0,determinants
0 1 0 1,determinants
0 0 1 0,determinants
"(a) What are the determinants of C1, C2, C3, C4?",determinants
(b) By cofactors ﬁnd the relation between Cn and Cn−1 and Cn−2. Find C10.,determinants
"29. Problem 28 has 1s just above and below the main diagonal. Going down the matrix,",determinants
which order of columns (if any) gives all 1s? Explain why that permutation is even,determinants
"for n = 4,8,12,... and odd for n = 2,6,10,...",determinants
Cn = 0 (odd n),determinants
"Cn = 1 (n = 4,8,...)",determinants
"Cn = −1 (n = 2,6,...).",determinants
30. Explain why this Vandermonde determinant contains x3 but not x4 or x5:,determinants
The determinant is zero at x =,determinants
. The cofactor of x3 is V3 =,determinants
(b−a)(c−a)(c−b). Then V4 = (x−a)(x−b)(x−c)V3.,determinants
"31. Compute the determinants S1, S2, S3 of these 1, 3, 1 tridiagonal matrices:",determinants
Make a Fibonacci guess for S4 and verify that you are right.,determinants
"32. Cofactors of those 1, 3, 1 matrices give Sn = 3Sn−1 − Sn−2. Challenge: Show that",determinants
Sn is the Fibonacci number F2n+2 by proving F2n+2 = 3F2n − F2n−2. Keep using,determinants
Fibonacci’s rule Fk = Fk−1 +Fk−2.,determinants
33. Change 3 to 2 in the upper left corner of the matrices in Problem 32. Why does,determinants
that subtract Sn−1 from the determinant Sn? Show that the determinants become the,determinants
"Fibonacci numbers 2, 5, 13 (always F2n+1).",determinants
Problems 34–36 are about block matrices and block determinants.,determinants
"34. With 2 by 2 blocks, you cannot always use block determinants!",determinants
(a) Why is the ﬁrst statement true? Somehow B doesn’t enter.,determinants
(b) Show by example that equality fails (as shown) when C enters.,determinants
(c) Show by example that the answer det(AD−CB) is also wrong.,determinants
"35. With block multiplication, A = LU has Ak = LkUk in the upper left corner:",determinants
"(a) Suppose the ﬁrst three pivots of A are 2, 3, −1. What are the determinants of L1,",determinants
"L2, L3 (with diagonal 1s), U1, U2, U3, and A1, A2, A3?",determinants
"(b) If A1, A2, A3 have determinants 5, 6, 7, ﬁnd the three pivots.",determinants
36. Block elimination subtracts CA−1 times the ﬁrst row [A B] from the second row,determinants
[C D]. This leaves the Schur complement D−CA−1B in the corner:,determinants
Take determinants of these matrices to prove correct rules for square blocks:,determinants
if AC = CA,determinants
4.4 Applications of Determinants,determinants
37. A 3 by 3 determinant has three products “down to the right” and three “down to the,determinants
left” with minus signs. Compute the six terms in the ﬁgure to ﬁnd D. Then explain,determinants
without determinants why this matrix is or is not invertible:,determinants
"38. For A4 in Problem 6, ﬁve of the 4! = 24 terms in the big formula (6) are nonzero.",determinants
Find those ﬁve terms to show that D4 = −1.,determinants
"39. For the 4 by 4 tridiagonal matrix (entries −1, 2, −1), ﬁnd the ﬁve terms in the big",determinants
formula that give detA = 16−4−4−4+1.,determinants
40. Find the determinant of this cyclic P by cofactors of row 1. How many exchanges,determinants
"reorder 4, 1, 2, 3 into 1, 2, 3, 4? Is |P2| = +1 or −1?",determinants
0 0 0 1,determinants
1 0 0 0,determinants
0 1 0 0,determinants
0 0 1 0,determinants
0 0 1 0,determinants
0 0 0 1,determinants
1 0 0 0,determinants
0 1 0 0,determinants
"41. A=2∗eye(n)−diag(ones(n−1, 1),1)−diag(ones(n−1, 1),−1) is the −1, 2, −1",determinants
"matrix. Change A(1,1) to 1 so detA = 1. Predict the entries of A−1 based on n = 3",determinants
and test the prediction for n = 4.,determinants
"42. (MATLAB) The −1, 2, −1 matrices have determinant n + 1. Compute (n + 1)A−1",determinants
"for n = 3 and 4, and verify your guess for n = 5. (Inverses of tridiagonal matrices",determinants
have the rank-1 form uvT above the diagonal.),determinants
"43. All Pascal matrices have determinant 1. If I subtract 1 from the n, n entry, why does",determinants
the determinant become zero? (Use rule 3 or a cofactor.),determinants
1 4 10 20,determinants
���� = 1 (known),determinants
1 4 10 19,determinants
���� = 0 (explain).,determinants
"This section follows through on four major applications: inverse of A, solving Ax = b,",determinants
"volumes of boxes, and pivots. They are among the key computations in linear algebra",determinants
(done by elimination). Determinants give formulas for the answers.,determinants
1. Computation of A−1. The 2 by 2 case shows how cofactors go into A−1:,determinants
"We are dividing by the determinant, and A is invertible exactly when detA is nonzero.",determinants
The number C11 = d is the cofactor of a. The number C12 = −c is the cofactor of b (note,determinants
"the minus sign). That number C12 goes in row 2, column 1!",determinants
"The row a, b times the column C11, C12 produces ad −bc. This is the cofactor expan-",determinants
sion of detA. That is the clue we need: A−1 divides the cofactors by detA.,determinants
Our goal is to verify this formula for A−1. We have to see why ACT = (detA)I:,determinants
"With cofactorsC11,...,C1n in the ﬁrst column and not the ﬁrst row, they multiply a11,...,a1n",determinants
and give the diagonal entry detA. Every row of A multiplies its cofactors (the cofactor,determinants
expansion) to give the same answer detA on the diagonal.,determinants
The critical question is: Why do we get zeros off the diagonal? If we combine the,determinants
"entries a1 j from row 1 with the cofactors C2j for row 2, why is the result zero?",determinants
"row 1 of A, row 2 of C",determinants
a11C21 +a12C22 +···+a1nC2n = 0.,determinants
"The answer is: We are computing the determinant of a new matrix B, with a new row 2.",determinants
"The ﬁrst row of A is copied into the second row of B. Then B has two equal rows, and",determinants
"detB = 0. Equation (3) is the expansion of detB along its row 2, where B has exactly",determinants
the same cofactors as A (because the second row is thrown away to ﬁnd those cofactors).,determinants
The remarkable matrix multiplication (2) is correct.,determinants
That multiplication ACT = (detA)I immediately gives A−1. Remember that the cofac-,determinants
tor from deleting row i and column j of A goes into row j and column i of CT. Dividing,determinants
by the number detA (if it is not zero!) gives A−1 = CT/detA.,determinants
Example 1. The inverse of a sum matrix is a difference matrix:,determinants
The minus signs enter because cofactors always include (−1)i+j.,determinants
4.4 Applications of Determinants,determinants
2. The Solution of Ax = b.,determinants
The multiplication x = A−1b is just CTb divided bydetA.,determinants
"There is a famous way in which to write the answer (x1,...,xn):",determinants
Cramer’s rule: The jth component of x = A−1b is the ratio,determinants
x j = detB j,determinants
�� has b in column j.,determinants
Proof. Expand detB j in cofactors of its jth column (which is b). Since the cofactors,determinants
"ignore that column, detB j is exactly the jth component in the product CTb:",determinants
detB j = b1C1 j +b2C2j +···+bnCnj.,determinants
Dividing this by detA gives x j. Each component of x is a ratio of two determinants. That,determinants
"fact might have been recognized from Gaussian elimination, but it never was.",determinants
Example 2. The solution of,determinants
has 0 and 6 in the ﬁrst column for x1 and in the second column for x2:,determinants
The denominators are always detA. For 1000 equations Cramer’s Rule would need 1001,determinants
determinants. To my dismay I found in a book called Mathematics for the Millions that,determinants
Cramer’s Rule was actually recommended (and elimination was thrown aside):,determinants
"To deal with a set involving the four variables u, v, w, z, we ﬁrst have to",determinants
eliminate one of them in each of three pairs to derive three equations in three,determinants
variables and then proceed as for the three-fold left-hand set to derive values,determinants
for two of them. The reader who does so as an exercise will begin to realize,determinants
"how formidably laborious the method of elimination becomes, when we have",determinants
to deal with more than three variables. This consideration invites us to explore,determinants
the possibility of a speedier method...,determinants
3. The Volume of a Box. The connection between the determinant and the volume is,determinants
"clearest when all angles are right angles—the edges are perpendicular, and the box is",determinants
rectangular. Then the volume is the product of the edge lengths: volume = ℓ1ℓ2···ℓn.,determinants
"We want to obtain the same ℓ1ℓ2···ℓn from detA, when the edges of that box are the",determinants
"rows of A. With right angles, these rows are orthogonal and AAT is diagonal:",determinants
The ℓi are the lengths of the rows (the edges). and the zeros off the diagonal come,determinants
"because the rows are orthogonal. Using the product and transposing rules,",determinants
n = det(AAT) = (detA)(detAT) = (detA)2.,determinants
The square root of this equation says that the determinant equals the volume. The sign,determinants
"of detA will indicate whether the edges form a “right-handed” set of coordinates, as in",determinants
"the usual x-y-z system, or a left-handed system like y-x-z.",determinants
"If the angles are not 90°, the volume is not the product of the lengths. In the plane",determinants
"(Figure 4.2), the “volume” of a parallelogram equals the base ℓ times the height h, The",determinants
"vector b− p of length h is the second row b = (a21,a22), minus its projection p onto the",determinants
"ﬁrst row. The key point is this: By rule 5, detA is unchanged when a multiple of row 1",determinants
"is subtracted from row 2. We can change the parallelogram to a rectangle, where it is",determinants
already proved that volume = determinant.,determinants
"In n dimensions, it takes longer to make each box rectangular, but the idea is the",determinants
same. The volume and determinant are unchanged if we subtract from each row its pro-,determinants
jection onto the space spanned by the preceding rows—leaving a perpendicular “height,determinants
"vector” like pb. This Gram-Schmidt process produces orthogonal rows, with volume =",determinants
determinant. So the same equality must have held for the original rows.,determinants
h = |b − p|,determinants
length ℓ = |a|,determinants
"b = (a21, a22)",determinants
"a = (a11, a12)",determinants
Figure 4.2: Volume (area) of the parallelogram = ℓ times h = |detA|.,determinants
"This completes the link between volumes and determinants, but it is worth coming",determinants
back one more time to the simplest case. We know that,determinants
4.4 Applications of Determinants,determinants
"These determinants give the volumes—or areas, since we are in two dimensions—drawn",determinants
in Figure 4.3. The parallelogram has unit base and unit height; its area is also 1.,determinants
"row 1 = (1, 0)",determinants
"row 2 = (0, 1)",determinants
"row 1 = (1, 0)",determinants
"row 2 = (c, 1)",determinants
Figure 4.3: The areas of a unit square and a unit parallelogram are both 1.,determinants
4. A Formula for the Pivots.,determinants
We can ﬁnally discover when elimination is possible,determinants
without row exchanges. The key observation is that the ﬁrst k pivots are completely,determinants
determined by the submatrix Ak in the upper left corner of A. The remaining rows and,determinants
columns of A have no effect on this corner of the problem:,determinants
(ad− bc)/a (af −ec)/a,determinants
"Certainly the ﬁrst pivot depended only on the ﬁrst row and column, The second pivot",determinants
(ad −bc)/a depends only on the 2 by 2 corner submatrix A2. The rest of A does not enter,determinants
"until the third pivot. Actually it is not just the pivots, but the entire upper-left corners of",determinants
"L, D, and U, that are determined by the upper-left corner of A:",determinants
A = LDU =,determinants
What we see in the ﬁrst two rows and columns is exactly the factorization of the corner,determinants
submatrix A2. This is a general rule if there are no row exchanges:,determinants
"4D If A is factored into LDU, the upper left corners satisfy Ak = LkDkUk. For",determinants
"every k, the submatrix Ak is going through a Gaussian elimination of its own.",determinants
"The proof is to see that this corner can be settled ﬁrst, before even looking at other",determinants
eliminations. Or use the laws for block multiplication:,determinants
"Comparing the last matrix with A, the corner LkDkUk coincides with Ak. Then:",determinants
detAk = detLk detDk detUk = detDk = d1d2···dk.,determinants
The product of the ﬁrst k pivots is the determinant of Ak. This is the same rule that,determinants
we know already for the whole matrix. Since the determinant of Ak−1 will be given by,determinants
"d1d2···dk−1, we can isolate each pivot dk as a ratio of determinants:",determinants
"In our example above, the second pivot was exactly this ratio (ad − bc)/a. It is the",determinants
"determinant of A2 divided by the determinant of A1. (By convention detA0 = 1, so that",determinants
"the ﬁrst pivot is a/1 = a.) Multiplying together all the individual pivots, we recover",determinants
From equation (5) we can ﬁnally read off the answer to our original question: The,determinants
pivot entries are all nonzero whenever the numbers detAk are all nonzero:,determinants
Elimination can be completed without row exchanges (so P = I and A =,determinants
"LU), if and only if the leading submatrices A1,A2,...,An are all nonsingular.",determinants
"That does it for determinants, except for an optional remark on property 2—the sign",determinants
reversal on row exchanges. The determinant of a permutation matrix P was the only,determinants
questionable point in the big formula. Independent of the particular row exchanges link-,determinants
"ing P to I, is the number of exchanges always even or always odd? If so, its determinant",determinants
is well deﬁned by rule 2 as either +1 or −1.,determinants
"Starting from (3,2,1), a single exchange of 3 and 1 would achieve the natural order",determinants
"(1,2,3). So would an exchange of 3 and 2, then 3 and 1, and then 2 and 1. In both",determinants
"sequences, the number of exchanges is odd. The assertion is that an even number of",determinants
"exchanges can never produce the natural order beginning with (3,2,1).",determinants
"Here is a proof. Look at each pair of numbers in the permutation, and let N count",determinants
the pairs in which the larger number comes ﬁrst. Certainly N = 0 for the natural order,determinants
"(1,2,3). The order (3,2,1) has N = 3 since all pairs (3,2), (3,1), and (2,1) are wrong.",determinants
We will show that every exchange alters N by an odd number. Then to arrive at N = 0,determinants
(the natural order) takes a number of exchanges having the same evenness or oddness as,determinants
"When neighbors are exchanged, N changes by +1 or −1. Any exchange can be",determinants
achieved by an odd number of exchanges of neighbors. This will complete the proof;,determinants
"an odd number of odd numbers is odd. To exchange the ﬁrst and fourth entries below,",determinants
"which happen to be 2 and 3, we use ﬁve exchanges (an odd number) of neighbors:",determinants
"(2,1,4,3) → (1,2,4,3) → (1,4,2,3) → (1,4,3,2) → (1,3,4,2) → (3,1,4,2).",determinants
We need ℓ − k exchanges of neighbors to move the entry in place k to place ℓ. Then,determinants
ℓ − k − 1 exchanges move the one originally in place ℓ (and now found in place ℓ − 1),determinants
"back down to place k. Since (ℓ − k) + (ℓ − k − 1) is odd, the proof is complete. The",determinants
"determinant not only has all the properties found earlier, it even exists.",determinants
4.4 Applications of Determinants,determinants
1. Find the determinant and all nine cofactors Cij of this triangular matrix:,determinants
Form CT and verify that ACT = (detA)I. What is A−1?,determinants
2. Use the cofactor matrix C to invert these symmetric matrices:,determinants
"3. Find x, y, and z by Cramer’s Rule in equation (4):",determinants
ax + by = 1,determinants
+ dy = 0,determinants
x + 4y −,determinants
+ 3z = 0.,determinants
4. (a) Find the determinant when a vector x replaces column j of the identity (consider,determinants
x j = 0 as a separate case):,determinants
"(b) If Ax = b, show that AM is the matrix B j in equation (4), with b in column j.",determinants
(c) Derive Cramer’s rule by taking determinants in AM = Bj.,determinants
"5. (a) Draw the triangle with vertices A = (2,2), B = (−1,3), and C = (0,0). By",determinants
"regarding it as half of a parallelogram, explain why its area equals",determinants
"(b) Move the third vertex to C = (1,−4) and justify the formula",determinants
Hint: Subtracting the last row from each of the others leaves,determinants
"Sketch A′ = (1,6), B′ = (−2,7), C′ = (0,0) and their relation to A, B, C.",determinants
6. Explain in terms of volumes why det3A = 3ndetA for an n by n matrix A.,determinants
"7. Predict in advance, and conﬁrm by elimination, the pivot entries of",determinants
"8. Find all the odd permutations of the numbers {1,2,3,4}. They come from an odd",determinants
number of exchanges and lead to detP = −1.,determinants
"9. Suppose the permutation P takes (1,2,3,4,5) to (5,4,1,2,3).",determinants
"(a) What does P2 do to (1,2,3,4,5)?",determinants
"(b) What does P−1 do to (1,2,3,4,5)?",determinants
"10. If P is an odd permutation, explain why P2 is even but P−1 is odd.",determinants
"11. Prove that if you keep multiplying A by the same permutation matrix P, the ﬁrst row",determinants
eventually comes back to its original place.,determinants
"12. If A is a 5 by 5 matrix with all |aij| ≤ 1, then detA ≤",determinants
. Volumes or the big,determinants
formula or pivots should give some upper bound on the determinant.,determinants
Problems 13–17 are about Cramer’s Rule for x = A−1b.,determinants
13. Solve these linear equations by Cramer’s Rule x j = detB j/detA:,determinants
14. Use Cramer’s Rule to solve for y (only). Call the 3 by 3 determinant D:,determinants
cx + dy = 0.,determinants
ax + by +,determinants
gx + hy +,determinants
4.4 Applications of Determinants,determinants
"15. Cramer’s Rule breaks down when detA = 0. Example (a) has no solution, whereas",determinants
(b) has inﬁnitely many. What are the ratios x j = detB j/detA?,determinants
2x1 +3x2 = 1,determinants
4x1 +6x2 = 1.,determinants
2x1 +3x2 = 1,determinants
4x1 +6x2 = 2.,determinants
16. Quick proof of Cramer’s rule. The determinant is a linear function of column 1. It is,determinants
zero if two columns are equal. When b = Ax = x1a1+x2a2+x3a3 goes into column,determinants
"1 to produce B1, the determinant is",determinants
(a) What formula for x1 comes from left side = right side?,determinants
(b) What steps lead to the middle equation?,determinants
"17. If the right side b is the last column of A, solve the 3 by 3 system Ax = b. Explain",determinants
how each determinant in Cramer’s Rule leads to your solution x.,determinants
Problems 18–26 are about A−1 = CT/detA. Remember to transpose C.,determinants
18. Find A−1 from the cofactor formula CT/detA. Use symmetry in part (b):,determinants
"19. If all the cofactors are zero, how do you know that A has no inverse? If none of the",determinants
"cofactors are zero, is A sure to be invertible?",determinants
20. Find the cofactors of A and multiply ACT to ﬁnd detA:,determinants
"If you change that corner entry from 4 to 100, why is detA unchanged?",determinants
21. Suppose detA = 1 and you know all the cofactors. How can you ﬁnd A?,determinants
22. From the formula ACT = (detA)I show that detC = (detA)n−1.,determinants
"23. (For professors only) If you know all 16 cofactors of a 4 by 4 invertible matrix A,",determinants
how would you ﬁnd A?,determinants
"24. If all entries of A are integers, and detA = 1 or −1, prove that all entries of A−1 are",determinants
integers. Give a 2 by 2 example.,determinants
25. L is lower triangular and S is symmetric. Assume they are invertible:,determinants
(a) Which three cofactors of L are zero? Then L−1 is lower triangular.,determinants
(b) Which three pairs of cofactors of S are equal? Then S−1 is symmetric.,determinants
26. For n = 5 the matrix C contains,determinants
cofactors and each 4 by 4 cofactor contains,determinants
terms and each term needs,determinants
multiplications. Compare with 53 = 125 for,determinants
the Gauss-Jordan computation of A−1.,determinants
Problems 27–36 are about area and volume by determinants.,determinants
"27. (a) Find the area of the parallelogram with edges v = (3,2) and w = (1,4).",determinants
"(b) Find the area of the triangle with sides v, w, and v+w. Draw it.",determinants
"(c) Find the area of the triangle with sides v, w, and w−v. Draw it.",determinants
"28. A box has edges from (0,0,0) to (3,1,1), (1,3,1), and (1,1,3). Find its volume and",determinants
also ﬁnd the area of each parallelogram face.,determinants
"29. (a) The corners of a triangle are (2,1), (3,4), and (0,5). What is the area?",determinants
"(b) A new corner at (−1,0) makes it lopsided (four sides). Find the area.",determinants
"30. The parallelogram with sides (2,1) and (2,3) has the same area as the parallelogram",determinants
"with sides (2,2) and (1,3). Find those areas from 2 by 2 determinants and say why",determinants
they must be equal. (I can’t see why from a picture. Please write to me if you do.),determinants
31. The Hadamard matrix H has orthogonal rows. The box is a hypercube!,determinants
What is detH =,determinants
= volume of a hypercube in R4?,determinants
"32. If the columns of a 4 by 4 matrix have lengths L1, L2, L3, L4, what is the largest",determinants
"possible value for the determinant (based on volume)? If all entries are 1 or −1,",determinants
what are those lengths and the maximum determinant?,determinants
33. Show by a picture how a rectangle with area x1y2 minus a rectangle with area x2y1,determinants
produces the area x1y2 −x2y1 of a parallelogram.,determinants
"34. When the edge vectors a, b, c are perpendicular, the volume of the box is ∥a∥ times",determinants
∥b∥ times ∥c∥. The matrix ATA is,determinants
. Find detATA and detA.,determinants
4.4 Applications of Determinants,determinants
35. An n-dimensional cube has how many corners? How many edges? How many (n−,determinants
1)-dimensional faces? The n-cube whose edges are the rows of 2I has volume,determinants
A hypercube computer has parallel processors at the corners with connections along,determinants
"36. The triangle with corners (0,0), (1,0), (0,1) has area 1",determinants
2. The pyramid with four,determinants
"corners (0,0,0), (1,0,0), (0,1,0), (0,0,1) has volume",determinants
. The pyramid in R4,determinants
"with ﬁve corners at (0,0,0,0) and the rows of I has what volume?",determinants
Problems 37–40 are about areas dA and volumes dV in calculus.,determinants
37. Polar coordinates satisfy x = rcosθ and y = rsinθ. Polar area J dr dθ includes J:,determinants
The two columns are orthogonal. Their lengths are,determinants
. Thus J =,determinants
"38. Spherical coordinates ρ, φ, θ give x = ρ sinφ cosθ, y = ρ sinφ sinθ, z = ρ cosφ.",determinants
"Find the Jacobian matrix of 9 partial derivatives: ∂x/∂ρ, ∂x/∂φ, ∂x/∂θ are in row",determinants
1. Simplify its determinant to J = ρ2sinφ. Then dV = ρ2sinφ dρ dφ dθ.,determinants
"39. The matrix that connects r, θ to x, y is in Problem 37. Invert that matrix:",determinants
It is surprising that ∂r/∂x = ∂x/∂r. The product JJ−1 = I gives the chain rule,determinants
"40. The triangle with corners (0,0), (6,0), and (1,4) has area",determinants
. When you rotate it,determinants
by θ = 60° the area is,determinants
. The rotation matrix has,determinants
"41. Let P = (1,0,−1), Q = (1,1,1), and R = (2,2,1). Choose S so that PQRS is a",determinants
"parallelogram, and compute its area. Choose T, U, V so that OPQRSTUV is a tilted",determinants
"box, and compute its volume.",determinants
"42. Suppose (x,y,z), (1,1,0), and (1,2,1) lie on a plane through the origin. What deter-",determinants
minant is zero? What equation does this give for the plane?,determinants
"43. Suppose (x,y,z) is a linear combination of (2,3,1) and (1,2,3). What determinant",determinants
is zero? What equation does this give for the plane of all combinations?,determinants
"44. If Ax = (1,0,...,0) show how Cramer’s Rule gives x = ﬁrst column of A−1.",determinants
"45. (VISA to AVIS) This takes an odd number of exchanges (IVSA, AVSI, AVIS). Count",determinants
the pairs of letters in VISA and AVIS that are reversed from alphabetical order. The,determinants
difference should be odd.,determinants
4.1 Find the determinants of,determinants
1 1 1 1,determinants
1 1 1 2,determinants
1 1 3 1,determinants
1 4 1 1,determinants
"4.2 If B = M−1AM, why is detB = detA? Show also that detA−1B = 1.",determinants
"4.3 Starting with A, multiply its ﬁrst row by 3 to produce B, and subtract the ﬁrst row",determinants
of B from the second to produce C. How is detC related to detA?,determinants
"4.4 Solve 3u+2v = 7, 4u+3v = 11 by Cramer’s rule.",determinants
"4.5 If the entries of A and A−1 are all integers, how do you know that both determinants",determinants
are 1 or −1? Hint: What is detA times detA−1?,determinants
"4.6 Find all the cofactors, and the inverse or the nullspace, of",determinants
"4.7 What is the volume of the parallelepiped with four of its vertices at (0,0,0), (−1,2,2),",determinants
"(2,−1,2), and (2,2,−1)? Where are the other four vertices?",determinants
"4.8 How many terms are in the expansion of a 5 by 5 determinant, and how many are",determinants
sure to be zero if a21 = 0?,determinants
"4.9 If P1 is an even permutation matrix and P2 is odd, deduce from P1 + P2 = P1(PT",determinants
2 )P2 that det(P1 +P2) = 0.,determinants
"4.10 If detA > 0, show that A can be connected to I by a continuous chain of matrices",determinants
A(t) all with positive determinants. (The straight path A(t) = A+t(I −A) does go,determinants
"from A(0) = A to A(1) = I, but in between A(t) might be singular. The problem is",determinants
"not so easy, and solutions are welcomed by the author.)",determinants
4.4 Applications of Determinants,determinants
"4.11 Explain why the point (x,y) is on the line through (2,8) and (4,7) if",determinants
"4.12 In analogy with the previous exercise, what is the equation for (x,y,z) to be on the",determinants
"plane through (2,0,0), (0,2,0), and (0,0,4)? It involves a 4 by 4 determinant.",determinants
"4.13 If the points (x,y,z), (2,1,0), and (1,1,1) lie on a plane through the origin, what",determinants
"determinant is zero? Are the vectors (1,0,−1), (2,1,0), (1,1,1) independent?",determinants
"4.14 If every row of A has either a single +1, or a single −1, or one of each (and is",determinants
"otherwise zero), show that detA = 1 or −1 or 0.",determinants
4.15 If C =,determinants
and D = [ u v,determinants
"w z], then CD = −DC yields 4 equations Ax = 0:",determinants
"(a) Show that detA = 0 if a+d = 0. Solve for u, v, w, z, the entries of D.",determinants
(b) Show that detA = 0 if ad = bc (so C is singular).,determinants
"In all other cases, CD = −DC is only possible with D = zero matrix.",determinants
"4.16 The circular shift permutes (1,2,...,n) into (2,3,...,1). What is the corresponding",determinants
"permutation matrix P, and (depending on n) what is its determinant?",determinants
4.17 Find the determinant of A = eye(5) + ones(5) and if possible eye(n) + ones(n).,determinants
This chapter begins the “second half” of linear algebra. The ﬁrst half was about Ax =,eigenvec_val
b. The new problem Ax = λx will still be solved by simplifying a matrix—making it,eigenvec_val
diagonal if possible. The basic step is no longer to subtract a multiple of one row from,eigenvec_val
"another: Elimination changes the eigenvalues, which we don’t want.",eigenvec_val
Determinants give a transition from Ax = b to Ax = λx. In both cases the determinant,eigenvec_val
"leads to a “formal solution”: to Cramer’s rule for x = A−1b, and to the polynomial",eigenvec_val
"det(A − λI), whose roots will be the eigenvalues. (All matrices are now square; the",eigenvec_val
eigenvalues of a rectangular matrix make no more sense than its determinant.) The,eigenvec_val
"determinant can actually be used if n = 2 or 3. For large n, computing λ is more difﬁcult",eigenvec_val
than solving Ax = b.,eigenvec_val
"The ﬁrst step is to understand how eigenvalues can be useful, One of their applications",eigenvec_val
is to ordinary differential equations. We shall not assume that the reader is an expert on,eigenvec_val
"differential equations! If you can differentiate xn, sinx, and ex, you know enough. As a",eigenvec_val
"speciﬁc example, consider the coupled pair of equations",eigenvec_val
This is an initial-value problem. The unknown is speciﬁed at time t = 0 by the given,eigenvec_val
initial values 8 and 5. The problem is to ﬁnd v(t) and w(t) for later times t > 0.,eigenvec_val
"It is easy to write the system in matrix form. Let the unknown vector be u(t), with",eigenvec_val
initial value u(0). The coefﬁcient matrix is A:,eigenvec_val
The two coupled equations become the vector equation we want:,eigenvec_val
u = u(0) at t = 0.,eigenvec_val
This is the basic statement of the problem. Note that it is a ﬁrst-order equation—no,eigenvec_val
"higher derivatives appear—and it is linear in the unknowns, It also has constant coefﬁ-",eigenvec_val
cients; the matrix A is independent of time.,eigenvec_val
"How do we ﬁnd u(t)? If there were only one unknown instead of two, that question",eigenvec_val
would be easy to answer. We would have a scalar instead of a vector equation:,eigenvec_val
u = u(0) at t = 0.,eigenvec_val
The solution to this equation is the one thing you need to know:,eigenvec_val
"At the initial time t = 0, u equals u(0) because e0 = 1. The derivative of eat has the",eigenvec_val
"required factor a, so that du/dt = au. Thus the initial condition and the equation are",eigenvec_val
"Notice the behavior of u for large times. The equation is unstable if a > 0, neutrally",eigenvec_val
"stable if a = 0, or stable if a < 0; the factor eat approaches inﬁnity, remains bounded,",eigenvec_val
"or goes to zero. If a were a complex number, a = α +iβ, then the same tests would be",eigenvec_val
applied to the real part α. The complex part produces oscillations eiβt = cosβt +isinβt.,eigenvec_val
Decay or growth is governed by the factor eαt.,eigenvec_val
"So much for a single equation. We shall take a direct approach to systems, and look",eigenvec_val
for solutions with the same exponential dependence on t just found in the scalar case:,eigenvec_val
or in vector notation,eigenvec_val
This is the whole key to differential equations du/dt = Au: Look for pure exponential,eigenvec_val
"solutions. Substituting v = eλty and w = eλtz into the equation, we ﬁnd",eigenvec_val
"The factor eλt is common to every term, and can be removed. This cancellation is the",eigenvec_val
reason for assuming the same exponent λ for both unknowns; it leaves,eigenvec_val
That is the eigenvalue equation. In matrix form it is Ax = λx. You can see it again if we,eigenvec_val
use u = eλtx—a number eλt that grows or decays times a ﬁxed vector x. Substituting,eigenvec_val
into du/dt = Au gives λeλtx = Aeλtx. The cancellation of eλt produces,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Now we have the fundamental equation of this chapter. It involves two unknowns,eigenvec_val
"λ and x. It is an algebra problem, and differential equations can be forgotten! The",eigenvec_val
"number λ (lambda) is an eigenvalue of the matrix A, and the vector x is the associated",eigenvec_val
"eigenvector. Our goal is to ﬁnd the eigenvalues and eigenvectors, λ’s and x’s, and to use",eigenvec_val
The Solution of Ax = λx,eigenvec_val
"Notice that Ax = λx is a nonlinear equation; λ multiplies x. If we could discover λ, then",eigenvec_val
"the equation for x would be linear. In fact we could write λIx in place of λx, and bring",eigenvec_val
this term over to the left side:,eigenvec_val
The identity matrix keeps matrices and vectors straight; the equation (A − λ)x = 0 is,eigenvec_val
"shorter, but mixed up. This is the key to the problem:",eigenvec_val
The vector x is in the nullspace of A−λI.,eigenvec_val
The number λ is chosen so that A−λI has a nullspace.,eigenvec_val
"Of course every matrix has a nullspace. It was ridiculous to suggest otherwise, but you",eigenvec_val
"see the point. We want a nonzero eigenvector x, The vector x = 0 always satisﬁes",eigenvec_val
"Ax = λx, but it is useless in solving differential equations. The goal is to build u(t) out",eigenvec_val
"of exponentials eλtx, and we are interested only in those particular values λ for which",eigenvec_val
"there is a nonzero eigenvector x. To be of any use, the nullspace of A−λI must contain",eigenvec_val
"vectors other than zero. In short, A−λI must be singular.",eigenvec_val
"For this, the determinant gives a conclusive test.",eigenvec_val
The number λ is an eigenvalue of A if and only if A−λI is singular:,eigenvec_val
This is the characteristic equation. Each λ is associated with eigenvectors x:,eigenvec_val
"In our example, we shift A by λI to make it singular:",eigenvec_val
Note that λ is subtracted only from the main diagonal (because it multiplies I).,eigenvec_val
λ 2 −λ −2.,eigenvec_val
"This is the characteristic polynomial. Its roots, where the determinant is zero, are the",eigenvec_val
"eigenvalues. They come from the general formula for the roots of a quadratic, or from",eigenvec_val
"factoring into λ 2 − λ − 2 = (λ + 1)(λ − 2). That is zero if λ = −1 or λ = 2, as the",eigenvec_val
= −1 and 2.,eigenvec_val
"There are two eigenvalues, because a quadratic has two roots. Every 2 by 2 matrix",eigenvec_val
A−λI has λ 2 (and no higher power of λ) in its determinant.,eigenvec_val
The values λ = −1 and λ = 2 lead to a solution of Ax = λx or (A−λI)x = 0. A matrix,eigenvec_val
"with zero determinant is singular, so there must be nonzero vectors x in its nullspace. In",eigenvec_val
fact the nullspace contains a whole line of eigenvectors; it is a subspace!,eigenvec_val
λ1 = −1 :,eigenvec_val
The solution (the ﬁrst eigenvector) is any nonzero multiple of x1:,eigenvec_val
The computation for λ2 is done separately:,eigenvec_val
λ2 = 2 :,eigenvec_val
The second eigenvector is any nonzero multiple of x2:,eigenvec_val
"You might notice that the columns of A − λ1I give x2, and the columns of A − λ2I are",eigenvec_val
multiples of x1. This is special (and useful) for 2 by 2 matrices.,eigenvec_val
"In the 3 by 3 case, I often set a component of x equal to 1 and solve (A−λI)x = 0 for",eigenvec_val
the other components. Of course if x is an eigenvector then so is 7x and so is −x. All,eigenvec_val
vectors in the nullspace of A−λI (which we call the eigenspace) will satisfy Ax = λx.,eigenvec_val
"In our example the eigenspaces are the lines through x1 = (1,1) and x2 = (5,2).",eigenvec_val
"Before going back to the application (the differential equation), we emphasize the",eigenvec_val
steps in solving Ax = λx:,eigenvec_val
"1. Compute the determinant of A − λI. With λ subtracted along the diagonal, this",eigenvec_val
determinant is a polynomial of degree n. It starts with (−λ)n.,eigenvec_val
2. Find the roots of this polynomial. The n roots are the eigenvalues of A.,eigenvec_val
3. For each eigenvalue solve the equation (A − λI)x = 0. Since the determinant is,eigenvec_val
"zero, there are solutions other than x = 0. Those are the eigenvectors.",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"In the differential equation, this produces the special solutions u = eλtx. They are the",eigenvec_val
pure exponential solutions to du/dt = Au. Notice e−t and e2t:,eigenvec_val
u(t) = eλ1tx1 = e−t,eigenvec_val
u(t) = eλ2tx2 = e2t,eigenvec_val
These two special solutions give the complete solution. They can be multiplied by any,eigenvec_val
"numbers c1 and c2, and they can be added together. When u1 and u2 satisfy the linear",eigenvec_val
"equation du/dt = Au, so does their sum u1 +u2:",eigenvec_val
u(t) = c1eλ1tx1 +c2eλ2tx2,eigenvec_val
"This is superposition, and it applies to differential equations (homogeneous and linear)",eigenvec_val
"just as it applied to matrix equations Ax = 0. The nullspace is always a subspace, and",eigenvec_val
combinations of solutions are still solutions.,eigenvec_val
"Now we have two free parameters c1 and c2, and it is reasonable to hope that they can",eigenvec_val
be chosen to satisfy the initial condition u = u(0) at t = 0:,eigenvec_val
c1x1 +c2x2 = u(0),eigenvec_val
"The constants are c1 = 3 and c2 = 1, and the solution to the original equation is",eigenvec_val
"Writing the two components separately, we have v(0) = 8 and w(0) = 5:",eigenvec_val
"v(t) = 3e−t +5e2t,",eigenvec_val
w(t) = 3e−t +2e2t.,eigenvec_val
The key was in the eigenvalues λ and eigenvectors x. Eigenvalues are important in,eigenvec_val
"themselves, and not just part of a trick for ﬁnding u. Probably the homeliest example",eigenvec_val
"is that of soldiers going over a bridge.1 Traditionally, they stop marching and just walk",eigenvec_val
across. If they happen to march at a frequency equal to one of the eigenvalues of the,eigenvec_val
"bridge, it would begin to oscillate. (Just as a child’s swing does; you soon notice the",eigenvec_val
"natural frequency of a swing, and by matching it you make the swing go higher.) An",eigenvec_val
engineer tries to keep the natural frequencies of his bridge or rocket away from those of,eigenvec_val
"the wind or the sloshing of fuel. And at the other extreme, a stockbroker spends his life",eigenvec_val
trying to get in line with the natural frequencies of the market. The eigenvalues are the,eigenvec_val
most important feature of practically any dynamical system.,eigenvec_val
"To summarize, this introduction has shown how λ and x appear naturally and auto-",eigenvec_val
matically when solving du/dt = Au. Such an equation has pure exponential solutions,eigenvec_val
1One which I never really believed—but a bridge did crash this way in 1831.,eigenvec_val
"u = eλtx; the eigenvalue gives the rate of growth or decay, and the eigenvector x develops",eigenvec_val
"at this rate. The other solutions will be mixtures of these pure solutions, and the mixture",eigenvec_val
is adjusted to ﬁt the initial conditions.,eigenvec_val
The key equation was Ax = λx. Most vectors x will not satisfy such an equation.,eigenvec_val
"They change direction when multiplied by A, so that Ax is not a multiple of x. This",eigenvec_val
"means that only certain special numbers are eigenvalues, and only certain special",eigenvec_val
"vectors x are eigenvectors. We can watch the behavior of each eigenvector, and then",eigenvec_val
combine these “normal modes” to ﬁnd the solution. To say the same thing in another,eigenvec_val
"way, the underlying matrix can be diagonalized.",eigenvec_val
"The diagonalization in Section 5.2 will be applied to difference equations, Fibonacci",eigenvec_val
"numbers, and Markov processes, and also to differential equations. In every example,",eigenvec_val
we start by computing the eigenvalues and eigenvectors; there is no shortcut to avoid,eigenvec_val
that. Symmetric matrices are especially easy. “Defective matrices” lack a full set of,eigenvec_val
"eigenvectors, so they are not diagonalizable. Certainly they have to be discussed, but we",eigenvec_val
will not allow them to take over the book.,eigenvec_val
We start with examples of particularly good matrices.,eigenvec_val
Example 1. Everything is clear when A is a diagonal matrix:,eigenvec_val
On each eigenvector A acts like a multiple of the identity: Ax1 = 3x1 and Ax2 = 2x2.,eigenvec_val
"Other vectors like x = (1,5) are mixtures x1 +5x2 of the two eigenvectors, and when A",eigenvec_val
multiplies x1 and x2 it produces the eigenvalues λ1 = 3 and λ2 = 2:,eigenvec_val
This is Ax for a typical vector x—not an eigenvector. But the action of A is determined,eigenvec_val
by its eigenvectors and eigenvalues.,eigenvec_val
Example 2. The eigenvalues of a projection matrix are 1 or 0!,eigenvec_val
"We have λ = 1 when x projects to itself, and λ = 0 when x projects to the zero vector.",eigenvec_val
"The column space of P is ﬁlled with eigenvectors, and so is the nullspace. If those spaces",eigenvec_val
"have dimension r and n −r, then λ = 1 is repeated r times and λ = 0 is repeated n −r",eigenvec_val
times (always n λ’s):,eigenvec_val
1 0 0 0,eigenvec_val
0 0 0 0,eigenvec_val
0 0 0 0,eigenvec_val
0 0 0 1,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"There is nothing exceptional about λ = 0. Like every other number, zero might be",eigenvec_val
"an eigenvalue and it might not. If it is, then its eigenvectors satisfy Ax = 0x. Thus x is",eigenvec_val
in the nullspace of A. A zero eigenvalue signals that A is singular (not invertible); its,eigenvec_val
determinant is zero. Invertible matrices have all λ ̸= 0.,eigenvec_val
Example 3. The eigenvalues are on the main diagonal when A is triangular:,eigenvec_val
"The determinant is just the product of the diagonal entries. It is zero if λ = 1, λ = 3",eigenvec_val
2; the eigenvalues were already sitting along the main diagonal.,eigenvec_val
"This example, in which the eigenvalues can be found by inspection, points to one",eigenvec_val
main theme of the chapter: To transform A into a diagonal or triangular matrix without,eigenvec_val
changing its eigenvalues. We emphasize once more that the Gaussian factorization A =,eigenvec_val
"LU is not suited to this purpose. The eigenvalues of U may be visible on the diagonal,",eigenvec_val
but they are not the eigenvalues of A.,eigenvec_val
"For most matrices, there is no doubt that the eigenvalue problem is computationally",eigenvec_val
"more difﬁcult than Ax = b. With linear systems, a ﬁnite number of elimination steps",eigenvec_val
"produced the exact answer in a ﬁnite time. (Or equivalently, Cramer’s rule gave an exact",eigenvec_val
"formula for the solution.) No such formula can give the eigenvalues, or Galois would",eigenvec_val
"turn in his grave. For a 5 by 5 matrix, det(A−λI) involves λ 5. Galois and Abel proved",eigenvec_val
that there can be no algebraic formula for the roots of a ﬁfth-degree polynomial.,eigenvec_val
"All they will allow is a few simple checks on the eigenvalues, after they have been",eigenvec_val
"computed, and we mention two good ones: sum and product.",eigenvec_val
The sum of the n eigenvalues equals the sum of the n diagonal entries:,eigenvec_val
A = λ1 +···+λn = a11 +···+ann.,eigenvec_val
"Furthermore, the product of the n eigenvalues equals the determinant of A.",eigenvec_val
The projection matrix P had diagonal entries 1,eigenvec_val
"2 and eigenvalues 1, 0. Then 1",eigenvec_val
"agrees with 1 + 0 as it should. So does the determinant, which is 0 · 1 = 0. A singular",eigenvec_val
"matrix, with zero determinant, has one or more of its eigenvalues equal to zero.",eigenvec_val
There should be no confusion between the diagonal entries and the eigenvalues. For,eigenvec_val
"a triangular matrix they are the same—but that is exceptional. Normally the pivots,",eigenvec_val
"diagonal entries, and eigenvalues are completely different, And for a 2 by 2 matrix, the",eigenvec_val
trace and determinant tell us everything:,eigenvec_val
"has trace a+d, and determinant ad −bc",eigenvec_val
����� = λ 2 −(trace)λ +determinant,eigenvec_val
The eigenvalues are λ = trace±,eigenvec_val
Those two λ’s add up to the trace; Exercise 9 gives ∑λi = trace for all matrices.,eigenvec_val
"There is a MATLAB demo (just type eigshow), displaying the eigenvalue problem for a",eigenvec_val
"2 by 2 matrix. It starts with the unit vector x = (1,0). The mouse makes this vector move",eigenvec_val
"around the unit circle. At the same time the screen shows Ax, in color and also moving.",eigenvec_val
Possibly Ax is ahead of x. Possibly Ax is behind x. Sometimes Ax is parallel to x. At that,eigenvec_val
"parallel moment, Ax = λx (twice in the second ﬁgure).",eigenvec_val
"y = (0, 1)",eigenvec_val
"x = (1, 0)",eigenvec_val
"Ay = (0.3, 0.7)",eigenvec_val
"Ax = (0.8, 0.2)",eigenvec_val
"The eigenvalue λ is the length of Ax, when the unit eigenvector x is parallel. The",eigenvec_val
"built-in choices for A illustrate three possibilities: 0, 1, or 2 real eigenvectors.",eigenvec_val
1. There are no real eigenvectors. Ax stays behind or ahead of x. This means the,eigenvec_val
"eigenvalues and eigenvectors are complex, as they are for the rotation Q.",eigenvec_val
2. There is only one line of eigenvectors (unusual). The moving directions Ax and x,eigenvec_val
meet but don’t cross. This happens for the last 2 by 2 matrix below.,eigenvec_val
3. There are eigenvectors in two independent directions. This is typical! Ax crosses x,eigenvec_val
"at the ﬁrst eigenvector x1, and it crosses back at the second eigenvector x2.",eigenvec_val
Suppose A is singular (rank 1). Its column space is a line. The vector Ax has to stay on,eigenvec_val
that line while x circles around. One eigenvector x is along the line. Another eigenvector,eigenvec_val
appears when Ax2 = 0. Zero is an eigenvalue of a singular matrix.,eigenvec_val
You can mentally follow x and Ax for these six matrices. How many eigenvectors and,eigenvec_val
"where? When does Ax go clockwise, instead of counterclockwise with x?",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
1. Find the eigenvalues and eigenvectors of the matrix A =,eigenvec_val
. Verify that the trace,eigenvec_val
"equals the sum of the eigenvalues, and the determinant equals their product.",eigenvec_val
"2. With the same matrix A, solve the differential equation du/dt = Au, u(0) =",eigenvec_val
What are the two pure exponential solutions?,eigenvec_val
"3. If we shift to A − 7I, what are the eigenvalues and eigenvectors and how are they",eigenvec_val
related to those of A?,eigenvec_val
B = A−7I =,eigenvec_val
"4. Solve du/dt = Pu, when P is a projection:",eigenvec_val
Part of u(0) increases exponentially while the nullspace part stays ﬁxed.,eigenvec_val
5. Find the eigenvalues and eigenvectors of,eigenvec_val
Check that λ1 +λ2 +λ3 equals the trace and λ1λ2λ3 equals the determinant.,eigenvec_val
6. Give an example to show that the eigenvalues can be changed when a multiple of one,eigenvec_val
row is subtracted from another. Why is a zero eigenvalue not changed by the steps,eigenvec_val
"7. Suppose that λ is an eigenvalue of A, and x is its eigenvector: Ax = λx.",eigenvec_val
"(a) Show that this same x is an eigenvector of B = A − 7I, and ﬁnd the eigenvalue.",eigenvec_val
This should conﬁrm Exercise 3.,eigenvec_val
"(b) Assuming λ ̸= 0, show that x is also an eigenvector of A−1—and ﬁnd the eigen-",eigenvec_val
8. Show that the determinant equals the product of the eigenvalues by imagining that,eigenvec_val
the characteristic polynomial is factored into,eigenvec_val
"det(A−λI) = (λ1 −λ)(λ2 −λ)···(λn −λ),",eigenvec_val
and making a clever choice of λ.,eigenvec_val
"9. Show that the trace equals the sum of the eigenvalues, in two steps. First, ﬁnd the",eigenvec_val
"coefﬁcient of (−λ)n−1 on the right side of equation (16). Next, ﬁnd all the terms in",eigenvec_val
that involve (−λ)n−1. They all come from the main diagonal! Find that coefﬁcient,eigenvec_val
of (−λ)n−1 and compare.,eigenvec_val
10. (a) Construct 2 by 2 matrices such that the eigenvalues of AB are not the products of,eigenvec_val
"the eigenvalues of A and B, and the eigenvalues of A+B are not the sums of the",eigenvec_val
"(b) Verify, however, that the sum of the eigenvalues of A + B equals the sum of all",eigenvec_val
"the individual eigenvalues of A and B, and similarly for products. Why is this",eigenvec_val
11. The eigenvalues of A equal the eigenvalues of AT. This is because det(A − λI),eigenvec_val
equals det(AT −λI). That is true because,eigenvec_val
. Show by an example that the eigen-,eigenvec_val
vectors of A and AT are not the same.,eigenvec_val
12. Find the eigenvalues and eigenvectors of,eigenvec_val
"13. If B has eigenvalues 1, 2, 3, C has eigenvalues 4, 5, 6, and D has eigenvalues 7, 8, 9,",eigenvec_val
what are the eigenvalues of the 6 by 6 matrix A =,eigenvec_val
14. Find the rank and all four eigenvalues for both the matrix of ones and the checker,eigenvec_val
1 1 1 1,eigenvec_val
1 1 1 1,eigenvec_val
1 1 1 1,eigenvec_val
1 1 1 1,eigenvec_val
0 1 0 1,eigenvec_val
1 0 1 0,eigenvec_val
0 1 0 1,eigenvec_val
1 0 1 0,eigenvec_val
Which eigenvectors correspond to nonzero eigenvalues?,eigenvec_val
15. What are the rank and eigenvalues when A and C in the previous exercise are n by n?,eigenvec_val
Remember that the eigenvalue λ = 0 is repeated n−r times.,eigenvec_val
"16. If A is the 4 by 4 matrix of ones, ﬁnd the eigenvalues and the determinant of A−I.",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
17. Choose the third row of the “companion matrix”,eigenvec_val
so that its characteristic polynomial |A−λI| is −λ 3 +4λ 2 +5λ +6.,eigenvec_val
"18. Suppose A has eigenvalues 0, 3, 5 with independent eigenvectors u, v, w.",eigenvec_val
(a) Give a basis for the nullspace and a basis for the column space.,eigenvec_val
(b) Find a particular solution to Ax = v+w. Find all solutions.,eigenvec_val
"(c) Show that Ax = u has no solution. (If it had a solution, then",eigenvec_val
19. The powers Ak of this matrix A approaches a limit as k → ∞:,eigenvec_val
The matrix A2 is halfway between A and A∞. Explain why A2 = 1,eigenvec_val
eigenvalues and eigenvectors of these three matrices.,eigenvec_val
20. Find the eigenvalues and the eigenvectors of these two matrices:,eigenvec_val
eigenvectors as A. Its eigenvalues are,eigenvec_val
21. Compute the eigenvalues and eigenvectors of A and A−1:,eigenvec_val
"eigenvectors as A. When A has eigenvalues λ1 and λ2, its inverse",eigenvec_val
22. Compute the eigenvalues and eigenvectors of A and A2:,eigenvec_val
A2 has the same,eigenvec_val
"as A. When A has eigenvalues λ1 and λ2, A2 has eigenvalues",eigenvec_val
"23. (a) If you know x is an eigenvector, the way to ﬁnd λ is to",eigenvec_val
"(b) If you know λ is an eigenvalue, the way to ﬁnd x is to",eigenvec_val
"24. What do you do to Ax = λx, in order to prove (a), (b), and (c)?",eigenvec_val
"(a) λ 2 is an eigenvalue of A2, as in Problem 22.",eigenvec_val
"(b) λ −1 is an eigenvalue of A−1, as in Problem 21.",eigenvec_val
"(c) λ +1 is an eigenvalue of A+I, as in Problem 20.",eigenvec_val
25. From the unit vector u =,eigenvec_val
", construct the rank-1 projection matrix P = uuT.",eigenvec_val
(a) Show that Pu = u. Then u is an eigenvector with λ = 1.,eigenvec_val
(b) If v is perpendicular to u show that Pv = zero vector. Then λ = 0.,eigenvec_val
(c) Find three independent eigenvectors of P all with eigenvalue λ = 0.,eigenvec_val
"26. Solve det(Q−λI) = 0 by the quadratic formula, to reach λ = cosθ ±isinθ:",eigenvec_val
rotates the xy-plane by the angle θ.,eigenvec_val
Find the eigenvectors of Q by solving (Q−λI)x = 0. Use i2 = −1.,eigenvec_val
"27. Every permutation matrix leaves x = (1,1,...,1) unchanged. Then λ = 1. Find two",eigenvec_val
more λ’s for these permutations:,eigenvec_val
"28. If A has λ1 = 4 and λ2 = 5, then det(A−λI) = (λ −4)(λ −5) = λ 2 −9λ +20. Find",eigenvec_val
"three matrices that have trace a+d = 9, determinant 20, and λ = 4,5.",eigenvec_val
"29. A 3 by 3 matrix B is known to have eigenvalues 0, 1, 2, This information is enough",eigenvec_val
to ﬁnd three of these:,eigenvec_val
"(a) the rank of B,",eigenvec_val
"(b) the determinant of BTB,",eigenvec_val
"(c) the eigenvalues of BTB, and",eigenvec_val
(d) the eigenvalues of (B+I)−1.,eigenvec_val
30. Choose the second row of A = [0 1,eigenvec_val
∗ ∗] so that A has eigenvalues 4 and 7.,eigenvec_val
"31. Choose a, b, c, so that det(A−λI) = 9λ −λ 3. Then the eigenvalues are −3, 0, 3:",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
32. Construct any 3 by 3 Markov matrix M: positive entries down each column add to,eigenvec_val
"1. If e = (1,1,1), verify that MTe = e. By Problem 11, λ = 1 is also an eigenvalue",eigenvec_val
of M. Challenge: A 3 by 3 singular Markov matrix with trace 1,eigenvec_val
33. Find three 2 by 2 matrices that have λ1 = λ2 = 0. The trace is zero and the determi-,eigenvec_val
nant is zero. The matrix A might not be 0 but check that A2 = 0.,eigenvec_val
34. This matrix is singular with rank 1. Find three λ’s and three eigenvectors:,eigenvec_val
"35. Suppose A and B have the same eigenvalues λ1,...,λn with the same independent",eigenvec_val
"eigenvectors x1,...,xn. Then A = B. Reason: Any vector x is a combination c1x1 +",eigenvec_val
···+cnxn. What is Ax? What is Bx?,eigenvec_val
"36. (Review) Find the eigenvalues of A, B, and C:",eigenvec_val
"37. When a+b = c+d, show that (1,1) is an eigenvector and ﬁnd both eigenvalues:",eigenvec_val
"38. When P exchanges rows 1 and 2 and columns 1 and 2, the eigenvalues don’t change.",eigenvec_val
Find eigenvectors of A and PAP for λ = 11:,eigenvec_val
39. Challenge problem: Is there a real 2 by 2 matrix (other than I) with A3 = I? Its,eigenvec_val
eigenvalues must satisfy λ 3 = I. They can be e2πi/3 and e−2πi/3. What trace and,eigenvec_val
determinant would this give? Construct A.,eigenvec_val
40. There are six 3 by 3 permutation matrices P. What numbers can be the determinants,eigenvec_val
of P? What numbers can be pivots? What numbers can be the trace of P? What four,eigenvec_val
numbers can be eigenvalues of P?,eigenvec_val
5.2 Diagonalization of a Matrix,eigenvec_val
Diagonalization of a Matrix,eigenvec_val
We start right off with the one essential computation. It is perfectly simple and will be,eigenvec_val
used in every section of this chapter. The eigenvectors diagonalize a matrix:,eigenvec_val
Suppose the n by n matrix A has n linearly independent eigenvectors.,eigenvec_val
"If these eigenvectors are the columns of a matrix S, then S−1AS is a diagonal",eigenvec_val
matrix Λ. The eigenvalues of A are on the diagonal of Λ:,eigenvec_val
S−1AS = Λ =,eigenvec_val
We call S the “eigenvector matrix” and Λ the “eigenvalue matrix”—using a capital,eigenvec_val
lambda because of the small lambdas for the eigenvalues on its diagonal.,eigenvec_val
"Proof. Put the eigenvectors xi in the columns of S, and compute AS by columns:",eigenvec_val
Then the trick is to split this last matrix into a quite different product SΛ:,eigenvec_val
It is crucial to keep these matrices in the right order. If Λ came before S (instead of,eigenvec_val
"after), then λ1 would multiply the entries in the ﬁrst row. We want λ1 to appear in the",eigenvec_val
"ﬁrst column. As it is, SΛ is correct. Therefore,",eigenvec_val
"S is invertible, because its columns (the eigenvectors) were assumed to be independent.",eigenvec_val
We add four remarks before giving any examples or applications.,eigenvec_val
"Remark 1. If the matrix A has no repeated eigenvalues—the numbers λ1,...,λn are",eigenvec_val
distinct—then its n eigenvectors are automatically independent (see 5D below). There-,eigenvec_val
fore any matrix with distinct eigenvalues can be diagonalized.,eigenvec_val
Remark 2. The diagonalizing matrix S is not unique. An eigenvector x can be multiplied,eigenvec_val
"by a constant, and remains an eigenvector. We can multiply the columns of S by any",eigenvec_val
"nonzero constants, and produce a new diagonalizing S. Repeated eigenvalues leave even",eigenvec_val
"more freedom in S. For the trivial example A = I, any invertible S will do: S−1IS is is",eigenvec_val
always diagonal (Λ is just I). All vectors are eigenvectors of the identity.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Remark 3. Other matrices S will not produce a diagonal Λ. Suppose the ﬁrst column,eigenvec_val
of S is y. Then the ﬁrst column of SΛ is λ1y. If this is to agree with the ﬁrst column of,eigenvec_val
"AS, which by matrix multiplication is Ay, then y must be an eigenvector: Ay = λ1y. The",eigenvec_val
order of the eigenvectors in S and the eigenvalues in Λ is automatically the same.,eigenvec_val
"Remark 4. Not all matrices possess n linearly independent eigenvectors, so not all ma-",eigenvec_val
trices are diagonalizable. The standard example of a “defective matrix” is,eigenvec_val
"Its eigenvalues are λ1 = λ2 = 0, since it is triangular with zeros on the diagonal:",eigenvec_val
"All eigenvectors of this A are multiples of the vector (1,0):",eigenvec_val
λ = 0 is a double eigenvalue—its algebraic multiplicity is 2. But the geometric multi-,eigenvec_val
plicity is 1—there is only one independent eigenvector. We can’t construct S.,eigenvec_val
"Here is a more direct proof that this A is not diagonalizable. Since λ1 = λ2 = 0, Λ",eigenvec_val
"would have to be the zero matrix, But if Λ = S−1AS = 0, then we premultiply by S and",eigenvec_val
"postmultiply by S−1, to deduce falsely that A = 0. There is no invertible S.",eigenvec_val
That failure of diagonalization was not a result of λ = 0. It came from λ1 = λ2:,eigenvec_val
"Their eigenvalues are 3, 3 and 1, 1. They are not singular! The problem is the shortage",eigenvec_val
of eigenvectors—which are needed for S. That needs to be emphasized:,eigenvec_val
Diagonalizability of A depends on enough eigenvectors.,eigenvec_val
Invertibility of A depends on nonzero eigenvalues.,eigenvec_val
There is no connection between diagonalizability (n independent eigenvector) and in-,eigenvec_val
vertibility (no zero eigenvalues). The only indication given by the eigenvalues is this:,eigenvec_val
"Diagonalization can fail only if there are repeated eigenvalues. Even then, it does not",eigenvec_val
"always fail. A = I has repeated eigenvalues 1,1,...,1 but it is already diagonal! There",eigenvec_val
is no shortage of eigenvectors in that case.,eigenvec_val
"The test is to check, for an eigenvalue that is repeated p times, whether there are p",eigenvec_val
"independent eigenvectors—in other words, whether A−λI has rank n− p. To complete",eigenvec_val
"that circle of ideas, we have to show that distinct eigenvalues present no problem.",eigenvec_val
5.2 Diagonalization of a Matrix,eigenvec_val
"If eigenvectors x1,...,xk correspond to different eigenvalues λ1,...,λk,",eigenvec_val
then those eigenvectors are linearly independent.,eigenvec_val
"Suppose ﬁrst that k = 2, and that some combination of x1 and x2 produces zero:",eigenvec_val
"c1x1 + c2x2 = 0. Multiplying by A, we ﬁnd c1λ1x1 + c2λ2x2 = 0. Subtracting λ2 times",eigenvec_val
"the previous equation, the vector x2 disappears:",eigenvec_val
c1(λ1 −λ2)x1 = 0.,eigenvec_val
"Since λ1 ̸= λ2 and x1 ̸= 0, we are forced into c1 = 0. Similarly c2 = 0, and the two",eigenvec_val
vectors are independent; only the trivial combination gives zero.,eigenvec_val
This same argument extends to any number of eigenvectors: If some combination pro-,eigenvec_val
"duces zero, multiply by A, subtract λk times the original combination, and xk disappears—",eigenvec_val
"leaving a combination of x1,...,xk−1, which produces zero. By repeating the same steps",eigenvec_val
(this is really mathematical induction) we end up with a multiple of x1 that produces,eigenvec_val
"zero. This forces c1 = 0, and ultimately every ci = 0. Therefore eigenvectors that come",eigenvec_val
from distinct eigenvalues are automatically independent.,eigenvec_val
A matrix with n distinct eigenvalues can be diagonalized. This is the typical case.,eigenvec_val
The main point of this section is S−1AS = A. The eigenvector matrix S converts A into,eigenvec_val
its eigenvalue matrix Λ (diagonal). We see this for projections and rotations.,eigenvec_val
Example 1. The projection A =,eigenvec_val
has eigenvalue matrix Λ =,eigenvec_val
vectors go into the columns of S:,eigenvec_val
AS = SΛ =,eigenvec_val
That last equation can be veriﬁed at a glance. Therefore S−1AS = Λ.,eigenvec_val
Example 2. The eigenvalues themselves are not so clear for a rotation:,eigenvec_val
det(K −λI) = λ 2 +1.,eigenvec_val
How can a vector be rotated and still have its direction unchanged? Apparently it,eigenvec_val
"can’t—except for the zero vector, which is useless. But there must be eigenvalues, and",eigenvec_val
we must be able to solve du/dt = Ku. The characteristic polynomial λ 2 +1 should still,eigenvec_val
have two roots—but those roots are not real.,eigenvec_val
"You see the way out. The eigenvalues of K are imaginary numbers, λ1 = i and λ2 =",eigenvec_val
"−i. The eigenvectors are also not real. Somehow, in turning through 90°, they are",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
multiplied by i or −i:,eigenvec_val
"The eigenvalues are distinct, even if imaginary, and the eigenvectors are independent.",eigenvec_val
They go into the columns of S:,eigenvec_val
"We are faced with an inescapable fact, that complex numbers are needed even for",eigenvec_val
"real matrices. If there are too few real eigenvalues, there are always n complex eigen-",eigenvec_val
"values. (Complex includes real, when the imaginary part is zero.) If there are too few",eigenvec_val
"eigenvectors in the real world R3, or in Rn, we look in C3 or Cn. The space Cn contains",eigenvec_val
"all column vectors with complex components, and it has new deﬁnitions of length and",eigenvec_val
"inner product and orthogonality. But it is not more difﬁcult than Rn, and in Section 5.5",eigenvec_val
we make an easy conversion to the complex case.,eigenvec_val
Powers and Products: Ak and AB,eigenvec_val
There is one more situation in which the calculations are easy. The eigenvalue of A2 are,eigenvec_val
"n , and every eigenvector of A is also an eigenvector of A2. We start",eigenvec_val
"from Ax = λx, and multiply again by A:",eigenvec_val
A2x = Aλx = λAx = λ 2x.,eigenvec_val
"Thus λ 2 is an eigenvalue of A2, with the same eigenvector x. If the ﬁrst multiplication",eigenvec_val
"by A leaves the direction of x unchanged, then so does the second.",eigenvec_val
"The same result comes from diagonalization, by squaring S−1AS = Λ:",eigenvec_val
"The matrix A2 is diagonalized by the same S, so the eigenvectors are unchanged. The",eigenvec_val
eigenvalues are squared. This continues to hold for any power of A:,eigenvec_val
5E The eigenvalues of Ak are λ k,eigenvec_val
"n, and each eigenvector of A is still an",eigenvec_val
"eigenvector of Ak. When S diagonalizes A, it also diagonalizes Ak:",eigenvec_val
Λk = (S−1AS)(S−1AS)···(S−1AS) = S−1AkS.,eigenvec_val
"Each S−1 cancels an S, except for the ﬁrst S−1 and the last S.",eigenvec_val
5.2 Diagonalization of a Matrix,eigenvec_val
If A is invertible this rule also applies to its inverse (the power k = −1). The eigen-,eigenvec_val
values of A−1 are 1/λi. That can be seen even without diagonalizing:,eigenvec_val
λ x = A−1x.,eigenvec_val
"Example 3. If K is rotation through 90°, then K2 is rotation through 180° (which means",eigenvec_val
−I) and K−1 is rotation through −90°:,eigenvec_val
The eigenvalues of K are i and −i; their squares are −1 and −1; their reciprocals are,eigenvec_val
1/i = −i and 1/(−i) = i. Then K4 is a complete rotation through 360°:,eigenvec_val
"For a product of two matrices, we can ask about the eigenvalues of AB—but we won’t",eigenvec_val
"get a good answer. It is very tempting to try the same reasoning, hoping to prove what",eigenvec_val
"is not in general true. If λ is an eigenvalue of A and µ is an eigenvalue of B, here is the",eigenvec_val
false proof that AB has the eigenvalue µλ:,eigenvec_val
ABx = Aµx = µAx = µλx.,eigenvec_val
"The mistake lies in assuming that A and B share the same eigenvector x. In general, they",eigenvec_val
"do not, We could have two matrices with zero eigenvalues, while AB has λ = 1:",eigenvec_val
"The eigenvectors of this A and B are completely different, which is typical. For the same",eigenvec_val
"reason, the eigenvalues of A+B generally have nothing to do with λ + µ.",eigenvec_val
This false proof does suggest what is true. If the eigenvector is the same for A and,eigenvec_val
"B, then the eigenvalues multiply and AB has the eigenvalue µλ. But there is something",eigenvec_val
more important. There is an easy way to recognize when A and B share a full set of,eigenvec_val
"eigenvectors, and that is a key question in quantum mechanics:",eigenvec_val
Diagonalizable matrices share the same eigenvector matrix S if and only,eigenvec_val
if AB = BA.,eigenvec_val
"Proof. If the same S diagonalizes both A = SΛ1S−1 and B = SΛ2S−1, we can multiply",eigenvec_val
AB = SΛ1S−1SΛ2S−1 = SΛ1Λ2S−1,eigenvec_val
BA = SΛ2S−1SΛ1S−1 = SΛ2Λ1S−1.,eigenvec_val
Since Λ1Λ2 = Λ2Λ1 (diagonal matrices always commute) we have AB = BA.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"In the opposite direction, suppose AB = BA. Starting from Ax = λx, we have",eigenvec_val
ABx = BAx = Bλx = λBx.,eigenvec_val
"Thus x and Bx are both eigenvectors of A, sharing the same λ (or else Bx = 0). If we",eigenvec_val
assume for convenience that the eigenvalues of A are distinct—the eigenspaces are all,eigenvec_val
one-dimensional—then Bx must be a multiple of x. in other words x is an eigenvector of,eigenvec_val
B as well as A. The proof with repeated eigenvalues is a little longer.,eigenvec_val
"Heisenberg’s uncertainty principle comes from noncommuting matrices, like posi-",eigenvec_val
"tion P and momentum Q. Position is symmetric, momentum is skew-symmetric, and",eigenvec_val
together they satisfy QP − PQ = I. The uncertainty principle follows directly from the,eigenvec_val
Schwarz inequality (Qx)T(Px) ≤ ∥Qx∥∥Px∥ of Section 3.2:,eigenvec_val
∥x∥2 = xTx = xT(QP−PQ)x ≤ 2∥Qx∥∥Px∥.,eigenvec_val
"The product of ∥Qx∥/∥x∥ and ∥Px∥/∥x∥—momentum and position errors, when the",eigenvec_val
wave function is x—is at least 1,eigenvec_val
"2. It is impossible to get both errors small, because when",eigenvec_val
you try to measure the position of a particle you change its momentum.,eigenvec_val
At the end we come back to A = SΛS−1. That factorization is particularly suited to,eigenvec_val
"take powers of A, and the simplest case A2 makes the point. The LU factorization is",eigenvec_val
"hopeless when squared, but SΛS−1 is perfect. The square is SΛ2S−1, and the eigenvec-",eigenvec_val
tors are unchanged. By following those eigenvectors we will solve difference equations,eigenvec_val
1. Factor the following matrices into SΛS−1:,eigenvec_val
"2. Find the matrix A whose eigenvalues are 1 and 4, and whose eigenvectors are",eigenvec_val
", respectively. (Hint: A = SΛS−1.)",eigenvec_val
3. Find all the eigenvalues and eigenvectors of,eigenvec_val
and write two different diagonalizing matrices S.,eigenvec_val
"4. If a 3 by 3 upper triangular matrix has diagonal entries 1, 2, 7, how do you know it",eigenvec_val
can be diagonalized? What is Λ?,eigenvec_val
5.2 Diagonalization of a Matrix,eigenvec_val
5. Which of these matrices cannot be diagonalized?,eigenvec_val
"6. (a) If A2 = I, what are the possible eigenvalues of A?",eigenvec_val
"(b) If this A is 2 by 2, and not I or −I, ﬁnd its trace and determinant.",eigenvec_val
"(c) If the ﬁrst row is (3,−1), what is the second row?",eigenvec_val
7. If A =,eigenvec_val
", ﬁnd A100 by diagonalizing A.",eigenvec_val
8. Suppose A = uvT is a column times a row (a rank-1 matrix).,eigenvec_val
"(a) By multiplying A times u, show that u is an eigenvector. What is λ?",eigenvec_val
(b) What are the other eigenvalues of A (and why)?,eigenvec_val
(c) Compute trace(A) from the sum on the diagonal and the sum of λ’s.,eigenvec_val
9. Show by direct calculation that AB and BA have the same trace when,eigenvec_val
Deduce that AB−BA = I is impossible (except in inﬁnite dimensions).,eigenvec_val
"10. Suppose A has eigenvalues 1, 2, 4. What is the trace of A2? What is the determinant",eigenvec_val
"11. If the eigenvalues of A are 1, 1, 2, which of the following are certain to be true? Give",eigenvec_val
a reason if true or a counterexample if false:,eigenvec_val
(a) A is invertible.,eigenvec_val
(b) A is diagonalizable.,eigenvec_val
(c) A is not diagonalizable.,eigenvec_val
"12. Suppose the only eigenvectors of A are multiples of x = (1,0,0). True or false:",eigenvec_val
(a) A is not invertible.,eigenvec_val
(b) A has a repeated eigenvalue.,eigenvec_val
(c) A is not diagonalizable.,eigenvec_val
13. Diagonalize the matrix A =,eigenvec_val
and ﬁnd one of its square roots—a matrix such that,eigenvec_val
R2 = A. How many square roots will there be?,eigenvec_val
14. Suppose the eigenvector matrix S has ST = S−1. Show that A = SΛS−1 is symmetric,eigenvec_val
and has orthogonal eigenvectors.,eigenvec_val
Problems 15–24 are about the eigenvalue and eigenvector matrices.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
15. Factor these two matrices into A = SΛS−1:,eigenvec_val
16. If A = SΛS−1 then A3 = (,eigenvec_val
) and A−1 = (,eigenvec_val
17. If A has λ1 = 2 with eigenvector x1 =,eigenvec_val
and λ2 = 5 with x2 =,eigenvec_val
", use SΛS−1 to",eigenvec_val
ﬁnd A. No other matrix has the same λ’s and x’s.,eigenvec_val
18. Suppose A = SΛS−1. What is the eigenvalue matrix for A + 2I? What is the eigen-,eigenvec_val
vector matrix? Check that A+2I = (,eigenvec_val
"19. True or false: If the n columns of S (eigenvectors of A) are independent, then",eigenvec_val
(a) A is invertible.,eigenvec_val
(b) A is diagonalizable.,eigenvec_val
(c) S is invertible.,eigenvec_val
(d) S is diagonalizable.,eigenvec_val
"20. If the eigenvectors of A are the columns of I, then A is a",eigenvec_val
matrix. If the eigen-,eigenvec_val
"vector matrix S is triangular, then S−1 is triangular and A is triangular.",eigenvec_val
21. Describe all matrices S that diagonalize this matrix A:,eigenvec_val
Then describe all matrices that diagonalize A−1.,eigenvec_val
22. Write the most general matrix that has eigenvectors,eigenvec_val
23. Find the eigenvalues of A and B and A+B:,eigenvec_val
Eigenvalues of A+B (are equal to)(are not equal to) eigenvalues of A plus eigenval-,eigenvec_val
"24. Find the eigenvalues of A, B, AB, and BA:",eigenvec_val
Eigenvalues of AB (are equal to)(are not equal to) eigenvalues of A times eigenvalues,eigenvec_val
of B. Eigenvalues of AB (are)(are not) equal to eigenvalues of BA.,eigenvec_val
5.2 Diagonalization of a Matrix,eigenvec_val
Problems 25–28 are about the diagonalizability of A.,eigenvec_val
"25. True or false: If the eigenvalues of A are 2, 2, 5, then the matrix is certainly",eigenvec_val
"26. If the eigenvalues of A are 1 and 0, write everything you know about the matrices A",eigenvec_val
"27. Complete these matrices so that detA = 25. Then trace = 10, and λ = 5 is repeated!",eigenvec_val
Find an eigcnvector with Ax = 5x. These matrices will nothe diagonalizabie because,eigenvec_val
there is no second line of eigenvectors.,eigenvec_val
28. The matrix A =,eigenvec_val
is not diagonalizable because the rank of A − 3I is,eigenvec_val
Change one entry to make A diagonalizable. Which entries could you change?,eigenvec_val
Problems 29–33 are about powers of matrices.,eigenvec_val
29. Ak = SΛkS−1 approaches the zero matrix as k → ∞ if and only if every λ has absolute,eigenvec_val
. Does Ak → 0 or Bk → 0?,eigenvec_val
30. (Recommended) Find Λ and S to diagonalize A in Problem 29. What is the limit of,eigenvec_val
Λk as k → ∞? What is the limit of SΛkS−1? In the columns of this limiting matrix,eigenvec_val
31. Find Λ and S to diagonalize B in Problem 29. What is B10u0 for these u0?,eigenvec_val
32. Diagonalize A and compute SΛkS−1 to prove this formula for Ak:,eigenvec_val
3k +1 3k −1,eigenvec_val
3k −1 3k +1,eigenvec_val
33. Diagonalize B and compute SΛkS−1 to prove this formula for Bk:,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Problems 34–44 are new applications of A = SΛS−1.,eigenvec_val
34. Suppose that A = SΛS−1. Take determinants to prove that detA = λ1λ2···λn = prod-,eigenvec_val
uct of λ’s. This quick proof only works when A is,eigenvec_val
35. The trace of S times ΛS−1 equals the trace of ΛS−1 times S. So the trace of a diago-,eigenvec_val
"nalizable A equals the trace of Λ, which is",eigenvec_val
"36. If A = SΛS−1, diagonalize the block matrix B =",eigenvec_val
. Find its eigenvalue and,eigenvec_val
37. Consider all 4 by 4 matrices A that are diagonalized by the same ﬁxed eigenvector,eigenvec_val
matrix S. Show that the A’s form a subspace (cA and A1 + A2 have this same S).,eigenvec_val
What is this subspace when S = I? What is its dimension?,eigenvec_val
38. Suppose A2 = A. On the left side A multiplies each column of A. Which of our four,eigenvec_val
subspaces contains eigenvectors with λ = 1? Which subspace contains eigenvectors,eigenvec_val
"with λ = 0? From the dimensions of those subspaces, A has a full set of independent",eigenvec_val
eigenvectors and can be diagonalized.,eigenvec_val
"39. Suppose Ax = λx. If λ = 0, then x is in the nullspace. If λ ̸= 0, then x is in the",eigenvec_val
column space. Those spaces have dimensions (n−r)+r = n. So why doesn’t every,eigenvec_val
square matrix have n linearly independent eigenvectors?,eigenvec_val
40. Substitute A = SΛS−1 into the product (A − λ1I)(A − λ2I)···(A − λnI) and explain,eigenvec_val
why this produces the zero matrix. We are substituting the matrix A for the number,eigenvec_val
λ in the polynomial p(λ) = det(A−λI). The Cayley-Hamilton Theorem says that,eigenvec_val
"this product is always p(A) = zero matrix, even if A is not diagonalizable.",eigenvec_val
41. Test the Cayley-Hamilton Theorem on Fibonacci’s matrix A =,eigenvec_val
"predicts that A2 −A−I = 0, since det(A−λI) is λ 2 −λ −1.",eigenvec_val
42. If A =,eigenvec_val
", then det(A−λI) is (λ −a)(λ −d). Check the Cayley-Hamilton state-",eigenvec_val
ment that (A−aI)(A−dI) = zero matrix.,eigenvec_val
43. If A =,eigenvec_val
"and AB = BA, show that B =",eigenvec_val
is also diagonal. B has the same,eigenvec_val
"as A, but different eigen",eigenvec_val
. These diagonal matrices B form a two-,eigenvec_val
dimensional subspace of matrix space. AB − BA = 0 gives four equations for the,eigenvec_val
"unknowns a, b, c, d—ﬁnd the rank of the 4 by 4 matrix.",eigenvec_val
44. If A is 5 by 5. then AB−BA = zero matrix gives 25 equations for the 25 entries in B.,eigenvec_val
Show that the 25 by 25 matrix is singular by noticing a simple nonzero solution B.,eigenvec_val
45. Find the eigenvalues and eigenvectors for both of these Markov matrices A and A∞.,eigenvec_val
Explain why A100 is close to A∞:,eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
Difference Equations and Powers Ak,eigenvec_val
Difference equations uk+1 = Auk move forward in a ﬁnite number of ﬁnite steps. A,eigenvec_val
"differential equation takes an inﬁnite number of inﬁnitesimal steps, but the two theories",eigenvec_val
stay absolutely in parallel. It is the same analogy between the discrete and the continuous,eigenvec_val
"that appears over and over in mathematics. A good illustration is compound interest,",eigenvec_val
when the time step gets shorter.,eigenvec_val
"Suppose you invest $1000 at 6% interest. Compounded once a year, the principal P",eigenvec_val
is multiplied by 1.06. This is a difference equation Pk+1 = APk = 1.06Pk with a time step,eigenvec_val
"of one year. After 5 years, the original P0 = 1000 has been multiplied 5 times:",eigenvec_val
Now suppose the time step is reduced to a month. The new difference equation is pk+1 =,eigenvec_val
"(1+.06/12)pk. After 5 years, or 60 months, you have $11 more:",eigenvec_val
"The next step is to compound every day, on 5(365) days. This only helps a little:",eigenvec_val
"Finally, to keep their employees really moving, banks offer continuous compounding.",eigenvec_val
"The interest is added on at every instant, and the difference equation breaks down. You",eigenvec_val
can hope that the treasurer does not know calculus (which is all about limits as ∆t → 0).,eigenvec_val
"The bank could compound the interest N times a year, so ∆t = 1/N:",eigenvec_val
1000 → e.301000 = $1349.87.,eigenvec_val
Or the bank can switch to a differential equation—the limit of the difference equation,eigenvec_val
"pk+1 = (1+.06∆t)pk. Moving pk to the left side and dividing by ∆t,",eigenvec_val
"The solution is p(t) = e.06t p0. After t = 5 years, this again amounts to $1349.87. The",eigenvec_val
"principal stays ﬁnite, even when it is compounded every instant—and the improvement",eigenvec_val
over compounding every day is only four cents.,eigenvec_val
The main object of this section is to solve uk+1 = Auk. That leads us to Ak and powers,eigenvec_val
of matrices. Our second example is the famous Fibonacci sequence:,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
You see the pattern: Every Fibonacci number is the sum of the two previous F’s:,eigenvec_val
Fk+2 = Fk+1 +Fk.,eigenvec_val
"That is the difference equation. It turns up in a most fantastic variety of applications,",eigenvec_val
"and deserves a book of its own. Leaves grow in a spiral pattern, and on the apple or oak",eigenvec_val
you ﬁnd ﬁve growths for every two turns around the stem. The pear tree has eight for,eigenvec_val
"every three turns, and the willow is 13:5. The champion seems to be a sunﬂower whose",eigenvec_val
seeds chose an almost unbelievable ratio of F12/F13 = 144/233.2,eigenvec_val
"How could we ﬁnd the 1000th Fibonacci number, without starting at F0 = 0 and",eigenvec_val
"F1 = 1, and working all the way out to F1000? The goal is to solve the difference equation",eigenvec_val
Fk+2 = Fk+1 +Fk. This can be reduced to a one-step equation uk+1 = Auk. Every step,eigenvec_val
"multiplies uk = (Fk+1,Fk) by a matrix A:",eigenvec_val
Fk+2 = Fk+1 +Fk,eigenvec_val
"The one-step system uk+1 = Auk is easy to solve, It starts from u0. After one step it",eigenvec_val
"produces u1 = Au0. Then u2 is Au1, which is A2u0. Every step brings a multiplication",eigenvec_val
"by A, and after k steps there are k multiplications:",eigenvec_val
The solution to a difference equation uk+1 = Auk is uk = Aku0.,eigenvec_val
"The real problem is to ﬁnd some quick way to compute the powers Ak, and thereby ﬁnd",eigenvec_val
the 1000th Fibonacci number. The key lies in the eigenvalues and eigenvectors:,eigenvec_val
"If A can be diagonalized, A = SΛS−1, then Ak comes from Λk:",eigenvec_val
uk = Aku0 = (SΛS−1)(SΛS−1)···(SΛS−1)u0 = SΛkS−1u0.,eigenvec_val
"The columns of S are the eigenvectors of A. Writing S−1u0 = c, the solution",eigenvec_val
uk = SΛkc =,eigenvec_val
�� = c1λ k,eigenvec_val
"After k steps, uk is a combination of the n “pure solutions” λ kx.",eigenvec_val
These formulas give two different approaches to the same solution uk = SΛkS−1u0.,eigenvec_val
"The ﬁrst formula recognized that Ak is identical with SΛkS−1, and we could stop there.",eigenvec_val
"2For these botanical applications, see D’Arcy Thompson’s book On Growth and Form (Cambridge University",eigenvec_val
"Press, 1942) or Peter Stevens’s beautiful Patterns in Nature (Little, Brown, 1974). Hundreds of other properties",eigenvec_val
of the Fn have been published in the Fibonacci Quarterly. Apparently Fibonacci brought Arabic numerals into,eigenvec_val
"Europe, about 1200 A.D.",eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
But the second approach brings out the analogy with a differential equation: The pure,eigenvec_val
exponential solutions eλitxi are now the pure powers λ k,eigenvec_val
i xi. The eigenvectors xi are,eigenvec_val
ampliﬁed by the eigenvalues λi. By combining these special solutions to match u0—that,eigenvec_val
is where c came from—we recover the correct solution uk = SΛkS−1u0.,eigenvec_val
"In any speciﬁc example like Fibonacci’s, the ﬁrst step is to ﬁnd the eigenvalues:",eigenvec_val
det(A−λI) = λ 2 −λ −1,eigenvec_val
"The second row of A−λI is (1,−λ). To get (A−λI)x = 0, the eigenvector is x = (λ,1),",eigenvec_val
"The ﬁrst Fibonacci numbers F0 = 0 and F1 = 1 go into u0, and S−1u0 = c:",eigenvec_val
Those are the constants in uk = c1λ k,eigenvec_val
1x1 + c2λ k,eigenvec_val
2x2. Both eigenvectors x1 and x2 have,eigenvec_val
second component 1. That leaves Fk = c1λ k,eigenvec_val
2 in the second component of uk:,eigenvec_val
This is the answer we wanted. The fractions and square roots look surprising because,eigenvec_val
"Fibonacci’s rule Fk+2 = Fk+1 +Fk must produce whole numbers, Somehow that formula",eigenvec_val
"for Fk must give an integer. In fact, since the second term [(1 −",eigenvec_val
"2, it must just move the ﬁrst term to the nearest integer:",eigenvec_val
F1000 = nearest integer to 1,eigenvec_val
"This is an enormous number, and F1001 will be even bigger. The fractions are becoming",eigenvec_val
"insigniﬁcant, and the ratio F1001/F1000 must be very close to (1+",eigenvec_val
5)/2 ≈ 1.618. Since,eigenvec_val
2 is insigniﬁcant compared to λ k,eigenvec_val
"1, the ratio Fk+1/Fk approaches λ1.",eigenvec_val
"That is a typical difference equation, leading to the powers of A =",eigenvec_val
5 because the eigenvalues did. If we choose a matrix with λ1 = 1 and λ2 = 6. we can,eigenvec_val
focus on the simplicity of the computation—after A has been diagonalized:,eigenvec_val
"λ = 1 and 6,",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"The powers 6k and 1k appear in that last matrix Ak, mixed in by the eigenvectors.",eigenvec_val
"For the difference equation uk+1 = Auk, we emphasize the main point. Every eigen-",eigenvec_val
vector x produces a “pure solution” with powers of λ:,eigenvec_val
"u2 = λ 2x,...",eigenvec_val
"When the initial u0 is an eigenvector x, this is the solution: uk = λ kx. In general u0",eigenvec_val
"is not an eigenvector. But if u0 is a combination of eigenvectors, the solution uk is the",eigenvec_val
same combination of these special solutions.,eigenvec_val
"If u0 = c1x1 + ··· + cnxn, then after k steps uk = c1λ k",eigenvec_val
1x1 + ··· + cnλ k,eigenvec_val
Choose the c’s to match the starting vector u0:,eigenvec_val
"There was an exercise in Chapter 1, about moving in and out of California, that is worth",eigenvec_val
another look. These were the rules:,eigenvec_val
"10 of the people outside California move in, and",eigenvec_val
10 of the people,eigenvec_val
inside California move out. We start with y0 people outside and z0 inside.,eigenvec_val
At the end of the ﬁrst year the numbers outside and inside are y1 and z1:,eigenvec_val
y1 = .9y0 +.2z0,eigenvec_val
z1 = .1y0 +.8z0,eigenvec_val
This problem and its matrix have the two essential properties of a Markov process:,eigenvec_val
1. The total number of people stays ﬁxed: Each column of the Markov matrix adds,eigenvec_val
up to 1. Nobody is gained or lost.,eigenvec_val
2. The numbers outside and inside can never become negative: The matrix has no,eigenvec_val
negative entries. The powers Ak are all nonnegative.3,eigenvec_val
We solve this Markov difference equation using uk = SΛkS−1u0. Then we show that,eigenvec_val
the population approaches a “steady state.” First A has to be diagonalized:,eigenvec_val
det(A−λI) = λ 2 −1.7λ +.7,eigenvec_val
"3Furthermore, history is completely disregarded; each new uk+1 depends only on the current uk. Perhaps even",eigenvec_val
"our lives are examples of Markov processes, but I hope not.",eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
λ1 and λ2 = .7 :,eigenvec_val
A = SΛS−1 =,eigenvec_val
"To ﬁnd Ak, and the distribution after k years, change SΛS−1 to SΛkS−1:",eigenvec_val
Those two terms are c1λ k,eigenvec_val
1x1 + c2λ k,eigenvec_val
2x2. The factor λ k,eigenvec_val
1 = 1 is hidden in the ﬁrst term. In,eigenvec_val
"the long run, the other factor (.7)k becomes extremely small. The solution approaches",eigenvec_val
"a limiting state u∞ = (y∞,z∞):",eigenvec_val
"The total population is still y0 +z0, but in the limit 2",eigenvec_val
3 of this population is outside Cali-,eigenvec_val
3 is inside. This is true no matter what the initial distribution may have been!,eigenvec_val
If the year starts with 2,eigenvec_val
3 outside and 1,eigenvec_val
"3 inside, then it ends the same way:",eigenvec_val
"The steady state is the eigenvector of A corresponding to λ = 1. Multiplication by A,",eigenvec_val
"from one time step to the next, leaves u∞ unchanged.",eigenvec_val
The theory of Markov processes is illustrated by that California example:,eigenvec_val
"A Markov matrix A has all aij ≥ 0, with each column adding to 1.",eigenvec_val
(a) λ1 = 1 is an eigenvalue of A.,eigenvec_val
"(b) Its eigenvector x1 is nonnegative—and it is a steady state, since Ax1 = x1.",eigenvec_val
(c) The other eigenvalues satisfy ∥λi∥ ≤ 1.,eigenvec_val
"(d) If A or any power of A has all positive entries, these other |λi| are below 1.",eigenvec_val
The solution Aku0 approaches a multiple of x1—which is the steady state,eigenvec_val
"To ﬁnd the right multiple of x1, use the fact that the total population stays the same. If",eigenvec_val
"California started with all 90 million people out, it ended with 60 million out and 30",eigenvec_val
million in. It ends the same way if all 90 million were originally inside.,eigenvec_val
We note that many authors transpose the matrix so its rows add to 1.,eigenvec_val
Remark. Our description of a Markov process was deterministic: populations moved in,eigenvec_val
"ﬁxed proportions. But if we look at a single individual, the fractions that move become",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
probabilities. With probability 1,eigenvec_val
"10, an individual outside California moves in. If inside,",eigenvec_val
the probability of moving out is 2,eigenvec_val
"10. The movement becomes a random process, and A is",eigenvec_val
called a transition matrix.,eigenvec_val
The components of uk = Aku0 specify the probability that the individual is outside,eigenvec_val
or inside the state. These probabilities are never negative and add to 1—everybody has,eigenvec_val
to be somewhere. That brings us back to the two fundamental properties of a Markov,eigenvec_val
"matrix: Each column adds to 1, and no entry is negative.",eigenvec_val
Why is λ = 1 always an eigenvalue? Each column of A − I adds up to 1 − 1 = 0.,eigenvec_val
"Therefore the rows of A − I add up to the zero row, they are linearly dependent, and",eigenvec_val
"Except for very special cases, uk will approach the corresponding eigenvector4. In",eigenvec_val
the formula uk = c1λ k,eigenvec_val
"nxn, no eigenvalue can be larger than 1. (Otherwise",eigenvec_val
the probabilities uk would blow up.) If all other eigenvalues are strictly smaller than,eigenvec_val
"λ1 = 1, then the ﬁrst term in the formula will be dominant. The other λ k",eigenvec_val
"i go to zero, and",eigenvec_val
uk → c1x1 = u∞ = steady state.,eigenvec_val
This is an example of one of the central themes of this chapter: Given information,eigenvec_val
"about A, ﬁnd information about its eigenvalues. Here we found λmax = 1.",eigenvec_val
Stability of uk+1 = Auk,eigenvec_val
There is an obvious difference between Fibonacci numbers and Markov processes. The,eigenvec_val
"numbers Fk become larger and larger, while by deﬁnition any “probability” is between 0",eigenvec_val
and 1. The Fibonacci equation is unstable. So is the compound interest equation Pk+1 =,eigenvec_val
1.06Pk; the principal keeps growing forever. If the Markov probabilities decreased to,eigenvec_val
"zero, that equation would be stable; but they do not, since at every stage they must add",eigenvec_val
to 1. Therefore a Markov process is neutrally stable.,eigenvec_val
We want to study the behavior of uk+1 = Auk as k → ∞. Assuming that A can be,eigenvec_val
"diagonalized, uk will be a combination of pure solutions:",eigenvec_val
Solution at time k,eigenvec_val
uk = SΛkS−1u0 = c1λ k,eigenvec_val
The growth of uk is governed by the λ k,eigenvec_val
i . Stability depends on the eigenvalues:,eigenvec_val
The difference equation uk+1 = Auk is,eigenvec_val
stable if all eigenvalues satisfy |λi| < 1;,eigenvec_val
neutrally stable if some |λi| = 1 and all the other |λi| < 1; and,eigenvec_val
unstable if at least one eigenvalue has |λi| > 1.,eigenvec_val
"In the stable case, the powers Ak approach zero and so does uk = Aku0.",eigenvec_val
"4If everybody outside moves in and everybody inside moves out, then the populations are reversed every year",eigenvec_val
and there is no steady state. The transition matrix is A =,eigenvec_val
and −1 is an eigenvalue as well as +1—which,eigenvec_val
cannot happen if all ai j > 0.,eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
Example 1. This matrix A is certainly stable:,eigenvec_val
has eigenvalues 0 and 1,eigenvec_val
"The λ’s are on the main diagonal because A is triangular. Starting from any u0, and",eigenvec_val
"following the rule uk+1 = Auk, the solution must eventually approach zero:",eigenvec_val
The larger eigenvalue λ = 1,eigenvec_val
2 governs the decay; after the ﬁrst step every uk is 1,eigenvec_val
real effect of the ﬁrst step is to split u0 into the two eigenvectors of A:,eigenvec_val
Positive Matrices and Applications in Economics,eigenvec_val
By developing the Markov ideas we can ﬁnd a small gold mine (entirely optional) of,eigenvec_val
matrix applications in economics.,eigenvec_val
Example 2 (Leontief’s input-output matrix).,eigenvec_val
"This is one of the ﬁrst great successes of mathematical economics. To illustrate it, we",eigenvec_val
"construct a consumption matrix—in which aij, gives the amount of product j that is",eigenvec_val
needed to create one unit of product i:,eigenvec_val
"The ﬁrst question is: Can we produce y1 units of steel, y2 units of food, and y3 units of",eigenvec_val
"labor? We must start with larger amounts p1, p2, p3, because some part is consumed",eigenvec_val
"by the production itself. The amount consumed is Ap, and it leaves a net production of",eigenvec_val
To ﬁnd a vector p such that,eigenvec_val
p = (I −A)−1y.,eigenvec_val
"On the surface, we are only asking if I −A is invertible. But there is a nonnegative twist",eigenvec_val
"to the problem. Demand and production, y and p, are nonnegative. Since p is (1−A)−1y,",eigenvec_val
the real question is about the matrix that multiplies y:,eigenvec_val
When is (I −A)−1 a nonnegative matrix?,eigenvec_val
"Roughly speaking, A cannot be too large. If production consumes too much, nothing is",eigenvec_val
"left as output. The key is in the largest eigenvalue λ1 of A, which must be below 1:",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"If λ1 > 1, (I −A)−1 fails to be nonnegative.",eigenvec_val
"If λ1 = 1, (I −A)−1 fails to exist.",eigenvec_val
"If λ1 < 1, (I −A)−1 is a converging sum of nonnegative matrices:",eigenvec_val
(I −A)−1 = I +A+A2 +A3 +··· .,eigenvec_val
"The 3 by 3 example has λ1 = .9, and output exceeds input. Production can go on.",eigenvec_val
"Those are easy to prove, once we know the main fact about a nonnegative matrix like",eigenvec_val
"A: Not only is the largest eigenvalue λ1 positive, but so is the eigenvector x1. Then",eigenvec_val
"(I −A)−1 has the same eigenvector, with eigenvalue 1/(1−λ1).",eigenvec_val
"If λ1 exceeds 1, that last number is negative. The matrix (I − A)−1 will take the",eigenvec_val
positive vector x1 to a negative vector x1/(1 − λ1). In that case (I − A)−1 is deﬁnitely,eigenvec_val
"not nonnegative. If λ1 = 1, then I −A is singular. The productive case is λ1 < 1, when",eigenvec_val
the powers of A go to zero (stability) and the inﬁnite series I + A + A2 + ··· converges.,eigenvec_val
Multiplying this series by I −A leaves the identity matrix—all higher powers cancel—so,eigenvec_val
"(I −A)−1 is a sum of nonnegative matrices, We give two examples:",eigenvec_val
has λ1 = 2 and the economy is lost,eigenvec_val
has λ1 = 1,eigenvec_val
2 and we can produce anything.,eigenvec_val
The matrices (I −A)−1 in those two cases are −1,eigenvec_val
Leontief’s inspiration was to ﬁnd a model that uses genuine data from the real econ-,eigenvec_val
"omy. The table for 1958 contained 83 industries in the United States, with a “trans-",eigenvec_val
actions table” of consumption and production for each one. The theory also reaches,eigenvec_val
"beyond (I −A)−1, to decide natural prices and questions of optimization. Normally la-",eigenvec_val
"bor is in limited supply and ought to be minimized. And, of course, the economy is not",eigenvec_val
Example 3 (The prices in a closed input-output model ).,eigenvec_val
The model is called “closed” when everything produced is also consumed. Nothing goes,eigenvec_val
outside the system. In that case A goes back to a Markov matrix. The columns add up,eigenvec_val
"to 1. We might be talking about the value of steel and food and labor, instead of the",eigenvec_val
"number of units, The vector p represents prices instead of production levels.",eigenvec_val
Suppose p0 is a vector of prices. Then Ap0 multiplies prices by amounts to give the,eigenvec_val
value of each product. That is a new set of prices which the system uses for the next,eigenvec_val
set of values A2p0. The question is whether the prices approach equilibrium. Are there,eigenvec_val
"prices such that p = Ap, and does the system take us there?",eigenvec_val
"You recognize p as the (nonnegative) eigenvector of the Markov matrix A, with λ = 1.",eigenvec_val
"It is the steady state p∞, and it is approached from any starting point p0. By repeating a",eigenvec_val
"transaction over and over, the price tends to equilibrium.",eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
The “Perron-Frobenius theorem” gives the key properties of a positive matrix—not,eigenvec_val
"to be confused with a positive deﬁnite matrix, which is symmetric and has all its eigen-",eigenvec_val
values positive. Here all the entries aij are positive.,eigenvec_val
"If A is a positive matrix, so is its largest eigenvalue: λ1 > all other |λi|.",eigenvec_val
Every component of the corresponding eigenvector x1 is also positive.,eigenvec_val
Proof. Suppose A > 0. The key idea is to look at all numbers t such that Ax ≥ tx for,eigenvec_val
some nonnegative vector x (other than x = 0). We are allowing inequality in Ax ≥ tx in,eigenvec_val
"order to have many positive candidates t. For the largest value tmax (which is attained),",eigenvec_val
we will show that equality holds: Ax = tmaxx.,eigenvec_val
"Otherwise, if Ax ≥ tmaxx is not an equality, multiply by A. Because A is positive, that",eigenvec_val
produces a strict inequality A2x > tmaxAx. Therefore the positive vector y = Ax satisﬁes,eigenvec_val
"Ay > tmaxy, and tmax could have been larger. This contradiction forces the equality Ax =",eigenvec_val
"tmaxx, and we have an eigenvalue. Its eigenvector x is positive (not just nonnegative)",eigenvec_val
because on the left-hand side of that equality Ax is sure to be positive.,eigenvec_val
"To see that no eigenvalue can be larger than tmax, suppose Az = λz. Since λ and z",eigenvec_val
"may involve negative or complex numbers, we take absolute values: |λ||z| = |Az| ≤ A|z|",eigenvec_val
"by the “triangle inequality.” This |z| is a nonnegative vector, so |λ| is one of the possible",eigenvec_val
"candidates t. Therefore |λ| cannot exceed λ1, which was tmax.",eigenvec_val
Example 4 (Von Neumann’s model of an expanding economy ).,eigenvec_val
"We go back to the 3 by 3 matrix A that gave the consumption of steel, food, and labor.",eigenvec_val
"If the outputs are s1, f1, ℓ1, then the required inputs are",eigenvec_val
In economics the difference equation is backward! Instead of u1 = Au0 we have u0 =,eigenvec_val
"Au1. If A is small (as it is), then production does not consume everything—and the",eigenvec_val
economy can grow. The eigenvalues of A−1 will govern this growth. But again there is,eigenvec_val
"a nonnegative twist, since steel, food, and labor cannot come in negative amounts. Von",eigenvec_val
Neumann asked for the maximum rate t at which the economy can expand and still stay,eigenvec_val
"nonnegative, meaning that u1 ≥ tu0 ≥ 0.",eigenvec_val
"Thus the problem requires u1 ≥ tAu1. It is like the Perron-Frobenius theorem, with A",eigenvec_val
"on the other side. As before, equality holds when t reaches tmax—which is the eigenvalue",eigenvec_val
associated with the positive eigenvector of A−1. In this example the expansion factor is,eigenvec_val
"With steel-food-labor in the ratio 1-5-5, the economy grows as quickly as possible: The",eigenvec_val
maximum growth rate is 1/λ1.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"1. Prove that every third Fibonacci number in 0,1,1.2,3,... is even.",eigenvec_val
2. Bernadelli studied a beetle “which lives three years only. and propagates in as third,eigenvec_val
year.” They survive the ﬁrst year with probability 1,eigenvec_val
"2, and the second with probability",eigenvec_val
"3, and then produce six females on the way out:",eigenvec_val
"Show that A3 = I, and follow the distribution of 3000 beetles for six years.",eigenvec_val
3. For the Fibonacci matrix A =,eigenvec_val
", compute A2, A3, and A4. Then use the text and",eigenvec_val
a calculator to ﬁnd F20.,eigenvec_val
4. Suppose each “Gibonacci” number Gk+2 is the average of the two previous numbers,eigenvec_val
Gk+1 and Gk. Then Gk+2 = 1,eigenvec_val
(a) Find the eigenvalues and eigenvectors of A.,eigenvec_val
(b) Find the limit as n → ∞ of the matrices An = SΛnS−1.,eigenvec_val
"(c) If G0 = 0 and G1 = 1, show that the Gibonacci numbers approach 2",eigenvec_val
5. Diagonalize the Fibonacci matrix by completing S−1:,eigenvec_val
Do the multiplication SΛkS−1�1,eigenvec_val
to ﬁnd its second component. This is the kth Fi-,eigenvec_val
bonacci number Fk = (λ k,eigenvec_val
6. The numbers λ k,eigenvec_val
1 and λ k,eigenvec_val
2 satisfy the Fibonacci rule Fk+2 = Fk+1 +Fk:,eigenvec_val
Prove this by using the original equation for the λ’s (multiply it by λ k). Then any,eigenvec_val
combination of λ k,eigenvec_val
1 and λ k,eigenvec_val
2 satisﬁes the rule. The combination Fk = (λ k,eigenvec_val
λ2) gives the right start of F0 = 0 and F1 = 1.,eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
"7. Lucas started with L0 = 2 and L1 = 1. The rule Lk+2 = Lk+1 +Lk is the same, so A",eigenvec_val
is still Fibonacci’s matrix. Add its eigenvectors x1 +x2:,eigenvec_val
"Multiplying by Ak, the second component is Lk = λ k",eigenvec_val
1 + λ k,eigenvec_val
2. Compute the Lucas,eigenvec_val
"number L10 slowly by Lk+2 = Lk+1 +Lk, and compute approximately by λ 10",eigenvec_val
8. Suppose there is an epidemic in which every month half of those who are well be-,eigenvec_val
"come sick, and a quarter of those who are sick become dead. Find the steady state",eigenvec_val
for the corresponding Markov process,eigenvec_val
"9. Write the 3 by 3 transition matrix for a chemistry course that is taught in two sections,",eigenvec_val
if every week 1,eigenvec_val
4 of those in Section A and 1,eigenvec_val
"3 of those in Section B drop the course,",eigenvec_val
6 of each section transfer to the other section.,eigenvec_val
10. Find the limiting values of yk and k (k → ∞) if,eigenvec_val
yk+1 = .8yk +.3zk,eigenvec_val
zk+1 = .2yk +.7zk,eigenvec_val
Also ﬁnd formulas for yk and zk from Ak = SΛkS−1.,eigenvec_val
"11. (a) From the fact that column 1 + column 2 = 2(column 3), so the columns are",eigenvec_val
linearly dependent ﬁnd one eigenvalue and one eigenvector of A:,eigenvec_val
(b) Find the other eigenvalues of A (it is Markov).,eigenvec_val
"(c) If u0 = (0,10,0), ﬁnd the limit of Aku0 as k → ∞.",eigenvec_val
12. Suppose there are three major centers for Move-It-Yourself trucks. Every month half,eigenvec_val
"of those in Boston and in Los Angeles go to Chicago, the other half stay here they",eigenvec_val
"are, and the trucks in Chicago are split equally between Boston and Los Angeles Set",eigenvec_val
"up the 3 by 3 transition matrix A, and ﬁnd the steady state u∞ corresponding to the",eigenvec_val
eigenvalue λ = 1.,eigenvec_val
13. (a) In what range of a and b is the following equation a Markov process?,eigenvec_val
uk+1 = Auk =,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
(b) Compute uk = SΛkS−1u0 for any a and b.,eigenvec_val
"(c) Under what condition on a and b does uk approach a ﬁnite limit as k → ∞, and",eigenvec_val
what is the limit? Does A have to be a Markov matrix?,eigenvec_val
"14. Multinational companies in the Americas, Asia, and Europe have assets of $4 trillion.",eigenvec_val
"At the start, $2 trillion are in the Americas and $2 trillion in Europe. Each year 1",eigenvec_val
"American money stays home, and 1",eigenvec_val
4 goes to each of Asia and Europe. For Asia and,eigenvec_val
2 stays home and 1,eigenvec_val
2 is sent to the Americas.,eigenvec_val
(a) Find the matrix that gives,eigenvec_val
(b) Find the eigenvalues and eigenvectors of A.,eigenvec_val
(c) Find the limiting distribution of the $4 trillion as the world ends.,eigenvec_val
(d) Find the distribution of the $4 trillion at year k.,eigenvec_val
"15. If A is a Markov matrix, show that the sum of the components of Ax equals the sum",eigenvec_val
"of the components of x. Deduce that if Ax = λx with λ ̸= 1, the components of the",eigenvec_val
eigenvector add to zero.,eigenvec_val
16. The solution to du/dt = Au =,eigenvec_val
u (eigenvalues i and −i) goes around in a circle:,eigenvec_val
"u = (cost,sint). Suppose we approximate du/dt by forward, backward, and centered",eigenvec_val
"differences F, B, C:",eigenvec_val
(F) un+1 −un = Aun or un+1 = (I +A)un (this is Euler’s method).,eigenvec_val
(B) un+1 −un = Aun+1 or un+1 = (I −A)−1un (backward Euler).,eigenvec_val
(C) un+1 −un = 1,eigenvec_val
2A(un+1 +un) or un+1 = (I − 1,eigenvec_val
"Find the eigenvalues of I +A, (IłA)−1, and (I − 1",eigenvec_val
2A). For which difference,eigenvec_val
equation does the solution un stay on a circle?,eigenvec_val
"17. What values of α produce instability in vn+1 = α(vn +wn), wn+1 = α(vn +wn)?",eigenvec_val
"18. Find the largest a, b, c for which these matrices are stable or neutrally stable:",eigenvec_val
"19. Multiplying term by term, check that (IłA)(I + A + A2 + ···) = I. This series rep-",eigenvec_val
"resents (IłA)−1. It is nonnegative when A is nonnegative, provided it has a ﬁnite",eigenvec_val
5.3 Difference Equations and Powers Ak,eigenvec_val
"sum; the condition for that is λmax < 1. Add up the inﬁnite series, and conﬁrm that",eigenvec_val
"it equals (IłA)−1, for the consumption matrix",eigenvec_val
which has λmax = 0.,eigenvec_val
20. For A =,eigenvec_val
", ﬁnd the powers Ak (including A0) and show explicitly that their sum",eigenvec_val
agrees with (I −A)−1.,eigenvec_val
21. Explain by mathematics or economics why increasing the “consumption matrix” A,eigenvec_val
must increase tmax = λ1 (and slow down the expansion).,eigenvec_val
22. What are the limits as k → ∞ (the steady states) of the following?,eigenvec_val
Problems 23–29 are about A = SΛS−1 and Ak = SΛkS−1,eigenvec_val
23. Diagonalize A and compute SΛkS−1 to prove this formula for Ak:,eigenvec_val
5k +1 5k −1,eigenvec_val
5k −1 5k +1,eigenvec_val
24. Diagonalize B and compute SΛkS−1 to prove this formula for Bk:,eigenvec_val
"25. The eigenvalues of A are 1 and 9, the eigenvalues of B are ł1 and 9:",eigenvec_val
Find a matrix square root of A from R = S,eigenvec_val
"ΛS−1, Why is there no real matrix square",eigenvec_val
"26. If A and B have the same λ’s with the same full set of independent eigenvectors, their",eigenvec_val
are the same. So A = B.,eigenvec_val
"27. Suppose A and B have the same full set of eigenvectors, so that A = SΛ1S−1 and",eigenvec_val
B = SΛ2S−1. Prove that AB = BA.,eigenvec_val
28. (a) When do the eigenvectors for λ = 0 span the nullspace N(A)?,eigenvec_val
(b) When do all the eigenvectors for λ ̸= 0 span the column space C(A)?,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"29. The powers Ak approach zero if all |λi| < 1, and they blow up if any |λi| > 1. Peter",eigenvec_val
Lax gives four striking examples in his book Linear Algebra.,eigenvec_val
Find the eigenvalues λ = eiθ of B and C to show that B4 = I and C3 = −I.,eigenvec_val
Differential Equations and eAt,eigenvec_val
"Wherever you ﬁnd a system of equations, rather than a single equation, matrix theory",eigenvec_val
"has a part to play. For difference equations, the solution uk = Aku0 depended on the owen",eigenvec_val
"of A. For differential equations, the solution u(t) = eAtu(0) depends on the exponential",eigenvec_val
"of A. To deﬁne this exponential. and to understand it, we turn right away to an example:",eigenvec_val
dt = Au =,eigenvec_val
The ﬁrst step is always to ﬁnd the eigenvalues (ł1 and −3) and the eigenvectors:,eigenvec_val
Then several approaches lead to u(t). Probably the best is to match the general solution,eigenvec_val
to the initial vector u(0) at t = 0.,eigenvec_val
The general solution is a combination of pure exponential solutions. These are so-,eigenvec_val
"lutions of the special form ceλtx, where λ is an eigenvalue of A and x is its eigenvec-",eigenvec_val
"tor. These pure solutions satisfy the differential equation, since d/dt(ceλtx) = A(ceλtx).",eigenvec_val
(They were our introduction to eigenvalues at the start of the chapter.) In this 2 by 2,eigenvec_val
"example, there are two pure exponentials to be combined:",eigenvec_val
u(t) = c1eλ1tx1 +c2eλ2tx2,eigenvec_val
"At time zero, when the exponentials are e0 = 1, u(0) determines c1 and c2:",eigenvec_val
u(0) = c1x1 +c2x2 =,eigenvec_val
"You recognize S, the matrix of eigenvectors. The constants c = S−1u(0) are the same as",eigenvec_val
"they were for difference equations. Substituting them back into equation (2), the solution",eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
Here is the fundamental formula of this section: SeΛtS−1u(0) solves the differential,eigenvec_val
"equation, just as SΛkS−1u0 solved the difference equation:",eigenvec_val
There are two more things to be done with this example. One is to complete the,eigenvec_val
"mathematics, by giving a direct deﬁnition of the exponential of a matrix. The other",eigenvec_val
is to give a physical interpretation of the equation and its solution. It is the kind of,eigenvec_val
differential equation that has useful applications.,eigenvec_val
The exponential of a diagonal matrix Λ is easy; eΛt just has the n numbers eλt on,eigenvec_val
"the diagonal. For a general matrix A, the natural idea is to imitate the power series",eigenvec_val
"ex = 1+x+x2/2!+x3/3!+···. If we replace x by At and 1 by I, this sum is an n by n",eigenvec_val
eAt = I +At + (At)2,eigenvec_val
"The series always converges, and its sum eAt has the right properties:",eigenvec_val
dt (eAt) = AeAt.,eigenvec_val
"From the last one, u(t) = eAtu(0) solves the differential equation. This solution must",eigenvec_val
be the same as the form SeΛtS−1u(0) used for computation. To prove directly that those,eigenvec_val
"solutions agree, remember that each power (SΛS−1)k telescopes into Ak = SΛkS−1 (be-",eigenvec_val
cause S−1 cancels S). The whole exponential is diagonalized by S:,eigenvec_val
eAt = I +SΛS−1t + SΛ2S−1t2,eigenvec_val
I +Λt + (Λt)2,eigenvec_val
"Example 1. In equation (1), the exponential of A =",eigenvec_val
eAt = SeΛtS−1 =,eigenvec_val
"At t = 0 we get e0 = I. The inﬁnite series eAt gives the answer for all t, but a series can be",eigenvec_val
hard to compute. The form SeΛtS−1 gives the same answer when A can be diagonalized;,eigenvec_val
it requires n independent eigenvectors in S. This simpler form leads to a combination of,eigenvec_val
n exponentials eλtx—which is the best solution of all:,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"If A can be diagonalized, A = SΛS−1, then du/dt = Au has the solution",eigenvec_val
u(t) = eAtu(0) = SeΛtS−1u(0).,eigenvec_val
"The columns of S are the eigenvectors x1,...,xn of A. Multiplying gives",eigenvec_val
= c1eλ1tx1 +···+cneλntxn = combination of eλtx.,eigenvec_val
The constants ci that match the initial conditions u(0) are c = S−1u(0).,eigenvec_val
This gives a complete analogy with difference equations and SΛS−1u0. In both cases,eigenvec_val
we assumed that A could be diagonalized. since otherwise it has fewer than n eigenvec-,eigenvec_val
"tors and we have not found enough special solutions. The missing Solutions do exist,",eigenvec_val
but they are more complicated than pure exponentials eλtx. They involve “generalized,eigenvec_val
eigenvectors” and factors like teλt. (To compute this defective case we can use the Jor-,eigenvec_val
"dan form in Appendix B, and ﬁnd eJt.) The formula u(t) = eAtu(0) remains completely",eigenvec_val
The matrix eAt is never singular. One proof is to look at its eigenvalues; if λ is an,eigenvec_val
"eigenvalue of A, then eλt is the corresponding eigenvalue of eAt—and eλt can never be",eigenvec_val
zero. Another approach is to compute the determinant of the exponential:,eigenvec_val
deteAt = eλ1teλ2t ···eλnt = etrace(At).,eigenvec_val
Quick proof that eAt is invertible: Just recognize e−At as its inverse.,eigenvec_val
This invertibility is fundamental for differential equations. If n solutions are linearly,eigenvec_val
"independent at t = 0, they remain linearly independent forever. If the initial vectors are",eigenvec_val
"v1,...,vn, we can put the solutions eAtv into a matrix:",eigenvec_val
"The determinant of the left-hand side is the Wronskian. It never becomes zero, because",eigenvec_val
it is the product of two nonzero determinants. Both matrices on the right-hand side are,eigenvec_val
Remark. Not all differential equations come to us as a ﬁrst-order system du/dt = Au.,eigenvec_val
"We may start from a single equation of higher order, like y′′′ −3y′′ +2y′ = 0. To convert",eigenvec_val
"to a 3 by 3 system, introduce v = y′ and w = v′ as additional unknowns along with y",eigenvec_val
itself. Then these two equations combine with the original one to give u′ = Au:,eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
Figure 5.1: A model of diffusion between four segments.,eigenvec_val
We are back to a ﬁrst-order system. The problem can be solved two ways. In a course,eigenvec_val
"on differential equations, you would substitute y = eλt into y′′′ −3y′′ +2y′ = 0:",eigenvec_val
(λ 3 −3λ 2 +2λ)eλt = 0,eigenvec_val
λ(λ −1)(λ −2)eλt = 0.,eigenvec_val
"The three pure exponential solutions are y = e0t, y = et, and y = e2t. No eigenvectors are",eigenvec_val
"involved. In a linear algebra course, we ﬁnd the eigenvalues of A:",eigenvec_val
�� = −λ 3 +3λ 2 −2λ = 0.,eigenvec_val
"Equations (10) and (11) are the same! The same three exponents appear: λ = 0, λ = 1,",eigenvec_val
and λ = 2. This is a general rule which makes the two methods consistent; the growth,eigenvec_val
rates of the solutions stay ﬁxed when the equations change form. It seems to us that,eigenvec_val
solving the third-order equation is quicker.,eigenvec_val
The physical signiﬁcance of du/dt =,eigenvec_val
u is easy to explain and at the same,eigenvec_val
time genuinely important. This differential equation describes a process of diffusion.,eigenvec_val
"Divide an inﬁnite pipe into four segments (Figure 5.1). At time t = 0, the middle seg-",eigenvec_val
"ments contain concentrations v(0) and w(0) of a chemical. At each time t, the diffusion",eigenvec_val
rate between two adjacent segments is the difference in concentrations. Within each,eigenvec_val
"segment, the concentration remains uniform (zero in the inﬁnite segments). The process",eigenvec_val
is continuous in time but discrete in space; the unknowns are v(t) and w(t) in the two,eigenvec_val
inner segments S1 and S2.,eigenvec_val
"The concentration v(t) in S1 is changing in two ways. There is diffusion into S0, and",eigenvec_val
"into or out of S2. The net rate of change is dv/dt, and dw/dt is similar:",eigenvec_val
Flow rate into S1,eigenvec_val
Flow rate into S2,eigenvec_val
This law of diffusion exactly matches our example du/dt = Au:,eigenvec_val
The eigenvalues −1 and −3 will govern the solution. They give the rate at which the,eigenvec_val
"concentrations decay, and λ1 is the more important because only an exceptional set of",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"starting conditions can lead to “superdecay” at the rate e−3t, In fact, those conditions",eigenvec_val
"must come from the eigenvector (1,−1). If the experiment admits only nonnegative",eigenvec_val
"concentrations, superdecay is impossible and the limiting rate must be e−t. The solution",eigenvec_val
"that decays at this slower rate corresponds to the eigenvector (1,1). Therefore the two",eigenvec_val
concentrations will become nearly equal (typical for diffusion) as t → ∞.,eigenvec_val
"One more comment on this example: It is a discrete approximation, with only two",eigenvec_val
"unknowns, to the continuous diffusion described by this partial differential equation:",eigenvec_val
∂t = ∂ 2u,eigenvec_val
"That heat equation is approached by dividing the pipe into smaller and smaller segments,",eigenvec_val
of length 1/N. The discrete system with N unknowns is governed by,eigenvec_val
"This is the ﬁnite difference matrix with the 1, −2, 1 pattern. The right side Au approaches",eigenvec_val
"the second derivative d2u/dx2, after a scaling factor N2 comes from the ﬂow problem.",eigenvec_val
"In the limit as N → ∞, we reach the heat equation ∂u/∂t = ∂ 2u/∂x2. Its solutions",eigenvec_val
"are still combinations of pure exponentials, but now there are inﬁnitely many. Instead",eigenvec_val
"of eigenvectors from Ax = λx, we have eigenfunctions from d2u/dx2 = λu. Those are",eigenvec_val
u(x) = sinnπx with λ = −n2π2. Then the solution to the heat equation is,eigenvec_val
The constants cn are determined by the initial condition. The novelty is that the eigen-,eigenvec_val
"vectors are functions u(x), because the problem is continuous and not discrete.",eigenvec_val
stability of differential equations,eigenvec_val
Just as for difference equations. the eigenvalues decide how u(t) behaves as t → ∞.,eigenvec_val
"As long as A can be diagonalized, there will be n pure exponential solutions to the",eigenvec_val
"differential equation, and any speciﬁc solution u(t) is some combination",eigenvec_val
u(t) = SeΛtS−1u0 = c1egl1tx1 +···+cneglntxn.,eigenvec_val
"Stability is governed by those factors eglit. If they all approach zero, then u(t) approaches",eigenvec_val
"zero: if they all stay bounded, then u(t) stays bounded; if one of them blows up, then",eigenvec_val
"except for very special starting conditions the solution will blow up. Furthermore, the",eigenvec_val
size of eλt depends only on the real part of λ. It is only the real parts of the eigenvalues,eigenvec_val
"that govern stability: If λ = a+ib, then",eigenvec_val
eλt = eateibt = eat(cosbt +isinbt),eigenvec_val
and the magnitude is,eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
"This decays for a < 0, it is constant for a = 0, and it explodes for a > 0. The imaginary",eigenvec_val
"part is producing oscillations, but the amplitude comes from the real part.",eigenvec_val
The differential equation du/dt = Au is,eigenvec_val
"stable and eAt → 0 whenever all Reλi < 0,",eigenvec_val
"neutrally stable when all Reλi ≤ 0 and Reλ1 = 0, and",eigenvec_val
unstable and eAt is unbounded if any eigenvalue has Reλi > 0.,eigenvec_val
"In some texts the condition Reλ < 0 is called asymptotic stability, because it guarantees",eigenvec_val
"decay for large times t. Our argument depended on having n pure exponential solutions,",eigenvec_val
but even if A is not diagonalizable (and there are terms like teλt) the result is still true:,eigenvec_val
All solutions approach zero if and only if all eigenvalues have Reλ < 0.,eigenvec_val
Stability is especially easy to decide for a 2 by 2 system (which is very common in,eigenvec_val
applications). The equation is,eigenvec_val
and we need to know when both eigenvalues of that matrix have negative real parts.,eigenvec_val
(Note again that the eigenvalues can be complex numbers.) The stability tests are,eigenvec_val
The trace a+d must be negative.,eigenvec_val
The determinant ad −bc must be positive.,eigenvec_val
"When the eigenvalues are real, those tests guarantee them to be negative. Their product",eigenvec_val
is the determinant; it is positive when the eigenvalues have the same sign. Their sum is,eigenvec_val
the trace; it is negative when both eigenvalues are negative.,eigenvec_val
"When the eigenvalues are a complex pair x ± iy, the tests still succeed. The trace",eigenvec_val
is their sum 2x (which is < 0) and the determinant is (x + iy)(x − iy) = x2 + y2 > 0.,eigenvec_val
"Figure 5.2 shows the one stable quadrant, trace < 0 and determinant > 0. It also shows",eigenvec_val
the parabolic boundary line between real and complex eigenvalues. The reason for the,eigenvec_val
parabola is in the quadratic equation for the eigenvalues:,eigenvec_val
= λ 2 −(trace)λ +(det) = 0.,eigenvec_val
The quadratic formula for λ leads to the parabola (trace)2 = 4(det):,eigenvec_val
λ1 and λ2 = 1,eigenvec_val
"Above the parabola, the number under the square root is negative—so λ is not real. On",eigenvec_val
"the parabola, the square root is zero and λ is repeated. Below the parabola the square",eigenvec_val
"roots are real. Every symmetric matrix has real eigenvalues, since if b = c, then",eigenvec_val
(trace)2 −4(det) = (a+d)2 −4(ad −b2) = (a−d)2 +4b2 ≥ 0.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
λ1 = λ2 and,eigenvec_val
both Reλ > 0,eigenvec_val
T 2 = 4D,eigenvec_val
both λ > 0,eigenvec_val
both Reλ < 0,eigenvec_val
both λ < 0,eigenvec_val
det < 0 gives λ1 < 0 and λ2 > 0: real and unstable,eigenvec_val
Figure 5.2: Stability and instability regions for a 2 by 2 matrix.,eigenvec_val
"For complex eigenvalues, b and c have opposite signs and are sufﬁciently large.",eigenvec_val
Example 2. One from each quadrant: only #2 is stable:,eigenvec_val
"On the boundaries of the second quadrant, the equation is neutrally stable. On the hori-",eigenvec_val
"zontal axis, one eigenvalue is zero (because the determinant is λ1λ2 = 0). On the vertical",eigenvec_val
"axis above the origin, both eigenvalues are purely imaginary (because the trace is Zero).",eigenvec_val
Crossing those axes are the two ways that stability is lost.,eigenvec_val
"The n by n case is more difﬁcult. A test for Reλi < 0 came from Routh and Hurwitz,",eigenvec_val
who found a series of inequalities on the entries aij. I do not think this approach is,eigenvec_val
much good for a large matrix; the computer can probably ﬁnd the eigenvalues with more,eigenvec_val
certainty than it can test these inequalities. Lyapunov’s idea was to ﬁnd a weighting,eigenvec_val
matrix W so that the weighted length ∥Wu(t)∥ is always decreasing. If there exists,eigenvec_val
"such a W, then ∥Wu∥ will decrease steadily to zero, and after a few ups and downs u",eigenvec_val
must get there too (stability). The real value of Lyapunov’s method is for a nonlinear,eigenvec_val
equation—then stability can be proved without knowing a formula for u(t).,eigenvec_val
Example 3. du/dt =,eigenvec_val
"u sends u(t) around a circle, starting from u(0) = (1,0).",eigenvec_val
"Since trace = 0 and det = 1, we have purely imaginary eigenvalues:",eigenvec_val
= λ 2 +1 = 0,eigenvec_val
λ = +i and −i.,eigenvec_val
"The eigenvectors are (1,−i) and (1,i). and the solution is",eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
"That is correct but not beautiful. By substituting cost ±isint for eit and e−it, real num-",eigenvec_val
"bers will reappear: The circling solution is u(t) = (cost,sint).",eigenvec_val
"Starting from a different u(0) = (a,b), the solution u(t) ends up as",eigenvec_val
"There we have something important! The last matrix is multiplying u(0), so it must be",eigenvec_val
the exponential eAt. (Remember that u(t) = eAtu(0).) That matrix of cosines and sines,eigenvec_val
"is our leading example of an orthogonal matrix. The columns have length 1, their inner",eigenvec_val
"product is zero, and we have a conﬁrmation of a wonderful fact:",eigenvec_val
If A is skew-symmetric (AT = −A) then eAt is an orthogonal matrix.,eigenvec_val
AT = −A gives a conservative system. No energy is lost in damping or diffusion:,eigenvec_val
That last equation expresses an essential property of orthogonal matrices. When they,eigenvec_val
"multiply a vector, the length is not changed. The vector u(0) is just rotated, and that",eigenvec_val
describes the solution to du/dt = Au: It goes around in a circle.,eigenvec_val
"In this very unusual case, eAt can also be recognized directly from the inﬁnite series.",eigenvec_val
Note that A =,eigenvec_val
"has A2 = −I, and use this in the series for eAt:",eigenvec_val
I +At + (At)2,eigenvec_val
Example 4. The diffusion equation is stable: A =,eigenvec_val
has λ = −1 and λ = −3.,eigenvec_val
"Example 5. If we close off the inﬁnite segments, nothing can escape:",eigenvec_val
"This is a continuous Markov process. Instead of moving every year, the particles move",eigenvec_val
every instant. Their total number v + w is constant. That comes from adding the two,eigenvec_val
equations on the right-hand side: the derivative of v+w is zero.,eigenvec_val
A discrete Markov matrix has its column sums equal to λmax = 1. A continuous,eigenvec_val
"Markov matrix, for differential equations, has its column sums equal to λmax = 0. A is",eigenvec_val
a discrete Markov matrix if and only if B = A − I is a continuous Markov matrix. The,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Figure 5.3: The slow and fast modes of oscillation.,eigenvec_val
steady state for both is the eigenvector for λmax. It is multiplied by 1k = 1 in difference,eigenvec_val
"equations and by e0t = 1 in differential equations, and it doesn’t move.",eigenvec_val
"In the example, the steady state has v = w.",eigenvec_val
"Example 6. In nuclear engineering, a reactor is called critical when it is neutrally",eigenvec_val
"stable; the ﬁssion balances the decay. Slower ﬁssion makes it stable, or subcritical, and",eigenvec_val
eventually it runs down. Unstable ﬁssion is a bomb.,eigenvec_val
The laws of diffusion led to a ﬁrst-order system du/dt = Au. So do a lot of other appli-,eigenvec_val
"cations, in chemistry, in biology, and elsewhere, but the most important law of physics",eigenvec_val
"does not. It is Newton’s law F = ma, and the acceleration a is a second derivative. In-",eigenvec_val
ertial terms produce second-order equations (we have to solve d2u/dt2 = Au instead of,eigenvec_val
"du/dt = Au), and the goal is to understand how this switch to second derivatives alters",eigenvec_val
"the solution5. It is optional in linear algebra, but not in physics.",eigenvec_val
The comparison will be perfect if we keep the same A:,eigenvec_val
dt2 = Au =,eigenvec_val
Two initial conditions get the system started—the “displacement” u(0) and the “veloc-,eigenvec_val
"ity” u′(0). To match these conditions, there will be 2n pure exponential solutions.",eigenvec_val
"Suppose we use ω rather than λ, and write these special solutions as u = eiωtx. Sub-",eigenvec_val
"stituting this exponential into the differential equation, it must satisfy",eigenvec_val
"The vector x must be an eigenvector of A, exactly as before. The corresponding eigen-",eigenvec_val
"value is now −ω2, so the frequency ω is connected to the decay rate λ by the law",eigenvec_val
"5Fourth derivatives are also possible, in the bending of beams, but nature seems to resist going higher than four.",eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
−ω2 = λ. Every special solution eλtx of the ﬁrst-order equation leads to two special,eigenvec_val
solutions eiωtx of the second-order equation. and the two exponents are ω = ±,eigenvec_val
"This breaks down only when λ = 0, which has just one square root; if the eigenvector is",eigenvec_val
"x, the two special solutions are x and tx.",eigenvec_val
"For a genuine diffusion matrix, the eigenvalues λ are all negative and the frequencies",eigenvec_val
ω are all real: Pure diffusion is converted into pure oscillation. The factors eiωt produce,eigenvec_val
"neutral stability, the solution neither grows or decays, and the total energy stays precisely",eigenvec_val
"constant. It just keeps passing around the system. The general solution to d2u/dt2 = Au,",eigenvec_val
"if A has negative eigenvalues λ1,...,λn and if ω j =",eigenvec_val
"As always, the constants are found from the initial conditions. This is easier to do (at the",eigenvec_val
expense of one extra formula) by switching from oscillating exponentials to the more,eigenvec_val
familiar sine and cosine:,eigenvec_val
u(t) = (a1cosω1t +b1sinω1t)x1 +···+(ancosωnt +bnsinωnt)xn.,eigenvec_val
The initial displacement u(0) is easy to keep separate: t = 0 means that sinωt = 0 and,eigenvec_val
"cosωt = 1, leaving only",eigenvec_val
"u(0) = a1x1 +···+anxn,",eigenvec_val
Then differentiating u(t) and setting t = 0. the b’s are determined by the initial velocity:,eigenvec_val
"u′(0) = b1ω1x1 +···+bnωnxn. Substituting the a’s and b’s into the formula for u(t), the",eigenvec_val
The matrix A =,eigenvec_val
has λ1 = −1 and λ2 = −3. The frequencies are ω1 = 1 and,eigenvec_val
"3. If the system starts from rest, u′(0) = 0, the terms in bsinωt will disappear:",eigenvec_val
Solution from u(0) =,eigenvec_val
"Physically, two masses are connected to each other and to stationary wails by three",eigenvec_val
"identical springs (Figure 5.3). The ﬁrst mass is held at v(0) = 1, the second mass is held",eigenvec_val
"at w(0) = 0, and at t = 0 we let go. Their motion u(t) becomes an average of two pure",eigenvec_val
"oscillations, corresponding to the two eigenvectors. In the ﬁrst mode x1 = (1,1), the",eigenvec_val
masses move together and the spring in the middle is never stretched (Figure 5.3a). The,eigenvec_val
frequency ω1 = 1 is the same as for a single spring and a single mass. In the faster mode,eigenvec_val
"x2 = (1,−1) with frequency",eigenvec_val
"3, the masses move oppositely but with equal speeds. The",eigenvec_val
general solution is a combination of these two normal modes. Our particular solution is,eigenvec_val
"As time goes on, the motion is “almost periodic.” If the ratio ω1/ω2 had been a",eigenvec_val
"fraction like 2/3, the masses would eventually return to u(0) = (1,0) and begin again.",eigenvec_val
A combination of sin2t and sin3t would have a period of 2π. But,eigenvec_val
3 is irrational. The,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"best we can say is that the masses will come arbitrarily close to (1,0) and also (0,1).",eigenvec_val
"Like a billiard ball bouncing forever on a perfectly smooth table, the total energy is ﬁxed.",eigenvec_val
Sooner or later the masses come near any state with this energy.,eigenvec_val
Again we cannot leave the problem without drawing a parallel to the continuous case.,eigenvec_val
"As the discrete masses and springs merge into a solid rod, the “second differences” given",eigenvec_val
"by the 1, −2, 1 matrix A turn into second derivatives. This limit is described by the",eigenvec_val
celebrated wave equation ∂ 2u/∂t2 = ∂ 2u/∂x2.,eigenvec_val
"1. Following the ﬁrst example in this section, ﬁnd the eigenvalues and eigenvectors,",eigenvec_val
"and the exponential eAt, for",eigenvec_val
"2. For the previous matrix, write the general solution to du/dt = Au, and the speciﬁc",eigenvec_val
"solution that matches u(0) = (3,1). What is the steady state as t → ∞? (This is a",eigenvec_val
continuous Markov process; λ = 0 in a differential equation corresponds to λ = 1 in,eigenvec_val
"a difference equation, since e0t = 1.)",eigenvec_val
3. Suppose the time direction is reversed to give the matrix −A:,eigenvec_val
Find u(t) and show that it blows up instead of decaying as t → ∞. (Diffusion is,eigenvec_val
"irreversible, and the heat equation cannot run backward.)",eigenvec_val
"4. If P is a projection matrix, show from the inﬁnite series that",eigenvec_val
eP ≈ I +1.718P.,eigenvec_val
5. A diagonal matrix like Λ =,eigenvec_val
"satisﬁes the usual rule eΛ(t+T) = eΛteΛT, because",eigenvec_val
the rule holds for each diagonal entry.,eigenvec_val
"(a) Explain why eA(t+T) = eAteAT, using the formula eAt = SeΛtS−1.",eigenvec_val
"(b) Show that eA+B = eAeB is not true for matrices, from the example",eigenvec_val
(use series for eA and eB).,eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
6. The higher order equation y′′ +y = 0 can be written as a ﬁrst-order system by intro-,eigenvec_val
ducing the velocity y′ as another unknown:,eigenvec_val
"If this is du/dt = Au, what is the 2 by 2 matrix A? Find its eigenvalues and eigen-",eigenvec_val
"vectors, and compute the solution that starts from y(0) = 2, y′(0) = 0.",eigenvec_val
7. Convert y′′ = 0 to a ﬁrst-order system du/dt = Au:,eigenvec_val
This 2 by 2 matrix A has only one eigenvector and cannot be diagonalized. Compute,eigenvec_val
"eAt from the series I +At +··· and write the solution eAtu(0) starting from y(0) = 3,",eigenvec_val
"y′(0) = 4. Check that your (y,y′) satisﬁes y′′ = 0.",eigenvec_val
8. Suppose the rabbit population r and the wolf population w are governed by,eigenvec_val
dt = 4r −2w,eigenvec_val
dt = r +w.,eigenvec_val
"(a) Is this system stable, neutrally stable, or unstable?",eigenvec_val
"(b) If initially r = 300 and w = 200, what are the populations at time t?",eigenvec_val
"(c) After a long time, what is the proportion of rabbits to wolves?",eigenvec_val
9. Decide the stability of u′ = Au for the following matrices:,eigenvec_val
"10. Decide on the stability or instability of dv/dt = w, dw/dt = v. Is there a solution",eigenvec_val
"11. From their trace and determinant, at what time t do the following matrices change",eigenvec_val
"between stable with real eigenvalues, stable with complex eigenvalues, and unstable?",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
12. Find the eigenvalues and eigenvectors for,eigenvec_val
dt = Au =,eigenvec_val
"Why do you know, without computing, that eAt will be an orthogonal matrix and",eigenvec_val
3 will be constant?,eigenvec_val
13. For the skew-symmetric equation,eigenvec_val
dt = Au =,eigenvec_val
(a) write out u′,eigenvec_val
3 and conﬁrm that u′,eigenvec_val
(b) deduce that the length u2,eigenvec_val
3 is a constant.,eigenvec_val
(c) ﬁnd the eigenvalues of A.,eigenvec_val
"The solution will rotate around the axis w = (a,b,c), because Au is the “cross prod-",eigenvec_val
uct” u×w—which is perpendicular to u and w.,eigenvec_val
"14. What are the eigenvalues λ and frequencies ω, and the general solution, of the fol-",eigenvec_val
15. Solve the second-order equation,eigenvec_val
"16. In most applications the second-order equation looks like Mu′′+Ku = 0, with a mass",eigenvec_val
matrix multiplying the second derivatives. Substitute the pure exponential u = eiωtx,eigenvec_val
and ﬁnd the “generalized eigenvalue problem” that must be solved for the frequency,eigenvec_val
ω and the vector x.,eigenvec_val
"17. With a friction matrix F in the equation u′′ + Fu′ − Au = 0, substitute a pure expo-",eigenvec_val
nential u = eλtx and ﬁnd a quadratic eigenvalue problem for λ.,eigenvec_val
"18. For equation (16) in the text, with ω = 1 and",eigenvec_val
"3, ﬁnd the motion if the ﬁrst mass is",eigenvec_val
"hit at t = 0; u(0) = (0,0) and u′(0) = (1,0).",eigenvec_val
19. Every 2 by 2 matrix with trace zero can be written as,eigenvec_val
Show that its eigenvalues are real exactly when a2 +b2 ≥ c2.,eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
"20. By back-substitution or by computing eigenvectors, solve",eigenvec_val
21. Find λ’s and x’s so that u = eλtx solves,eigenvec_val
"What combination u = c1eλ1tx1 +c2eλ2tx2 starts from u(0) = (5,−2)?",eigenvec_val
"22. Solve Problem 21 for u(t) = (y(t),z(t)) by back-substitution:",eigenvec_val
The solution for y will be a combination of e4t and et.,eigenvec_val
"23. Find A to change y′′ = 5y′ +4y into a vector equation for u(t) = (y(t),y′(t)):",eigenvec_val
What are the eigenvalues of A? Find them also by substituting y = eλt into the scalar,eigenvec_val
equation y′′ = 5y′ +4y.,eigenvec_val
24. A door is opened between rooms that hold v(0) = 30 people and w(0) = 10 people.,eigenvec_val
The movement between rooms is proportional to the difference v−w:,eigenvec_val
"Show that the total v+w is constant (40 people). Find the matrix in du/dt = Au, and",eigenvec_val
its eigenvalues and eigenvectors.,eigenvec_val
What are v and w at t = 1?,eigenvec_val
25. Reverse the diffusion of people in Problem 24 to du/dt = −Au:,eigenvec_val
The total v+w still remains constant. How are the λ’s changed now that A is changed,eigenvec_val
to −A? But show that v(t),eigenvec_val
grows to inﬁnity from v(0) = 30.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
26. The solution to y′′ = 0 is a straight line y = C +Dt. Convert to a matrix equation:,eigenvec_val
This matrix A cannot be diagonalized. Find A2 and compute eAt = I +At + 1,eigenvec_val
"···. Multiply your eAt times (y(0),y′(0)) to check the straight line y(t) = y(0) +",eigenvec_val
27. Substitute y = eλt into y′′ = 6y′ − 9y to show that λ = 3 is a repeated root. This is,eigenvec_val
trouble; we need a second solution after e3t. The matrix equation is,eigenvec_val
"Show that this matrix has λ = 3,3 and only one line of eigenvectors. Trouble here",eigenvec_val
too. Show that the second solution is y = te3t.,eigenvec_val
28. Figure out how to write my′′ +by′ +ky = 0 as a vector equation Mu′ = Au.,eigenvec_val
29. (a) Find two familiar functions that solve the equation d2y/dt2 = −y. Which one,eigenvec_val
starts with y(0) = 1 and y′(0) = 0?,eigenvec_val
(b) This second-order equation y′′ = −y produces a vector equation u′ = Au:,eigenvec_val
"Put y(t) from part (a) into u(t) = (y,y′). This solves Problem 6 again.",eigenvec_val
"30. A particular solution to du/dt = Au−b is up = A−1b, if A is invertible. The solutions",eigenvec_val
to du/dt = Au give un. Find the complete solution up +un to,eigenvec_val
"31. If c is not an eigenvalue of A, substitute u = ectv and ﬁnd v to solve du/dt = Au −",eigenvec_val
ectb. This u = ectv is a particular solution. How does it break down when c is an,eigenvec_val
32. Find a matrix A to illustrate each of the unstable regions in Figure 5.2:,eigenvec_val
(a) λ1 < 0 and λ2 > 0.,eigenvec_val
(b) λ1 > 0 and λ2 > 0.,eigenvec_val
(c) Complex λ’s with real part a > 0.,eigenvec_val
Problems 33–41 are about the matrix exponential eAt.,eigenvec_val
5.4 Differential Equations and eAt,eigenvec_val
33. Write ﬁve terms of the inﬁnite series for eAt. Take the t derivative of each term. Show,eigenvec_val
that you have four terms of AeAt. Conclusion: eAtu(0) solves u′ = Au.,eigenvec_val
34. The matrix B =,eigenvec_val
has B2 = 0. Find eBt from a (short) inﬁnite series. Check that,eigenvec_val
the derivative of eBt is BeBt.,eigenvec_val
"35. Starting from u(0), the solution at time T is eATu(0). Go an additional time t to reach",eigenvec_val
eAt(eATu(0)). This solution at time t + T can also be written as,eigenvec_val
eAt times eAT equals,eigenvec_val
36. Write A =,eigenvec_val
in the form SΛS−1. Find eAt from SeΛtS−1.,eigenvec_val
"37. If A2 = A, show that the inﬁnite series produces eAt = I +(et −1)A. For A =",eigenvec_val
"Problem 36, this gives eAt =",eigenvec_val
38. Generally eAeB is different from eBeA. They are both different from eA+B. Check this,eigenvec_val
using Problems 36–37 and 34:,eigenvec_val
39. Write A =,eigenvec_val
as SΛS−1. Multiply SeΛtS−1 to ﬁnd the matrix exponential eAt.,eigenvec_val
Check eAt = I when t = 0.,eigenvec_val
40. Put A =,eigenvec_val
into the inﬁnite series to ﬁnd eAt. First compute A2:,eigenvec_val
41. Give two reasons why the matrix exponential eAt is never singular:,eigenvec_val
(a) Write its inverse.,eigenvec_val
(b) Write its eigenvalues. If Ax = λx then eAtx =,eigenvec_val
"42. Find a solution x(t), y(t) of the ﬁrst system that gets large as t → ∞. To avoid this",eigenvec_val
instability a scientist thought of exchanging the two equations!,eigenvec_val
= −2x + 2y,eigenvec_val
is stable. It has λ < 0. Comment on this craziness.,eigenvec_val
"43. From this general solution to du/dt = Au, ﬁnd the matrix A:",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
It is no longer possible to work only with real vectors and real matrices In the ﬁrst half of,eigenvec_val
"this book, when the basic problem was Ax−b, the solution was real when A and b were",eigenvec_val
real. Complex numbers could have been permitted. but would have contributed nothing,eigenvec_val
"new. Now we cannot avoid them. A real matrix has real coefﬁcients in det(A−λI), but",eigenvec_val
the eigenvalues (as in rotations) may be complex.,eigenvec_val
We now introduce the space Cn of vectors with n complex components. Addition and,eigenvec_val
matrix multiplication follow the same rules as before. Length is computed differently.,eigenvec_val
"The old way, the vector in C2 with components (1,i) would have zero length: 12+i2 = 0,",eigenvec_val
not good. The correct length squared is 12 +|i|2 = 2.,eigenvec_val
This change to ∥x∥2 = |x1|2 +···+|xn|2 forces a whole series of other changes. The,eigenvec_val
"inner product, the transpose, the deﬁnitions of symmetric and orthogonal matrices, all",eigenvec_val
need to be modiﬁed for complex numbers. The new deﬁnitions coincide with the old,eigenvec_val
when the vectors and matrices are real. We have listed these changes in a table at the,eigenvec_val
end of the section. and we explain them as we go.,eigenvec_val
That table virtually amounts to a dictionary for translating real into complex. We,eigenvec_val
hope it will be useful to the reader. We particularly want to ﬁnd out about symmetric,eigenvec_val
"matrices and Hermitian matrices: Where are their eigenvalues, and what is special",eigenvec_val
"about their eigenvectors? For practical purposes, those are the most important questions",eigenvec_val
in the theory of eigenvalues. We call attention in advance to the answers:,eigenvec_val
1. Every symmetric matrix (and Hermitian matrix) has real eigenvalues.,eigenvec_val
2. Its eigenvectors can be chosen to be orthonormal.,eigenvec_val
"Strangely, to prove that the eigenvalues are real we begin with the opposite possibility—",eigenvec_val
"and that takes us to complex numbers, complex vectors, and complex matrices.",eigenvec_val
Complex Numbers and Their Conjugates,eigenvec_val
Probably the reader has already met complex numbers; a review is easy to give. The,eigenvec_val
important ideas are the complex conjugate ¯x and the absolute value |x|. Everyone knows,eigenvec_val
"that whatever i is, it satisﬁes the equation i2 = −1. It is a pure imaginary number, and",eigenvec_val
"so are its multiples ib; b is real. The sum a+ib is a complex number, and it is plotted in",eigenvec_val
a natural way on the complex plane (Figure 5.4).,eigenvec_val
The real numbers a and the imaginary numbers ib are special cases of complex num-,eigenvec_val
bers; they lie on the axes. Two complex numbers are easy to add:,eigenvec_val
a + ib = reiθ,eigenvec_val
a − ib = a + ib = re−iθ,eigenvec_val
r = |a + ib|,eigenvec_val
r2 = a2 + b2,eigenvec_val
"Figure 5.4: The complex plane, with a+ib = reiθ and its conjugate a−ib = re−iθ.",eigenvec_val
Multiplying a+ib times c+id uses the rule that i2 = −1:,eigenvec_val
(a+ib)(c+id) = ac+ibc+iad +i2bd,eigenvec_val
The complex conjugate of a + ib is the number a − ib. The sign of the imaginary,eigenvec_val
part is reversed. It is the mirror image across the real axis; any real number is its own,eigenvec_val
"conjugate, since b = 0. The conjugate is denoted by a bar or a star: (a+ib)∗ = a+ib =",eigenvec_val
a−ib. It has three important properties:,eigenvec_val
1. The conjugate of a product equals the product of the conjugates:,eigenvec_val
(a+ib)(c+id) = (ac−bd)−i(bc+ad) = (a+ib)(c+id).,eigenvec_val
2. The conjugate of a sum equals the sum of the conjugates:,eigenvec_val
(a+c)+i(b+d) = (a+c)−i(b+d) = (a+ib)+(c+id).,eigenvec_val
3. Multiplying any a+ib by its conjugate a−ib produces a real number a2 +b2:,eigenvec_val
(a+ib)(a−ib) = a2 +b2 = r2.,eigenvec_val
This distance r is the absolute value |a+ib| =,eigenvec_val
"Finally, trigonometry connects the sides a and b to the hypotenuse r by a = rcosθ",eigenvec_val
and b = rsinθ. Combining these two equations moves us into polar coordinates:,eigenvec_val
a+ib = r(cosθ +isinθ) = reiθ.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
The most important special case is when r = 1. Then a + ib is eiθ = cosθ + isinθ. It,eigenvec_val
"falls on the unit circle in the complex plane. As θ varies from 0 to 2π, this number eiθ",eigenvec_val
circles around zero at the constant radial distance |eiθ| =,eigenvec_val
cos2 θ +sin2θ = 1.,eigenvec_val
Example 1. x = 3+4i times its conjugate x = 3−4i is the absolute value squared:,eigenvec_val
xx = (3+4i)(3−4i) = 25 = |x|2,eigenvec_val
r = |x| = 5.,eigenvec_val
"To divide by 3+4i, multiply numerator and denominator by its conjugate 3−4i:",eigenvec_val
"In polar coordinates, multiplication and division are easy:",eigenvec_val
reiθ times Reiα has absolute value rR and angle θ +α.,eigenvec_val
reiθ divided by Reiα has absolute value r/R and angle θ −α.,eigenvec_val
Lengths and Transposes in the Complex Case,eigenvec_val
"We return to linear algebra, and make the conversion from real to complex. By deﬁnition,",eigenvec_val
the complex vector space Cn contains all vectors x with n complex components:,eigenvec_val
x j = a j +ibj.,eigenvec_val
Vectors x and y are still added component by component. Scalar multiplication cx is,eigenvec_val
"now done with complex numbers c. The vectors v1,...,vk are linearly dependent if",eigenvec_val
some nontrivial combination gives c1v1 + ... + ckvk = 0; the c j may now be complex.,eigenvec_val
The unit coordinate vectors are still in Cn; they are still independent; and they still form,eigenvec_val
a basis. Therefore Cn is a complex vector space of dimension n.,eigenvec_val
"In the new deﬁnition of length, each x2",eigenvec_val
j is replaced by its modulus |x j|2:,eigenvec_val
∥x∥2 = |x1|2 +···+|xn|2.,eigenvec_val
Example 2. x =,eigenvec_val
For real vectors there was a close connection between the length and the inner product:,eigenvec_val
∥x∥2 = xTx. This connection we want to preserve. The inner product must be modiﬁed,eigenvec_val
"to match the new deﬁnition of length, and we conjugate the ﬁrst vector in the inner",eigenvec_val
"product. Replacing x by x, the inner product becomes",eigenvec_val
xTy = x1y1 +···+xnyn.,eigenvec_val
"If we take the inner product of x = (1+3i,3i) with itself, we are back to ∥x∥2:",eigenvec_val
xTx = (1+i)(1+i)+(3i)(3i) = 2+9,eigenvec_val
Note that yTx is different from xTy; we have to watch the order of the vectors.,eigenvec_val
"This leaves only one more change in notation, condensing two symbols into one.",eigenvec_val
"Instead of a bar for the conjugate and a T for the transpose, those are combined into the",eigenvec_val
"conjugate transpose. For vectors and matrices, a superscript H (or a star) combines both",eigenvec_val
operations. This matrix AT = AH = A∗ is called “A Hermitian”:,eigenvec_val
(AH)i j = A ji.,eigenvec_val
"You have to listen closely to distinguish that name from the phrase “A is Hermitian,”",eigenvec_val
"which means that A equals AH. If A is an m by n matrix, then AH is n by m:",eigenvec_val
"This symbol AH gives ofﬁcial recognition to the fact that, with complex entries, it is",eigenvec_val
very seldom that we want only the transpose of A. It is the conjugate transpose AH that,eigenvec_val
"becomes appropriate, and xH is the row vector [x1 ··· xn].",eigenvec_val
1. The inner product of x and y is xHy. Orthogonal vectors have xHy = 0.,eigenvec_val
2. The squared length of x is ∥x∥2 = xHx = |x1|2 +···+|xn|2.,eigenvec_val
3. Conjugating (AB)T = BTAT produces (AB)H = BHAH.,eigenvec_val
"We spoke in earlier chapters about symmetric matrices: A = AT. With complex entries,",eigenvec_val
this idea of symmetry has to be extended. The right generalization is not to matrices that,eigenvec_val
"equal their transpose, but to matrices that equal their conjugate transpose. These are",eigenvec_val
"the Hermitian matrices, and a typical example is A:",eigenvec_val
The diagonal entries must be real; they are unchanged by conjugation. Each off-diagonal,eigenvec_val
"entry is matched with its mirror image across the main diagonal, and 3−3i is the conju-",eigenvec_val
"gate of 3+3i. In every case, aij = a ji.",eigenvec_val
Our main goal is to establish three basic properties of Hermitian matrices. These,eigenvec_val
properties apply equally well to symmetric matrices. A real symmetric matrix is cer-,eigenvec_val
tainly Hermitian. (For real matrices there is no difference between AT and AH.) The,eigenvec_val
eigenvalues of A are real—as we now prove.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"Property 1 If A = AH, then for all complex vectors x, the number xHAx is real.",eigenvec_val
"Every entry of A contributes to xHAx. Try the 2 by 2 case with x = (u,v):",eigenvec_val
= real+real+(sum of complex conjugates).,eigenvec_val
"For a proof in general. (xHAx)H is the conjugate of the 1 by 1 matrix xHAx, but we",eigenvec_val
actually get the same number back again: (xHAx)H = xHAHxHH = xHAx. So that number,eigenvec_val
"Property 2 If A = AH, every eigenvalue is real.",eigenvec_val
Proof. Suppose Ax = λx. The trick is to multiply by xH: xHAx = λxHx. The left-hand,eigenvec_val
"side is real by Property 1, and the right-hand side xHx = ∥x∥2 is real and positive, because",eigenvec_val
x ̸= 0. Therefore λ = xHAx/xHx must be real. Our example has λ = 8 and λ = −1:,eigenvec_val
����� = λ 2 −7λ +10−|3−3i|2,eigenvec_val
= λ 2 −7λ −8 = (λ −8)(λ +1).,eigenvec_val
Note. This proof of real eigenvalues looks correct for any real matrix:,eigenvec_val
There must be a catch: The eigenvector x might be complex. It is when A = AT that,eigenvec_val
"we can be sure λ and x stay real. More than that, the eigenvectors are perpendicular:",eigenvec_val
xTy = 0 in the real symmetric case and xHy = 0 in the complex Hermitian case.,eigenvec_val
Property 3 Two eigenvectors of a real symmetric matrix or a Hermitian ma-,eigenvec_val
"trix, if they come from different eigenvalues, are orthogonal to one another.",eigenvec_val
"The proof starts with Ax = λ1x, Ay = λ1y, and A = AH:",eigenvec_val
(λ1x)Hy = (Ax)Hy = xHAy = xH(λ2y).,eigenvec_val
"The outside numbers are λ1xHy = λ2xHy, since the λ’s are real. Now wc use the assump-",eigenvec_val
"tion λ1 ̸= λ2, which forces the conclusion that xHy = 0. In our example,",eigenvec_val
These two eigenvectors are orthogonal:,eigenvec_val
Of course any multiples x/α and y/β are equally good as eigenvectors. MATLAB,eigenvec_val
"picks α = ∥x∥ and β = ∥y∥, so that x/α and y/β are unit vectors; the eigenvectors",eigenvec_val
are normalized to have length 1. They are now orthonormal. If these eigenvectors are,eigenvec_val
"chosen to be the columns of S, then we have S−1AS = Λ as always. The diagonalizing",eigenvec_val
matrix can be chosen with orthonormal columns when A = AH.,eigenvec_val
"In case A is real and symmetric, its eigenvalues are real by Property 2. Its unit",eigenvec_val
eigenvectors are orthogonal by Property 3. Those eigenvectors are also real; they solve,eigenvec_val
"(A − λI)x = 0. These orthonormal eigenvectors go into an orthogonal matrix Q, with",eigenvec_val
QTQ = I and QT = Q−1.,eigenvec_val
Then S−1AS = Λ becomes special—it is Q−1AQ = Λ or,eigenvec_val
A = QΛQ−1 = QΛQT. We can state one of the great theorems of linear algebra:,eigenvec_val
5O A real symmetric matrix can be factored into A = QΛQT. Its orthonormal,eigenvec_val
eigenvectors are in the orthogonal matrix Q and its eigenvalues are in Λ.,eigenvec_val
"In geometry or mechanics, this is the principal axis theorem. It gives the right choice",eigenvec_val
"of axes for an ellipse. Those axes are perpendicular, and they point along the eigen-",eigenvec_val
vectors of the corresponding matrix. (Section 6.2 connects symmetric matrices to n-,eigenvec_val
"dimensional ellipses.) In mechanics the eigenvectors give the principal directions, along",eigenvec_val
which there is pure compression or pure tension—with no shear.,eigenvec_val
In mathematics the formula A = QΛQT is known as the spectral theorem. If we,eigenvec_val
"multiply columns by rows, the matrix A becomes a combination of one-dimensional",eigenvec_val
"projections—which are the special matrices xxT of rank 1, multiplied by λ:",eigenvec_val
A = QΛQT =,eigenvec_val
Our 2 by 2 example has eigenvalues 3 and 1:,eigenvec_val
Example 3. A =,eigenvec_val
= combination of two projections.,eigenvec_val
"The eigenvectors, with length scaled to 1, are",eigenvec_val
Then the matrices on the right-hand side are x1xT,eigenvec_val
they are projections onto the line through x1 and the line through x2.,eigenvec_val
All symmetric matrices are combinations of one-dimensional projections—which are,eigenvec_val
symmetric matrices of rank 1.,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"Remark. If A is real and its eigenvalues happen to be real, then its eigenvectors are also",eigenvec_val
real. They solve (A −λI)x = 0 and can be computed by elimination. But they will not,eigenvec_val
be orthogonal unless A is symmetric: A = QΛQT leads to AT = A.,eigenvec_val
"If A is real, all complex eigenvalues come in conjugate pairs: Ax = λx and Ax = λx.",eigenvec_val
"If a+ib is an eigenvalue of a real matrix, so is a−ib. (If A = AT then b = 0.)",eigenvec_val
"Strictly speaking, the spectral theorem A = QΛQT has been proved only when the",eigenvec_val
"eigenvalues of A are distinct. Then there are certainly n independent eigenvectors, and",eigenvec_val
A can be safely diagonalized. Nevertheless it is true (see Section 5.6) that even with,eigenvec_val
"repeated eigenvalues, a symmetric matrix still has a complete set of orthonormal eigen-",eigenvec_val
"vectors. The extreme case is the identity matrix, which has λ = 1 repeated n times—and",eigenvec_val
no shortage of eigenvectors.,eigenvec_val
To ﬁnish the complex case we need the analogue of a real orthogonal matrix—and you,eigenvec_val
can guess what happens to the requirement QTQ = I. The transpose will be replaced by,eigenvec_val
the conjugate transpose. The condition will become UHU = I. The new letter U reﬂects,eigenvec_val
the new name: A complex matrix with orthonormal columns is called a unitary matrix.,eigenvec_val
May we propose two analogies? A Hermitian (or symmetric) matrix can be compared,eigenvec_val
to a real number. A unitary (or orthogonal) matrix can be compared to a number on,eigenvec_val
"the unit circle—a complex number of absolute value 1. The λ’s are real if AH = A, and",eigenvec_val
they are on the unit circle if UHU = I. The eigenvectors can be scaled to unit length and,eigenvec_val
Those statements are not yet proved for unitary (including orthogonal) matrices.,eigenvec_val
Therefore we go directly to the three properties of U that correspond to the earlier Prop-,eigenvec_val
erties 1–3 of A. Remember that U has orthonormal columns:,eigenvec_val
"This leads directly to Property 1′, that multiplication by U has no effect on inner prod-",eigenvec_val
"ucts, angles, or lengths. The proof is on one line, just as it was for Q:",eigenvec_val
Property 1′ (Ux)H(Uy) = xHUHUy = xHy and lengths are preserved by U:,eigenvec_val
∥Ux∥2 = xHUHUx = ∥x∥2.,eigenvec_val
Property 2′ Every eigenvalue of U has absolute value |λ| = 1.,eigenvec_val
"This follows directly from Ux = λx, by comparing the lengths of the two sides:",eigenvec_val
"∥Ux∥ = ∥x∥ by Property 1′, and always ∥λx∥ = |λ|∥x∥. Therefore |λ| = 1.",eigenvec_val
"6Later we compare “skew-Hermitian” matrices with pure imaginary numbers, and “normal” matrices with all",eigenvec_val
"complex numbers a + ib. A nonnormal matrix without orthogonal eigenvectors belongs to none of these classes,",eigenvec_val
and is outside the whole analogy.,eigenvec_val
Property 3′ Eigenvectors corresponding to different eigenvalues are orthonor-,eigenvec_val
"Start with Ux = λ1x and Uy = λ2y, and take inner products by Property 1′:",eigenvec_val
xHy = (Ux)H(Uy) = (λ1x)H(λ2y) = λ 1λ2xHy.,eigenvec_val
"Comparing the left to the right, λ 1λ2 = 1 or xHy = 0. But Property 2′ is λ 1λ1 = 1, so we",eigenvec_val
cannot also have λ 1λ2 = 1. Thus xHy = 0 and the eigenvectors are orthogonal.,eigenvec_val
Example 4. U =,eigenvec_val
has eigenvalues eit and e−it.,eigenvec_val
"The orthogonal eigenvectors are x = (1,−i) and y = (1,i). (Remember to take conjugates",eigenvec_val
in xHy = 1+i2 = 0.) After division by,eigenvec_val
2 they are orthonormal.,eigenvec_val
Here is the most important unitary matrix by far.,eigenvec_val
Example 5. U = 1,eigenvec_val
���� = Fourier matrix,eigenvec_val
The complex number w is on the unit circle at the angle θ = 2π/n. It equals e2πi/n. Its,eigenvec_val
powers are spaced evenly around the circle. That spacing assures that the sum of all n,eigenvec_val
"powers of w—all the nth roots of 1—is zero. Algebraically, the sum 1+w+···+wn−1",eigenvec_val
is (wn −1)/(w−1). And wn −1 is zero!,eigenvec_val
row 1 of UH times column 2 of U is 1,eigenvec_val
n(1+w+w2 +···+wn−1) = wn −1,eigenvec_val
row i of UH times column j of U is 1,eigenvec_val
n(1+W +W 2 +···+W n−1) = W n −1,eigenvec_val
W −1 = 0.,eigenvec_val
"In the second case, W = w j−i. Every entry of the original F has absolute value 1. The",eigenvec_val
factor √n shrinks the columns of U into unit vectors. The fundamental identity of the,eigenvec_val
ﬁnite Fourier transform is UHU = I.,eigenvec_val
Thus U is a unitary matrix. Its inverse looks the same except that w is replaced by,eigenvec_val
"w−1 = e−iθ = w. Since U is unitary, its inverse is found by transposing (which changes",eigenvec_val
nothing) and conjugating (which changes w to w). The inverse of this U is U. Ux can be,eigenvec_val
computed quickly by the Fast Fourier Transform as found in Section 3.5.,eigenvec_val
"By Property 1′ of unitary matrices, the length of a vector x is the same as the length",eigenvec_val
of Ux. The energy in state space equals the energy in transform space. The energy is,eigenvec_val
"the sum of |x j|2, and it is also the sum of the energies in the separate frequencies. The",eigenvec_val
"vector x = (1,0,...,0) contains equal amounts of every frequency component, and its",eigenvec_val
"Discrete Fourier Transform Ux = (1,1,...,1)/√n also has length 1.",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
0 1 0 0,eigenvec_val
0 0 1 0,eigenvec_val
0 0 0 1,eigenvec_val
1 0 0 0,eigenvec_val
"This is an orthogonal matrix, so by Property 3′ it must have orthogonal eigenvectors.",eigenvec_val
They are the columns of the Fourier matrix! Its eigenvalues must have absolute value 1.,eigenvec_val
"They are the numbers 1,w,...,wn−1 (or 1,i,i2,i3 in this 4 by 4 ease). It is a real matrix,",eigenvec_val
but its eigenvalues and eigenvectors are complex.,eigenvec_val
"One ﬁnal note, Skew-Hermitian matrices satisfy KH = −K, just as skew-symmetric",eigenvec_val
matrices satisfy KT = −K. Their properties follow immediately from their close link to,eigenvec_val
If A is Hermitian then K = iA is skew-Hermitian.,eigenvec_val
The eigenvalues of K are purely imaginary instead of purely real; we multiply i. The,eigenvec_val
eigenvectors are not changed. The Hermitian example on the previous pages would lead,eigenvec_val
K = iA =,eigenvec_val
The diagonal entries are multiples of i (allowing zero). The eigenvalues are 8i and −i.,eigenvec_val
"The eigenvectors are still orthogonal, and we still have K = UΛUH—with a unitary U",eigenvec_val
"instead of a real orthogonal Q, and with 8i and −i on the diagonal of Λ.",eigenvec_val
This section is summarized by a table of parallels between real and complex.,eigenvec_val
Rn (n real components),eigenvec_val
Cn (n complex components),eigenvec_val
length: ∥x∥2 = x2,eigenvec_val
length: ∥x∥2 = |x1|2 +···+|xn|2,eigenvec_val
ij = A ji,eigenvec_val
ij = A ji,eigenvec_val
inner product: xTy = x1y1 +···+xnyn,eigenvec_val
inner product: xHy = x1y1 +···+xnyn,eigenvec_val
orthogonality: xTy = 0,eigenvec_val
orthogonality: xHy = 0,eigenvec_val
symmetric matrices: AT = A,eigenvec_val
Hermitian matrices: AH = A,eigenvec_val
A = QΛQ−1 = QΛQT (real Λ),eigenvec_val
A = UΛU−1 = UΛUH (real Λ),eigenvec_val
skew-symmetric KT = −K,eigenvec_val
skew-Hermitian KH = −K,eigenvec_val
orthogonal QTQ = I or QT = Q−1,eigenvec_val
unitary UHU = I or UH = U−1,eigenvec_val
(Qx)T(Qy) = xTy and ∥Qx∥ = ∥x∥,eigenvec_val
(Ux)H(Uy) = xHy and ∥Ux∥ = ∥x∥,eigenvec_val
"The columns, rows, and eigenvectors of Q and U are orthonormal, and every |λ| = 1",eigenvec_val
"1. For the complex numbers 3+4i and 1−i,",eigenvec_val
(a) ﬁnd their positions in the complex plane.,eigenvec_val
(b) ﬁnd their sum and product.,eigenvec_val
(c) ﬁnd their conjugates and their absolute values.,eigenvec_val
Do the original numbers lie inside or outside the unit circle?,eigenvec_val
2. What can you say about,eigenvec_val
(a) the sum of a complex number and its conjugate?,eigenvec_val
(b) the conjugate of a number on the unit circle?,eigenvec_val
(c) the product of two numbers on the unit circle?,eigenvec_val
(d) the sum of two numbers on the unit circle?,eigenvec_val
"3. If x = 2+i and y = 1+3i, ﬁnd x, xx, 1/x, and x/y. Check that the absolute value |xy|",eigenvec_val
"equals |x| times |y|, and the absolute value |1/x| equals 1 divided by |x|.",eigenvec_val
"4. Find a and b for the complex numbers a + ib at the angles θ = 30°,60°,90° on the",eigenvec_val
"unit circle. Verify by direct multiplication that the square of the ﬁrst is the second,",eigenvec_val
and the cube of the ﬁrst is the third.,eigenvec_val
"5. (a) If x = reiθ what are x2, x−1, and x in polar coordinates? Where are the complex",eigenvec_val
numbers that have x−1 = x?,eigenvec_val
"(b) At t = 0, the complex number e(−1+i)t equals one. Sketch its path in the complex",eigenvec_val
plane as t increases from 0 to 2π.,eigenvec_val
6. Find the lengths and the inner product of,eigenvec_val
7. Write out the matrix AH and compute C = AHA if,eigenvec_val
What is the relation between C and CH? Does it hold whenever C is constructed from,eigenvec_val
"8. (a) With the preceding A, use elimination to solve Ax = 0.",eigenvec_val
(b) Show that the nullspace you just computed is orthogonal to C(AH) and not to,eigenvec_val
the usual row space C(AT). The four fundamental spaces in the complex case,eigenvec_val
"are N(A) and C(A) as before, and then N(AH) and C(AH).",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
9. (a) How is the determinant of AH related to the determinant of A?,eigenvec_val
(b) Prove that the determinant of any Hermitian matrix is real.,eigenvec_val
"10. (a) How many degrees of freedom are there in a real symmetric matrix, a real diag-",eigenvec_val
"onal matrix, and a real orthogonal matrix? (The ﬁrst answer is the sum of the",eigenvec_val
"other two, because A = QΛQT.)",eigenvec_val
(b) Show that 3 by 3 Hermitian matrices A and also unitary U have 9 real degrees of,eigenvec_val
freedom (columns of U can be multiplied by any eiθ).,eigenvec_val
"11. Write P, Q and R in the form λ1x1xH",eigenvec_val
2 of the spectral theorem:,eigenvec_val
12. Give a reason if true or a counterexample if false:,eigenvec_val
"(a) If A is Hermitian, then A+iI is invertible.",eigenvec_val
(b) If Q is orthogonal. then Q+ 1,eigenvec_val
"(c) If A is real, then A+iI is invertible.",eigenvec_val
"13. Suppose A is a symmetric 3 by 3 matrix with eigenvalues 0, 1, 2.",eigenvec_val
"(a) What properties can be guaranteed for the corresponding unit eigenvectors u, v,",eigenvec_val
"(b) In terms of u, v, w, describe the nullspace, left nullspace, row space and column",eigenvec_val
(c) Find a vector x that satisﬁes Ax = v+w. Is x unique?,eigenvec_val
(d) Under what conditions on b does Ax = b have a solution?,eigenvec_val
"(e) If u, v, w are the columns of S, what are S−1 and S−1AS?",eigenvec_val
"14. In the list below, which classes of matrices contain A and which contain B?",eigenvec_val
0 1 0 0,eigenvec_val
0 0 1 0,eigenvec_val
0 0 0 1,eigenvec_val
1 0 0 0,eigenvec_val
1 1 1 1,eigenvec_val
1 1 1 1,eigenvec_val
1 1 1 1,eigenvec_val
1 1 1 1,eigenvec_val
"Orthogonal, invertible, projection, permutation, Hermitian, rank-1, diagonalizable,",eigenvec_val
Markov. Find the eigenvalues of A and B.,eigenvec_val
15. What is the dimension of the space S of all n by n real symmetric matrices? The,eigenvec_val
spectral theorem says that every symmetric matrix is a combination of n projection,eigenvec_val
"matrices. Since the dimension exceeds n, how is this difference explained?",eigenvec_val
16. Write one signiﬁcant fact about the eigenvalues of each of the following.,eigenvec_val
(a) A real symmetric matrix.,eigenvec_val
(b) A stable matrix: all solutions to du/dt = Au approach zero.,eigenvec_val
(c) An orthogonal matrix.,eigenvec_val
(d) A Markov matrix.,eigenvec_val
(e) A defective matrix (nondiagonalizable).,eigenvec_val
(f) A singular matrix.,eigenvec_val
"17. Show that if U and V are unitary, so is UV. Use the criterion UHU = I.",eigenvec_val
"18. Show that a unitary matrix has |detU| = 1, but possibly detU is different from",eigenvec_val
detUH. Describe all 2 by 2 matrices that are unitary.,eigenvec_val
19. Find a third column so that U is unitary. How much freedom in column 3?,eigenvec_val
20. Diagonalize the 2 by 2 skew-Hermitian matrix K =,eigenvec_val
", whose entries are all √−1.",eigenvec_val
"Compute eKt = SeΛtS−1, and verify that eKt is unitary. What is the derivative of eKt",eigenvec_val
at t = 0?,eigenvec_val
"21. Describe all 3 by 3 matrices that are simultaneously Hermitian, unitary, and diagonal.",eigenvec_val
How many are there?,eigenvec_val
"22. Every matrix Z can be split into a Hermitian and a skew-Hermitian part, Z = A+K,",eigenvec_val
"just as a complex number z is split into a+ib, The real part of z is half of z+ z, and",eigenvec_val
the “real part” of Z is half of Z +ZH. Find a similar formula for the “imaginary part”,eigenvec_val
"K, and split these matrices into A+K:",eigenvec_val
23. Show that the columns of the 4 by 4 Fourier matrix F in Example 5 are eigenvectors,eigenvec_val
of the permutation matrix P in Example 6.,eigenvec_val
"24. For the permutation of Example 6, write out the circulant matrix C = c0I + c1P +",eigenvec_val
c2P2 + c3P3. (Its eigenvector matrix is again the Fourier matrix.) Write out also,eigenvec_val
"the four components of the matrix-vector product Cx, which is the convolution of",eigenvec_val
"c = (c0,c1,c2,c3) and x = (x0,x1,x2,x3).",eigenvec_val
"25. For a circulant C = FΛF−1, why is it faster to multiply by F−1, then Λ, then F (the",eigenvec_val
"convolution rule), than to multiply directly by C?",eigenvec_val
"26. Find the lengths of u = (1+i,1−i,1+2i) and v = (i,i,i). Also ﬁnd uHv and vHu.",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"27. Prove that AHA is always a Hermitian matrix, Compute AHA and AAH:",eigenvec_val
"28. If Az = 0, then AHAz = 0. If AHAz = 0, multiply by zH to prove that Az = 0. The",eigenvec_val
nullspaces of A and AHA are,eigenvec_val
. AHA is an invertible Hermitian matrix when the,eigenvec_val
nullspace of A contains only z =,eigenvec_val
"29. When you multiply a Hermitian matrix by a real number c, is cA still Hermitian? If",eigenvec_val
"c = i, show that iA is skew-Hermitian. The 3 by 3 Hermitian matrices are a subspace,",eigenvec_val
provided that the “scalars” are real numbers.,eigenvec_val
"30. Which classes of matrices does P belong to: orthogonal, invertible, Hermitian, uni-",eigenvec_val
"tary, factorizable into LU, factorizable into QR?",eigenvec_val
"31. Compute P2, P3, and P100 in Problem 30. What are the eigenvalues of P?",eigenvec_val
"32. Find the unit eigenvectors of P in Problem 30, and put them into the columns of a",eigenvec_val
unitary matrix U. What property of P makes these eigenvectors orthogonal?,eigenvec_val
33. Write down the 3 by 3 circulant matrix C = 2I + 5P + 4P2. It has the same eigen-,eigenvec_val
vectors as P in Problem 30. Find its eigenvalues.,eigenvec_val
"34. If U is unitary and Q is a real orthogonal matrix, show that U−1 is unitary and also",eigenvec_val
UQ is unitary. Start from UHU = I and QTQ = I.,eigenvec_val
35. Diagonalize A (real λ’s) and K (imaginary λ’s) to reach UΛUH:,eigenvec_val
36. Diagonalize this orthogonal matrix to reach Q = UΛUH. Now all λ’s are,eigenvec_val
37. Diagonalize this unitary matrix V to reach V = UΛUH. Again all |λ| = 1:,eigenvec_val
"38. If v1,...,vn is an orthonormal basis for Cn, the matrix with those columns is a",eigenvec_val
matrix. Show that any vector z equals (vH,eigenvec_val
39. The functions e−ix and e−ix are orthogonal on the interval 0 ≤ x ≤ 2π because their,eigenvec_val
complex inner product is,eigenvec_val
"40. The vectors v = (1,i,1), w = (i,1,0) and z =",eigenvec_val
are an orthogonal basis for,eigenvec_val
"41. If A = R+iS is a Hermitian matrix, are the real matrices R and S symmetric?",eigenvec_val
42. The (complex) dimension of Cn is,eigenvec_val
. Find a nonreal basis for Cn.,eigenvec_val
43. Describe all 1 by 1 matrices that are Hermitian and also unitary. Do the same for 2,eigenvec_val
44. How are the eigenvalues of AH (square matrix) related to the eigenvalues of A?,eigenvec_val
"45. If uHu = 1, show that I −2uuH is Hermitian and also unitary. The rank-1 matrix uuH",eigenvec_val
is the projection onto what line in Cn?,eigenvec_val
"46. If A+iB is a unitary matrix (A and B are real), show that Q =",eigenvec_val
"47. If A+iB is a Hermitian matrix (A and B are real), show that",eigenvec_val
48. Prove that the inverse of a Hermitian matrix is again a Hermitian matrix.,eigenvec_val
49. Diagonalize this matrix by constructing its eigenvalue matrix Λ and its eigenvector,eigenvec_val
50. A matrix with orthonormal eigenvectors has the form A = UΛU−1 = UΛUH. Prove,eigenvec_val
that AAH = AHA. These are exactly the normal matrices.,eigenvec_val
Virtually every step in this chapter has involved the combination S−1AS. The eigenvec-,eigenvec_val
"tors of A went into the columns of S, and that made S−1AS a diagonal matrix (called",eigenvec_val
"Λ). When A was symmetric, we wrote Q instead of S, choosing the eigenvectors to be",eigenvec_val
"orthonormal. In the complex case, when A is Hermitian we write U—it is still the matrix",eigenvec_val
of eigenvectors. Now we look at all combinations M−1AM—formed with any invertible,eigenvec_val
M on the right and its inverse on the left. The invertible eigenvector matrix S may fail to,eigenvec_val
"exist (the defective case), or we may not know it, or we may not want to use it.",eigenvec_val
First a new word: The matrices A and M−1AM are “similar”. Going from one to,eigenvec_val
the other is a similarity transformation. It is the natural step for differential equations,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
or matrix powers or eigenvalues—just as elimination steps were natural for Ax = b.,eigenvec_val
"Elimination multiplied A on the left by L−1, but not on the right by L. So U is not",eigenvec_val
"similar to A, and the pivots are not the eigenvalues.",eigenvec_val
"A whole family of matrices M−1AM is similar to A, and there are two questions:",eigenvec_val
1. What do these similar matrices M−1AM have in common?,eigenvec_val
"2. With a special choice of M, what special form can be achieved by M−1AM?",eigenvec_val
"The ﬁnal answer is given by the Jordan form, with which the chapter ends.",eigenvec_val
"These combinations M−1AM arise in a differential or difference equation, when a",eigenvec_val
“change of variables” u = Mv introduces the new unknown v:,eigenvec_val
"The new matrix in the equation is M−1AM. In the special case M = S, the system is",eigenvec_val
uncoupled because Λ = S−1AS is diagonal. The eigenvectors evolve independently. This,eigenvec_val
"is the maximum simpliﬁcation, but other M’s are also useful. We try to make M−1AM",eigenvec_val
easier to work with than A.,eigenvec_val
"The family of matrices M−1AM includes A itself, by choosing M = I. Any of these",eigenvec_val
"similar matrices can appear in the differential and difference equations, by the change",eigenvec_val
"u = Mv, so they ought to have something in common, and they do: Similar matrices",eigenvec_val
share the same eigenvalues.,eigenvec_val
Suppose that B = M−1AM. Then A and B have the same eigenvalues.,eigenvec_val
Every eigenvector x of A corresponds to an eigenvector M−1x of B.,eigenvec_val
Start from Ax = λx and substitute A = MBM−1:,eigenvec_val
The eigenvalue of B is still λ. The eigenvector has changed from x to M−1x.,eigenvec_val
We can also check that A−λI and B−λI have the same determinant:,eigenvec_val
B−λI = M−1AM −λI = M−1(A−λI)M,eigenvec_val
det(B−λI) = detM−1det(A−λI)detM = det(A−λI).,eigenvec_val
The polynomials det(A − λI) and det(B − λI) are equal. Their roots—the eigenvalues,eigenvec_val
of A and B—are the same. Here are matrices B similar to A.,eigenvec_val
Example 1. A =,eigenvec_val
has eigenvalues 1 and 0. Each B is M−1AM:,eigenvec_val
", then B =",eigenvec_val
triangular with λ = 0 and 0.,eigenvec_val
", then B =",eigenvec_val
projection with λ = 0 and 0.,eigenvec_val
", then B = an arbitrary matrix with λ = 0 and 0.",eigenvec_val
"In this case we can produce any B that has the correct eigenvalues. It is an easy case,",eigenvec_val
"because the eigenvalues 1 and 0 are distinct. The diagonal A was actually Λ, the out-",eigenvec_val
standing member of this family of similar matrices (the capo). The Jordan form will,eigenvec_val
worry about repeated eigenvalues and a possible shortage of eigenvectors. All we say,eigenvec_val
no is that every M−1AM has the same number of independent eigenvectors as A (each,eigenvec_val
eigenvector is multiplied by M−1).,eigenvec_val
The ﬁrst step is to look at the linear transformations that lie behind the matrices.,eigenvec_val
"Rotations, reﬂections, and projections act on n-dimensional space. The transformation",eigenvec_val
"can happen without linear algebra, but linear algebra turns it into matrix multiplication.",eigenvec_val
Change of Basis = Similarity Transformation,eigenvec_val
"The similar matrix B = M−1AM is closely connected to A, if we go back to linear trans-",eigenvec_val
formations. Remember the key idea: Every linear transformation is represented by a,eigenvec_val
matrix. The matrix depends on the choice of basis! If we change the basis by M we,eigenvec_val
change the matrix A to a similar matrix B.,eigenvec_val
Similar matrices represent the same transformation T with respect so different,eigenvec_val
"bases. The algebra is almost straightforward. Suppose we have a basis v1,...,vn. The",eigenvec_val
jth column of A comes from applying T to v j:,eigenvec_val
Tvj = combination of the basis vectors = a1jv1 +···+anjvn.,eigenvec_val
"For a new basis V1,...,Vn, the new matrix B is constructed in the same way: TVj =",eigenvec_val
combination of the V’s = b1 jV1 + ··· + bnjVn. But also each V must be a combination,eigenvec_val
of the old basis vectors: Vj = ∑mijvi. That matrix M is really representing the identity,eigenvec_val
transformation (!) when the only thing happening is the change of basis (T is I). The in-,eigenvec_val
verse matrix M−1 also represents the identity transformation. when the basis is changed,eigenvec_val
from the v’s back to the V’s. Now the product rule gives the result we want:,eigenvec_val
The matrices A and B that represent the same linear transformation T,eigenvec_val
with respect to two different bases (the v’s and the V’s) are similar:,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
I think an example is the best way to explain B = M−1AM. Suppose T is projection,eigenvec_val
onto the line L at angle θ. This linear transformation is completely described without the,eigenvec_val
"help of a basis. But to represent T by a matrix, we do need a basis. Figure 5.5 offers two",eigenvec_val
"choices, the standard basis v1 = (1,0), v2 = (0,1) and a basis V1, V2 chosen especially",eigenvec_val
Figure 5.5: Change of basis to make the projection matrix diagonal.,eigenvec_val
In fact TV1 = V1 (since V1 is already on the line L) and TV2 = 0 (since V2 is perpen-,eigenvec_val
"dicular to the line). In that eigenvector basis, the matrix is diagonal:",eigenvec_val
B = [T]V to V =,eigenvec_val
The other thing is the change of basis matrix M. For that we express V1 as a combination,eigenvec_val
"v1cosθ⊥v2sinθ and put those coefﬁcients into column 1. Similarly V2 (or IV2, the",eigenvec_val
"transformation is the identity) is −v1sinθ +v2cosθ, producing column 2:",eigenvec_val
M = [I]V to v =,eigenvec_val
The inverse matrix M−1 (which is here the transpose) goes from v to V. Combined with,eigenvec_val
"B and M, it gives the projection matrix in the standard basis of v’s:",eigenvec_val
A = MBM−1 =,eigenvec_val
We can summarize the main point. The way to simplify that matrix A—in fact to diag-,eigenvec_val
onalize it—is to ﬁnd its eigenvectors. They go into the columns of M (or S) and M−1AM,eigenvec_val
is diagonal. The algebraist says the same thing in the language of linear transformations:,eigenvec_val
"Choose a basis consisting of eigenvectors. The standard basis led to A, which was not",eigenvec_val
"simple. The right basis led to B, which was diagonal.",eigenvec_val
We emphasize again that M−1AM does not arise in solving Ax = b. There the basic,eigenvec_val
operation was to multiply A (on the left side only!) by a matrix that subtracts a multiple,eigenvec_val
of one row from another. Such a transformation preserved the nullspace and row space,eigenvec_val
of A; it normally changes the eigenvalues.,eigenvec_val
Eigenvalues are actually calculated by a sequence of simple similarities. The matrix,eigenvec_val
"goes gradually toward a triangular form, and the eigenvalues gradually appear on the",eigenvec_val
main diagonal. (Such a sequence is described in Chapter 7.) This is much better than,eigenvec_val
"trying to compute det(A−λI), whose roots should be the eigenvalues. For a large matrix,",eigenvec_val
it is numerically impossible to concentrate all that information into the polynomial and,eigenvec_val
then get it out again.,eigenvec_val
Triangular Forms with a Unitary M,eigenvec_val
Our ﬁrst move beyond the eigenvector matrix M = S is a little bit crazy: Instead of a,eigenvec_val
"more general M, we go the other way and restrict M to be unitary. M−1AM can achieve",eigenvec_val
a triangular form T under this restriction. The columns of M = U are orthonormal (in,eigenvec_val
"the real case, we would write M = Q). Unless the eigenvectors of Λ are orthogonal, a",eigenvec_val
diagonal U−1AU is impossible. But “Schur’s lemma” in 5R is very useful—at least to,eigenvec_val
the theory. (The rest of this chapter is devoted more to theory than to applications. The,eigenvec_val
Jordan form is independent of this triangular form.),eigenvec_val
There is a unitary matrix M = U such that U−1AU = T is triangular.,eigenvec_val
The eigenvalues of A appear along the diagonal of this similar matrix T.,eigenvec_val
"Proof. Every matrix, say 4 by 4, has at least one eigenvalue λ1. In the worst case, it",eigenvec_val
"could be repeated four times. Therefore A has at least one unit eigenvector x1, which we",eigenvec_val
place in the ﬁrst column of U. At this stage the other three columns are impossible to,eigenvec_val
"determine, so we complete the matrix in any way that leaves it unitary, and call it U1.",eigenvec_val
(The Gram-Schmidt process guarantees that this can be done.) Ax1 = λ1x1 column 1,eigenvec_val
means that the product U−1,eigenvec_val
1 AU1 starts in the right form:,eigenvec_val
Now work with the 3 by 3 submatrix in the lower right-hand corner. It has a unit,eigenvec_val
"eigenvector x2, which becomes the ﬁrst column of a unitary matrix M2:",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"At the last step, an eigenvector of the 2 by 2 matrix in the lower right-hand corner goes",eigenvec_val
"into a unitary M3, which is put into the corner of U3:",eigenvec_val
"The product U = U1U2U3 is still a unitary matrix, and U−1AU = T.",eigenvec_val
"This lemma applies to all matrices, with no assumption that A is diagoalizable. We",eigenvec_val
"could use it to prove that the powers Ak approach zero when all |λi| < 1, and the expo-",eigenvec_val
nentials eAt approach zero when all Reλi < 0—even without the full set of eigenvectors,eigenvec_val
which was assumed in Sections 5.3 and 5.4.,eigenvec_val
Example 2. A =,eigenvec_val
has the eigenvalue λ = 1 (twice).,eigenvec_val
"The only line of eigenvectors goes through (1,1). After dividing by",eigenvec_val
"2, this is the ﬁrst",eigenvec_val
"column of U, and the triangular U−1AU = T has the eigenvalues on its diagonal:",eigenvec_val
Diagonalizing Symmetric and Hermitian Matrices,eigenvec_val
This triangular form will show that any symmetric or Hermitian matrix—whether its,eigenvec_val
eigenvalues are distinct or not—has a complete set of orthonormal eigenvectors. We,eigenvec_val
need a unitary matrix such that U−1AU is diagonal. Schur’s lemma has just found it.,eigenvec_val
"This triangular T must be diagonal, because it is also Hermitian when A = AH:",eigenvec_val
T = T H,eigenvec_val
(U−1AU)H = UHAH(U−1)H = U−1AU.,eigenvec_val
The diagonal matrix U−1AU represents a key theorem in linear algebra.,eigenvec_val
(Spectral Theorem) Every real symmetric A can be diagonalized by an,eigenvec_val
orthogonal matrix Q. Every Hermitian matrix can be diagonalized by a unitary,eigenvec_val
The columns of Q (or U) contain orthonormal eigenvectors of A.,eigenvec_val
"Remark 1. In the real symmetric case, the eigenvalues and eigenvectors are real at every",eigenvec_val
step. That produces a real unitary U—an orthogonal matrix.,eigenvec_val
Remark 2. A is the limit of symmetric matrices with distinct eigenvalues. As the limit,eigenvec_val
"approaches, the eigenvectors stay perpendicular. This can fail if A ̸= AT:",eigenvec_val
"As θ → 0, the only eigenvector of the nondiagonalizable matrix",eigenvec_val
Example 3. The spectral theorem says that this A = AT can be diagonalized:,eigenvec_val
λ1 = λ2 = 1 and λ3 = −1.,eigenvec_val
"λ = 1 has a plane of eigenvectors, and we pick an orthonormal pair x1 and x2:",eigenvec_val
for λ3 = −1.,eigenvec_val
These are the columns of Q. Splitting A = QΛQT into 3 columns times 3 rows gives,eigenvec_val
"Since λ1 = λ2, those ﬁrst two projections x1xT",eigenvec_val
2 (each of rank 1) combine to give,eigenvec_val
a projection P1 of rank 2 (onto the plane of eigenvectors). Then A is,eigenvec_val
�� = λ1P1 +λ3P3 = (+1),eigenvec_val
Every Hermitian matrix with k different eigenvalues has a spectral decomposition into,eigenvec_val
"A = λ1P1+···+λkPk, where Pi is the projection onto the eigenspace for λi. Since there is",eigenvec_val
"a full set of eigenvectors, the projections add up to the identity. And since the eigenspace",eigenvec_val
"are orthogonal, two projections produce zero: PjPi = 0.",eigenvec_val
"We are very close to answering an important question, so we keep going: For which",eigenvec_val
"matrices is T = Λ? Symmetric, skew-symmetric, and orthogonal T’s are all diagonal!",eigenvec_val
"Hermitian, skew-Hermitian, and unitary matrices are also in this class. They correspond",eigenvec_val
"to numbers on the real axis, the imaginary axis, and the unit circle. Now we want the",eigenvec_val
"whole class, corresponding to all complex numbers. The matrices are called “normal”.",eigenvec_val
The matrix N is normal if it commutes with NH: NNH = NHN. For,eigenvec_val
"such matrices, and no others, the triangular T = U−1NU is the diagonal Λ.",eigenvec_val
Normal matrices are exactly those that have a complete set of orthonormal,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"Symmetric and Hermitian matrices are certainly normal: If A = AH, then AAH and",eigenvec_val
AHA both equal A2. Orthogonal and unitary matrices are also normal: UUH and UHU,eigenvec_val
both equal I. Two steps will work for any normal matrix:,eigenvec_val
"1. If N is normal, then so is the triangular T = U−1NU:",eigenvec_val
TT H = U−1NUUHNHU = U−1NNHU = U−1NHNU = UHNHUU−1NU = T HT.,eigenvec_val
2. A triangular T that is normal must be diagonal! (See Problems 19–20 at the end of,eigenvec_val
"Thus, if N is normal, the triangular T =U−1NU must be diagonal. Since T has the same",eigenvec_val
"eigenvalues as N, it must be Λ. The eigenvectors of N are the columns of U, and they",eigenvec_val
are orthonormal. That is the good case. We turn now from the best possible matrices,eigenvec_val
(normal) to the worst possible (defective).,eigenvec_val
This section has done its best while requiring M to be a unitary matrix U. We got,eigenvec_val
M−1AM into a triangular form T. Now we lift this restriction on M. Any matrix is,eigenvec_val
"allowed, and the goal is to make M−1AM as nearly diagonal as possible.",eigenvec_val
The result of this supreme effort at diagonalization is the Jordan form J. If A has a full,eigenvec_val
"set of eigenvectors, we take M = S and arrive at J = S−1AS = Λ. Then the Jordan form",eigenvec_val
coincides with the diagonal Λ. This is impossible for a defective (nondiagonalizable),eigenvec_val
"matrix. For every missing eigenvector, the Jordan form will have a 1 just above its main",eigenvec_val
diagonal. The eigenvalues appear on the diagonal because J is triangular. And distinct,eigenvec_val
eigenvalues can always be decoupled.,eigenvec_val
It is only a repeated λ that may (or may not!) require an off-diagonal 1 in J.,eigenvec_val
"5U If A has s independent eigenvectors, it is similar to a matrix with s blocks:",eigenvec_val
J = M−1AM =,eigenvec_val
Each Jordan block Ji is a triangular matrix that has only a single eigenvalue λi,eigenvec_val
and only one eigenvector:,eigenvec_val
"The same λi will appear in several blocks, if it has several independent eigen-",eigenvec_val
vectors. Two matrices are similar if and only if they share the same Jordan,eigenvec_val
Many authors have made this theorem the climax of their linear algebra course.,eigenvec_val
"Frankly, I think that is a mistake. It is certainly true that not all matrices are diagonaliz-",eigenvec_val
"able, and the Jordan form is the most general case. For that very reason, its construction",eigenvec_val
is both technical and extremely unstable. (A slight change in A can put back all the,eigenvec_val
"missing eigenvectors, and remove the off-diagonal is.) Therefore the right place for the",eigenvec_val
"details is in the appendix, and the best way to start on the Jordan form is to look at some",eigenvec_val
speciﬁc and manageable examples.,eigenvec_val
Example 4. T =,eigenvec_val
all lead to J =,eigenvec_val
These four matrices have eigenvalues 1 and 1 with only one eigenvector—so J con-,eigenvec_val
sists of one block. We now check that. The determinants all equal 1. The traces (the,eigenvec_val
sums down the main diagonal) are 2. The eigenvalues satisfy 1·1 = 1 and 1+1 = 2. For,eigenvec_val
"T, B, and J, which are triangular, the eigenvalues are on the diagonal. We want to show",eigenvec_val
that these matrices are similar—they all belong to the same family.,eigenvec_val
"(T) From T to J, the job is to change 2 to 1. and a diagonal M will do it:",eigenvec_val
"(B) From B to J, the job is to transpose the matrix. A permutation does that:",eigenvec_val
"(A) From A to J, we go ﬁrst to T as in equation (4). Then change 2 to 1:",eigenvec_val
Example 5. A =,eigenvec_val
"Zero is a triple eigenvalue for A and B, so it will appear in all their Jordan blocks. There",eigenvec_val
"can be a single 3 by 3 block, or a 2 by 2 and a 1 by I block, or three I by I blocks. Then",eigenvec_val
A and B have three possible Jordan forms:,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"The only eigenvector of A is (1,0,0). Its Jordan form has only one block, and A must",eigenvec_val
"be similar to J1. The matrix B has the additional eigenvector (0,1,0), and its Jordan",eigenvec_val
"form is J2 with two blocks, As for J3 = zero matrix, it is in a family by itself; the only",eigenvec_val
matrix similar to J3 is M−10M = 0. A count of the eigenvectors will determine J when,eigenvec_val
there is nothing more complicated than a triple eigenvalue.,eigenvec_val
Example 6. Application to difference and differential equations (powers and expo-,eigenvec_val
"nentials). If A can be diagonalized, the powers of A = SΛS−1 are easy: Ak = SΛkS−1. In",eigenvec_val
"every case we have Jordan’s similarity A = MJM−1, so now we need the powers of J:",eigenvec_val
Ak = (MJM−1)(MJM−1)···(MJM−1) = MJkM−1.,eigenvec_val
"J is block-diagonal, and the powers of each block can be taken separately:",eigenvec_val
This block Ji will enter when λ is a triple eigenvalue with a single eigenvector. Its,eigenvec_val
exponential is in the solution to the corresponding differential equation:,eigenvec_val
Here I +Jit +(Jit)2/2!+··· produces 1+λt +λ 2t2/2!+··· = eλt on the diagonal.,eigenvec_val
The third column of this exponential comes directly from solving du/dt = Jiu:,eigenvec_val
This can be solved by back-substitution (since Ji is triangular). The last equation du3/dt =,eigenvec_val
"λu3 yields u3 = eλt. The equation for u2 is du2/dt = λu2 +u3, and its solution is teλt.",eigenvec_val
"The top equation is du1/dt = λu1 + u2, and its solution is 1",eigenvec_val
2t2eλt. When λ has multi-,eigenvec_val
"plicity m with only one eigenvector, the extra factor t appears m−1 times.",eigenvec_val
These powers and exponentials of J are a part of the solutions uk and u(t). The other,eigenvec_val
part is the M that connects the original A to the more convenient matrix J:,eigenvec_val
if uk+1 = Auk,eigenvec_val
then uk = Aku0 = MJkM−1u0,eigenvec_val
if du/dt = Au then u(t) = eAtu(0) = MeJtM−1u(0).,eigenvec_val
When M and J are S and Λ (the diagonalizable case) those are the formulas of Sections,eigenvec_val
"5.3 and 5.4. Appendix B returns to the nondiagonalizable case, and shows how the",eigenvec_val
Jordan form can be reached. I hope the following table will be a convenient summary.,eigenvec_val
1. A is diagonalizable: The columns of S are eigenvectors and S−1AS = Λ.,eigenvec_val
"2. A is arbitrary: The columns of M include “generalized eigenvectors” of A, and the",eigenvec_val
Jordan form M−1AM = J is block diagonal.,eigenvec_val
3. A is arbitrary: The unitary U can be chosen so that U−1AU = T is triangular.,eigenvec_val
"4. A is normal, AAH = AHA: then U can be chosen so that U−1AU = Λ.",eigenvec_val
"Special cases of normal matrices, all with orthonormal eigenvectors:",eigenvec_val
"(a) If A = AH is Hermitian, then all λi are real.",eigenvec_val
"(b) If A = AT is real symmetric, then Λ is real and U = Q is orthogonal.",eigenvec_val
"(c) If A = −AH is skew-Hermitian, then all λi are purely imaginary.",eigenvec_val
"(d) If A is orthogonal or unitary, then all |λi| = 1 are on the unit circle.",eigenvec_val
"1. If B is similar to A and C is similar to B, show that C is similar to A. (Let B = M−1AM",eigenvec_val
and C = N−1BN.) Which matrices are similar to I?,eigenvec_val
2. Describe in words all matrices that are similar to,eigenvec_val
", and ﬁnd two of them.",eigenvec_val
3. Explain why A is never similar to A+I.,eigenvec_val
"4. Find a diagonal M, made up of 1s and −1s, to show that",eigenvec_val
5. Show (if B is invertible) that BA is similar to AB.,eigenvec_val
"6. (a) If CD = −DC (and D is invertible), show that C is similar to −C.",eigenvec_val
(b) Deduce that the eigenvalues of C must come in plus-minus pairs.,eigenvec_val
"(c) Show directly that if Cx = λx, then C(Dx) = −λ(Dx).",eigenvec_val
7. Consider any A and a “Givens rotation” M in the 1–2 plane:,eigenvec_val
"Choose the rotation angle θ to produce zero in the (3,1) entry of M−1AM.",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"Note. This “zeroing” is not so easy to continue, because the rotations that produce",eigenvec_val
zero in place of d and h will spoil the new zero in the corner. We have to leave one,eigenvec_val
"diagonal below the main one, and ﬁnish the eigenvalue calculation in a different way.",eigenvec_val
"Otherwise, if we could make A diagonal and see its eigenvalues, we would be ﬁnding",eigenvec_val
the roots of the polynomial det(A−λI) by using only the square roots that determine,eigenvec_val
cosθ—and that is impossible.,eigenvec_val
"8. What matrix M changes the basis V1 = (1,1), V2 = (1,4) to the basis v1 = (2,5),",eigenvec_val
"v2 = (1,4)? The columns of M come from expressing V1 and V2 as combinations",eigenvec_val
∑mijvi of the v’s.,eigenvec_val
"9. For the same two bases, express the vector (3,9) as a combination c1V1 + c2V2 and",eigenvec_val
also as d1v1 +d2v2. Check numerically that M connects c to d: Mc = d.,eigenvec_val
"10. Conﬁrm the last exercise: If V1 = m11v1 + m21v2 and V2 = m12v1 + m22v2, and",eigenvec_val
"m11c1 +m12c2 = d1 and m21c1 +m22c2 = d2, the vectors c1V1 +c2V2 and d1v1 +d2v2",eigenvec_val
are the same. This is the “change of basis formula” Mc = d.,eigenvec_val
"11. If the transformation T is a reﬂection across the 45° line in the plane, ﬁnd its matrix",eigenvec_val
"with respect to the standard basis v1 = (1,0), v2 = (0,1), and also with respect to",eigenvec_val
"V1 = (1,1), V2 = (1,−1). Show that those matrices are similar.",eigenvec_val
12. The identity transformation takes every vector to itself: Tx = x. Find the corre-,eigenvec_val
"sponding matrix, if the ﬁrst basis is v1 = (1,2), v2 = (3,4) and the second basis is",eigenvec_val
"w1 = (1,0), w2 = (0,1). (It is not the identity matrix!)",eigenvec_val
13. The derivative of a+bx+cx2 is b+2cx+0x2.,eigenvec_val
(a) Write the 3 by 3 matrix D such that,eigenvec_val
(b) Compute D3 and interpret the results in terms of derivatives.,eigenvec_val
(c) What are the eigenvalues and eigenvectors of D?,eigenvec_val
"14. Show that every number is an eigenvalue for T f(x) = d f/dx, but the transformation",eigenvec_val
0 f(t)dt has no eigenvalues (here −∞ < x < ∞).,eigenvec_val
"15. On the space of 2 by 2 matrices, let T be the transformation that transposes every",eigenvec_val
matrix. Find the eigenvalues and “eigenmatrices” for AT = λA.,eigenvec_val
16. (a) Find an orthogonal Q so that Q−1AQ = Λ if,eigenvec_val
"Then ﬁnd a second pair of orthonormal eigenvectors x1, x2 for λ = 0.",eigenvec_val
(b) Verify that P = x1xT,eigenvec_val
2 is the same for both pairs.,eigenvec_val
"17. Prove that every unitary matrix A is diagonalizable, in two steps:",eigenvec_val
"(i) If A is unitary, and U is too, then so is T = U−1AU.",eigenvec_val
(ii) An upper triangular T that is unitary must be diagonal. Thus T = Λ.,eigenvec_val
Any unitary matrix A (distinct eigenvalues or not) has a complete set of orthonormal,eigenvec_val
eigenvectors. All eigenvalues satisfy |λ| = 1.,eigenvec_val
"18. Find a normal matrix (NNH = NHN) that is not Hermitian, skew-Hermitian, unitary,",eigenvec_val
or diagonal. Show that all permutation matrices are normal.,eigenvec_val
"19. Suppose T is a 3 by 3 upper triangular matrix, with entries tij. Compare the entries of",eigenvec_val
"TT H and T HT, and show that if they are equal, then T must be diagonal. All normal",eigenvec_val
triangular matrices are diagonal.,eigenvec_val
"20. If N is normal, show that ∥Nx∥ = ∥NHx∥ for every vector x. Deduce that the ith row",eigenvec_val
"of N has the same length as the ith column. Note: If N is also upper triangular, this",eigenvec_val
leads again to the conclusion that it must be diagonal.,eigenvec_val
"21. Prove that a matrix with orthonormal eigenvectors must be normal, as claimed in 5T:",eigenvec_val
"If U−1NU = A, or N = UΛUH, then NNH = NHN.",eigenvec_val
"22. Find a unitary U and triangular T so that U−1AU = T, for",eigenvec_val
"23. If A has eigenvalues 0, 1, 2, what are the eigenvalues of A(A−I)(A−2I)?",eigenvec_val
"24. (a) Show by direct multiplication that every triangular matrix T, say 3 by 3, satisﬁes",eigenvec_val
its own characteristic equation: (T −λ1I)(T −λ2I)(T −λ3I) = 0.,eigenvec_val
"(b) SubstitutingU−1AU for T, deduce the famous Cayley-Hamilton theorem: Every",eigenvec_val
matrix satisﬁes its own characteristic equation. For 3 by 3 this is (A−λ1I)(A−,eigenvec_val
25. The characteristic polynomial of A =,eigenvec_val
is λ 2 −(a+d)λ +(ad −bc). By direct,eigenvec_val
"substitution, verify Cayley-Hamilton: A2 −(a+d)A+(ad −bc)I = 0.",eigenvec_val
"26. If aij = 1 above the main diagonal and aij = 0 elsewhere, ﬁnd the Jordan form (say",eigenvec_val
4 by 4) by ﬁnding all the eigenvectors.,eigenvec_val
"27. Show, by trying for an M and failing, that no two of the three Jordan forms in equa-",eigenvec_val
"tion (8) are similar: J1 ̸= M−1J2M, J1 ̸= M−1J3M, and J2 ̸= M−1J3M.",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
"28. Solve u′ = Ju by back-substitution, solving ﬁrst for u2(t):",eigenvec_val
dt = Ju =,eigenvec_val
Notice te5t in the ﬁrst component u1(t).,eigenvec_val
29. Compute A10 and eA if A = MJM−1:,eigenvec_val
30. Show that A and B are similar by ﬁnding M so that B = M−1AM:,eigenvec_val
31. Which of these matrices A1 to A6 are similar? Check their eigenvalues.,eigenvec_val
32. There are sixteen 2 by 2 matrices whose entries are 0s and 1s. Similar matrices go,eigenvec_val
into the same family. How many families? How many matrices (total 16) in each,eigenvec_val
"33. (a) If x is in the nullspace of A, show that M−1x is in the nullspace of M−1AM.",eigenvec_val
(b) The nullspaces of A and M−1AM have the same (vectors)(basis)(dimension).,eigenvec_val
"34. If A and B have the exactly the same eigenvalues and eigenvectors, does A = B? With",eigenvec_val
"n independent eigenvectors, we do have A = B. Find A ̸= B when λ = 0,0 (repeated),",eigenvec_val
"but there is only one line of eigenvectors (x1,0).",eigenvec_val
Problems 35–39 are about the Jordan form.,eigenvec_val
"35. By direct multiplication, ﬁnd J2 and J3 when",eigenvec_val
Guess the form of Jk. Set k = 0 to ﬁnd J0. Set k = −1 to ﬁnd J−1.,eigenvec_val
"36. If J is the 5 by 5 Jordan block with λ = 0, ﬁnd J2 and count its eigenvectors, and",eigenvec_val
ﬁnd its Jordan form (two blocks).,eigenvec_val
37. The text solved du/dt = Ju for a 3 by 3 Jordan block J. Add a fourth equation,eigenvec_val
"dw/dt = 5w+x. Follow the pattern of solutions for z, y, x to ﬁnd w.",eigenvec_val
"38. These Jordan matrices have eigenvalues 0, 0, 0, 0. They have two eigenvectors (ﬁnd",eigenvec_val
them). But the block sizes don’t match and J is not similar to K:,eigenvec_val
0 1 0 0,eigenvec_val
0 0 0 0,eigenvec_val
0 0 0 1,eigenvec_val
0 0 0 0,eigenvec_val
0 1 0 0,eigenvec_val
0 0 1 0,eigenvec_val
0 0 0 0,eigenvec_val
0 0 0 0,eigenvec_val
"For any matrix M, compare JM with MK. If they are equal, show that M is not",eigenvec_val
invertible. Then M−1JM = K is impossible.,eigenvec_val
39. Prove in three steps that AT is always similar to A (we know that the λ’s are the,eigenvec_val
"same, the eigenvectors are the problem):",eigenvec_val
"(a) For A = one block, ﬁnd Mi = permutation so that M−1",eigenvec_val
"(b) For A = any J, build M0 from blocks so that M−1",eigenvec_val
0 JM0 = JT.,eigenvec_val
(c) For any A = MJM−1: Show that AT is similar to JT and so to J and to A.,eigenvec_val
"40. Which pairs are similar? Choose a, b, c, d to prove that the other pairs aren’t:",eigenvec_val
"41. True or false, with a good reason:",eigenvec_val
(a) An invertible matrix can’t be similar to a singular matrix.,eigenvec_val
(b) A symmetric matrix can’t be similar to a nonsymmetric matrix.,eigenvec_val
(c) A can’t be similar to −A unless A = 0.,eigenvec_val
(d) A−I can’t be similar to A+I.,eigenvec_val
42. Prove that AB has the same eigenvalues as BA.,eigenvec_val
"43. If A is 6 by 4 and B is 4 by 6, AB and BA have different sizes. Nevertheless,",eigenvec_val
(a) What sizes are the blocks of G? They are the same in each matrix.,eigenvec_val
"(b) This equation is M−1FM = G, so F and G have the same 10 eigenvalues. F has",eigenvec_val
the eigenvalues of AB plus 4 zeros; G has the eigenvalues of BA plus 6 zeros. AB,eigenvec_val
has the same eigenvalues as BA plus,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
44. Why is each of these statements true?,eigenvec_val
"(a) If A is similar to B, then A2 is similar to B2.",eigenvec_val
"(b) A2 and B2 can be similar when A and B are not similar (try λ = 0,0).",eigenvec_val
is not similar to,eigenvec_val
"(e) If we exchange rows 1 and 2 of A, and then exchange columns 1 and 2, the",eigenvec_val
eigenvalues stay the same.,eigenvec_val
Properties of Eigenvalues and Eigenvectors,eigenvec_val
How are the properties of a matrix reﬂected in its eigenvalues and eigenvectors? This,eigenvec_val
question is fundamental throughout Chapter 5. A table that organizes the key facts may,eigenvec_val
"be helpful. For each class of matrices, here are the special properties of the eigenvalues",eigenvec_val
λi and eigenvectors xi.,eigenvec_val
Symmetric: AT = A,eigenvec_val
i x j = 0,eigenvec_val
Orthogonal: QT = Q−1,eigenvec_val
all |λ| = 1,eigenvec_val
i x j = 0,eigenvec_val
Skew-symmetric: AT = −A,eigenvec_val
i x j = 0,eigenvec_val
Complex Hermitian: AT = A,eigenvec_val
i x j = 0,eigenvec_val
Positive deﬁnite: xTAx > 0,eigenvec_val
all λ > 0,eigenvec_val
Similar matrix: B = M−1AM,eigenvec_val
Projection: P = P2 = PT,eigenvec_val
Stable powers: An → 0,eigenvec_val
all |λ| < 1,eigenvec_val
Stable exponential: eAt → 0,eigenvec_val
all Reλ < 0,eigenvec_val
"Markov: mij > 0, ∑n",eigenvec_val
steady state x > 0,eigenvec_val
Cyclic permutation: Pn = I,eigenvec_val
"xk = (1,λk,...,λ n−1",eigenvec_val
columns of S are independent,eigenvec_val
diagonal of Λ (real),eigenvec_val
columns of Q are orthonormal,eigenvec_val
Jordan: J = M−1AM,eigenvec_val
each block gives 1 eigenvector,eigenvec_val
Every matrix: A = UΣV T,eigenvec_val
"eigenvectors of ATA, AAT in V, U",eigenvec_val
"5.1 Find the eigenvalues and eigenvectors, and the diagonalizing matrix S, for",eigenvec_val
5.2 Find the determinants of A and A−1 if,eigenvec_val
"5.3 If A has eigenvalues 0 and 1, corresponding to the eigenvectors",eigenvec_val
how can you tell in advance that A is symmetric? What are its trace and determi-,eigenvec_val
nant? What is A?,eigenvec_val
"5.4 In the previous problem, what will be the eigenvalues and eigenvectors of A2? What",eigenvec_val
is the relation of A2 to A?,eigenvec_val
5.5 Does there exist a matrix A such that the entire family A + cI is invertible for all,eigenvec_val
complex numbers c? Find a real matrix with A+rI invertible for all real r.,eigenvec_val
5.6 Solve for both initial values and then ﬁnd eAt:,eigenvec_val
"5.7 Would you prefer to have interest compounded quarterly at 40% per year, or annu-",eigenvec_val
5.8 True or false (with counterexample if false):,eigenvec_val
"(a) If B is formed from A by exchanging two rows, then B is similar to A.",eigenvec_val
"(b) If a triangular matrix is similar to a diagonal matrix, it is already diagonal.",eigenvec_val
"(c) Any two of these statements imply the third: A is Hermitian, A is unitary, A2 = I.",eigenvec_val
"(d) If A and B are diagonalizable, so is AB.",eigenvec_val
"5.9 What happens to the Fibonacci sequence if we go backward in time, and how is F−k",eigenvec_val
"related to Fk? The law Fk+2 = Fk+1 +Fk is still in force, so F−1 = 1.",eigenvec_val
5.10 Find the general solution to du/dt = Au if,eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
Can you ﬁnd a time T at which the solution u(T) is guaranteed to return to the,eigenvec_val
"5.11 If P is the matrix that projects Rn onto a subspace S, explain why every vector in",eigenvec_val
"S is an eigenvector, and so is every vector in S⊥. What are the eigenvai (Note the",eigenvec_val
"connection to P2 = P, which means that λ 2 = λ.)",eigenvec_val
5.12 Show that every matrix of order > 1 is the sum of two singular matrices.,eigenvec_val
5.13 (a) Show that the matrix differential equation dX/dt = AX + XB has the solution,eigenvec_val
(b) Prove that the solutions of dX/dt = AX −XA keep the same eigenvalues for all,eigenvec_val
"5.14 If the eigenvalues of A are 1 and 3 with eigenvectors (5,2) and (2,1), ﬁnd the",eigenvec_val
"solutions to du/dt = Au and uk+1 = Auk, starting from u = (9,4).",eigenvec_val
5.15 Find the eigenvalues and eigenvectors of,eigenvec_val
"What property do you expect for the eigenvectors, and is it true?",eigenvec_val
5.16 By trying to solve,eigenvec_val
show that A has no square root. Change the diagonal entries of A to 4 and ﬁnd a,eigenvec_val
5.17 (a) Find the eigenvalues and eigenvectors of A =,eigenvec_val
"(b) Solve du/dt = Au starting from u(0) = (100,100).",eigenvec_val
"(c) If v(t) = income to stockbrokers and w(t) = income to client, and they help",eigenvec_val
each other by dv/dt = 4w and dw/dt = 1,eigenvec_val
"4v, what does the ratio v/w approach",eigenvec_val
as t → ∞?,eigenvec_val
"5.18 True or false, with reason if true and counterexample if false:",eigenvec_val
"(a) For every matrix A, there is a solution to du/dt = Au starting from u(0) =",eigenvec_val
(b) Every invertible matrix can be diagonalized.,eigenvec_val
(c) Every diagonalizable matrix can be inverted.,eigenvec_val
(d) Exchanging the rows of a 2 by 2 matrix reverses the signs of its eigenvalues.,eigenvec_val
"(e) If eigenvectors x and y correspond to distinct eigenvalues, then xHy = 0.",eigenvec_val
"5.19 If K is a skew-symmetric matrix, show that Q = (I −K)(I +K)−1 is an orthogonal",eigenvec_val
matrix. Find Q if K =,eigenvec_val
"5.20 If KH = −K (skew-Hermitian), the eigenvalues are imaginary and the eigenvectors",eigenvec_val
(a) How do you know that K −I is invertible?,eigenvec_val
(b) How do you know that K = UΛUH for a unitary U?,eigenvec_val
(c) Why is eΛt unitary?,eigenvec_val
(d) Why is eKt unitary?,eigenvec_val
"5.21 If M is the diagonal matrix with entries d, d2, d3, what is M−1AM? What are its",eigenvec_val
eigenvalues in the following case?,eigenvec_val
"5.22 If A2 = −I, what are the eigenvalues of A? If A is a real n by n matrix show that n",eigenvec_val
"must be even, and give an example.",eigenvec_val
"5.23 If Ax = λ1x and ATy = λ2y (all real), show that xTy = 0.",eigenvec_val
5.24 A variation on the Fourier matrix is the “sine matrix”:,eigenvec_val
"Verify that ST = S−1. (The columns are the eigenvectors of the tridiagonal −1, 2,",eigenvec_val
5.25 (a) Find a nonzero matrix N such that N3 = 0.,eigenvec_val
"(b) If Nx = λx, show that λ must be zero.",eigenvec_val
(c) Prove that N (called a “nilpotent” matrix) cannot be symmetric.,eigenvec_val
5.26 (a) Find the matrix P = aaT/aTa that projects any vector onto the line through,eigenvec_val
"(b) What is the only nonzero eigenvalue of P, and what is the corresponding eigen-",eigenvec_val
"(c) Solve uk+1 = Puk, starting from u0 = (9,9,0).",eigenvec_val
"5.27 Suppose the ﬁrst row of A is 7, 6 and its eigenvalues are i, −i. Find A.",eigenvec_val
Chapter 5 Eigenvalues and Eigenvectors,eigenvec_val
5.28 (a) For which numbers c and d does A have real eigenvalues and orthogonal eigen-,eigenvec_val
(b) For which c and d can we ﬁnd three orthonormal vectors that are combinations,eigenvec_val
of the columns (don’t do it!)?,eigenvec_val
"5.29 If the vectors x1 and x2 are in the columns of S, what are the eigenvalues and eigen-",eigenvec_val
5.30 What is the limit as k → ∞ (the Markov steady state) of,eigenvec_val
"Minima, Maxima, and Saddle Points",pos_def_matrices
"Up to now, we have hardly thought about the signs of the eigenvalues. We couldn’t",pos_def_matrices
ask whether λ was positive before it was known to be real. Chapter 5 established that,pos_def_matrices
every symmetric matrix has real eigenvalues. Now we will ﬁnd a test that can be applied,pos_def_matrices
"directly to A, without computing its eigenvalues, which will guarantee that all those",pos_def_matrices
eigenvalues are positive. The test brings together three of the most basic ideas in the,pos_def_matrices
"book—pivots, determinants, and eigenvalues.",pos_def_matrices
"The signs of the eigenvalues are often crucial. For stability in differential equations,",pos_def_matrices
we needed negative eigenvalues so that eλt would decay. The new and highly important,pos_def_matrices
problem is to recognize a minimum point. This arises throughout science and engi-,pos_def_matrices
neering and every problem of optimization. The mathematical problem is to move the,pos_def_matrices
second derivative test F′′ > 0 into n dimensions. Here are two examples:,pos_def_matrices
"F(x,y) = 7+2(x+y)2 −ysiny−x3",pos_def_matrices
"f(x,y) = 2x2 +4xy+y2.",pos_def_matrices
"Does either F(x,y) or f(x,y) have a minimum at the point x = y = 0?",pos_def_matrices
"Remark 3. The zero-order terms F(0,0) = 7 and f(0,0) = 0 have no effect on the an-",pos_def_matrices
swer. They simply raise or lower the graphs of F and f.,pos_def_matrices
Remark 4. The linear terms give a necessary condition: To have any chance of a mini-,pos_def_matrices
"mum, the ﬁrst derivatives must vanish at x = y = 0:",pos_def_matrices
∂x = 4(x+y)−3x2 = 0,pos_def_matrices
∂y = 4(x+y)−ycosy−siny = 0,pos_def_matrices
∂x = 4x+4y = 0,pos_def_matrices
∂y = 4x+2y = 0.,pos_def_matrices
"Thus (x,y) = (0,0) is a stationary point for both functions. The surface z = F(x,y) is",pos_def_matrices
"tangent to the horizontal plane z = 7, and the surface z = f(x,y) is tangent to the plane",pos_def_matrices
"z = 0. The question is whether the graphs go above those planes or not, as we move",pos_def_matrices
away from the tangency point x = y = 0.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"Remark 5. The second derivatives at (0,0) are decisive:",pos_def_matrices
∂x2 = 4−6x = 4,pos_def_matrices
∂x∂y = ∂ 2F,pos_def_matrices
∂y2 = 4+ysiny−2cosy = 2,pos_def_matrices
∂x∂y = ∂ 2 f,pos_def_matrices
"These second derivatives 4, 4, 2 contain the answer. Since they are the same for F and",pos_def_matrices
"f, they must contain the same answer. The two functions behave in exactly the same",pos_def_matrices
way near the origin. F has a minimum if and only if f has a minimum. I am going to,pos_def_matrices
show that those functions don’t!,pos_def_matrices
Remark 6. The higher-degree terms in F have no effect on the question of a local min-,pos_def_matrices
"imum, but they can prevent it from being a global minimum. In our example the term",pos_def_matrices
"−x3 must sooner or later pull F toward −∞. For f(x,y), with no higher terms, all the",pos_def_matrices
"action is at (0,0).",pos_def_matrices
"Every quadratic form f = ax2+2bxy+cy2 has a stationary point at the origin, where",pos_def_matrices
"∂ f/∂x = ∂ f/∂y = 0. A local minimum would also be a global minimum, The surface",pos_def_matrices
"z = f(x,y) will then be shaped like a bowl, resting on the origin (Figure 6.1). If the",pos_def_matrices
"stationary point of F is at x = α, y = β, the only change would be to use the second",pos_def_matrices
"derivatives at α, β:",pos_def_matrices
"∂x2 (α,β)+xy ∂ 2F",pos_def_matrices
"This f(x,y) behaves near (0,0) in the same way that F(x,y) behaves near (α,β).",pos_def_matrices
Figure 6.1: A bowl and a saddle: Deﬁnite A =,pos_def_matrices
and indeﬁnite A =,pos_def_matrices
The third derivatives are drawn into the problem when the second derivatives fail to,pos_def_matrices
give a deﬁnite decision. That happens when the quadratic part is singular. For a true,pos_def_matrices
"minimum, f is allowed to vanish only at x = y = 0. When f(x,y) is strictly positive at",pos_def_matrices
"all other points (the bowl goes up), it is called positive deﬁnite.",pos_def_matrices
"6.1 Minima, Maxima, and Saddle Points",pos_def_matrices
Deﬁnite versus Indeﬁnite: Bowl versus Saddle,pos_def_matrices
"The problem comes down to this: For a function of two variables x and y, what is the",pos_def_matrices
"correct replacement for the condition ∂ 2F/∂x2 > 0? With only one variable, the sign of",pos_def_matrices
the second derivative decides between a minimum or a maximum. Now we have three,pos_def_matrices
"second derivatives: Fxx, Fxy = Fyx, and Fyy. These three numbers (like 4, 4, 2) must",pos_def_matrices
determine whether or not F (as well as f) has a minimum.,pos_def_matrices
"What conditions on a, b, and c ensure that the quadratic f(x,y) = ax2 +2bxy+cy2",pos_def_matrices
is positive deﬁnite? One necessary condition is easy:,pos_def_matrices
"(i) If ax2 +2bxy+cy2 is positive deﬁnite, then necessarily a > 0.",pos_def_matrices
"We look at x = 1, y = 0, where ax2 + 2bxy + cy2 is equal to a. This must be positive.",pos_def_matrices
"Translating back to F, that means that ∂ 2F/∂x2 > 0. The graph must go up in the x",pos_def_matrices
"direction. Similarly, ﬁx x = 0 and look in the y direction where f(0,y) = cy2:",pos_def_matrices
"(ii) If f(x,y) is positive deﬁnite, then necessarily c > 0.",pos_def_matrices
"Do these conditions a > 0 and c > 0 guarantee that f(x,y) is always positive? The",pos_def_matrices
answer is no. A large cross term 2bxy can pull the graph below zero.,pos_def_matrices
"Example 1. f(x,y) = x2 −10xy+y2. Here a = 1 and c = 1 are both positive. But f is",pos_def_matrices
"not positive deﬁnite, because f(1,1) = −8. The conditions a > 0 and c > 0 ensure that",pos_def_matrices
"f(x,y) is positive on the x and y axes. But this function is negative on the line x = y,",pos_def_matrices
because b = −10 overwhelms a and c.,pos_def_matrices
Example 2. In our original f the coefﬁcient 2b = 4 was positive. Does this ensure a,pos_def_matrices
minimum? Again the answer is no; the sign of b is of no importance! Even though its,pos_def_matrices
"second derivatives are positive, 2x2 +4xy+y2 is not positive deﬁnite. Neither F nor f",pos_def_matrices
"has a minimum at (0,0) because f(1,−1) = 2−4+1 = −1.",pos_def_matrices
"It is the size of b, compared to a and c, that must be controlled. We now want a",pos_def_matrices
necessary and sufﬁcient condition for positive deﬁniteness. The simplest technique is to,pos_def_matrices
f = ax2 +2bxy+cy2 = a,pos_def_matrices
"The ﬁrst term on the right is never negative, when the square is multiplied by a > 0.",pos_def_matrices
"But this square can be zero, and the second term must then be positive. That term",pos_def_matrices
has coefﬁcient (ac − b2)/a. The last requirement for positive deﬁniteness is that this,pos_def_matrices
coefﬁcient must be positive:,pos_def_matrices
"(iii) If ax2 +2bxy+cy2 stays positive, then necessarily ac > b2.",pos_def_matrices
Test for a minimum:,pos_def_matrices
The conditions a > 0 and ac > b2 are just right. They guarantee,pos_def_matrices
"c > 0. The right side of (2) is positive, and we have found a minimum:",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
6A ax2 +2bxy+cy2 is positive deﬁnite if and only if a > 0 and ac > b2. Any,pos_def_matrices
"f(x,y) has a minimum at a point where ∂F/∂x = ∂F/∂y = 0 with",pos_def_matrices
Test for a maximum:,pos_def_matrices
"Since f has a maximum whenever −f has a minimum, we just",pos_def_matrices
"reverse the signs of a, b, and c. This actually leaves ac > b2 unchanged: The quadratic",pos_def_matrices
form is negative deﬁnite if and only if a < 0 and ac > b2. The same change applies for,pos_def_matrices
"a maximum of F(x,y).",pos_def_matrices
Singular case ac = b2:,pos_def_matrices
The second term in equation (2) disappears to leave only,pos_def_matrices
"the ﬁrst square—which is either positive semideﬁnite, when a > 0, or negative semidef-",pos_def_matrices
"inite, when a < 0. The preﬁx semi allows the possibility that f can equal zero, as it will",pos_def_matrices
"at the point x = b, y = −a. The surface z = f(x,y) degenerates from a bowl into a valley.",pos_def_matrices
"For f = (x+y)2, the valley runs along the line x+y = 0.",pos_def_matrices
Saddle Point ac < b2:,pos_def_matrices
"In one dimension, F(x) has a minimum or a maximum, or",pos_def_matrices
"F′′ = 0. In two dimensions, a very important possibility still remains: The combination",pos_def_matrices
"ac−b2 may be negative. This occurred in both examples, when b dominated a and c. It",pos_def_matrices
also occurs if a and c have opposite signs. Then two directions give opposite results—in,pos_def_matrices
"one direction f increases, in the other it decreases. It is useful to consider two special",pos_def_matrices
"Saddle points at (0,0)",pos_def_matrices
f2 = x2 −y2,pos_def_matrices
"In the ﬁrst, b = 1 dominates a = c = 0. In the second, a = 1 and c = −1 have opposite",pos_def_matrices
sign. The saddles 2xy and x2 − y2 are practically the same; if we turn one through 45°,pos_def_matrices
we get the other. They are also hard to draw.,pos_def_matrices
"These quadratic forms are indeﬁnite, because they can take either sign. So we have",pos_def_matrices
a stationary point that is neither a maximum or a minimum. It is called a saddle point.,pos_def_matrices
"The surface z = x2 −y2 goes down in the direction of the y axis, where the legs ﬁt (if you",pos_def_matrices
"still ride a horse). In case you switched to a car, think of a road going over a mountain",pos_def_matrices
"pass. The top of the pass is a minimum as you look along the range of mountains, but it",pos_def_matrices
is a maximum as you go along the road.,pos_def_matrices
Higher Dimensions: Linear Algebra,pos_def_matrices
Calculus would be enough to ﬁnd our conditions Fxx > 0 and FxxFyy > F2,pos_def_matrices
xy for a minimum.,pos_def_matrices
"But linear algebra is ready to do more, because the second derivatives ﬁt into a symmetric",pos_def_matrices
matrix A. The terms ax2 and cy2 appear on the diagonal. The cross derivative 2bxy is,pos_def_matrices
"6.1 Minima, Maxima, and Saddle Points",pos_def_matrices
"split between the same entry b above and below. A quadratic f(x,y) comes directly from",pos_def_matrices
a symmetric 2 by 2 matrix!,pos_def_matrices
This identity (please multiply it out) is the key to the whole chapter. It generalizes,pos_def_matrices
"immediately to n dimensions, and it is a perfect shorthand for studying maxima and",pos_def_matrices
"minima. When the variables are x1,...,xn, they go into a column vector x. For any",pos_def_matrices
"symmetric matrix A, the product xTAx is a pure quadratic form f(x1,...,xn):",pos_def_matrices
The diagonal entries a11 to ann multiply x2,pos_def_matrices
n. The pair aij = a ji combines into,pos_def_matrices
2aijxix j. Then f = a11x2,pos_def_matrices
There are no higher-order terms or lower-order terms—only second-order. The func-,pos_def_matrices
"tion is zero at x = (0,...,0), and its ﬁrst derivatives are zero. The tangent is ﬂat; this is",pos_def_matrices
a stationary point. We have to decide if x = 0 is a minimum or a maximum or a saddle,pos_def_matrices
point of the function f = xTAx.,pos_def_matrices
Example 3. f = 2x2 +4xy+y2 and A =,pos_def_matrices
Example 4. f = 2xy and A =,pos_def_matrices
Example 5. A is 3 by 3 for 2x2,pos_def_matrices
"�� → minimum at (0,0,0).",pos_def_matrices
"Any function F(x1,...,xn) is approached in the same way. At a stationary point",pos_def_matrices
all ﬁrst derivatives are zero. A is the “second derivative matrix” with entries aij =,pos_def_matrices
"∂ 2F/∂xi∂x j. This automatically equals a ji = ∂ 2F/∂x j∂xi, so A is symmetric. Then F",pos_def_matrices
has a minimum when the pure quadratic xTAx is positive deﬁnite. These second-order,pos_def_matrices
terms control F near the stationary point:,pos_def_matrices
F(x) = F(0)+xT(grad F)+ 1,pos_def_matrices
"At a stationary point, grad F = (∂F/∂x1,...,∂F/∂xn) is a vector of zeros. The second",pos_def_matrices
derivatives in xTAx take the graph up or down (or saddle). If the stationary point is at x0,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"instead of 0, F(x) and all derivatives are computed at x0. Then x changes to x − x0 on",pos_def_matrices
The next section contains the tests to decide whether xTAx is positive (the bowl goes,pos_def_matrices
"up from x = 0). Equivalently, the tests decide whether the matrix A is positive deﬁ-",pos_def_matrices
nite—which is the main goal of the chapter.,pos_def_matrices
"1. The quadratic f = x2 +4xy+2y2 has a saddle point at the origin, despite the fact that",pos_def_matrices
its coefﬁcients are positive. Write f as a difference of two squares.,pos_def_matrices
"2. Decide for or against the positive deﬁniteness of these matrices, and write out the",pos_def_matrices
corresponding f = xTAx:,pos_def_matrices
"The determinant in (b) is zero; along what line is f(x,y) = 0?",pos_def_matrices
"3. If a 2 by 2 symmetric matrix passes the tests a > 0, ac > b2, solve the quadratic",pos_def_matrices
equation det(A−λI) = 0 and show that both eigenvalues are positive.,pos_def_matrices
"4. Decide between a minimum, maximum, or saddle point for the following functions.",pos_def_matrices
(a) F = −1+4(ex −x)−5xsiny+6y2 at the point x = y = 0.,pos_def_matrices
"(b) F = (x2 −2x)cosy, with stationary point at x = 1, y = π.",pos_def_matrices
5. (a) For which numbers b is the matrix A =,pos_def_matrices
(b) Factor A = LDLT when b is in the range for positive deﬁniteness.,pos_def_matrices
(c) Find the minimum value of 1,pos_def_matrices
2(x2 +2bxy+9y2)−y for b in this range.,pos_def_matrices
(d) What is the minimum if b = 3?,pos_def_matrices
6. Suppose the positive coefﬁcients a and c dominate b in the sense that a + c > 2b.,pos_def_matrices
"Find an example that has ac < b2, so the matrix is not positive deﬁnite.",pos_def_matrices
7. (a) What 3 by 3 symmetric matrices A1 and A2 correspond to f1 and f2?,pos_def_matrices
3 −2x1x2 −2x1x3 +2x2x3,pos_def_matrices
3 −2x1x2 −2x1x3 −4x2x3.,pos_def_matrices
(b) Show that f1 is a single perfect square and not positive deﬁnite. Where is f1,pos_def_matrices
"(c) Factor A2 into LLT, Write f2 = xTA2x as a sum of three squares.",pos_def_matrices
8. If A =,pos_def_matrices
"is positive deﬁnite, test A−1 = [ p q",pos_def_matrices
q r ] for positive deﬁniteness.,pos_def_matrices
"6.1 Minima, Maxima, and Saddle Points",pos_def_matrices
"9. The quadratic f(x1,x2) = 3(x1 + 2x2)2 + 4x2",pos_def_matrices
"2 is positive. Find its matrix A, factor it",pos_def_matrices
"into LDLT, and connect the entries in D and L to 3, 2, 4 in f.",pos_def_matrices
10. If R = [ p q,pos_def_matrices
"q r ], write out R2 and check that it is positive deﬁnite unless R is singular.",pos_def_matrices
11. (a) If A =,pos_def_matrices
"is Hermitian (complex b), ﬁnd its pivots and determinant.",pos_def_matrices
(b) Complete the square for xHAx. Now xH = [x1 x2] can be complex,pos_def_matrices
a|x1|2 +2Rebx1x2 +c|x2|2 = a|x1 +(b/a)x2|2 +,pos_def_matrices
(c) Show that a > 0 and ac > |b|2 ensure that A is positive deﬁnite.,pos_def_matrices
(d) Are the matrices,pos_def_matrices
12. Decide whether F = x2y2 − 2x − 2y has a minimum at the point x = y = 1 (after,pos_def_matrices
showing that the ﬁrst derivatives are zero at that point).,pos_def_matrices
"13. Under what conditions on a, b, c is ax2 +2bxy+cy2 > x2 +y2 for all x, y?",pos_def_matrices
Problems 14–18 are about tests for positive deﬁniteness.,pos_def_matrices
"14. Which of A1, A2, A3, A4 has two positive eigenvalues? Test a > 0 and ac > b2, don’t",pos_def_matrices
compute the eigenvalues. Find an x so that xTA1x < 0.,pos_def_matrices
15. What is the quadratic f = ax2 +2bxy+cy2 for each of these matrices? Complete the,pos_def_matrices
square to write f as a sum of one or two squares d1(,pos_def_matrices
"16. Show that f(x,y) = x2 + 4xy + 3y2 does not have a minimum at (0,0) even though",pos_def_matrices
"it has positive coefﬁcients. Write f as a difference of squares and ﬁnd a point (x,y)",pos_def_matrices
where f is negative.,pos_def_matrices
"17. (Important) If A has independent columns, then ATA is square and symmetric and",pos_def_matrices
invertible (Section 4.2). Rewrite xTATAx to show why it is positive except when,pos_def_matrices
x = 0. Then ATA is positive deﬁnite.,pos_def_matrices
18. Test to see if ATA is positive deﬁnite in each case:,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"19. Find the 3 by 3 matrix A and its pivots, rank, eigenvalues, and determinant:",pos_def_matrices
�� = 4(x1 −x2 +2x3)2.,pos_def_matrices
"20. For F1(x,y) = 1",pos_def_matrices
"4x4 + x2y + y2 and F2(x,y) = x3 + xy − x, ﬁnd the second derivative",pos_def_matrices
matrices A1 and A2:,pos_def_matrices
"A1 is positive deﬁnite, so F1 is concave up (= convex). Find the minimum point of",pos_def_matrices
F1 and the saddle point of F2 (look where ﬁrst derivatives are zero).,pos_def_matrices
21. The graph of z = x2 + y2 is a bowl opening upward. The graph of z = x2 − y2 is a,pos_def_matrices
saddle. The graph of z = −x2 − y2 is a bowl opening downward. What is a test on,pos_def_matrices
"F(x,y) to have a saddle at (0,0)?",pos_def_matrices
22. Which values of c give a bowl and which give a saddle point for the graph of z =,pos_def_matrices
4x2 +12xy+cy2? Describe this graph at the borderline value of c.,pos_def_matrices
Tests for Positive Deﬁniteness,pos_def_matrices
Which symmetric matrices have the property that xTAx > 0 for all nonzero vectors x?,pos_def_matrices
"There are four or ﬁve different ways to answer this question, and we hope to ﬁnd all of",pos_def_matrices
them. The previous section began with some hints about the signs of eigenvalues. but,pos_def_matrices
"that gave place to the tests on a, b, c:",pos_def_matrices
is positive deﬁnite when,pos_def_matrices
"From those conditions, both eigenvalues are positive. Their product λ1λ2 is determinant",pos_def_matrices
"ac − b2 > 0, so the eigenvalues are either both positive or both negative. They must be",pos_def_matrices
positive because their sum is the trace a+c > 0.,pos_def_matrices
"Looking at a and ac−b2, it is even possible to spot the appearance of the pivots. They",pos_def_matrices
turned up when we decomposed xTAx into a sum of squares:,pos_def_matrices
ax2 +2bxy+cy2 = a,pos_def_matrices
Those coefﬁcients a and (ac − b2)/a are the pivots for a 2 by 2 matrix. For larger,pos_def_matrices
matrices the pivots still give a simple test for positive deﬁniteness: xTAx stays positive,pos_def_matrices
when n independent squares are multiplied by positive pivots.,pos_def_matrices
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
One more preliminary remark. The two parts of this hook were linked by the chapter,pos_def_matrices
on determinants. Therefore we ask what part determinants play. It is not enough to,pos_def_matrices
require that the determinant of A is positive. If a = c = −1 and b = 0. then detA = 1,pos_def_matrices
"but A = −I = negative deﬁnite. The determinant test is applied not only to A itself,",pos_def_matrices
"giving ac−b2 > 0, but also to the 1 by 1 submatrix a in the upper left-hand corner.",pos_def_matrices
The natural generalization will involve all n of the upper left submatrices of A:,pos_def_matrices
"Here is the main theorem on positive deﬁniteness, and a reasonably detailed proof:",pos_def_matrices
Each of the following tests is a necessary and sufﬁcient condition for the,pos_def_matrices
real symmetric matrix A to be positive deﬁnite:,pos_def_matrices
(I) xTkx > 0 for all nonzero real vectors x.,pos_def_matrices
(II) All the eigenvalues of A satisfy λi > 0.,pos_def_matrices
(III) All the upper left submatrices Ak have positive determinants.,pos_def_matrices
(IV) All the pivots (without row exchanges) satisfy dk > 0.,pos_def_matrices
Proof. Condition I deﬁnes a positive deﬁnite matrix. Our ﬁrst step shows that each,pos_def_matrices
eigenvalue will be positive:,pos_def_matrices
xTAx = xTλx = λ∥x∥2.,pos_def_matrices
"A positive deﬁnite matrix has positive eigenvalues, since xTAx > 0.",pos_def_matrices
"Now we go in the other direction. If all λi > 0, we have to prove xTAx > 0 for",pos_def_matrices
every vector x (not just the eigenvectors). Since symmetric matrices have a full set of,pos_def_matrices
"orthonormal eigenvectors, any x is a combination c1x1 +···+cnxn. Then",pos_def_matrices
Ax = c1Ax1 +···+cnAxn = c1λ1x1 +···+cnλnxn.,pos_def_matrices
Because of the orthogonality xT,pos_def_matrices
"i xi = 0, and the normalization xT",pos_def_matrices
"i xi = 1,",pos_def_matrices
"If every λi > 0, then equation (2) shows that xTAx > 0. Thus condition II implies condi-",pos_def_matrices
"If condition I holds, so does condition III: The determinant of A is the product of",pos_def_matrices
"the eigenvalues. And if condition I holds, we already know that these eigenvalues are",pos_def_matrices
positive. But we also have to deal with every upper left submatrix Ak. The trick is to,pos_def_matrices
look at all nonzero vectors whose last n−k components are zero:,pos_def_matrices
k Akxk > 0.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
Thus Ak is positive deﬁnite. Its eigenvalues (not the same λ1!) must be positive. Its,pos_def_matrices
"determinant is their product, so all upper left determinants are positive.",pos_def_matrices
"If condition III holds, so does condition IV: According to Section 4.4, the kth pivot",pos_def_matrices
"dk is the ratio of detAk to detAk−1. If the determinants are all positive, so are the pivots.",pos_def_matrices
"If condition IV holds, so does condition I: We are given positive pivots, and must",pos_def_matrices
"deduce that xTAx > 0. This is what we did in the 2 by 2 case, by completing the square.",pos_def_matrices
The pivots were the numbers outside the squares. To see how that happens for symmetric,pos_def_matrices
"matrices of any size, we go back to elimination on a symmetric matrix: A = LDLT.",pos_def_matrices
"Example 1. Positive pivots 2, 3",pos_def_matrices
I want to split xTAx into xTLDLTx:,pos_def_matrices
"So xTAx is a sum of squares with the pivots 2, 3",pos_def_matrices
xTAx = (LTx)TD(LTx) = 2,pos_def_matrices
Those positive pivots in D multiply perfect squares to make xTAx positive. Thus condi-,pos_def_matrices
"tion IV implies condition I, and the proof is complete.",pos_def_matrices
It is beautiful that elimination and completing the square are actually the same. Elim-,pos_def_matrices
"ination removes x1 from all later equations. Similarly, the ﬁrst square accounts for all",pos_def_matrices
terms in xTAx involving x1. The sum of squares has the pivots outside. The multipliers,pos_def_matrices
ℓij are inside! You can see the numbers −1,pos_def_matrices
3 inside the squares in the example.,pos_def_matrices
"Every diagonal entry aii must be positive. As we know from the examples, however,",pos_def_matrices
it is far from sufﬁcient to look only at the diagonal entries.,pos_def_matrices
The pivots di are not to be confused with the eigenvalues. For a typical positive,pos_def_matrices
"deﬁnite matrix, they are two completely different sets of positive numbers, In our 3 by 3",pos_def_matrices
"example, probably the determinant test is the easiest:",pos_def_matrices
detA3 = detA = 4.,pos_def_matrices
"The pivots are the ratios d1 = 2, d2 = 3",pos_def_matrices
"2, d3 = 4",pos_def_matrices
3. Ordinarily the eigenvalue test is the,pos_def_matrices
longest computation. For this A we know the λ’s are all positive:,pos_def_matrices
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
"Even though it is the hardest to apply to a single matrix, eigenvalues can be the most",pos_def_matrices
useful test for theoretical purposes. Each test is enough by itself.,pos_def_matrices
Positive Deﬁnite Matrices and Least Squares,pos_def_matrices
I hope you will allow one more test for positive deﬁniteness. It is already close. We,pos_def_matrices
"connected positive deﬁnite matrices to pivots (Chapter 1), determinants (Chapter 4), and",pos_def_matrices
"eigenvalues (Chapter 5). Now we see them in the least-squares problems in Chapter 3,",pos_def_matrices
coming from the rectangular matrices of Chapter 2.,pos_def_matrices
The rectangular matrix will be R and the least-squares problem will be Rx = b. It has,pos_def_matrices
m equations with m ≥ n (square systems are included). The least-square choice �x is the,pos_def_matrices
"solution of RTR�x = RTb. That matrix ARTR is not only symmetric but positive deﬁnite,",pos_def_matrices
as we now show—provided that the n columns of R are linearly independent:,pos_def_matrices
The symmetric matrix A is positive deﬁnite if and only if,pos_def_matrices
(V) There is a matrix R with independent columns such that A = RTR.,pos_def_matrices
The key is to recognize xTAx as xTRTRx = (Rx)T(Rx). This squared length ∥Rx∥2 is,pos_def_matrices
"positive (unless x = 0), because R has independent columns. (If x is nonzero then Rx is",pos_def_matrices
nonzero.) Thus xTRTRx > 0 and RTR is positive deﬁnite.,pos_def_matrices
It remains to ﬁnd an R For which A = RTR. We have almost done this twice already:,pos_def_matrices
A = LDLT = (L,pos_def_matrices
So take R =,pos_def_matrices
This Cholesky decomposition has the pivots split evenly between L and LT.,pos_def_matrices
A = QΛQT = (Q,pos_def_matrices
So take R =,pos_def_matrices
A third possibility is R = Q,pos_def_matrices
"ΛQT, the symmetric positive deﬁnite square root of A.",pos_def_matrices
"There are many other choices, square or rectangular, and we can see why. If you multiply",pos_def_matrices
"any R by a matrix Q with orthonormal columns, then (QR)T(QR) = RTQTQR = RTIR =",pos_def_matrices
A. Therefore QR is another choice.,pos_def_matrices
Applications of positive deﬁnite matrices are developed in my earlier book Intro-,pos_def_matrices
duction to Applied Mathematics and also the new Applied Mathematics and Scientiﬁc,pos_def_matrices
Computing (see www.wellesleycambridge.com). We mention that Ax = λMx,pos_def_matrices
"arises constantly in engineering analysis. If A and M are positive deﬁnite, this general-",pos_def_matrices
"ized problem is parallel to the familiar Ax = λx, and λ > 0. M is a mass matrix for the",pos_def_matrices
ﬁnite element method in Section 6.4.,pos_def_matrices
"The tests for semideﬁniteness will relax xTAx > 0, λ > 0, d > 0, and det > 0, to allow",pos_def_matrices
zeros to appear. The main point is to see the analogies with the positive deﬁnite case.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
Each of the following tests is a necessary and sufﬁcient condition for a,pos_def_matrices
symmetric matrix A to be positive semideﬁnite:,pos_def_matrices
(I′) xTAx ≥ 0 for all vectors x (this deﬁnes positive semideﬁnite).,pos_def_matrices
(II′) All the eigenvalues of A satisfy λi ≥ 0.,pos_def_matrices
(III′) No principal submatrices have negative determinants.,pos_def_matrices
(IV′) No pivots are negative.,pos_def_matrices
"(V′) There is a matrix R, possibly with dependent columns, such that A = RTR.",pos_def_matrices
"The diagonalization A = QΛQT leads to xTAx = xTQΛQTx = yTΛy. If A has rank r, there",pos_def_matrices
are r nonzero λ’s and r perfect squares in yTΛy = λ1y2,pos_def_matrices
"Note. The novelty is that condition III′ applies to all the principal submatrices, not only",pos_def_matrices
"those in the upper left-hand corner. Otherwise, we could not distinguish between two",pos_def_matrices
matrices whose upper left determinants were all zero:,pos_def_matrices
"is positive semideﬁnite, and",pos_def_matrices
A row exchange comes with the same column exchange to maintain symmetry.,pos_def_matrices
"is positive semideﬁnite, by all ﬁve tests:",pos_def_matrices
(I′) xTAx = (x1 −x2)2 +(x1 −x3)2 +(x2 −x3)2 ≥ 0 (zero if x1 = x2 = x3).,pos_def_matrices
"(II′) The eigenvalues are λ1 = 0, λ2 = λ3 = 3 (a zero eigenvalue).",pos_def_matrices
(III′) detA = 0 and smaller determinants are positive.,pos_def_matrices
(V′) A = RTR with dependent columns in R:,pos_def_matrices
"(1,1,1) in the nullspace.",pos_def_matrices
Remark. The conditions for semideﬁniteness could also be deduced from the origin con-,pos_def_matrices
ditions I-V for deﬁniteness by the following trick: Add a small multiple of the identity,pos_def_matrices
giving a positive deﬁnite matrix A + εI. Then let ε approach zero. Since the determi-,pos_def_matrices
"nants and eigenvalues depend continuously on ε, they will be positive until the very last",pos_def_matrices
moment. At ε = 0 they must still be nonnegative.,pos_def_matrices
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
My class often asks about unsymmetric positive deﬁnite matrices. I never use that,pos_def_matrices
term. One reasonable deﬁnition is that the symmetric part 1,pos_def_matrices
2(A+AT) should be positive,pos_def_matrices
deﬁnite. That guarantees that the real parts of the eigenvalues are positive. But it is not,pos_def_matrices
has λ > 0 but 1,pos_def_matrices
"If Ax = λx, then xHAx = λxHx and xHAHx = λxHx.",pos_def_matrices
"2xH(A+AH)x = (Reλ)xHx > 0, so that Reλ > 0.",pos_def_matrices
Ellipsoids in n Dimensions,pos_def_matrices
"Throughout this book, geometry has helped the matrix algebra. A linear equation pro-",pos_def_matrices
duced a plane. The system Ax = b gives an intersection of planes. Least squares gives,pos_def_matrices
"a perpendicular projection. The determinant is the volume of a box. Now, for a positive",pos_def_matrices
"deﬁnite matrix and its xTAx, we ﬁnally get a ﬁgure that is curved. It is an ellipse in two",pos_def_matrices
"dimensions, and an ellipsoid in n dimensions.",pos_def_matrices
"The equation to consider is xTAx = 1. If A is the identity matrix, this simpliﬁes to",pos_def_matrices
2 + ··· + x2,pos_def_matrices
"n = 1. This is the equation of the “unit sphere” in Rn. If A = 4I, the",pos_def_matrices
sphere gets smaller. The equation changes to 4x2,pos_def_matrices
"n = 1. Instead of (1,0,...,0),",pos_def_matrices
it goes through (1,pos_def_matrices
"2,0,...,0). The center is at the origin, because if x satisﬁes xTAx = 1,",pos_def_matrices
so does the opposite vector −x. The important step is to go from the identity matrix to a,pos_def_matrices
the equation is xTAx = 4x2,pos_def_matrices
Since the entries are unequal (and positive!) the sphere changes to an ellipsoid.,pos_def_matrices
One solution is x = (1,pos_def_matrices
"2,0,0) along the ﬁrst axis. Another is x = (0,1,0). The major",pos_def_matrices
"axis has the farthest point x = (0,0,3). It is like a football or a rugby ball, but not quite—",pos_def_matrices
those are closer to x2,pos_def_matrices
3 = 1. The two equal coefﬁcients make them circular in,pos_def_matrices
"the x1-x2 plane, and much easier to throw!",pos_def_matrices
"Now comes the ﬁnal step, to allow nonzeros away from the diagonal of A.",pos_def_matrices
Example 3. A =,pos_def_matrices
and xTAx = 5u2 + 8uv + 5v2 = 1. That ellipse is centered at,pos_def_matrices
"u = v = 0, but the axes are not so clear. The off-diagonal 4s leave the matrix positive",pos_def_matrices
"deﬁnite, but they rotate the ellipse—its axes no longer line up with the coordinate axes",pos_def_matrices
(Figure 6.2). We will show that the axes of the ellipse point toward the eigenvector of,pos_def_matrices
"A. Because A = AT, those eigenvectors and axes are orthogonal. The major axis of the",pos_def_matrices
ellipse corresponds to the smallest eigenvalue of A.,pos_def_matrices
To locate the ellipse we compute λ1 = 1 and λ2 = 9. The unit eigenvectors are,pos_def_matrices
"2. Those are at 45° angles with the u-v axes, and they are",pos_def_matrices
lined up with the axes of the ellipse. The way to see the ellipse properly is to rewrite,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
Figure 6.2: The ellipse xTAx = 5u2 +8uv+5v2 = 1 and its principal axes.,pos_def_matrices
λ = 1 and λ = 9 are outside the squares. The eigenvectors are inside. This is different,pos_def_matrices
from completing the square to 5(u+ 4,pos_def_matrices
"5v2, with the pivots outside.",pos_def_matrices
The ﬁrst square equals 1 at (1/,pos_def_matrices
2) at the end of the major axis. The minor,pos_def_matrices
"axis is one-third as long, since we need (1",pos_def_matrices
3)2 to cancel the 9.,pos_def_matrices
Any ellipsoid xTAx = 1 can be simpliﬁed in the same way. The key step is to diago-,pos_def_matrices
"nalize A = QΛQT. We straightened the picture by rotating the axes. Algebraically, the",pos_def_matrices
change to y = QTx produces a sum of squares:,pos_def_matrices
xTAx = (xTQ)Λ(QTx) = yTΛy = λ1y2,pos_def_matrices
The major axis has y1 = 1/,pos_def_matrices
λ1 along the eigenvector with the smallest eigenvalue.,pos_def_matrices
The other axes are along the other eigenvectors. Their lengths are 1/,pos_def_matrices
Notice that the λ’s must be positive—the matrix must be positive deﬁnite—or these,pos_def_matrices
square roots are in trouble. An indeﬁnite equation y2,pos_def_matrices
2 = 1 describes a hyperbola,pos_def_matrices
"and not an ellipse. A hyperbola is a cross-section through a saddle, and an ellipse is a",pos_def_matrices
cross-section through a bowl.,pos_def_matrices
"The change from x to y = QTx rotates the axes of the space, to match the axes of",pos_def_matrices
"the ellipsoid. In the y variables we can see that it is an ellipsoid, because the equation",pos_def_matrices
Suppose A = QΛQT with λi > 0. Rotating y = QTx simpliﬁes xTAx = 1:,pos_def_matrices
This is the equation of an ellipsoid. Its axes have lengths 1/,pos_def_matrices
from the center. In the original x-space they point along the eigenvectors of A.,pos_def_matrices
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
The Law of Inertia,pos_def_matrices
"For elimination and eigenvalues, matrices become simpler by elementary operations",pos_def_matrices
The essential thing is to know which properties of the matrix stay unchanged. When,pos_def_matrices
"a multiple of one row is subtracted from another, the row space, nullspace. rant and",pos_def_matrices
"determinant all remain the same. For eigenvalues, the basic operation was a similarity",pos_def_matrices
transformation A → S−1AS (or A → M−1AM). The eigenvalues are unchanged (and also,pos_def_matrices
the Jordan form). Now we ask the same question for symmetric matrices: What are the,pos_def_matrices
elementary operations and their invariants for xTAx?,pos_def_matrices
The basic operation on a quadratic form is to change variables. A new vector y is,pos_def_matrices
"related to x by some nonsingular matrix, x =Cy. The quadratic form becomes yTCTACy.",pos_def_matrices
This shows the fundamental operation on A:,pos_def_matrices
for some nonsingular C.,pos_def_matrices
"The symmetry of A is preserved, since CTAC remains symmetric. The real question is,",pos_def_matrices
What other properties are shared by A and CTAC? The answer is given by Sylvester’s,pos_def_matrices
"6F CTAC has the same number of positive eigenvalues, negative eigenvalues,",pos_def_matrices
and zero eigenvalues as A.,pos_def_matrices
The signs of the eigenvalues (and not the eigenvalues themselves) are preserved by a,pos_def_matrices
"congruence transformation. In the proof, we will suppose that A is nonsingular. Then",pos_def_matrices
"CTAC is also nonsingular, and there are no zero eigenvalues to worry about. (Otherwise",pos_def_matrices
"we can work with the nonsingular A+ εI and A−εI, and at the end let ε → 0.)",pos_def_matrices
Proof. We want to borrow a trick from topology. Suppose C is linked to an orthogonal,pos_def_matrices
"matrix Q by a continuous chain of nonsingular matrices C(t). At t = 0 and t = 1, C(0) =",pos_def_matrices
"C and C(1) = Q. Then the eigenvalues of C(t)TAC(t) will change gradually, as t goes",pos_def_matrices
"from 0 to 1, from the eigenvalues of CTAC to the eigenvalues of QTAQ. Because C(t) is",pos_def_matrices
"never singular, none of these eigenvalues can touch zero (not to mention cross over it!).",pos_def_matrices
"Therefore the number of eigenvalues to the right of zero, and the number to the left, is",pos_def_matrices
the same for CTAC as for QTAQ. And A has exactly the same eigenvalues as the similar,pos_def_matrices
matrix Q−1AQ = QTAQ.,pos_def_matrices
"One good choice for Q is to apply Gram-Schmidt to the columns of C. Then C = QR,",pos_def_matrices
and the chain of matrices is C(t) = tQ+(1−t)QR. The family C(t) goes slowly through,pos_def_matrices
"Gram-Schmidt, from QR to Q. It is invertible, because Q is invertible and the triangular",pos_def_matrices
factor tI +(1−t)R has positive diagonal. That ends the proof.,pos_def_matrices
Example 4. Suppose A = I. Then CTAC = CTC is positive deﬁnite. Both I and CTC,pos_def_matrices
"have n positive eigenvalues, conﬁrming the law of inertia.",pos_def_matrices
Example 5. If A =,pos_def_matrices
", then CTAC has a negative determinant:",pos_def_matrices
detCTAC = (detCT)(detA)(detC) = −(detC)2 < 0.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"Then CTAC must have one positive and one negative eigenvalue, like A.",pos_def_matrices
Example 6. This application is the important one:,pos_def_matrices
"For any symmetric matrix A, the signs of the pivots agree with the signs",pos_def_matrices
of the eigenvalues. The eigenvalue matrix Λ and the pivot matrix D have the,pos_def_matrices
"same number of positive entries, negative entries, and zero entries.",pos_def_matrices
We will assume that A allows the symmetric factorization A = LDLT (without row ex-,pos_def_matrices
"changes). By the law of inertia, A has the same number of positive eigenvalues as D.",pos_def_matrices
But the eigenvalues of D are just its diagonal entries (the pivots). Thus the number of,pos_def_matrices
positive pivots matches the number of positive eigenvalues of A.,pos_def_matrices
That is both beautiful and practical. It is beautiful because it brings together (for,pos_def_matrices
symmetric matrices) two parts of this book that were previously separate: pivots and,pos_def_matrices
"eigenvalues. It is also practical, because the pivots can locate the eigenvalues:",pos_def_matrices
A has positive pivots,pos_def_matrices
A−2I has a negative pivot,pos_def_matrices
"A has positive eigenvalues, by our test. But we know that λmin is smaller than 2, because",pos_def_matrices
"subtracting 2 dropped it below zero. The next step looks at A−I, to see if λmin < 1. (It",pos_def_matrices
"is, because A−I has a negative pivot.) That interval containing λ is cut in half at every",pos_def_matrices
step by checking the signs of the pivots.,pos_def_matrices
This was almost the ﬁrst practical method of computing eigenvalues. It was dominant,pos_def_matrices
"about 1960, after one important improvement—to make A tridiagonal ﬁrst. Then the",pos_def_matrices
pivots are computed in 2n steps instead of 1,pos_def_matrices
"6n3. Elimination becomes fast, and the search",pos_def_matrices
for eigenvalues (by halving the intervals) becomes simple. The current favorite is the,pos_def_matrices
QR method in Chapter 7.,pos_def_matrices
The Generalized Eigenvalue Problem,pos_def_matrices
"Physics, engineering, and statistics are usually kind enough to produce symmetric ma-",pos_def_matrices
trices in their eigenvalue problems. But sometimes Ax = λx is replaced by Ax = λMx.,pos_def_matrices
There are two matrices rather than one.,pos_def_matrices
An example is the motion of two unequal masses in a line of springs:,pos_def_matrices
dt2 +2v−w = 0,pos_def_matrices
dt2 −v+2w = 0,pos_def_matrices
"When the masses were equal, m1 = m2 = 1, this was the old system u′′ +Au = 0. Now",pos_def_matrices
"it is Mu′′ +Au = 0, with a mass matrix M. The eigenvalue problem arises when we look",pos_def_matrices
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
for exponential solutions eiωtx:,pos_def_matrices
Mu′′ +Au = 0,pos_def_matrices
"Canceling eiωt, and writing λ for ω2, this is an eigenvalue problem:",pos_def_matrices
Generalized problem Ax = λMx,pos_def_matrices
There is a solution when A−λM is singular. The special choice M = I brings back the,pos_def_matrices
usual det(A−λI) = 0. We work out det(A−λM) with m1 = 1 and m2 = 2:,pos_def_matrices
= 2λ 2 −6λ +3 = 0,pos_def_matrices
For the eigenvector x1(,pos_def_matrices
"3−1,1), the two masses oscillate together—but the ﬁrst mass",pos_def_matrices
only moves as far as,pos_def_matrices
"3 − 1 ≈ .73. In the fastest mode, the components of x2 = (1 +",pos_def_matrices
"3,−1) have opposite signs and the masses move in opposite directions. This time the",pos_def_matrices
smaller mass goes much further.,pos_def_matrices
The underlying theory is easier to explain if M is split into RTR. (M is assumed to be,pos_def_matrices
positive deﬁnite.) Then the substitution y = Rx changes,pos_def_matrices
Ax = λMx = λRTRx,pos_def_matrices
"Writing C for R−1, and multiplying through by (RT)−1 = CT, this becomes a standard",pos_def_matrices
eigenvalue problem for the single symmetric matrix CTAC:,pos_def_matrices
The eigenvalues λj are the same as for the original Ax = λMx. and the eigenvectors are,pos_def_matrices
"related by y j = Rx j. The properties of CTAC lead directly to thc properties of Ax = λMx,",pos_def_matrices
when A = AT and M is positive deﬁnite:,pos_def_matrices
"1. The eigenvalues for Ax = λMx are real, because CTAC is symmetric.",pos_def_matrices
"2. The λ’s have the same signs as the eigenvalues of A, by the law of inertia.",pos_def_matrices
3. CTAC has orthogonal eigenvectors y j. So the eigenvectors of Ax = λMx have,pos_def_matrices
i Mx j = xT,pos_def_matrices
i RTRx j = yT,pos_def_matrices
i y j = 0.,pos_def_matrices
"A and M are being simultaneously diagonalized. If S has the x j in its columns, then",pos_def_matrices
"STAS = Λ and STMS = I. This is a congruence transformation, with ST on the left,",pos_def_matrices
and not a similarity transformation with S−1. The main point is easy to summarize: As,pos_def_matrices
"long as M is positive deﬁnite, the generalized eigenvalue problem Ax = −λMx behaves",pos_def_matrices
exactly like Ax = λx.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
1. For what range of numbers a and b are the matrices A and B positive deﬁnite?,pos_def_matrices
2. Decide for or against the positive deﬁniteness of,pos_def_matrices
3. Construct an indeﬁnite matrix with its largest entries on the main diagonal:,pos_def_matrices
with |b| < 1 can have detA < 0.,pos_def_matrices
"4. Show from the eigenvalues that if A is positive deﬁnite, so is A2 and so is A−1.",pos_def_matrices
"5. If A and B are positive deﬁnite, then A+B is positive deﬁnite. Pivots and eigenvalues",pos_def_matrices
are not convenient for A+B. Much better to prove xT(A+B)x > 0.,pos_def_matrices
"6. From the pivots, eigenvalues, and eigenvectors of A =",pos_def_matrices
", write A as RTR in three",pos_def_matrices
"7. If A = QΛQT is symmetric positive deﬁnite, then R = Q",pos_def_matrices
ΛQT is its symmetric pos-,pos_def_matrices
itive deﬁnite square root. Why does R have positive eigenvalues? Compute R and,pos_def_matrices
verify R2 = A for,pos_def_matrices
"8. If A is symmetric positive deﬁnite and C is nonsingular, prove that B = CTAC is also",pos_def_matrices
9. If A = RTR prove the generalized Schwarz inequality |xTAy|2 ≤ (xTAx)(yTAy).,pos_def_matrices
10. The ellipse u2 +4v2 = 1 corresponds to A =,pos_def_matrices
. Write the eigenvalues and eigen-,pos_def_matrices
"vectors, and sketch the ellipse.",pos_def_matrices
11. Reduce the equation 3u2 − 2,pos_def_matrices
2uv + 2v2 = 1 to a sum of squares by ﬁnding the,pos_def_matrices
"eigenvalues of the corresponding A, and sketch the ellipse.",pos_def_matrices
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
"12. In three dimensions, λ1y2",pos_def_matrices
3 = 1 represents an ellipsoid when all λi > 0.,pos_def_matrices
Describe all the different kinds of surfaces that appear in the positive semideﬁnite,pos_def_matrices
case when one or more of the eigenvalues is zero.,pos_def_matrices
13. Write down the ﬁve conditions for a 3 by 3 matrix to be negative deﬁnite (−A is,pos_def_matrices
positive deﬁnite) with special attention to condition III: How is det(−A) related to,pos_def_matrices
"14. Decide whether the following matrices are positive deﬁnite, negative deﬁnite, semidef-",pos_def_matrices
Is there a real solution to −x2 −5y2 −9z2 −4xy−6xz−8yz = 1?,pos_def_matrices
15. Suppose A is symmetric positive deﬁnite and Q is an orthogonal matrix. True or,pos_def_matrices
(a) QTAQ is a diagonal matrix.,pos_def_matrices
(b) QTAQ is symmetric positive deﬁnite.,pos_def_matrices
(c) QTAQ has the same eigenvalues as A.,pos_def_matrices
(d) e−A is symmetric positive deﬁnite.,pos_def_matrices
"16. If A is positive deﬁnite and a11 is increased, prove from cofactors that the determinant",pos_def_matrices
is increased. Show by example that this can fail if A is indeﬁnite.,pos_def_matrices
17. From A = RTR. show for positive deﬁnite matrices that detA ≤ a11a22···ann. (The,pos_def_matrices
length squared of column j of R is a j j. Use determinant = volume.),pos_def_matrices
18. (Lyapunov test for stability of M) Suppose AM + MHA = −I with positive deﬁnite,pos_def_matrices
A. If Mx = λx show that ReA < 0. (Hint: Multiply the ﬁrst equation by xH and x.),pos_def_matrices
19. Which 3 by 3 symmetric matrices A produce these functions f = xTAx? Why is the,pos_def_matrices
ﬁrst matrix positive deﬁnite but not the second one?,pos_def_matrices
(a) f = 2(x2,pos_def_matrices
(b) f = 2(x2,pos_def_matrices
3 −x1x2 −x1x3 −x2x3).,pos_def_matrices
20. Compute the three upper left determinants to establish positive deﬁniteness. Verify,pos_def_matrices
that their ratios give the second and third pivots.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"21. A positive deﬁnite matrix cannot have a zero (or even worse, a negative number) on",pos_def_matrices
its diagonal. Show that this matrix fails to have xTAx > 0:,pos_def_matrices
is not positive when,pos_def_matrices
22. A diagonal entry a j j of a symmetric matrix cannot be smaller than all λ’s. If it,pos_def_matrices
"were, then A−a j jI would have",pos_def_matrices
eigenvalues and would be positive deﬁnite. But,pos_def_matrices
A−a j jI has a,pos_def_matrices
on the main diagonal.,pos_def_matrices
23. Give a quick reason why each of these statements is true:,pos_def_matrices
(a) Every positive deﬁnite matrix is invertible.,pos_def_matrices
(b) The only positive deﬁnite projection matrix is P = I.,pos_def_matrices
(c) A diagonal matrix with positive diagonal entries is positive deﬁnite.,pos_def_matrices
(d) A symmetric matrix with a positive determinant might not be positive deﬁnite!,pos_def_matrices
24. For which s and t do A and B have all λ > 0 (and are therefore positive deﬁnite)?,pos_def_matrices
25. You may have seen the equation for an ellipse as (x,pos_def_matrices
a)2 + ( y,pos_def_matrices
b)2 = 1. What are a and,pos_def_matrices
b when the equation is written as λ1x2 + λ2y2 = 1? The ellipse 9x2 + 16y2 = 1 has,pos_def_matrices
half-axes with lengths a =,pos_def_matrices
", and b =",pos_def_matrices
26. Draw the tilted ellipse x2 +xy+y2 = 1 and ﬁnd the half-lengths of its axes from the,pos_def_matrices
eigenvalues of the corresponding A.,pos_def_matrices
"27. With positive pivots in D, the factorization A = LDLT becomes L",pos_def_matrices
roots of the pivots give D =,pos_def_matrices
D.) Then C = L,pos_def_matrices
D yields the Cholesky factor-,pos_def_matrices
"ization A = CCT, which is “symmetrized LU”:",pos_def_matrices
"28. In the Cholesky factorization A =CCT, with C = L",pos_def_matrices
"D, the square roots of the pivots",pos_def_matrices
are on the diagonal of C. Find C (lower triangular) for,pos_def_matrices
6.2 Tests for Positive Deﬁniteness,pos_def_matrices
29. The symmetric factorization A = LDLT means that xTAx = xTLDLTx:,pos_def_matrices
The left-hand side is ax2 + 2bxy + cy2. The right-hand side is a(x + b,pos_def_matrices
"The second pivot completes the square! Test with a = 2, b = 4, c = 10.",pos_def_matrices
30. Without multiplying A =,pos_def_matrices
the determinant of A.,pos_def_matrices
the eigenvalues of A.,pos_def_matrices
the eigenvectors of A.,pos_def_matrices
a reason why A is symmetric positive deﬁnite.,pos_def_matrices
31. For the semideﬁnite matrices,pos_def_matrices
write xTAx as a sum of two squares and xTBx as one square.,pos_def_matrices
32. Apply any three tests to each of the matrices,pos_def_matrices
"to decide whether they are positive deﬁnite, positive semideﬁnite, or indeﬁnite.",pos_def_matrices
33. For C =,pos_def_matrices
", conﬁrm that CTAC has eigenvalues of the same signs",pos_def_matrices
as A. Construct a chain of nonsingular matrices C(t) linking C to an orthogonal,pos_def_matrices
Q. Why is it impossible to construct a nonsingular chain linking C to the identity,pos_def_matrices
"34. If the pivots of a matrix are all greater than 1, are the eigenvalues all greater than 1?",pos_def_matrices
"Test on the tridiagonal −1, 2, −1 matrices.",pos_def_matrices
35. Use the pivots of A− 1,pos_def_matrices
2I to decide whether A has an eigenvalue smaller than 1,pos_def_matrices
36. An algebraic proof of the law of inertia starts with the orthonormal eigenvectors,pos_def_matrices
"x1,...,xp of A corresponding to eigenvalues λi > 0. and the orthonormal eigenvectors",pos_def_matrices
"y1,...,yq of CTAC corresponding to eigenvalues µi < 0.",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"(a) To prove that the p +q vectors x1,...,xp, Cy1,...,Cyq are independent, assume",pos_def_matrices
that some combination gives zero:,pos_def_matrices
a1x1 +···+apxp = b1Cy1 +···+bqCyq,pos_def_matrices
Show that zTAz = λ1a2,pos_def_matrices
p ≥ 0 and zTAz = µ1b2,pos_def_matrices
(b) Deduce that the a’s and b’s are zero (proving linear independence). From that,pos_def_matrices
deduce p+q ≤ n.,pos_def_matrices
(c) The same argument for the n − p negative λ’s and the n − q positive µ’s gives,pos_def_matrices
n − p + n − q ≤ n. (We again assume no zero eigenvalues—which are handled,pos_def_matrices
"separately). Show that p + q = n, so the number p of positive λ’s equals the",pos_def_matrices
number n−q of positive µ’s—which is the law of inertia.,pos_def_matrices
"37. If C is nonsingular, show that A and CTAC have the same rank. Thus they have the",pos_def_matrices
same number of zero eigenvalues.,pos_def_matrices
"38. Find by experiment the number of positive, negative, and zero eigenvalues of",pos_def_matrices
when the block B (of order 1,pos_def_matrices
39. Do A and CTAC always satisfy the law of inertia when C is not square?,pos_def_matrices
"40. In equation (9) with m1 = 1 and m2 = 2, verify that the normal modes are M-",pos_def_matrices
41. Find the eigenvalues and eigenvectors of Ax = λMx:,pos_def_matrices
"42. If the symmetric matrices A and M are indeﬁnite, Ax = λMx might not have real",pos_def_matrices
eigenvalues. Construct a 2 by 2 example.,pos_def_matrices
43. A group of nonsingular matrices includes AB and A−1 if it includes A and B. “Prod-,pos_def_matrices
ucts and inverses stay in the group.” Which of these sets are groups? Positive deﬁnite,pos_def_matrices
"symmetric matrices A, orthogonal matrices Q, all exponentials etA of a ﬁxed matrix",pos_def_matrices
"A, matrices P with positive eigenvalues, matrices D with determinant 1. Invent a",pos_def_matrices
group containing only positive deﬁnite matrices.,pos_def_matrices
6.3 Singular Value Decomposition,pos_def_matrices
A great matrix factorization has been saved for the end of the basic course. UΣV T joins,pos_def_matrices
with LU from elimination and QR from orthogonalization (Gauss and Gram-Schmidt).,pos_def_matrices
Nobody’s name is attached; A = UΣV T is known as the “SVD” or the singular value,pos_def_matrices
"decomposition. We want to describe it, to prove it, and to discuss its applications—",pos_def_matrices
which are many and growing.,pos_def_matrices
The SVD is closely associated with the eigenvalue-eigenvector factorization QΛQT of,pos_def_matrices
a positive deﬁnite matrix. The eigenvalues are in the diagonal matrix Λ. The eigenvector,pos_def_matrices
matrix Q is orthogonal (QTQ = I) because eigenvectors of a symmetric matrix can be,pos_def_matrices
"chosen to be orthonormal. For most matrices that is not true, and for rectangular matrices",pos_def_matrices
it is ridiculous (eigenvalues undeﬁned). But now we allow the Q on the left and the QT,pos_def_matrices
on the right to be any two orthogonal matrices U and V T—not necessarily transposes of,pos_def_matrices
each other. Then every matrix will split into A = UΣV T.,pos_def_matrices
"The diagonal (but rectangular) matrix Σ has eigenvalues from ATA, not from A! Those",pos_def_matrices
"positive entries (also called sigma) will be σ1,...,σr. They are the singular values of A.",pos_def_matrices
They ﬁll the ﬁrst r places on the main diagonal of Σ—when A has rank r. The rest of Σ,pos_def_matrices
"With rectangular matrices, the key is almost always to consider ATA and AAT.",pos_def_matrices
Any m by n matrix A can be factored,pos_def_matrices
A = UΣV T = (orthogonal)(diagonal)(orthogonal).,pos_def_matrices
"The columns of U (m by m) are eigenvectors of AAT, and the columns of V (n",pos_def_matrices
by n) are eigenvectors of ATA. The r singular values on the diagonal of Σ (m,pos_def_matrices
by n) are the square roots of the nonzero eigenvalues of both AAT and ATA.,pos_def_matrices
"Remark 1. For positive deﬁnite matrices, Σ is Λ and UΣV T is identical to QΛQT. For",pos_def_matrices
"other symmetric matrices, any negative eigenvalues in Λ become positive in Σ. For",pos_def_matrices
"complex matrices, Σ remains real but U and V become unitary (the complex version of",pos_def_matrices
orthogonal). We take complex conjugates in UHU = I and V HV = I and A = UΣV H.,pos_def_matrices
Remark 2. U and V give orthonormal bases for all four fundamental subspaces:,pos_def_matrices
columns of U: column space of A,pos_def_matrices
m−r columns of U: left nullspace of A,pos_def_matrices
row space of A,pos_def_matrices
Remark 3. The SVD chooses those bases in an extremely special way. They are more,pos_def_matrices
"than just orthonormal. When A multiplies a column vj of V, it produces σ j times a",pos_def_matrices
"column of U. That comes directly from AV = UΣ, looked at a column at a time.",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
Remark 4. Eigenvectors of AAT and ATA must go into the columns of U and V:,pos_def_matrices
AAT = (UΣV T)(VΣTUT) = UΣΣTUT,pos_def_matrices
ATA = VΣTΣV T.,pos_def_matrices
U must be the eigenvector matrix for AAT. The eigenvalue matrix in the middle is ΣΣT—,pos_def_matrices
which is m by m with σ2,pos_def_matrices
r on the diagonal.,pos_def_matrices
"From the ATA = VΣTΣV T, the V matrix must be the eigenvector matrix for ATA. The",pos_def_matrices
diagonal matrix ΣTΣ has the same σ2,pos_def_matrices
"r , but it is n by n.",pos_def_matrices
Remark 5. Here is the reason that Avj = σ ju j. Start with ATAvj = σ2,pos_def_matrices
AATAv j = σ 2,pos_def_matrices
This says that Avj is an eigenvector of AAT! We just moved parentheses to (AAT)(Avj).,pos_def_matrices
"The length of this eigenvector Avj is σ j, because",pos_def_matrices
vTATAv j = σ2,pos_def_matrices
"So the unit eigenvector is Avj/σj = u j. In other words, AV = UΣ.",pos_def_matrices
Example 1. This A has only one column: rank r = 1. Then Σ has only σ1 = 3:,pos_def_matrices
"ATA is 1 by 1, whereas AAT is 3 by 3. They both have eigenvalue 9 (whose square root",pos_def_matrices
is the 3 in Σ). The two zero eigenvalues of AAT leave some freedom for the eigenvectors,pos_def_matrices
in columns 2 and 3 of U. We kept that matrix orthogonal.,pos_def_matrices
"Example 2. Now A has rank 2, and AAT =",pos_def_matrices
with λ = 3 and 1:,pos_def_matrices
= UΣV T = 1,pos_def_matrices
1. The columns of U are left singular vectors (unit eigenvectors of,pos_def_matrices
AAT). The columns of V are right singular vectors (unit eigenvectors of ATA).,pos_def_matrices
Application of the SVD,pos_def_matrices
"We will pick a few important applications, after emphasizing one key point. The SVD is",pos_def_matrices
terriﬁc for numerically stable computations. because U and V are orthogonal matrices.,pos_def_matrices
"They never change the length of a vector. Since ∥Ux∥2 = xTUTUx = ∥x∥2, multiplication",pos_def_matrices
by U cannot destroy the scaling.,pos_def_matrices
6.3 Singular Value Decomposition,pos_def_matrices
"Of course Σ could multiply by a large σ or (more commonly) divide by a small σ,",pos_def_matrices
and overﬂow the computer. But still Σ is as good as possible. It reveals exactly what is,pos_def_matrices
large and what is small. The ratio σmax/σmin is the condition number of an invertible n,pos_def_matrices
by n matrix. The availability of that information is another reason for the popularity of,pos_def_matrices
the SVD. We come back to this in the second application.,pos_def_matrices
"1. Image processing Suppose a satellite takes a picture, and wants to send it to Earth.",pos_def_matrices
"The picture may contain 1000 by 1000 “pixels”—a million little squares, each with a",pos_def_matrices
"deﬁnite color. We can code the colors, and send back 1,000,000 numbers. It is better to",pos_def_matrices
"ﬁnd the essential information inside the 1000 by 1000 matrix, and send only that.",pos_def_matrices
"Suppose we know the SVD. The key is in the singular values (in Σ). Typically, some",pos_def_matrices
"σ’s are signiﬁcant and others are extremely small. If we keep 20 and throw away 980,",pos_def_matrices
then we send only the corresponding 20 columns of U and V. The other 980 columns,pos_def_matrices
are multiplied in UΣV T by the small σ’s that are being ignored. We can do the matrix,pos_def_matrices
multiplication as columns times rows:,pos_def_matrices
A = UΣV T = u1σ1vT,pos_def_matrices
"Any matrix is the sum of r matrices of rank 1. If only 20 terms are kept, we send 20",pos_def_matrices
times 2000 numbers instead of a million (25 to 1 compression).,pos_def_matrices
"The pictures are really striking, as more and more singular values are included. At",pos_def_matrices
"ﬁrst you see nothing, and suddenly you recognize everything. The cost is in computing",pos_def_matrices
"the SVD—this has become much more efﬁcient, but it is expensive for a big matrix.",pos_def_matrices
"2. The effective rank The rank of a matrix is the number of independent rows, and",pos_def_matrices
the number of independent columns. That can be hard to decide in computations! In,pos_def_matrices
"exact arithmetic, counting the pivots is correct. Real arithmetic can be misleading—but",pos_def_matrices
discarding small pivots is not the answer. Consider the following:,pos_def_matrices
"The ﬁrst has rank 1, although roundoff error will probably produce a second pivot. Both",pos_def_matrices
"pivots will be small; how many do we ignore? The second has one small pivot, but we",pos_def_matrices
"cannot pretend that its row is insigniﬁcant. The third has two pivots and its rank is 2, but",pos_def_matrices
its “effective rank” ought to be 1.,pos_def_matrices
"We go to a more stable measure of rank. The ﬁrst step is to use ATA or AAT, which",pos_def_matrices
are symmetric but share the same rank as A. Their eigenvalues—the singular values,pos_def_matrices
"squared—are not misleading. Based on the accuracy of the data, we decide on a toler-",pos_def_matrices
ance like 10−6 and count the singular values above it—that is the effective rank. The,pos_def_matrices
examples above have effective rank 1 (when ε is very small).,pos_def_matrices
3. Polar decomposition Every nonzero complex number z is a positive number r times,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
a number eiθ on the unit circle: z = reiθ. That expresses z in “polar coordinates.” If,pos_def_matrices
"we think of z as a 1 by 1 matrix, r corresponds to a positive deﬁnite matrix and eiθ",pos_def_matrices
"corresponds to an orthogonal matrix. More exactly, since eiθ is complex and satisﬁes",pos_def_matrices
"e−iθeiθ = 1, it forms a 1 by 1 unitary matrix: UHU = I. We take the complex conjugate",pos_def_matrices
"as well as the transpose, for UH.",pos_def_matrices
The SVD extends this “polar factorization” to matrices of any size:,pos_def_matrices
"Every real square matrix can be factored into A = QS, where Q is orthogonal",pos_def_matrices
and S is symmetric positive semideﬁnite. If A is invertible then S is positive,pos_def_matrices
For proof we just insert V TV = I into the middle of the SVD:,pos_def_matrices
A = UΣV T = (UV T)(VΣV T).,pos_def_matrices
The factor S =VΣV T is symmetric and semideﬁnite (because Σ is). The factor Q =UV T,pos_def_matrices
"is an orthogonal matrix (because QTQ =VUTUV T = I). In the complex case, S becomes",pos_def_matrices
Hermitian instead of symmetric and Q becomes unitary instead of orthogonal. In the,pos_def_matrices
invertible case Σ is deﬁnite and so is S.,pos_def_matrices
Example 3. Polar decomposition:,pos_def_matrices
Example 4. Reverse polar decomposition:,pos_def_matrices
"The exercises show how, in the reverse order. S changes but Q remains the same. Both",pos_def_matrices
S and S′ are symmetric positive deﬁnite because this A is invertible.,pos_def_matrices
Application of A = QS:,pos_def_matrices
A major use of the polar decomposition is in continuum,pos_def_matrices
"mechanics (and recently in robotics). In any deformation, it is important to separate",pos_def_matrices
"stretching from rotation, and that is exactly what QS achieves. The orthogonal matrix",pos_def_matrices
"Q is a rotation, and possibly a reﬂection. The material feels no strain. The symmetric",pos_def_matrices
"matrix S has eigenvalues σ1,...,σr, which are the stretching factors (or compression",pos_def_matrices
factors). The diagonalization that displays those eigenvalues is the natural choice of,pos_def_matrices
axes—called principal axes: as in the ellipses of Section 6.2. It is S that requires work,pos_def_matrices
"on the material, and stores up elastic energy.",pos_def_matrices
"We note that S2 is ATA, which is symmetric positive deﬁnite when A is invertible. S",pos_def_matrices
"is the symmetric positive deﬁnite square root of ATA, and Q is AS−1. In fact, A could be",pos_def_matrices
"rectangular, as long as ATA is positive deﬁnite. (That is the condition we keep meeting,",pos_def_matrices
6.3 Singular Value Decomposition,pos_def_matrices
"that A must have independent columns.) In the reverse order A = S′Q, the matrix S′ is",pos_def_matrices
the symmetric positive deﬁnite square root of AAT.,pos_def_matrices
4. Least Squares For a rectangular system Ax = b. the least-squares solution comes,pos_def_matrices
from the normal equations ATA�x = ATb. If A has dependent columns then ATA is not,pos_def_matrices
invertible and �x is not determined. Any vector in the nullspace could be added to �x. We,pos_def_matrices
"can now complete Chapter 3, by choosing a “best” (shortest) �x for every Ax = b.",pos_def_matrices
Ax = b has two possible difﬁculties: Dependent rows or dependent columns. With,pos_def_matrices
"dependent rows, Ax = b may have no solution. That happens when b is outside the",pos_def_matrices
column space of A. Instead of Ax = b. we solve ATA�x = ATb. But if A has dependent,pos_def_matrices
"columns, this �x will not be unique. We have to choose a particular solution of ATA�x =",pos_def_matrices
"ATb, and we choose the shortest.",pos_def_matrices
The optimal solution of Ax = b is the minimum length solution of ATA�x = ATb.,pos_def_matrices
That minimum length solution will be called x+. It is our preferred choice as the best,pos_def_matrices
"solution to Ax = b (which had no solution), and also to ATA�x = ATb (which had too",pos_def_matrices
many). We start with a diagonal example.,pos_def_matrices
"Example 5. A is diagonal, with dependent rows and dependent columns:",pos_def_matrices
"The columns all end with zero. In the column space, the closest vector to b = (b1,b2,b3)",pos_def_matrices
"is p = (b1,b2,0). The best we can do with Ax = b is to solve the ﬁrst two equations,",pos_def_matrices
"since the third equation is 0 = b3. That error cannot be reduced, but the errors in the ﬁrst",pos_def_matrices
two equations will be zero. Then,pos_def_matrices
"Now we face the second difﬁculty. To make �x as short as possible, we choose the",pos_def_matrices
totally arbitrary �x3 and �x4 to be zero. The minimum length solution is x+:,pos_def_matrices
x+ = A+b is shortest,pos_def_matrices
"This equation ﬁnds x+, and it also displays the matrix that produces x+ from b. That",pos_def_matrices
"matrix is the pseudoinverse A+ of our diagonal A. Based on this example, we know Σ+",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
and x+ for any diagonal matrix Σ:,pos_def_matrices
"The matrix Σ is m by n, with r nonzero entries σi. Its pseudoinverse Σ+ is n by m, with",pos_def_matrices
r nonzero entries 1/σi. All the blank spaces are zeros. Notice that (Σ+)+ is Σ again.,pos_def_matrices
"That is like (A−1)−1 = A, but here A is not invertible.",pos_def_matrices
Now we ﬁnd x+ in the general case. We claim that the shortest solution x+ is always,pos_def_matrices
in the row space of A. Remember that any vector �x can be split into a row space compo-,pos_def_matrices
nent xr and a nullspace component: �x = xr +xn. There are three important points about,pos_def_matrices
"1. The row space component also solves ATA�xr = ATb, because Axn = 0.",pos_def_matrices
"2. The components are orthogonal, and they obey Pythagoras’s law:",pos_def_matrices
"∥�x∥2 = ∥xr∥2 +∥xn∥2,",pos_def_matrices
so �x is shortest when xn = 0.,pos_def_matrices
3. All solutions of ATA�x = ATb have the same xr. That vector is x+.,pos_def_matrices
The fundamental theorem of linear algebra was in Figure 3.4. Every p in the column,pos_def_matrices
space comes from one and only one vector xr in the row space. All we are doing is to,pos_def_matrices
"choose that vector, x+ = xr, as the best solution to Ax = b.",pos_def_matrices
The pseudoinverse in Figure 6.3 starts with b and comes back to x+. It inverts A where,pos_def_matrices
A is invertible—between row space and column space. The pseudoinverse knocks out,pos_def_matrices
"the left nullspace by sending it to zero, and it knocks out the nullspace by choosing xr as",pos_def_matrices
We have not yet shown that there is a matrix A+ that always gives x+—but there is.,pos_def_matrices
"It will be n by m, because it takes b and p in Rm back to x+ in Rn. We look at one more",pos_def_matrices
example before ﬁnding A+ in general.,pos_def_matrices
"Example 6. Ax = b is −x1 +2x2 +2x3 = 18, with a whole plane of solutions.",pos_def_matrices
"According to our theory, the shortest solution should be in the row space of A =",pos_def_matrices
"[−1 2 2]. The multiple of that row that satisﬁes the equation is x+ = (−2,4,4). There",pos_def_matrices
"are longer solutions like (−2,5,3), (−2,7,1), or (−6,3,3), but they all have nonzero",pos_def_matrices
components from the nullspace. The matrix that produces x+ from b = [18] is the pseu-,pos_def_matrices
"doinverse A+. Whereas A was 1 by 3, this A+ is 3 by 1:",pos_def_matrices
6.3 Singular Value Decomposition,pos_def_matrices
Figure 6.3: The pseudoinverse A+ inverts A where it can on the column space.,pos_def_matrices
The row space of A is the column space of A+. Here is a formula for A+:,pos_def_matrices
"If A = UΣV T (the SVD), then its pseudoinverse is A+ = VΣ+UT.",pos_def_matrices
Example 6 had σ = 3—the square root of the eigenvalue of AAT = [9]. Here it is again,pos_def_matrices
with Σ and Σ+:,pos_def_matrices
= UΣV T =,pos_def_matrices
The minimum length least-squares solution is x+ = A+b = VΣ+UTb.,pos_def_matrices
Proof. Multiplication by the orthogonal matrix UT leaves lengths unchanged:,pos_def_matrices
∥Ax−b∥ = ∥UΣV Tx−b∥ = ∥ΣV Tx−UTb∥.,pos_def_matrices
"Introduce the new unknown y = V Tx = V −1x, which has the same length as x. Then,",pos_def_matrices
minimizing ∥Ax−b∥ is the same as minimizing ∥Σy−UTb∥. Now Σ is diagonal and we,pos_def_matrices
know the best y+. It is y+ = Σ+UTb so the best x+ is Vy+:,pos_def_matrices
x+ = Vy+ = VΣ+UTb = A+b.,pos_def_matrices
"Vy+ is in the row space, and ATAx+ = ATb from the SVD.",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
Problems 1–2 compute the SVD of a square singular matrix A.,pos_def_matrices
1. Compute ATA and its eigenvalues σ2,pos_def_matrices
"1, 0 and unit eigenvectors v1, v2:",pos_def_matrices
2. (a) Compute AAT and its eigenvalues σ2,pos_def_matrices
"1, 0 and unit eigenvectors u1, u2.",pos_def_matrices
(b) Choose signs so that Av1 = σ1u1 and verify the SVD:,pos_def_matrices
"(c) Which four vectors give orthonormal bases for C(A), N(A), C(AT), N(AT)?",pos_def_matrices
Problems 3–5 ask for the SVD of matrices of rank 2.,pos_def_matrices
"3. Find the SVD from the eigenvectors v1, v2 of ATA and Avi = σiui:",pos_def_matrices
4. Use the SVD part of the MATLAB demo eigshow (or Java on the course page,pos_def_matrices
web.mit.edu/18.06) to ﬁnd the same vectors v1 and v2 graphically.,pos_def_matrices
"5. Compute ATA and AAT, and their eigenvalues and unit eigenvectors, for",pos_def_matrices
Multiply the three matrices UΣV T to recover A.,pos_def_matrices
Problems 6–13 bring out the underlying ideas of the SVD.,pos_def_matrices
"6. Suppose u1,...,un and v1,...,vn are orthonormal bases for Rn. Construct the matrix",pos_def_matrices
"A that transforms each vj into u j to give Av1 = u1,...,Avn = un.",pos_def_matrices
7. Construct the matrix with rank 1 that has Av = 12u for v = 1,pos_def_matrices
"2(1,1,1,1) and u =",pos_def_matrices
"3(2,2,1). Its only singular value is σ1 =",pos_def_matrices
"8. Find UΣV T if A has orthogonal columns w1,...,wn of lengths σ1,...,σn.",pos_def_matrices
9. Explain how UΣV T expresses A as a sum of r rank-1 matrices in equation (3):,pos_def_matrices
6.3 Singular Value Decomposition,pos_def_matrices
10. Suppose A is a 2 by 2 symmetric matrix with unit eigenvectors u1 and u2. If its,pos_def_matrices
"eigenvalues are λ1 = 3 and λ2 = −2, what are U, Σ, and V T?",pos_def_matrices
11. Suppose A is invertible (with σ1 > σ2 > 0). Change A by as small a matrix as possible,pos_def_matrices
to produce a singular matrix A0. Hint: U and V do not change:,pos_def_matrices
"12. (a) If A changes to 4A, what is the change in the SVD?",pos_def_matrices
(b) What is the SVD for AT and for A−1?,pos_def_matrices
13. Why doesn’t the SVD for A+I just use Σ+I?,pos_def_matrices
14. Find the SVD and the pseudoinverse 0+ of the m by n zero matrix.,pos_def_matrices
15. Find the SVD and the pseudoinverse VΣ+UT of,pos_def_matrices
1 1 1 1,pos_def_matrices
"16. If an m by n matrix Q has orthonormal columns, what is Q+?",pos_def_matrices
17. Diagonalize ATA to ﬁnd its positive deﬁnite square root S = VΣ1/2V T and its polar,pos_def_matrices
decomposition A = QS:,pos_def_matrices
18. What is the minimum-length least-squares solution x+ = A+b to the following?,pos_def_matrices
"You can compute A+, or ﬁnd the general solution to ATA�x = ATb and choose the",pos_def_matrices
solution that is in the row space of A. This problem ﬁts the best plane C+Dt +Ez to,pos_def_matrices
b = 0 and also b = 2 at t = z = 0 (and b = 2 at t = z = 1).,pos_def_matrices
"(a) If A has independent columns, its left-inverse (ATA)−1AT is A+.",pos_def_matrices
"(b) If A has independent rows, its right-inverse AT(AAT)−1 is A+.",pos_def_matrices
"In both cases, verify that x+ = A+b is in the row space. and ATAx+ = ATb.",pos_def_matrices
19. Split A = UΣV T into its reverse polar decomposition QS′.,pos_def_matrices
20. Is (AB)+ = B+A+ always true for pseudoinverses? I believe not.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"21. Removing zero rows of U leaves A = LU, where the r columns or L span the column",pos_def_matrices
space of A and the r rows of U span the row space. Then A+ has the explicit formula,pos_def_matrices
"Why is A+b in the row space with UT at the front? Why does ATAA+b = ATb, so",pos_def_matrices
that x+ = A+b satisﬁes the normal equation as it should?,pos_def_matrices
22. Explain why AA+ and A+A are projection matrices (and therefore symmetric). What,pos_def_matrices
fundamental subspaces do they project onto?,pos_def_matrices
In this section we escape for the ﬁrst time from linear equations. The unknown x will not,pos_def_matrices
"be given as the solution to Ax = b or Ax = λx. Instead, the vector x will be determined",pos_def_matrices
by a minimum principle.,pos_def_matrices
It is astonishing how many natural laws can be expressed as minimum principles. Just,pos_def_matrices
the fact that heavy liquids sink to the bottom is a consequence of minimizing their po-,pos_def_matrices
"tential energy. And when you sit on a chair or lie on a bed, the springs adjust themselves",pos_def_matrices
so that the energy is minimized. A straw in a glass of water looks bent because light,pos_def_matrices
reaches your eye as quickly as possible. Certainly there are more highbrow examples:,pos_def_matrices
The fundamental principle of structural engineering is the minimization of total energy.1,pos_def_matrices
We have to say immediately that these “energies” are nothing but positive deﬁnite,pos_def_matrices
quadratic functions. And the derivative of a quadratic is linear. We get back to the,pos_def_matrices
"familiar linear equations, when we set the ﬁrst derivatives to zero. Our ﬁrst goal in",pos_def_matrices
"this section is to ﬁnd the minimum principle that is equivalent to Ax = b, and the",pos_def_matrices
minimization equivalent to Ax = λx. We will be doing in ﬁnite dimensions exactly,pos_def_matrices
"what the theory of optimization does in a continuous problem, where “ﬁrst derivatives",pos_def_matrices
"= 0” gives a differential equation. In every problem, we are free to solve the linear",pos_def_matrices
equation or minimize the quadratic.,pos_def_matrices
The ﬁrst step is straightforward: We want to ﬁnd the “parabola” P(x) whose minimum,pos_def_matrices
"occurs when Ax = b. If A is just a scalar, that is easy to do:",pos_def_matrices
has zero slope when,pos_def_matrices
dx = Ax−b = 0.,pos_def_matrices
This point x = A−1b will be a minimum if A is positive. Then the parabola P(x) opens,pos_def_matrices
upward (Figure 6.4). In more dimensions this parabola turns into a parabolic bowl (a,pos_def_matrices
"paraboloid). To assure a minimum of P(x), not a maximum or a saddle point, A must be",pos_def_matrices
1I am convinced that plants and people also develop in accordance with minimum principles. Perhaps civilization,pos_def_matrices
is based on a law of least action. There must be new laws (and minimum principles) to be found in the social,pos_def_matrices
sciences and life sciences.,pos_def_matrices
"If A is symmetric positive deﬁnite, then P(x) = 1",pos_def_matrices
2xTAx − xTb reaches its,pos_def_matrices
minimum at the point where Ax = b. At that point Pmin = −1,pos_def_matrices
Figure 6.4: The graph of a positive quadratic P(x) is a parabolic bowl.,pos_def_matrices
"Proof. Suppose Ax = b. For any vector y, we show that P(y) ≥ P(x):",pos_def_matrices
(set b = Ax),pos_def_matrices
This can’t be negative since A is positive deﬁnite—and it is zero only if y−x = 0. At all,pos_def_matrices
"other points P(y) is larger than P(x), so the minimum occurs at x.",pos_def_matrices
Example 1. Minimize P(x) = x2,pos_def_matrices
1 − x1x2 + x2,pos_def_matrices
"2 − b1x1 − b2x2. The usual approach, by",pos_def_matrices
"calculus, is to set the partial derivatives to zero. This gives Ax = b:",pos_def_matrices
∂P/∂x1 = 2x1 −x2 −b1 = 0,pos_def_matrices
∂P/∂x2 = −x1 +2x2 −b2 = 0,pos_def_matrices
Linear algebra recognizes this P(x) as 1,pos_def_matrices
"2xTAx−xTb, and knows immediately that Ax = b",pos_def_matrices
gives the minimum. Substitute x = A−1b into P(x):,pos_def_matrices
2xTAx is the internal energy and −xTb is the external work. The system,pos_def_matrices
"automatically goes to x = A−1b, where the total energy P(x) is a minimum.",pos_def_matrices
Many applications add extra equations Cx = d on top of the minimization problem.,pos_def_matrices
These equations are constraints. We minimize P(x) subject to the extra requirement,pos_def_matrices
Cx = d. Usually x can’t satisfy n equations Ax = b and also ℓ extra constraints Cx = d.,pos_def_matrices
We have too many equations and we need ℓ more unknowns.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"Those new unknowns y1,...,yℓ are called Lagrange multipliers. They build the",pos_def_matrices
"constraint into a function L(x,y). This was the brilliant insight of Lagrange:",pos_def_matrices
"L(x,y) = P(x)+yT(Cx−d) = 1",pos_def_matrices
That term in L is chosen exactly so that ∂L/∂y = 0 brings back Cx = d. When we set,pos_def_matrices
"the derivatives of L to zero, we have n+ℓ equations for n+ℓ unknowns x and y:",pos_def_matrices
∂L/∂x = 0 :,pos_def_matrices
∂L/∂y = 0 :,pos_def_matrices
The ﬁrst equations involve the mysterious unknowns y. You might well ask what they,pos_def_matrices
represent. Those “dual unknowns” y tell how much the constrained minimum PC/min,pos_def_matrices
(which only allows x when Cx = d) exceeds the unconstrained Pmin (allowing all x):,pos_def_matrices
PC/min = Pmin + 1,pos_def_matrices
"Example 2. Suppose P(x1,x2) = 1",pos_def_matrices
2. Its smallest value is certainly Pmin = 0.,pos_def_matrices
"This unconstrained problem has n = 2, A = I, and b = 0. So the minimizing equation",pos_def_matrices
Ax = b just gives x1 = 0 and x2 = 0.,pos_def_matrices
Now add one constraint c1x1 + c2x2 = d. This puts x on a line in the x1-x2 plane.,pos_def_matrices
"The old minimizer x1 = x2 = 0 is not on the line. The Lagrangian function L(x,y) =",pos_def_matrices
2 +y(c1x1 +c2x2 −d) has n+ℓ = 2+1 partial derivatives:,pos_def_matrices
x1 +c1y = 0,pos_def_matrices
x2 +c2y = 0,pos_def_matrices
c1x1 +c2x2 = d.,pos_def_matrices
Substituting x1 = −c1y and x2 = −c2y into the third equation gives −c2,pos_def_matrices
The constrained minimum of P = 1,pos_def_matrices
2xTx is reached at that solution point:,pos_def_matrices
"2yd as predicted in equation (5), since b = 0 and Pmin = 0.",pos_def_matrices
"Figure 6.5 shows what problem the linear algebra has solved, if the constraint keeps",pos_def_matrices
"x on a line 2x1 −x2 = 5. We are looking for the closest point to (0,0) on this line. The",pos_def_matrices
"solution is x = (2,−1). We expect this shortest vector x to be perpendicular to the line,",pos_def_matrices
and we are right.,pos_def_matrices
Figure 6.5: Minimizing 1,pos_def_matrices
2∥x∥2 for all x on the constraint line 2x1 −x2 = 5.,pos_def_matrices
"In minimization, our big application is least squares. The best �x is the vector that mini-",pos_def_matrices
mizes the squared error E2 = ∥Ax−b∥2. This is a quadratic and it ﬁts our framework! I,pos_def_matrices
will highlight the parts that look new:,pos_def_matrices
E2 = (Ax−b)T(Ax−b) = xTATAx−2xTATb+bTb.,pos_def_matrices
"2xTAx−xTb at the start of the section, which led to Ax = b:",pos_def_matrices
A changes to ATA,pos_def_matrices
b changes to ATb,pos_def_matrices
The constant bTb raises the whole graph—this has no effect on the best �x. The other,pos_def_matrices
"two changes, A to ATA and b to ATb, give a new way to reach the least-squares equation",pos_def_matrices
(normal equation). The minimizing equation Ax = b changes into the,pos_def_matrices
Optimization needs a whole book. We stop while it is pure linear algebra.,pos_def_matrices
Our second goal is to ﬁnd a minimization problem equivalent to Ax = λx. That is not so,pos_def_matrices
"easy. The function to minimize cannot be a quadratic, or its derivative would be linear—",pos_def_matrices
and the eigenvalue problem is nonlinear (λ times x). The trick that succeeds is to divide,pos_def_matrices
one quadratic by another one:,pos_def_matrices
6I Rayleigh’s Principle: The minimum value of the Rayleigh quotient is,pos_def_matrices
the smallest eigenvalue λ1. R(x) reaches that minimum at the ﬁrst eigenvector,pos_def_matrices
Minimum where Ax1 = λx1,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
"If we keep xTAx = 1, then R(x) is a minimum when xTx = ∥x∥2 is as large as possible.",pos_def_matrices
We are looking for the point on the ellipsoid xTAx = 1 farthest from the origin—the,pos_def_matrices
"vector x of greatest length. From our earlier description of the ellipsoid, its longest axis",pos_def_matrices
points along the ﬁrst eigenvector. So R(x) is a minimum at x1.,pos_def_matrices
"Algebraically, we can diagonalize the symmetric A by an orthogonal matrix: QTAQ =",pos_def_matrices
Λ. Then set x = Qy and the quotient becomes simple:,pos_def_matrices
"The minimum of R is λ1, at the point where y1 = 1 and y2 = ··· = yn = 0:",pos_def_matrices
The Rayleigh quotient in equation (11) is never below λ1 and never above λn (the largest,pos_def_matrices
eigenvalue). Its minimum is at the eigenvector x1 and its maximum is at xn:,pos_def_matrices
Maximum where Axn = λnxn,pos_def_matrices
"One small yet important point: The Rayleigh quotient equals a11, when the trial vector",pos_def_matrices
"is x = (1,0,...,0). So a11 (on the main diagonal) is between λ1 and λn. You can see this",pos_def_matrices
"in Figure 6.6, where the horizontal distance to the ellipse (where a11x2 = 1) is between",pos_def_matrices
the shortest distance and the longest distance:,pos_def_matrices
λ1 ≤ a11 ≤ λn.,pos_def_matrices
The diagonal entries of any symmetric matrix are between λ1 and λn. We drew Figure,pos_def_matrices
6.6 for a 2 by 2 positive deﬁnite matrix to see it clearly.,pos_def_matrices
Intertwining of the Eigenvalues,pos_def_matrices
"The intermediate eigenvectors x2,...,xn−1 are saddle points of the Rayleigh quotient",pos_def_matrices
"(zero derivatives, but not minima or maxima). The difﬁculty with saddle points is that",pos_def_matrices
we have no idea whether R(x) is above or below them. That makes the intermediate,pos_def_matrices
"eigenvalues λ2,...,λn−1 harder to estimate.",pos_def_matrices
"For this optional topic, the key is to ﬁnd a constrained minimum or maximum. The",pos_def_matrices
constraints come from the basic property of symmetric matrices: xj is perpendicular to,pos_def_matrices
The minimum of R(x) subject to xTx1 = 0 is λ2. The minimum of R(x),pos_def_matrices
subject to any other constraint xTv = 0 is not above λ2:,pos_def_matrices
Figure 6.6: The farthest x = x1/,pos_def_matrices
λ1 and the closet x = xn/√λn both give xTAx = xTλx = 1. These are the major,pos_def_matrices
axes of the ellipse.,pos_def_matrices
This “maximin principle” makes λ2 the maximum over all v of the minimum of R(x) with,pos_def_matrices
xTv = 0. That offers a way to estimate λ2 without knowing λ1.,pos_def_matrices
Example 3. Throw away the last row and column of any symmetric matrix:,pos_def_matrices
The second eigenvalue λ2(A) = 2 is above the lowest eigenvalue λ1(B) = 1. The lowest,pos_def_matrices
eigenvalue λ1(A) = 2−,pos_def_matrices
2 is below λ1(B). So λ1(B) is caught between.,pos_def_matrices
"This example chose v = (0,0,1) so the constraint xTv = 0 knocked out the third com-",pos_def_matrices
ponent of x (thereby reducing A to B).,pos_def_matrices
The complete picture is an intertwining of eigenvalues:,pos_def_matrices
λ1(A) ≤ λ1(B) ≤ λ2(A) ≤ λ2(B) ≤ ··· ≤ λn−1(B) ≤ λn(A).,pos_def_matrices
"This has a natural interpretation for an ellipsoid, when it is cut by a plane through the",pos_def_matrices
origin. The cross section is an ellipsoid of one lower dimension. The major axis Of this,pos_def_matrices
cross section cannot be longer than the major axis of the whole ellipsoid: λ1(B) ≥ λ1(A).,pos_def_matrices
But the major axis of the cross section is at least as long as the second axis of the original,pos_def_matrices
ellipsoid: λ1(B) ≤ λ2(A). Similarly the minor axis of the cross section is smaller than,pos_def_matrices
"the original second axis, and larger than the original minor axis: λ2(A) ≤ λ2(B) ≤ λ3(A).",pos_def_matrices
"You can see the same thing in mechanics. When springs and masses are oscillating,",pos_def_matrices
suppose one mass is held at equilibrium. Then the lowest frequency is increased but not,pos_def_matrices
"above λ2. The highest frequency is decreased, but not below λn−1.",pos_def_matrices
"We close with three remarks, I hope your intuition says that they are correct.",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
Remark 1. The maximin principle extends to j-dimensional subspaces S j:,pos_def_matrices
Remark 2. There is also a minimax principle for λn−j:,pos_def_matrices
"If j = 1, we are maximizing R(x) over one constraint xTv = 0. That maximum is between",pos_def_matrices
the unconstrained λn−1 and λn. The toughest constraint makes x perpendicular to the top,pos_def_matrices
eigenvector v = xn. Then the best x is the next eigenvector xn−1. The “minimum of the,pos_def_matrices
"Remark 3. For the generalized problem Ax = λMx, the same principles hold if M is",pos_def_matrices
"positive deﬁnite. In the Rayleigh quotient, xTx becomes xTMx:",pos_def_matrices
"Even for unequal masses in an oscillating system (M ̸= I), holding one mass at equilib-",pos_def_matrices
rium will raise the lowest frequency and lower the highest frequency.,pos_def_matrices
1. Consider the system Ax = b given by,pos_def_matrices
"Construct the corresponding quadratic P(x1,x2,x3), compute its partial derivatives",pos_def_matrices
"∂P/∂xi, and verify that they vanish exactly at the desired solution.",pos_def_matrices
2. Complete the square in P = 1,pos_def_matrices
2xTAx − xTb = 1,pos_def_matrices
2(x − A−1b)TA(x − A−1b) + constant.,pos_def_matrices
This constant equals Pmin because the term before it is never negative. (Why?),pos_def_matrices
"3. Find the minimum, if there is one of P1 = 1",pos_def_matrices
2x2+xy+y2−3y and P2 = 1,pos_def_matrices
matrix A is associated with P2?,pos_def_matrices
4. (Review) Another quadratic that certainly has its minimum at Ax = b is,pos_def_matrices
"Comparing Q with P, and ignoring the constant 1",pos_def_matrices
"2bTb, what system of equations do",pos_def_matrices
we get at the minimum of Q? What are these equations called in the theory of least,pos_def_matrices
"5. For any symmetric matrix A, compute the ratio R(x) for the special choice x =",pos_def_matrices
"(1,...,1). How is the sum of all entries aij related to λ1 and λn?",pos_def_matrices
6. With A =,pos_def_matrices
", ﬁnd a choice of x that gives a smaller R(x) than the bound λ1 ≤ 2",pos_def_matrices
that comes from the diagonal entries. What is the minimum value of R(x)?,pos_def_matrices
"7. If B is positive deﬁnite, show from the Rayleigh quotient that the smallest eigenvalue",pos_def_matrices
of A+B is larger than the smallest eigenvalue of A.,pos_def_matrices
"8. If λ1 and µ1 are the smallest eigenvalues of A and B, show that the smallest eigen-",pos_def_matrices
value θ1 of A+B is at least as large as λ1 + µ1. (Try the corresponding eigenvector,pos_def_matrices
x in the Rayleigh quotients.),pos_def_matrices
Note. Problems 7 and 8 are perhaps the most typical and most important results,pos_def_matrices
"that come easily from Rayleigh’s principle, but only with great difﬁculty from the",pos_def_matrices
"9. If B is positive deﬁnite, show from the minimax principle (12) that the second small-",pos_def_matrices
est eigenvalue is increased by adding B : λ2(A+B) > λ2(A).,pos_def_matrices
"10. If you throw away two rows and columns of A, what inequalities do you expect",pos_def_matrices
between the smallest eigenvalue µ of the new matrix and the original λ’s?,pos_def_matrices
11. Find the minimum values of,pos_def_matrices
12. Prove from equation (11) that R(x) is never larger than the largest eigenvalue λn.,pos_def_matrices
13. The minimax principle for λj involves j-dimensional subspaces S j:,pos_def_matrices
Equivalent to equation (15),pos_def_matrices
λ j = min,pos_def_matrices
x in S j R(x),pos_def_matrices
"(a) If λj is positive, infer that every S j contains a vector x with R(x) > 0.",pos_def_matrices
(b) Deduce that S j contains a vector y = C−1x with yTcTACy/yTy > 0.,pos_def_matrices
"(c) Conclude that the jth eigenvalue of CTAC, from its minimax principle, is also",pos_def_matrices
positive—proving again the law of inertia in Section 6.2.,pos_def_matrices
14. Show that the smallest eigenvalue λ1 of Ax = λMx is not larger than the ratio a11/m11,pos_def_matrices
of the corner entries.,pos_def_matrices
15. Which particular subspace S2 in Problem 13 gives the minimum value λ2? In other,pos_def_matrices
"words, over which S2 is the maximum of R(x) equal to λ2?",pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
16. (Recommended) From the zero submatrix decide the signs of the n eigenvalues:,pos_def_matrices
17. (Constrained minimum) Suppose the unconstrained minimum x = A−1b happens to,pos_def_matrices
satisfy the constraint Cx = d. Verify that equation (5) correctly gives PC/min = Pmin;,pos_def_matrices
the correction term is zero.,pos_def_matrices
The Finite Element Method,pos_def_matrices
There were two main ideas in the preceding section on minimum principles:,pos_def_matrices
(i) Solving Ax = b is equivalent to minimizing P(x) = 1,pos_def_matrices
(ii) Solving Ax = λ1x is equivalent to minimizing R(x) = xTAx/xTx.,pos_def_matrices
Now we try to explain how these ideas can be applied.,pos_def_matrices
"The story is a long one, because these principles have been known for more than a",pos_def_matrices
"century. In engineering problems like plate bending, or physics problems like the ground",pos_def_matrices
"state (eigenfunction) of an atom, minimization was used to get a rough approximation",pos_def_matrices
to the true solution. The approximations had to be rough; the computers were human.,pos_def_matrices
"The principles (i) and (ii) were there, but they could not be implemented.",pos_def_matrices
Obviously the computer was going to bring about a revolution. It was the method,pos_def_matrices
"of ﬁnite differences that jumped ahead, because it is easy to “discretize” a differential",pos_def_matrices
"equation. Already in Section 1.7, derivatives were replaced by differences. The physical",pos_def_matrices
"region is covered by a mesh, and u′′ = f(x) became u j+1 − 2u j + u j−1 = h2 f j. The",pos_def_matrices
1950s brought new ways to solve systems Au = f that are very large and very sparse—,pos_def_matrices
algorithms and hardware are both much faster now.,pos_def_matrices
What we did not fully recognize was that even ﬁnite differences become incredibly,pos_def_matrices
"complicated for real engineering problems, like the stresses on an airplane. The real",pos_def_matrices
"difﬁculty is not to solve the equations, but to set them up. For an irregular region we",pos_def_matrices
piece the mesh together from triangles or quadrilaterals or tetrahedra. Then we need a,pos_def_matrices
systematic way to approximate the underlying physical laws. The computer has to help,pos_def_matrices
"not only in the solution of Au = f and Ax = λx, but in its formulation.",pos_def_matrices
"You can guess what happened. The old methods came back, with a new idea and a",pos_def_matrices
new name. The new name is the ﬁnite element method. The new ides uses more of,pos_def_matrices
"the power of the computer—in constructing a discrete approximation, solving it, and",pos_def_matrices
displaying the results—than any other technique in scientiﬁc computation2. If the basic,pos_def_matrices
2Please forgive this enthusiasm: I know the method may not be immortal.,pos_def_matrices
6.5 The Finite Element Method,pos_def_matrices
"idea is simple, the applications can be complicated. For problems on this scale, the one",pos_def_matrices
undebatable point is their cost—I am afraid a billion dollars would be a conservative,pos_def_matrices
estimate of the expense so far. I hope some readers will be vigorous enough to master,pos_def_matrices
the ﬁnite element method and put it to good use.,pos_def_matrices
"Starting from the classical Rayleigh-Ritz principle, I will introduce the new idea of ﬁnite",pos_def_matrices
elements. The equation can be −u′′ = f(x) with boundary conditions u(0) = u(1) =,pos_def_matrices
"0. This problem is inﬁnite-dimensional (the vector b is replaced by a function f, and",pos_def_matrices
the matrix A becomes −d2/dx2). We can write down the energy whose minimum is,pos_def_matrices
"required, replacing inner products vT f by integrals of v(x)f(x):",pos_def_matrices
2vTAv−vT f = 1,pos_def_matrices
P(v) is to be minimized over all functions v(x) that satisfy v(0) = v(1) = 0. The function,pos_def_matrices
that gives the minimum will be the solution u(x). The differential equation has been,pos_def_matrices
"converted to a minimum principle, and it only remains to integrate by parts:",pos_def_matrices
"The term vv′ is zero at both limits, because v is. Now",pos_def_matrices
� (v′(x))2dx is positive like xTAx.,pos_def_matrices
We are guaranteed a minimum.,pos_def_matrices
To compute the minimum exactly is equivalent to solving the differential equation ex-,pos_def_matrices
actly. The Rayleigh-Ritz principle produces an n-dimensional problem by choosing only,pos_def_matrices
"n trial functions V1(x),...,Vn(x). From all combinations V = y1V1(x) + ··· + ynVn(x),",pos_def_matrices
we look for the particular combination (call it U) that minimizes P(V). This is the key,pos_def_matrices
"idea, to minimize over a subspace of V’s instead of over all possible v(x). The function",pos_def_matrices
that gives the minimum is U(x). We hope and expect that U(x) is near the correct u(x).,pos_def_matrices
"Substituting V for v, the quadratic turns into",pos_def_matrices
The trial functions V are chosen in advance. That is the key step! The unknowns,pos_def_matrices
"y1,...,yn go into a vector y. Then P(V) = 1",pos_def_matrices
2yTAy − yTb is recognized as one of the,pos_def_matrices
quadratics we are accustomed to. The matrix entries Aij are,pos_def_matrices
jdx = coefﬁcient of,pos_def_matrices
yiy j. The components b j are,pos_def_matrices
� Vj fdx. We can certainly ﬁnd the minimum of 1,pos_def_matrices
by solving Ay = b. Therefore the Rayleigh-Ritz method has three steps:,pos_def_matrices
"1. Choose the trial functions V1,...,Vn.",pos_def_matrices
2. Compute the coefﬁcients Aij and b j.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
3. Solve Ay = b to ﬁnd U(x) = y1V1(x)+···+ynVn(x).,pos_def_matrices
"Everything depends on step 1. Unless the functions Vj(x) are extremely simple, the",pos_def_matrices
other steps will be virtually impossible. And unless some combination of the Vj is close,pos_def_matrices
"to the true solution u(x), those steps will be useless. To combine both computability",pos_def_matrices
"and accuracy, the key idea that makes ﬁnite elements successful is the use of piecewise",pos_def_matrices
polynomials as the trial functions V(x).,pos_def_matrices
The simplest and most widely used ﬁnite element is piecewise linear. Place nodes at,pos_def_matrices
"the interior points x1 = h,x2 = 2h,...,xn = nh, just as for ﬁnite differences. Then Vj is",pos_def_matrices
"the “hat function” that equals 1 at the node x j, and zero at all the other nodes (Figure",pos_def_matrices
"6.7a). It is concentrated in a small interval around its node, and it is zero everywhere",pos_def_matrices
else (including x = 0 and x = 1). Any combination y1V1 +···+ynVn must have the value,pos_def_matrices
"y j at node j (the other V’s are zero there), so its graph is easy to draw (Figure 6.7b).",pos_def_matrices
V (x) = y1V1 + · · · + y5V5,pos_def_matrices
Figure 6.7: Hat functions and their linear combinations.,pos_def_matrices
Step 2 computes the coefﬁcients Aij =,pos_def_matrices
jdx in the “stiffness matrix” A. The slope,pos_def_matrices
"j equals 1/h in the small interval to the left of x j, and −1/h in the interval to the right.",pos_def_matrices
"If these “double intervals” do not overlap, the product V ′",pos_def_matrices
j is zero and Aij = 0. Each,pos_def_matrices
hat function overlaps itself and only two neighbors:,pos_def_matrices
i = j ±1,pos_def_matrices
Then the stiffness matrix is actually tridiagonal:,pos_def_matrices
6.5 The Finite Element Method,pos_def_matrices
This looks just like ﬁnite differences! It has led to a thousand discussions about the,pos_def_matrices
relation between these two methods. More complicated ﬁnite elements—polynomials of,pos_def_matrices
higher degree. deﬁned on triangles or quadrilaterals for partial differential equations—,pos_def_matrices
also produce sparse matrices A. You could think of ﬁnite elements as a systematic way,pos_def_matrices
to construct accurate difference equations on irregular meshes. The essential thing is the,pos_def_matrices
"simplicity of these piecewise polynomials. Inside every element, their slopes arc easy to",pos_def_matrices
ﬁnd and to integrate.,pos_def_matrices
"The components b j on the right side are new. Instead of just the value of f at x j,",pos_def_matrices
"as for ﬁnite differences, they are now an average of f around that point: b j =",pos_def_matrices
"Then, in step 3, we solve the tridiagonal system Ay = b, which gives the coefﬁcients in",pos_def_matrices
the minimizing trial function U = y1V1 +···+ynVn. Connecting all these heights y j by,pos_def_matrices
"a broken line, we have the approximate solution U(x).",pos_def_matrices
"Example 1. u′′ = 2 with u(0) = u(1) = 0, and solution u(x) = x−x2.",pos_def_matrices
"The approximation will use three intervals and two hat functions, with h = 1",pos_def_matrices
matrix A is 2 by 2. The right side requires integration of the hat function times f(x) = 2.,pos_def_matrices
That produces twice the area 1,pos_def_matrices
3 under the hat:,pos_def_matrices
The solution to Ay = b is y = (2,pos_def_matrices
9). The best U(x) is 2,pos_def_matrices
"9V2, which equals 2",pos_def_matrices
mesh points. This agrees with the exact solution u(x) = x−x2 = 1,pos_def_matrices
"In a more complicated example, the approximation will not be exact at the nodes.",pos_def_matrices
But it is remarkably close. The underlying theory is explained in the author’s book An,pos_def_matrices
Analysis of the Finite Element Method (see www.wellesleycambridge.com) written,pos_def_matrices
"jointly with George Fix. Other books give more detailed applications, and the subject",pos_def_matrices
of ﬁnite elements has become an important part of engineering education. It is treated,pos_def_matrices
"in Introduction to Applied Mathematics, and also in my new book Applied Mathematics",pos_def_matrices
"and Scientiﬁc Computing. There we discuss partial differential equations, where the",pos_def_matrices
method really comes into its own.,pos_def_matrices
The Rayleigh-Ritz idea—to minimize over a ﬁnite-dimensional family of V’s in place,pos_def_matrices
of all admissible v’s—is also useful for eigenvalue problems. The true minimum of the,pos_def_matrices
Rayleigh quotient is the fundamental frequency λ1. Its approximate minimum Λ1 will,pos_def_matrices
be larger—because the class of trial functions is restricted to the V’s. This step was,pos_def_matrices
completely natural and inevitable: to apply the new ﬁnite element ideas to this long-,pos_def_matrices
established variational form of the eigenvalue problem.,pos_def_matrices
The best example of an eigenvalue problem has u(x) = sinπx and λ1 = π2:,pos_def_matrices
u(0) = u(1) = 0.,pos_def_matrices
Chapter 6 Positive Deﬁnite Matrices,pos_def_matrices
That function sinπx minimizes the Rayleigh quotient vTAv/vTv:,pos_def_matrices
"This is a ratio of potential to kinetic energy, and they are in balance at the eigenvector.",pos_def_matrices
"Normally this eigenvector would be unknown, and to approximate it we admit only the",pos_def_matrices
trial candidates V = y1V1 +···+ynVn:,pos_def_matrices
"Now we face a matrix problem: Minimize yTAy/yTMy. With M = I, this leads to the",pos_def_matrices
"standard eigenvalue problem Ay = λy. But our matrix M will be tridiagonal, because",pos_def_matrices
neighboring hat functions overlap. It is exactly this situation that brings in the general-,pos_def_matrices
ized eigenvalue problem. The minimum value Λ1 will be the smallest eigenvalue of,pos_def_matrices
Ay = λMy. That Λ1 will be close to (and above) π2. The eigenvector y will give the,pos_def_matrices
approximation U = y1V1 +···+ynVn to the eigenfunction.,pos_def_matrices
As in the static problem. The method can be summarized in three steps: (1) choose,pos_def_matrices
"the Vj, (2) compute A and M, and (3) solve Ay = λMy. I don’t know why that costs a",pos_def_matrices
"1. Use three hat functions, with h = 1",pos_def_matrices
"4, to solve −u′′ = 2 with u(0) = u(1) = 0. Verify",pos_def_matrices
that the approximation U matches u = x−x2 at the nodes.,pos_def_matrices
2. Solve −u′′ = x with u(0) = u(1) = 0. Then solve approximately with two hat func-,pos_def_matrices
tions and h = 1,pos_def_matrices
3. Where is the largest error?,pos_def_matrices
"3. Suppose −u′′ = 2, with the boundary condition u(1) = 0 changed to u′(1) = 0. This",pos_def_matrices
“natural” condition on u′ need not be imposed on the trial functions V. With h = 1,pos_def_matrices
"there is an extra half-hat V3, which goes from 0 to 1 between x = 2",pos_def_matrices
3 and x = 1.,pos_def_matrices
3)2dx and f3 =,pos_def_matrices
� 2V3dx. Solve Ay = f for the ﬁnite element,pos_def_matrices
solution y1V1 +y2V2 +y3V3.,pos_def_matrices
"4. Solve −u′′ = 2 with a single hat function, but place its node at x = 1",pos_def_matrices
4 instead of x = 1,pos_def_matrices
"(Sketch this function V1.) With boundary conditions u(0) = u(1) = 0, compare the",pos_def_matrices
ﬁnite element approximation with the true u = x−x2.,pos_def_matrices
5. Galerkin’s method starts with the differential equation (say −u′′ = f(x)) instead of,pos_def_matrices
"the energy P. The trial solution is still u = y1V1 + y2V2 + ··· + ynVn, and the y’s are",pos_def_matrices
chosen to make the difference between −u′′ and f orthogonal to every Vj:,pos_def_matrices
6.5 The Finite Element Method,pos_def_matrices
"integrate the left side by parts to reach Ay = f, proving that Galerkin gives the same",pos_def_matrices
A and f as Rayleigh-Ritz for symmetric problems.,pos_def_matrices
6. A basic identity for quadratics shows y = A−1b as minimizing:,pos_def_matrices
The minimum over a subspace of trial functions is at the y nearest to A−1b. (That,pos_def_matrices
makes the ﬁrst term on the right as small as possible; it is the key to convergence of,pos_def_matrices
"U to u.) If A = I and b = (1,0,0), which multiple of V = (1,1,1) gives the smallest",pos_def_matrices
value of P(y) = 1,pos_def_matrices
7. For a single hat function V(x) centered at x = 1,pos_def_matrices
"2, compute A =",pos_def_matrices
� (V ′)2dx and M =,pos_def_matrices
"� V 2dx. In the 1 by 1 eigenvalue problem, is λ = A/M larger or smaller than the true",pos_def_matrices
eigenvalue λ = π2?,pos_def_matrices
8. For the hat functions V1 and V2 centered at x = h = 1,pos_def_matrices
3 and x = 2h = 2,pos_def_matrices
"3, compute the 2",pos_def_matrices
by 2 mass matrix Mi j =,pos_def_matrices
"� ViVjdx, and solve the eigenvalue problem Ax = λMx.",pos_def_matrices
9. What is the mass matrix Mi j =,pos_def_matrices
� ViVjdx for n hat functions with h =,pos_def_matrices
One aim of this book is to explain the useful parts of matrix theory. In comparison,computations
"with older texts in abstract linear algebra, the underlying theory has not been radically",computations
changed. One of the best things about the subject is that the theory is really essential,computations
for the applications. What is different is the change in emphasis which comes with a,computations
new point of view. Elimination becomes more than just a way to ﬁnd a basis for the,computations
"row space, and the Gram-Schmidt process is not just a proof that every subspace has an",computations
"orthonormal basis. Instead, we really need these algorithms. And we need a convenient",computations
"description, A = LU or A = QR, of what they do.",computations
This chapter will take a few more steps in the same direction. I suppose these steps are,computations
"governed by computational necessity, rather than by elegance, and I don’t know whether",computations
"to apologize for that; it makes them sound very superﬁcial, and that is wrong. They deal",computations
"with the oldest and most fundamental problems of the subject, Ax = b and Ax = λx, but",computations
they are continually changing and improving. In numerical analysis there is a survival,computations
"of the ﬁttest, and we want to describe some ideas that have survived so far. They fall",computations
1. Techniques for Solving Ax = b.,computations
"Elimination is a perfect algorithm, except",computations
when the particular problem has special properties—as almost every problem has. Sec-,computations
"tion 7.4 will concentrate on the property of sparseness, when most of the entries in A",computations
are zero. We develop iterative rather than direct methods for solving Ax = b. An iter-,computations
"ative method is “self-correcting,” and never reaches the exact answer. The object is to",computations
"get close more quickly than elimination. In some problems, that can be done; in many",computations
"others, elimination is safer and faster if it takes advantage of the zeros. The competition",computations
"is far from over, and we will identify the spectral radius that controls the speed of con-",computations
vergence to x = A−1b.,computations
2. Techniques for Solving Ax = λx.,computations
The eigenvalue problem is one of the out-,computations
7.2 Matrix Norm and Condition Number,computations
"standing successes of numerical analysis. It is clearly deﬁned, its importance is obvious,",computations
but until recently no one knew how to solve it. Dozens of algorithms have been sug-,computations
"gested, and everything depends on the size and the properties of A (and on the number",computations
"of eigenvalues that are wanted). You can ask LAPACK for an eigenvalue subroutine,",computations
"without knowing its contents, but it is better to know. We have chosen two or three ideas",computations
"that have superseded almost all of their predecessors: the QR algorithm, the family of",computations
"“power methods,” and the preprocessing of a symmetric matrix to make it tridiagonal.",computations
"The ﬁrst two methods are iterative, and the last is direct. It does its job in a ﬁnite",computations
"number of steps, but it does not end up with the eigenvalues themselves. This produces",computations
a much simpler matrix to use in the iterative steps.,computations
3. The Condition Number of a Matrix.,computations
Section 7.2 attempts to measure the,computations
"“sensitivity” of a problem: If A and b are slightly changed, how great is the effect on",computations
"x = A−1b? Before starting on that question, we need a way to measure A and the change",computations
"∆A. The length of a vector is already deﬁned, and now we need the norm of a matrix.",computations
"Then the condition number, and the sensitivity of A will follow from multiplying the",computations
norms of A and A−1. The matrices in this chapter are square.,computations
Matrix Norm and Condition Number,computations
"An error and a blunder are very different things. An error is a small mistake, probably",computations
unavoidable even by a perfect mathematician or a perfect computer. A blunder is much,computations
"more serious, and larger by at least an order of magnitude. When the computer rounds",computations
"oft a number after 16 bits, that is an error, But when a problem is so excruciatingly",computations
"sensitive that this roundoff error completely changes the solution, then almost certainly",computations
someone has committed a blunder. Our goal in this section is to analyze the effect of,computations
"errors, so that blunders can be avoided.",computations
We are actually continuing a discussion that began in Chapter 1 with,computations
"We claimed that B is well-conditioned, and not particularly sensitive to roundoff—except",computations
"that if Gaussian elimination is applied in a stupid way, the matrix becomes completely",computations
"vulnerable. It is a blunder to accept .0001 as the ﬁrst pivot, and we must insist on a larger",computations
and safer choice by exchanging the rows of B. When “partial pivoting” is built into the,computations
"elimination algorithm, the computer automatically looks for the largest pivot. Then the",computations
natural resistance to roundoff error is no longer compromised.,computations
"How do we measure this natural resistance, and decide whether a matrix is well-",computations
"conditioned or ill-conditioned? If there is a small change in b or in A, how large a",computations
change does that produce in the solution x?,computations
Chapter 7 Computations with Matrices,computations
"We begin with a change in the right-hand side, from b to b +δb. This error might",computations
"come from experimental data or from roundoff. We may suppose that δb is small, but",computations
its direction is outside our control. The solution is changed from x to x+δx:,computations
An error δb leads to δx = A−1δb. There will be a large change in the solution x when,computations
A−1 is large—A is nearly singular. The change in x is especially large when δb points in,computations
the direction that is ampliﬁed most by A−1.,computations
Suppose A is symmetric and its eigenvalues are positive: 0 < λ1 ≤ ··· ≤ λn. Any,computations
"vector δb is a combination of the corresponding unit eigenvectors x1,...,xn. The worst",computations
"error δx, coming from A−1, is in the direction of the ﬁrst eigenvector x1:",computations
"The error ∥δb∥ is ampliﬁed by 1/λ1, which is the largest eigenvalue of A−1. This",computations
"ampliﬁcation is greatest when λ1 is near zero, and A is nearly singular.",computations
Measuring sensitivity entirely by λ1 has a serious drawback. Suppose we multiply all,computations
the entries of A by 1000; then λ1 will be multiplied by 1000 and the matrix will look,computations
much less singular. This offends our sense of fair play; such a simple rescaling cannot,computations
"make an ill-conditioned matrix well. It is true that δx will be 1000 times smaller, but so",computations
will the solution x = A−1b. The relative error ∥δx∥/∥x∥ will be the same. Dividing by,computations
∥x∥ normalizes the problem against a trivial change of scale. At the same time there is,computations
a normalization for δb; our problem is to compare the relative change ∥δb∥/∥b∥ with,computations
the relative error ∥δx∥/∥x∥.,computations
The worst case is when ∥δx∥ is large—with δb in the direction of the eigenvector,computations
x1—and when ∥x∥ is small. The true solution x should be as small as possible compared,computations
"to the true b. This means that the original problem Ax = b should be at the other extreme,",computations
"in the direction of the last eigenvector xn: if b = xn, then x = A−1b = b/λn.",computations
"It is this combination, b = xn and δb = εx1, that makes the relative error as large as",computations
possible. These are the extreme cases in the following inequalities:,computations
"For a positive deﬁnite matrix, the solution x = A−1b and the error δx =",computations
The ratio c = λmax/λmin is the condition number of a positive deﬁnite matrix,computations
Example 1. The eigenvalues of A are approximately λ1 = 10−4/2 and λ2 = 2:,computations
has condition number about,computations
7.2 Matrix Norm and Condition Number,computations
We must expect a violent change in the solution from ordinary changes in the data.,computations
Chapter 1 compared the equations Ax = b and Ax′ = b′:,computations
u + 1.0001v = 2,computations
u + 1.0001v = 2.0001.,computations
"The right-hand sides are changed only by ∥δb∥ = .0001 = 10−4. At the same time, the",computations
"solution goes from u = 2, v = 0 to u = v = 1. This is a relative error of",computations
"Without having made any special choice of the perturbation, there was a relatively large",computations
"change in the solution. Our x and δb make 45° angles with the worst cases, which",computations
accounts for the missing 2 between 2·104 and the extreme possibility c = 4·104.,computations
"If A = I or even if A = I/10, its condition number is c = λmax/λmin = 1. By compari-",computations
"son, the determinant is a terrible measure of ill-conditioning. It depends not only on the",computations
"scaling but also on the order n; if A = I/10, then the determinant of A is 10−n. In fact,",computations
this “nearly singular” matrix is as well-conditioned as possible.,computations
Example 2. The n by n ﬁnite difference matrix A has λmax ≈ 4 and λmin ≈ π2/n2:,computations
The condition number is approximately c(A) = 1,computations
"2n2, and this time the dependence on the",computations
"order n is genuine. The better we approximate −u′′ = f, by increasing the number of",computations
"unknowns, the harder it is to compute the approximation. At a certain crossover point,",computations
an increase in n will actually produce a poorer answer.,computations
"Fortunately for the engineer, this crossover occurs where the accuracy is already",computations
"pretty good. Working in single precision, a typical computer might make roundoff errors",computations
"of order 10−9. With n = 100 unknowns and c = 5000, the error is ampliﬁed at most to",computations
be of order 10−5—which is still more accurate than any ordinary measurements. But,computations
"there will be trouble with 10,000 unknowns, or with a 1, −4, 6, −4, 1 approximation to",computations
d4u/dx4 = f(x)—for which the condition number grows as n4.1,computations
Our analysis so far has applied to symmetric matrices with positive eigenvalues. We,computations
"could easily drop the positivity assumption, and use absolute values |λ|. But to go",computations
"1The usual rule of thumb, experimentally veriﬁed, is that the computer can lose logc decimal places to the",computations
roundoff errors in Gaussian elimination.,computations
Chapter 7 Computations with Matrices,computations
"beyond symmetry, as we certainly want to do, there will have to be a major change. This",computations
is easy to see for the very unsymmetric matrices,computations
"The eigenvalues all equal one, but the proper condition number is not λmax/λmin = 1.",computations
The relative change in x is not bounded by the relative change in b. Compare,computations
A 1% change in b has produced a hundredfold change in x; the ampliﬁcation factor is,computations
"1002. Since c represents an upper bound, the condition number must be at least 10,000.",computations
The difﬁculty here is that a large off-diagonal entry in A means an equally large entry in,computations
A−1. Expecting A−1 to get smaller as A gets bigger is often wrong.,computations
"For a proper deﬁnition of the condition number, we look back at equation (3). We",computations
"were trying to make x small and b = Ax large. When A is not symmetric, the maximum of",computations
∥Ax∥/∥x∥ may be found at a vector x that is not one of the eigenvectors. This maximum,computations
is an excellent measure of the size of A. It is the norm of A.,computations
The norm of A is the number ∥A∥ deﬁned by,computations
"In other words, ∥A∥ bounds the “amplifying power” of the matrix:",computations
for all vectors x.,computations
The matrices A and A−1 in equation (4) have norms somewhere between 100 and 101.,computations
"They can be calculated exactly, but ﬁrst we want to complete the connection between",computations
"norms and condition numbers. Because b = Ax and δx = A−1δb, equation (6) gives",computations
"This is the replacement for equation (3), when A is not symmetric. In the symmetric case,",computations
"∥A∥ is the same as λmax, and ∥A−1∥ is the same as 1/λmin. The correct replacement for",computations
λmax/λmin is the product ∥A∥∥A−1∥—which is the condition number.,computations
The condition number of A is c = ∥A∥∥A−1∥. The relative error satisﬁes,computations
directly from equation (7).,computations
"If we perturb the matrix A instead of the right-hand side b, then",computations
from equation (10) below.,computations
7.2 Matrix Norm and Condition Number,computations
"What is remarkable is that the same condition number appears in equation (9), when the",computations
"matrix itself is perturbed: If Ax = b and (A+δA)(x+δx) = b, then by subtraction",computations
"Multiplying by δA ampliﬁes a vector by no more than ∥δA∥, and multiplying by A−1",computations
"ampliﬁes by no more than ∥A−1∥. Then ∥δx∥ < ∥A−1∥∥δA∥∥x+δx∥, which is",computations
∥x+δx∥ ≤ ∥A−1∥∥δA∥ = c∥δA∥,computations
These inequalities mean that roundoff error comes from two sources. One is the,computations
"natural sensitivity of the problem, measured by c. The other is the actual error δb",computations
or δA. This was the basis of Wilkinson’s error analysis. Since elimination actually,computations
"produces approximate factors L′ and U′, it solves the equation with the wrong matrix",computations
A + δA = L′U′ instead of the right matrix A = LU. He proved that partial pivoting,computations
controls δA—so the burden of the roundoff error is carried by the condition number c.,computations
A Formula for the Norm,computations
The norm of A measures the largest amount by which any vector (eigenvector or not),computations
is ampliﬁed by matrix multiplication: ∥A∥ = max(∥Ax∥/∥x∥). The norm of the identity,computations
"matrix is 1. To compute the norm, square both sides to reach the symmetric ATA:",computations
∥A∥2 = max ∥Ax∥2,computations
∥x∥2 = max xTATAx,computations
7D ∥A∥ is the square root of the largest eigenvalue of ATA: ∥A∥2 = λmax(ATA).,computations
The vector that A ampliﬁes the most is the corresponding eigenvector of ATA:,computations
= λmax(ATA) = ∥A∥2.,computations
Figure 7.1 shows an unsymmetric matrix with eigenvalues λ1 = λ2 = 1 and norm,computations
∥A∥ = 1.618. In this case A−1 has the same norm. The farthest and closest points Ax on,computations
"the ellipse come from eigenvectors x of ATA, not of A.",computations
"Note 1. The norm and condition number are not actually computed in practice, only",computations
"estimated, There is not time to solve an eigenvalue problem for λmax(ATA).",computations
"Note 2. In the least-squares equation ATAx = ATb, the condition number c(ATA) is the",computations
square of c(A). Forming ATA can turn a healthy problem into a sick one. It may be,computations
"necessary to orthogonalize A by Gram-Schmidt, instead of computing with ATA.",computations
Note 3. The singular values of A in the SVD are the square roots of the eigenvalues of,computations
"ATA. By equation (12), another formula for the norm is ∥A∥ = σmax. The orthogonal",computations
U and V leave lengths unchanged in ∥Ax∥ = ∥UΣV Tx∥. So the largest ∥Ax∥/∥x∥ comes,computations
from the largest σ in the diagonal matrix Σ.,computations
Chapter 7 Computations with Matrices,computations
ellipse of all Ax,computations
circle ∥x∥ = 1,computations
∥A∥ = 1 +,computations
∥A∥2 = λmax(ATA) ≈ 2.618,computations
∥A−1∥2 = λmin(ATA) ≈ 0.382,computations
c(A) = ∥A∥∥A−1∥ ≈ (1.618)2,computations
Figure 7.1: The norms of A and A−1 come from the longest and shortest Ax.,computations
Note 4. Roundoff error also enters Ax = λx. What is the condition number of the eigen-,computations
value problem? The condition number of the diagonalizing S measures the sensitivity,computations
"of the eigenvalues. If µ is an eigenvalue of A + E, then its distance from one of the",computations
eigenvalues of A is,computations
|µ −λ| ≤ ∥S∥∥S−1∥∥E∥ = c(S)∥E∥.,computations
"With orthonormal eigenvectors and S = Q, the eigenvalue problem is perfectly condi-",computations
tioned: c(Q) = 1. The change δλ in the eigenvalues is no greater than the change δA.,computations
"Therefore the best case is when A is symmetric, or more generally when AAT = ATA.",computations
Then A is a normal matrix; its diagonalizing S is an orthogonal Q (Section 5.6).,computations
"If xk is the kth column of S and yk is the kth row of S−1, then λk changes by",computations
δλk = ykExk +terms of order ∥E∥2.,computations
"In practice, ykExk is a realistic estimate of δλ. The idea in every good algorithm is to",computations
"keep the error matrix E as small as possible—usually by insisting, as in the next section,",computations
on orthogonal matrices at every step of the computation of λ.,computations
"1. For an orthogonal matrix Q, show that ∥Q∥ = 1 and also c(Q) = 1. Orthogonal",computations
matrices (and their multiples αQ) are the only perfectly conditioned matrices.,computations
"2. Which “famous” inequality gives ∥(A+B)x∥ ≤ ∥Ax∥+∥Bx∥, and why does it follow",computations
from equation (5) that ∥A+B∥ ≤ ∥A∥+∥B∥?,computations
"3. Explain why ∥ABx∥ ≤ ∥A∥∥B∥∥x∥, and deduce from equation (5) that ∥AB∥ ≤ ∥A∥∥B∥.",computations
Show that this also implies c(AB) ≤ c(A)c(B).,computations
7.2 Matrix Norm and Condition Number,computations
4. For the positive deﬁnite A =,computations
", compute ∥A−1∥ = 1/λ1, ∥A∥ = λ2, and c(A) =",computations
λ2/λ1. Find a right-hand side b and a perturbation δb so that the error is the worst,computations
"possible, ∥δx∥/∥x∥ = c∥δb∥/∥b∥.",computations
"5. Show that if λ is any eigenvalue of A, Ax = λx, then |λ| ≤ ∥A∥.",computations
6. The matrices in equation (4) have norms between 100 and 101. Why?,computations
"7. Comparing the eigenvalues of ATA and AAT, prove that ∥A∥ = ∥AT∥.",computations
"8. For a positive deﬁnite A, the Cholesky decomposition is A = LDLT = RTR, where",computations
DLT. Show directly from equation (12) that the condition number of c(R) is,computations
the square root of c(A). Elimination without row exchanges cannot hurt a positive,computations
"deﬁnite matrix, since c(A) = c(RT)c(R).",computations
"9. Show that max|λ| is not a true norm, by ﬁnding 2 by 2 counterexamples to λmax(A+",computations
B) ≤ λmax(A)+λmax(B) and λmax(AB) ≤ λmax(A)λmax(B).,computations
10. Show that the eigenvalues of B =,computations
"are ±σi, the singular values of A. Hint: Try",computations
11. (a) Do A and A−1 have the same condition number c?,computations
"(b) In parallel with the upper bound (8) on the error, prove a lower bound:",computations
(Consider A−1b = x instead of Ax = b.),computations
12. Find the norms λmax and condition numbers λmax/λmin of these positive deﬁnite,computations
13. Find the norms and condition numbers from the square roots of λmax(ATA) and,computations
14. Prove that the condition number ∥A∥∥A−1∥ is at least 1.,computations
15. Why is I the only symmetric positive deﬁnite matrix that has λmax = λmin = 1? Then,computations
the only matrices with ∥A∥ = 1 and ∥A−1∥ = 1 must have ATA = I. They are,computations
"16. Orthogonal matrices have norm ∥Q∥ = 1. If A = QR, show that ∥A∥ ≤ ∥R∥ and also",computations
∥R∥ ≤ ∥A∥. Then ∥A∥ = ∥Q∥∥R∥. Find an example of A = LU with ∥A∥ < ∥L∥∥U∥.,computations
Chapter 7 Computations with Matrices,computations
17. (Suggested by Moler and Van Loan) Compute b−Ay and b−Az when,computations
Is y closer than z to solving Ax = b? Answer in two ways: Compare the residual,computations
"b−Ay to b−Az. Then compare y and z to the true x = (1,−1), Sometimes we want",computations
"a small residual, sometimes a small δx.",computations
Problems 18–20 are about vector norms other than the usual ∥x∥ = √x·x.,computations
18. The “ℓ1 norm” is ∥x∥1 = |x|1+···+|x|n. The “ℓ∞ norm” is ∥x∥∞ = max|xi|. Compute,computations
"∥x∥, ∥x∥1 and ∥x∥∞ for the vectors",computations
19. Prove that ∥x∥∞ ≤ ∥x∥ ≤ ∥x∥1. Show from the Schwarz inequality that the ratios,computations
"∥x∥/∥x∥∞ and ∥x∥1/∥x∥ are never larger than √n. Which vector (x1,...,xn) gives",computations
ratios equal to √n?,computations
20. All vector norms must satisfy the triangle inequality. Prove that,computations
∥x+y∥∞ ≤ ∥x∥∞ +∥y∥∞,computations
∥x+y∥1 ≤ ∥x∥1 +∥y∥1.,computations
21. Compute the exact inverse of the Hilbert matrix A by elimination. Then compute,computations
A−1 again by rounding all numbers to three ﬁgures:,computations
A = hilb(3) =,computations
"22. For the same A, compute b = Ax for x = (1,1,1) and x = (0,6,−3.6). A small change",computations
∆b produces a large change ∆x.,computations
23. Compute λmax and λmin for the 8 by 8 Hilbert matrix aij = 1/(i + j − 1). If Ax = b,computations
"with ∥b∥ = 1, how large can ∥x∥ be? If b has roundoff error less than 10−16, how",computations
large an error can this cause in x?,computations
"24. If you know L, U, Q, and R, is it faster to solve LUx = b or QRx = b?",computations
"25. Choosing the largest available pivot in each column (partial pivoting), factor each A",computations
into PA = LU:,computations
7.3 Computation of Eigenvalues,computations
26. Find the LU factorization of A =,computations
". On your computer, solve by elimination",computations
"when ε = 10−3,10−6,10−9,10−12,10−15:",computations
"The true x is (1,1). Make a table to show the error for each ε. Exchange the two",computations
equations and solve again—the errors should almost disappear.,computations
There is no one best way to ﬁnd the eigenvalues of a matrix. But there are certainly,computations
"some terrible ways which should never be tried, and also some ideas that do deserve a",computations
"permanent place. We begin by describing one very rough and ready approach, the power",computations
"method, whose convergence properties are easy to understand. We added a graphic",computations
"animation (with sound) to the course page web.mit.edu/18.06, to show the power method",computations
"We move steadily toward a more sophisticated algorithm, which starts by making a",computations
symmetric matrix tridiagonal and ends by making it virtually diagonal. That second step,computations
"is done by repeating Gram-Schmidt, so it is known as the QR method.",computations
The ordinary power method operates on the principle of a difference equation. It,computations
"starts with an initial guess u0 and then successively forms u1 = Au0, u2 = Au1, and",computations
in general uk+1 = Auk. Each step is a matrix-vector multiplication. After k steps it,computations
"produces uk = Aku0, although the matrix Ak will never appear. The essential thing is",computations
"that multiplication by A should be easy—if the matrix is large, it had better be sparse—",computations
because convergence to the eigenvector is often very slow. Assuming A has a full set of,computations
"eigenvectors x1,...,xn, the vector uk will be given by the usual formula:",computations
Eigenvectors weighted by λ k,computations
uk = c1λ k,computations
Suppose the largest eigenvalue λn is all by itself; there is no other eigenvalue of the same,computations
"magnitude, and |λ1| ≤ ··· ≤ |λn−1| < |λn|. Then as long as the initial guess u0 contained",computations
"some component of the eigenvector xn, so that cn ̸= 0, this component will gradually",computations
The vectors uk point more and more accurately toward the direction of xn. Their conver-,computations
"gence factor is the ratio r = |λn−1|/|λn|. It is just like convergence to a steady state, for",computations
"a Markov matrix, except now λn may not equal 1. The scaling factor λ k",computations
n in equation (1),computations
"prevents uk from growing very large or very small, in case |λn| > 1 or |λn| < 1.",computations
Chapter 7 Computations with Matrices,computations
Often we can just divide each uk by its ﬁrst component αk before taking the next step.,computations
"With this simple scaling, the power method uk+1 = Auk/αk converges to a multiple of",computations
xn. The scaling factors αk will approach λn.,computations
Example 1. The uk approach the eigenvector,computations
matrix of population shifts in Section 1.3:,computations
"If r = |λn−1|/|λn| is close to 1, then convergence is very slow. In many applications",computations
"r > .9, which means that more than 20 iterations are needed to achieve one more digit.",computations
"(The example had r = .7, and it was still slow.) If r = 1, which means |λn−1| = |λn|, then",computations
convergence will probably not occur at all. That happens (in the applet with sound) for a,computations
"complex conjugate pair λn−1 = λ n. There are several ways to get around this limitation,",computations
and we shall describe three of them:,computations
"1. The block power method works with several vectors at once, in place of uk. If we",computations
"multiply p orthonormal vectors by A, and then apply Gram-Schmidt to orthogonal-",computations
ize them again—that is a single step of the method—the convergence ratio becomes,computations
r′ = |λn−p|/|λn|. We will obtain approximations to p different eigenvalues and their,computations
2. The inverse power method operates with A−1 instead of A. A single step is vk+1 =,computations
"A−1vk, which means that we solve the linear system Avk+1 = vk (and save the factors",computations
"L and U!). Now we converge to the smallest eigenvalue λ1 and its eigenvector x1,",computations
"provided |λ1| < |λ2|. Often it is λ1 that is wanted in the applications, and then",computations
inverse iteration is an automatic choice.,computations
3. The shifted inverse power method is best of all. Replace A by A−αI. Each eigen-,computations
"value is shifted by α, and the convergence factor for the inverse method will change",computations
"to r′′ = |λ1 −α|/|λ2 −α|. If α is a good approximation to λ1, r′′ will be very small",computations
and the convergence is enormously accelerated. Each step of the method solves,computations
"When α is close to λ1, the ﬁrst term dominates after only one or two steps. If",computations
"λ1 has already been computed by another algorithm (such as QR), then α is this",computations
computed value. One standard procedure is to factor A − αI into LU and to solve,computations
"Ux1 = (1,1,...,1) by back-substitution.",computations
"If λ1 is not already approximated, the shifted inverse power method has to generate its",computations
"own choice of α. We can vary α = αk at every step if we want to, so (A−αkI)wk+1 = wk.",computations
7.3 Computation of Eigenvalues,computations
"When A is symmetric, a very accurate choice is the Rayleigh quotient:",computations
αk = R(wk) = wT,computations
This quotient R(x) has a minimum at the true eigenvector x1. Its graph is like the bottom,computations
"of a parabola, so the error λ1 − αk is roughly the square of the error in the eigenvector.",computations
The convergence factors |λ1 − αk|/|λ2 − αk| are themselves converging to zero. Then,computations
these Rayleigh quotient shifts give cubic convergence of αk to λ1.2,computations
Tridiagonal and Hessenberg Forms,computations
The power method is reasonable only for a matrix that is large and sparse. When too,computations
"many entries are nonzero, this method is a mistake. Therefore we ask whether there is",computations
any simple way to create zeros. That is the goal of the following paragraphs.,computations
It should be said that after computing a similar matrix Q−1AQ with more zeros than,computations
"A, we do not intend to go back to the power method. There are much more powerful",computations
"variants, and the best of them seems to be the QR algorithm. (The shifted inverse power",computations
"method has its place at the very end, in ﬁnding the eigenvector.) The ﬁrst step is to pro-",computations
"duce quickly as many zeros as possible, using an orthogonal matrix Q. If A is symmetric,",computations
then so is Q−1AQ. No entry can become dangerously large because Q preserves lengths.,computations
"To go from A to Q−1AQ, there are two main possibilities: We can produce one zero at",computations
"every step (as in elimination), or we can work with a whole column at once. For a single",computations
"zero, it is easy to use a plane rotation as illustrated in equation (7), found near the end",computations
"of this section, that has cosθ and sinθ in a 2 by 2 block. Then we could cycle through",computations
"all the entries below the diagonal, choosing at each step a rotation θ that will produce a",computations
"zero; this is Jacobi’s method. It fails to diagonalize A after a ﬁnite number of rotations,",computations
since the zeros from early steps will be destroyed when later zeros are created.,computations
"To preserve the zeros and stop, we have to settle for less than a triangular form.",computations
The Hessenberg form accepts one nonzero diagonal below the main diagonal. If a,computations
"Hessenberg matrix is symmetric, it only has three nonzero diagonals.",computations
A series of rotations in the right planes will produce the required zeros. Householder,computations
found a new way to accomplish exactly the same thing. A Householder transformation,computations
is a reﬂection matrix determined by one vector v:,computations
H = I −2 vvT,computations
"Often v is normalized to become a unit vector u = v/∥v∥, and then H becomes I −2uuT.",computations
In either case H is both symmetric and orthogonal:,computations
HTH = (I −2uuT)(I −2uuT) = I −4uuT +4uuTuuT = I.,computations
2Linear convergence means that every step multiplies the error by a ﬁxed factor r < 1. Quadratic convergence,computations
"means that the error is squared at every step, as in Newton’s method xk+1 −xk = −f(xk)/ f ′(xk) for solving f(x) =",computations
0. Cubic convergence takes 10−1 to 10−3 to 10−9.,computations
Chapter 7 Computations with Matrices,computations
"Thus H = HT = H−1. Householder’s plan was to produce zeros with these matrices, and",computations
its success depends on the following identity Hx = −σz:,computations
"Suppose z is the column vector (1,0,...,0), σ = ∥x∥, and v = x + σz.",computations
"Then Hx = −σz = (−σ,0,...,0). The vector Hx ends in zeros as desired.",computations
The proof is to compute Hx and reach −σz:,computations
Hx = x− 2vvTx,computations
(because xTx = σ2),computations
"This identity can be used right away, on the ﬁrst column of A. The ﬁnal Q−1AQ is",computations
allowed one nonzero diagonal below the main diagonal (Hessenberg form). Therefore,computations
only the entries strictly below the diagonal will be involved:,computations
"At this point Householder’s matrix H is only of order n − 1, so it is embedded into the",computations
lower right-hand corner of a full-size matrix U1:,computations
∗ ∗ ∗ ∗,computations
∗ ∗ ∗ ∗,computations
∗ ∗ ∗ ∗,computations
∗ ∗ ∗ ∗,computations
∗ ∗ ∗ ∗,computations
"The ﬁrst stage is complete, and U−1",computations
1 AU1 has the required ﬁrst column. At the second,computations
"stage, x consists of the last n − 2 entries in the second column (three bold stars). Then",computations
"H2 is of order n−2. When it is embedded in U2, it produces",computations
∗ ∗ ∗ ∗ ∗,computations
∗ ∗ ∗ ∗ ∗,computations
0 ∗ ∗ ∗ ∗,computations
0 0 ∗ ∗ ∗,computations
0 0 ∗ ∗ ∗,computations
"U3 will take care of the third column. For a 5 by 5 matrix, the Hessenberg form is",computations
"achieved (it has six zeros). In general Q is the product of all the matrices U1U2···Un−2,",computations
and the number of operations required to compute it is of order n3.,computations
7.3 Computation of Eigenvalues,computations
Example 2. (to change a13 = a31 to zero),computations
"Embedding H into Q, the result Q−1AQ is tridiagonal:",computations
Q−1AQ is a matrix that is ready to reveal its eigenvalues—the QR algorithm is ready to,computations
begin—but we digress for a moment to mention two other applications of these same,computations
1. The Gram-Schmidt factorization A = QR. Remember that R is to be upper trian-,computations
"gular. We no longer have to accept an extra nonzero diagonal below the main one,",computations
since no matrices are multiplying on the right to spoil the zeros. The ﬁrst step in,computations
constructing Q is to work with the whole ﬁrst column of A:,computations
H1 = I −2 vvT,computations
"The ﬁrst column of H1A equals −∥x∥z. It is zero below the main diagonal, and it",computations
"is the ﬁrst column of R. The second step works with the second column of H1A,",computations
"from the pivot on down, and produces an H2H1A which is zero below that pivot.",computations
"(The whole algorithm is like elimination, but slightly slower.) The result of n − 1",computations
"steps is an upper triangular R, but the matrix that records the steps is not a lower",computations
"triangular L. Instead it is the product Q = H1H2···Hn−1, which can be stored in this",computations
factored form (keep only the v’s) and never computed explicitly. That completes,computations
2. The singular value decomposition UTAV = Σ. The diagonal matrix Σ has the same,computations
"shape as A, and its entries (the singular values) are the square roots of the eigenval-",computations
ues of ATA. Since Householder transformations can only prepare for the eigenvalue,computations
"problem, we cannot expect them to produce Σ. Instead, they stably produce a bidi-",computations
"agonal matrix, with zeros everywhere except along the main diagonal and the one",computations
"The ﬁrst step toward the SVD is exactly as in QR above: x is the ﬁrst column of A,",computations
and H1x is zero below the pivot. The next step is to multiply on the right by an H(1),computations
Chapter 7 Computations with Matrices,computations
which will produce zeros as indicated along the ﬁrst row:,computations
A → H1A =,computations
∗ ∗ ∗ ∗,computations
0 ∗ ∗ ∗,computations
0 ∗ ∗ ∗,computations
�� → H1AH(1) =,computations
∗ ∗ 0 0,computations
0 ∗ ∗ ∗,computations
0 ∗ ∗ ∗,computations
Then two ﬁnal Householder transformations quickly achieve the bidiagonal form:,computations
∗ ∗ 0 0,computations
0 ∗ ∗ ∗,computations
0 0 ∗ ∗,computations
∗ ∗ 0 0,computations
0 ∗ ∗ 0,computations
0 0 ∗ ∗,computations
The QR Algorithm for Computing Eigenvalues,computations
"The algorithm is almost magically simple. It starts with A0, factors it by Gram-Schmidt",computations
"into Q0R0, and then reverses the factors: A1 = R0Q0. This new matrix A1 is similar to",computations
the original one because Q−1,computations
0 A0Q0 = Q−1,computations
0 (Q0R0)Q0 = A1. So the process continues with,computations
no change in the eigenvalues:,computations
All Ak are similar,computations
"This equation describes the unshifted QR algorithm, and almost always Ak approaches a",computations
"triangular form, Its diagonal entries approach its eigenvalues, which are also the eigen-",computations
"values of A0. If there was already some processing to obtain a tridiagonal form, then A0",computations
is connected to the absolutely original A by Q−1AQ = A0.,computations
"As it stands, the QR algorithm is good but not very good. To make it special, it needs",computations
"two reﬁnements: We must allow shifts to Ak − αkI, and we must ensure that the QR",computations
factorization at each step is very quick.,computations
1. The Shifted Algorithm.,computations
"If the number αk is close to an eigenvalue, the step in",computations
equation (5) should be shifted immediately by αk (which changes Qk and Rk):,computations
Ak = αkI = QkRk,computations
Ak+1 = RkQk +αkI.,computations
This matrix Ak+1 is similar to Ak (always the same eigenvalues):,computations
k AkQk = Q−1,computations
k (QkRk +αkI)Qk = Ak+1.,computations
"What happens in practice is that the (n,n) entry of Ak—the one in the lower right-hand",computations
corner—is the ﬁrst to approach an eigenvalue. That entry is the simplest and most pop-,computations
"ular choice for the shift αk. Normally this produces quadratic convergence, and in the",computations
"symmetric case even cubic convergence, to the smallest eigenvalue. After three or four",computations
7.3 Computation of Eigenvalues,computations
"steps of the shifted algorithm, the matrix Ak looks like this:",computations
We accept the computed λ ′,computations
1 as a very close approximation to the true λ1. To ﬁnd the,computations
"next eigenvalue, the QR algorithm continues with the smaller matrix (3 by 3, in the",computations
illustration) in the upper left-hand corner. Its subdiagonal elements will be somewhat,computations
"reduced by the ﬁrst QR steps, and another two steps are sufﬁcient to ﬁnd λ2. This gives",computations
"a systematic procedure for ﬁnding all the eigenvalues. In fact, the QR method is now",computations
completely described. It only remains to catch up on the eigenvectors—that is a single,computations
inverse power step—and to use the zeros that Householder created.,computations
"2. When A0 is tridiagonal or Hessenberg, each QR step is very fast. The Gram-Schmidt",computations
process (factoring into QR) takes O(n3) operations for a full matrix A. For a Hessenberg,computations
"matrix this becomes O(n2), and for a tridiagonal matrix it is O(n). Fortunately, each new",computations
Ak is again in Hessenberg or tridiagonal form:,computations
∗ ∗ ∗ ∗,computations
∗ ∗ ∗ ∗,computations
0 ∗ ∗ ∗,computations
0 0 ∗ ∗,computations
∗ ∗ ∗ ∗,computations
0 ∗ ∗ ∗,computations
0 0 ∗ ∗,computations
0 0 0 ∗,computations
You can easily check that this multiplication leaves Q0 with the same three zeros as A0.,computations
Hessenberg times triangular is Hessenberg. So is triangular times Hessenberg:,computations
A1 = R0Q0 =,computations
∗ ∗ ∗ ∗,computations
0 ∗ ∗ ∗,computations
0 0 ∗ ∗,computations
0 0 0 ∗,computations
∗ ∗ ∗ ∗,computations
∗ ∗ ∗ ∗,computations
0 ∗ ∗ ∗,computations
0 0 ∗ ∗,computations
"The symmetric case is even better, since A1 = Q−1",computations
0 A0Q0 = QT,computations
0A0Q0 stays symmetric. By,computations
"the reasoning just completed, A1 is also Hessenberg. So A1 must be tridiagonal. The",computations
"same applies to A2,A3,..., and every QR step begins with a tridiagonal matrix.",computations
"The last point is the factorization itself, producing the Qk and Rk from each Ak (or",computations
"really from Ak − αkI). We may use Householder again, but it is simpler to annihilate",computations
each subdiagonal element in turn by a “plane rotation” Pij. The ﬁrst is P21:,computations
Rotation to kill a21,computations
Chapter 7 Computations with Matrices,computations
"The (2,1) entry in this product is a11sinθ + a21cosθ, and we choose the angle θ that",computations
"makes this combination zero. The next rotation P32 is chosen in a similar way, to remove",computations
"the (3,2) entry of P32P21Ak. After n−1 rotations, we have R0:",computations
Rk = Pn n−1···P32P21Ak.,computations
Books on numerical linear algebra give more information about this remarkable algo-,computations
rithm in scientiﬁc computing. We mention one more method—Arnoldi in ARPACK—,computations
"for large sparse matrices. It orthogonalizes the Krylov sequence x,Ax,A2x,... by Gram-",computations
"Schmidt. If you need the eigenvalues of a large matrix, don’t use det(A−λI)!",computations
1. For the matrix A =,computations
"with eigenvalues λ1 = 1 and λ2 = 3, apply the power",computations
method uk+1 = Auk three times to the initial guess u0 =,computations
. What is the limiting,computations
2. For the same A and the initial guess u0 =,computations
", compare three inverse power steps to",computations
one shifted step with α = uT,computations
uk+1 = A−1uk = 1,computations
"The limiting vector u∞ is now a multiple of the other eigenvector (1,1).",computations
3. Explain why |λn/λn−1| controls the convergence of the usual power method. Con-,computations
struct a matrix A for which this method does not converge.,computations
4. The Markov matrix A =,computations
"has λ = 1 and .6, and the power method uk = Aku0",computations
. Find the eigenvectors of A−1. What does the inverse power,computations
method u−k = A−ku0 converge to (after you multiply by .6k)?,computations
"5. Show that for any two different vectors of the same length, ∥x∥ = ∥y∥, the House-",computations
holder transformation with v = x−y gives Hx = y and Hy = x.,computations
"6. Compute σ = ∥x∥, v = x+σz, and H = I −2vvT/vTv, Verify Hx = −σz:",computations
"7. Using Problem 6, ﬁnd the tridiagonal HAH−1 that is similar to",computations
7.4 Iterative Methods for Ax = b,computations
8. Show that starting from A0 =,computations
", the unshifted QR algorithm produces only the",computations
modest improvement A1 = 1,computations
9. Apply to the following matrix A a single QR step with the shift α = a22—which in,computations
"this case means without shift, since a22 = 0. Show that the off-diagonal entries go",computations
"from sinθ to −sin3θ, which is cubic convergence.",computations
10. Check that the tridiagonal A =,computations
is left unchanged by the QR algorithm. It is one,computations
of the (rare) counterexamples to convergence (so we shift).,computations
"11. Show by induction that, without shifts, (Q0Q1···Qk)(Rk ···R1R0) is exactly the QR",computations
factorization of Ak+1. This identity connects QR to the power method and leads to,computations
"an explanation of its convergence. If |λ1| > |λ2| > ··· > |λn|, these eigenvalues will",computations
gradually appear on the main diagonal.,computations
"12. Choose sinθ and cosθ in the rotation P to triangularize A, and ﬁnd R:",computations
13. Choose sinθ and cosθ to make P21AP−1,computations
21 triangular (same A). What are the eigen-,computations
"14. When A is multiplied by Pi j (plane rotation), which entries are changed? When Pi jA",computations
is multiplied on the right by P−1,computations
"i j , which entries are changed now?",computations
15. How many multiplications and how many additions are used to compute PA? (A,computations
careful organization of all the rotations gives 2,computations
"3n3 multiplications and additions, the",computations
same as for QR by reﬂectors and twice as many as for LU.),computations
16. (Turning a robot hand) A robot produces any 3 by 3 rotation A from plane rotations,computations
"around the x, y, and z axes. If P32P31P21A = I, the three robot turns are in A =",computations
32 . The three angles are Euler angles. Choose the ﬁrst θ so that,computations
"is zero in the (2,1) position.",computations
Iterative Methods for Ax = b,computations
"In contrast to eigenvalues, for which there was no choice, we do not absolutely need",computations
an iterative method to solve Ax = b. Gaussian elimination will reach the solution x in,computations
Chapter 7 Computations with Matrices,computations
"a ﬁnite number of steps (n3/3 for a full matrix, less than that for the large matrices we",computations
"actually meet), Often that number is reasonable. When it is enormous, we may have to",computations
settle for an approximate x that can be obtained more quickly—and it is no use to go,computations
part way through elimination and then stop.,computations
"Our goal is to describe methods that start from any initial guess x0, and produce an",computations
improved approximation xk+1 from the previous xk. We can stop when we want to.,computations
"An iterative method is easy to invent, by splitting the matrix A. If A = S − T, then",computations
the equation Ax = b is the same as Sx = Tx+b. Therefore we can try,computations
Iteration from xk to xk+1,computations
Sxk+1 = Txk +b.,computations
There is no guarantee that this method is any good. A successful splitting S−T satisﬁes,computations
1. The new vector xk+1 should be easy to compute. Therefore S should be a simple,computations
(and invertible!) matrix; it may be diagonal or triangular.,computations
2. The sequence xk should converge to the true solution x. If we subtract the iteration,computations
"in equation (1) from the true equation Sx = Tx+b, the result is a formula involving",computations
only the errors ek = x−xk:,computations
"This is just a difference equation. It starts with the initial error e0, and after k steps",computations
it produces the new error ek = (S−1T)ke0. The question of convergence is exactly,computations
the same as the question of stability: xk → x exactly when ek → 0.,computations
The iterative method in equation (1) is convergent if and only if every,computations
eigenvalue of S−1T satisﬁes |λ| < 1. Its rate of convergence depends on the,computations
maximum size of |λ|:,computations
Remember that a typical solution to ek+1 = S−1Tek is a combination of eigenvectors:,computations
Error after k steps,computations
ek = c1λ k,computations
"The largest |λi| will eventually be dominant, so the spectral radius ρ = |λmax| will govern",computations
the rate at which ek converges to zero. We certainly need ρ < 1.,computations
Requirements 1 and 2 above are conﬂicting. We could achieve immediate conver-,computations
gence with S = A and T = 0; the ﬁrst and only step of the iteration would be Ax1 = b. In,computations
"that case the error matrix S−1T is zero, its eigenvalues and spectral radius are zero, and",computations
the rate of convergence (usually deﬁned as −logρ) is inﬁnite. But Ax1 = b may be hard,computations
"to solve; that was the reason for a splitting. A simple choice of S can often succeed, and",computations
we start with three possibilities:,computations
7.4 Iterative Methods for Ax = b,computations
1. S = diagonal part of A (Jacobi’s method).,computations
2. S = triangular pail of A (Gauss-Seidel method).,computations
3. S = combination of 1 and 2 (successive overrelaxation or SOR).,computations
"S is also called a preconditioner, and its choice is crucial in numerical analysis.",computations
Example 1 (Jacobi). Here S is the diagonal part of A:,computations
"If the components of x are v and w, the Jacobi step Sxk+1 = Txk +b is",computations
2vk+1 = wk +b1,computations
"2wk+1 = vk +b2,",computations
The decisive matrix S−1T has eigenvalues ±1,computations
"2, which means that the error is cut in half",computations
"(one more binary digit becomes correct) at every step. In this example, which is much",computations
"too small to be typical, the convergence is fast.",computations
"For a larger matrix A, there is a very practical difﬁculty. The Jacobi iteration re-",computations
quires us to keep all components of xk until the calculation of xk+1 is complete. A,computations
"much more natural idea, which requires only half as much storage, is to start using each",computations
component of the new xk+1 as soon as it is computed; xk+1 takes the place of xk a com-,computations
"ponent at a time. Then xk can be destroyed as fast as xk+1 is created, The ﬁrst component",computations
a11(x1)k+1 = (−a12x2 −a13x3 −···−a1nxn)k +b1.,computations
"The next step operates immediately with this new value of x1, to ﬁnd (x2)k+1:",computations
a22(x2)k+1 = −a21(x1)k+1 +(−a23x3 −···−a2nxn)k +b2.,computations
And the last equation in the iteration step will use new values exclusively:,computations
ann(xn)k+1 = (−an1x1 −an2x2 −···−ann−1xn−1)k+1 +bn.,computations
"This is called the Gauss-Seidel method, even though it was apparently unknown to",computations
"Gauss and not recommended by Seidel. That is a surprising bit of history, because it is",computations
"not a bad method. When the terms in xk+1 are moved to the left-hand side, S is seen as",computations
"the lower triangular part of A. On the right-hand side, T is strictly upper triangular.",computations
Example 2 (Gauss-Seidel). Here S−1T has smaller eigenvalues:,computations
Chapter 7 Computations with Matrices,computations
A single Gauss-Seidel step takes the components vk and wk into,computations
2vk+1 = wk +b1,computations
"2wk+1 = vk +b2,",computations
The eigenvalues of S−1T are 1,computations
"4 and 0. The error is divided by 4 every time, so a sin-",computations
gle Gauss-Seidel step is worth two Jacobi steps. Since both methods require the same,computations
"number of operations—we just use the new value instead of the old, and actually save",computations
on storage—the Gauss-Seidel method is better.,computations
"This rule holds in many applications, even though there are examples in which Jacobi",computations
converges and Gauss-Seidel fails (or conversely). The symmetric case is straightfor-,computations
"ward: When all aii > 0, Gauss-Seidel converges if and only if A is positive deﬁnite.",computations
It was discovered during the years of hand computation (probably by accident) that,computations
convergence is faster if we go beyond the Gauss-Seidel correction xk+1 − xk. Roughly,computations
"speaking, those approximations stay on the same side of the solution x. An overrelax-",computations
"ation factor ω moves us closer to the solution. With ω = 1, we recover Gauss-Seidel;",computations
"with ω > 1, the method is known as successive overrelaxation (SOR). The optimal",computations
choice of ω never exceeds 2. It is often in the neighborhood of 1.9.,computations
"To describe overrelaxation, let D, L, and U be the parts of A on, below, and above",computations
"the diagonal, respectively. (This splitting has nothing to do with the A = LDU of elim-",computations
ination. In fact we now have A = L + D +U.) The Jacobi method has S = D on the,computations
left-hand side and T = −L −U on the right-hand side. Gauss-Seidel chose S = D + L,computations
"and T = −U. To accelerate the convergence, we move to",computations
[D+ωL]xk+1 = [(1−ω)D−ωU]xk +ωb.,computations
"Regardless of ω, the matrix on the left is lower triangular and the one on the right is",computations
"upper triangular. Therefore xk+1 can still replace xk, component by component, as soon",computations
as it is computed. A typical step is,computations
aii(xi)k+1 = aii(xi)k +ω[(−ai1x1 −···−aii−1xi−1)k+1 +(−aiixi −···−ainxn)k +bi].,computations
"If the old guess xk happened to coincide with the true solution x, then the new guess xk+1",computations
"would stay the same, and the quantity in brackets would vanish.",computations
Example 3 (SOR). For the same A =,computations
", each overrelaxation step is",computations
"If we divide by ω, these two matrices are the S and T in the splitting AS−T; the iteration",computations
is back to Sxk+1 = Txk +b. The crucial matrix L = S−1T is,computations
2ω(1−ω) 1−ω + 1,computations
7.4 Iterative Methods for Ax = b,computations
The optimal ω makes the largest eigenvalue of L (its spectral radius) as small as possible.,computations
The whole point of overrelaxation is to discover this optimal ω. The product of the,computations
eigenvalues equals detL = detT/detS:,computations
λ1λ2 = detL = (1−ω)2.,computations
"Always detS = detD because L lies below the diagonal, and detT = det(1 − ω)D be-",computations
cause U lies above the diagonal. Their product is detL = (1− ω)n. (This explains why,computations
"we never go as far as ω = 2. The product of the eigenvalues would be too large, and",computations
the iteration could not converge.) We also get a clue to the behavior of the eigenvalues:,computations
At the optimal ω the two eigenvalues are equal. They must both equal ω − 1 so their,computations
"product will match detL. This value of ω is easy to compute, because the sum of the",computations
eigenvalues always agrees with the sum of the diagonal entries (the trace of L):,computations
λ1 +λ2 = (ωopt −1)+(ωopt −1) = 2−2ωopt + 1,computations
This quadratic equation gives ωopt = 4(2−,computations
3) ≈ 1.07. The two equal eigenvalues are,computations
"approximately ω − 1 = 1.07, which is a major reduction from the Gauss-Seidel value",computations
"4 at ω = 1. In this example, the right choice of ω has again doubled the rate of",computations
"4)2 ≈ .07. If ω is further increased, the eigenvalues become a",computations
"complex conjugate pair—both have |λ| = ω −1, which is now increasing with ω.",computations
"The discovery that such an improvement could be produced so easily, almost as if by",computations
"magic, was the starting point for 20 years of enormous activity in numerical analysis.",computations
The ﬁrst problem was solved in Young’s 1950 thesis—a simple formula for the optimal,computations
ω. The key step was to connect the eigenvalues λ of L to the eigenvalues µ of the,computations
original Jacobi matrix D−1(−L−U). That connection is expressed by,computations
(λ +ω −1)2 = λω2µ2.,computations
"This is valid for a wide class of ﬁnite difference matrices, and if we take ω = 1 (Gauss-",computations
"Seidel) it yields λ 2 = λµ2. Therefore λ = 0 and λ = µ2 as in Example 2, where µ = ±1",computations
"and λ = 0, λ = 1",computations
4. All the matrices in Young’s class have eigenvalues µ that occur in,computations
"plus-minus pairs, and the corresponding λ are 0 and µ2. So Gauss-Seidel doubles the",computations
Jacobi rate of convergence.,computations
"The important problem is to choose ω so that λmax will be minimized. Fortunately,",computations
Young’s equation (7) is exactly our 2 by 2 example! The best ω makes the two roots λ,computations
both equal to ω −1:,computations
"(ω −1)+(ω −1) = 2−2ω + µ2ω2,",computations
"For a large matrix, this pattern will be repeated for a number of different pairs ±µi—and",computations
we can only make a single choice of ω. The largest µ gives the largest value of ω and,computations
Chapter 7 Computations with Matrices,computations
"of λ = ω − 1. Since our goal is to make λmax as small as possible, that extremal pair",computations
speciﬁes the best choice ωopt:,computations
λmax = ωopt −1.,computations
"The splittings of the −1, 2, −1 matrix of order n yield these eigenvalues",computations
"Jacobi (S = 0, 2, 0 matrix):",computations
S−1T has |λ|max = cos,computations
"Gauss-Seidel (S = −1, 2, 0 matrix):",computations
S−1T has |λ|max =,computations
SOR (with the best ω):,computations
"This can only be appreciated by an example. Suppose A is of order 21, which is very",computations
moderate. Then h = 1,computations
"22, cosπh = .99, and the Jacobi method is slow; cos2πh = .98",computations
means that even Gauss-Seidel will require a great many iterations. But since sinπh =,computations
".02 = .14, the optimal overrelaxation method will have the convergence factor",computations
ωopt = 1+λmax = 1.75.,computations
"The error is reduced by 25% at every step, and a single SOR step is the equivalent of",computations
30 Jacobi steps: (.99)30 = .75.,computations
That is a striking result from such a simple idea. Its real applications are not in one-,computations
dimensional problems like −uxx = f. A tridiagonal system Ax = b is already easy. It is,computations
for partial differential equations that overrelaxation (and other ideas) will be important.,computations
"Changing to −uxx −uyy = f leads to the “ﬁve-point scheme.” The entries −1, 2, −1 in",computations
"the x direction combine with −1, 2, −1 in the y direction to give a main diagonal of",computations
+4 and four off-diagonal entries of −1. The matrix A does not have a small bandwidth!,computations
There is no way to number the N2 mesh points in a square so that each point stays,computations
"close to all four of its neighbors. That is the true curse of dimensionality, and parallel",computations
computers will partly relieve it.,computations
"If the ordering goes a row at a time, every point must wait a whole row for the neigh-",computations
bor above it to turn up. The “ﬁve-point matrix” has bandwidth N: This matrix has had,computations
"more attention, and been attacked in more different ways, than any other linear equa-",computations
"tion Ax = b. The trend now is back to direct methods, based on an idea of Golub and",computations
Hockney; certain special matrices will fall apart when they are dropped the right way.,computations
(It is comparable to the Fast Fourier Transform.) Before that came the iterative methods,computations
"of alternating direction, in which the splitting separated the tridiagonal matrix in the x",computations
"direction from the one in the y direction, A recent choice is S = L0U0, in which small",computations
7.4 Iterative Methods for Ax = b,computations
"−1, 2, −1 in x and y",computations
"gives −1, −1, 4, −1, −1",computations
entries of the true L and U are set to zero while factoring A. It is called incomplete LU,computations
and it can be terriﬁc.,computations
"We cannot close without mentioning the conjugate gradient method, which looked",computations
dead hut is suddenly very much alive (Problem 33 gives the steps). It is direct rather,computations
"than iterative, but unlike elimination, it can be stopped part way. And needless to say,",computations
a completely new idea may still appear and win. But it seems fair to say that it was the,computations
change from .99 to .75 that revolutionized the solution of Ax = b.,computations
1. This matrix has eigenvalues 2−,computations
"2, 2, and 2+",computations
Find the Jacobi matrix D−1(−L −U) and the Gauss-Seidel matrix (D + L)−1(−U),computations
"and their eigenvalues, and the numbers ωopt and λmax for SOR.",computations
"2. For this n by n matrix, describe the Jacobi matrix J = D−1(−L−U):",computations
"Show that the vector x1 = (sinπh,sin2πh,...,sinnπh) is an eigenvector of J with",computations
eigenvalue λ1 = cosπh = cosπ/(n+1).,computations
"3. In Problem 2, show that xk = (sinkπh,sin2kπh,...,sinnkπh) is an eigenvector of A.",computations
Multiply xk by A to ﬁnd the corresponding eigenvalue αk. Verify that in the 3 by 3,computations
case these eigenvalues are 2−,computations
Note. The eigenvalues of the Jacobi matrix J = 1,computations
2(−L −U) = I − 1,computations
2A are λk = 1 −,computations
2αk = coskπh. They occur in plus-minus pairs and λmax is cosπh.,computations
Chapter 7 Computations with Matrices,computations
Problems 4–5 require Gershgorin’s “circle theorem”: Every eigenvalue of A lies in at,computations
"least one of the circles C1,...,Cn, where Ci has its center at the diagonal entry aii. Its",computations
radius ri = ∑i̸=j |aij| is equal to the absolute sum along the rest of the row.,computations
Proof. Suppose xi is the largest component of x. Then Ax = λx leads to,computations
(λ −aii)xi = ∑,computations
|λ −aii| ≤ ∑,computations
|ai j| = ri.,computations
is called diagonally dominant because every |aii| > ri. Show that zero cannot lie in,computations
"any of the circles, and conclude that A is nonsingular.",computations
"5. Write the Jacobi matrix J for the diagonally dominant A of Problem 4, and ﬁnd the",computations
"three Gershgorin circles for J. Show that all the radii satisfy ri < 1, and that the",computations
6. The true solution to Ax = b is slightly different from the elimination solution to,computations
LUx0 = b; A−LU misses zero because of roundoff. One strategy is to do everything,computations
"in double precision, but a better and faster way is iterative reﬁnement: Compute only",computations
"one vector r = b−Ax0 in double precision, solve LUy = r, and add the correction y to",computations
"x0. Problem: Multiply x1 = x0+y by LU, write the result as a splitting Sx1 = Tx0+b,",computations
and explain why T is extremely small. This single step brings us almost exactly to x.,computations
7. For a general 2 by 2 matrix,computations
ﬁnd the Jacobi iteration matrix S−1T = −D−1(L +U) and its eigenvalues µi. Find,computations
"also the Gauss-Seidel matrix −(D+L)−1U and its eigenvalues λi, and decide whether",computations
8. Change Ax = b to x = (I −A)x+b. What are S and T for this splitting? What matrix,computations
S−1T controls the convergence of xk+1 = (1−A)xk +b?,computations
"9. If λ is an eigenvalue of A, then",computations
is an eigenvalue of B = I −A. The real eigen-,computations
values of B have absolute value less than 1 if the real eigenvalues of A lie between,computations
10. Show why the iteration xk+1 = (I −A)xk +b does not converge for A =,computations
7.4 Iterative Methods for Ax = b,computations
11. Why is the norm of Bk never larger than ∥B∥k? Then ∥B∥ < 1 guarantees that the,computations
"powers Bk approach zero (convergence). This is no surprise, since |λ|max is below",computations
"12. If A is singular, then all splittings A = S − T must fail. From Ax = 0, show that",computations
S−1Tx = x. So this matrix B = S−1T has λ = 1 and fails.,computations
13. Change the 2s to 3s and ﬁnd the eigenvalues of S−1T for both methods:,computations
Does |λ|max for Gauss-Seidel equal |λ|2,computations
14. Write a computer code (MATLAB or other) for Gauss-Seidel. You can deﬁne S and,computations
"T from A, or set up the iteration loop directly from the entries aij. Test it on the −1,",computations
"2, −1 matrices A of order 10, 20, 50, with b = (1,0,...,0).",computations
15. The SOR splitting matrix S is the same as for Gauss-Seidel except that the diagonal,computations
"is divided by ω. Write a program for SOR on an n by n matrix. Apply it with ω = 1,",computations
"1.4, 1.8, 2.2 when A is the −1, 2, −1 matrix of order 10.",computations
"16. When A = AT, the Arnoldi-Lanczos method ﬁnds orthonormal q’s so that Aq j =",computations
b j−1q j−1 +a jq j +b jq j+1 (with q0 = 0). Multiply by qT,computations
j to ﬁnd a formula for a j. The,computations
equation says that AQ = QT where T is a,computations
17. What bound on |λ|max does Gershgorin give For these matrices (see Problem 4)?,computations
What are the three Gershgorin circles that contain all the eigenvalues?,computations
The key point for large matrices is that matrix-vector multiplication is much,computations
faster than matrix-matrix multiplication. A crucial construction starts with a vec-,computations
"tor b and computes Ab,A2b,... (but never A2!). The ﬁrst N vectors span the Nth",computations
Krylov subspace. They are the columns of the Krylov matrix KN:,computations
b Ab A2b ···,computations
"The Arnoldi-Lanczos iteration orthogonalizes the columns of KN, and the conjugate",computations
gradient iteration solves Ax = b when A is symmetric positive deﬁnite.,computations
Chapter 7 Computations with Matrices,computations
"x0 = 0, r0 = b, p0 = r0",computations
for n = 1 to N −1,computations
for n = 1 to N,computations
step length xn−1 to xn,computations
for j = 1 to n,computations
xn = xn−1 +αnpn−1,computations
h jn = qT,computations
rn = rn−1 −αnApn−1,computations
v = v−h jnq j,computations
pn = rn +βnpn−1,computations
Note: Only 1 matrix vector multiplication Aq and Ap,computations
"18. In Arnoldi, show that q2 is orthogonal to q1. The Arnoldi method is Gram-Schmidt",computations
orthogonalization applied to the Krylov matrix: KN = QNRN. The eigenvalues of,computations
"NAQN are often very close to those of A, even for N ≪ n. The Lanczos iteration is",computations
Arnoldi for symmetric matrices (all coded in ARPACK).,computations
"19. In conjugate gradients, show that r1 is orthogonal to r0 (orthogonal residuals), and",computations
pTAp0 = 0 (search directions are A-orthogonal). The iteration solves Ax = b by,computations
minimizing the error eTAe in the Krylov subspace. It is a fantastic algorithm.,computations
Linear Programming and Game Theory,linear_prog
"Algebra is about equations, and analysis is often about inequalities. The line between",linear_prog
them has always seemed clear. But I have realized that this chapter is a counterexam-,linear_prog
"ple: linear programming is about inequalities, but it is unquestionably a part of linear",linear_prog
algebra. It is also extremely useful—business decisions are more likely to involve linear,linear_prog
programming than determinants or eigenvalues.,linear_prog
There are three ways to approach the underlying mathematics: intuitively through,linear_prog
"the geometry, computationally through the simplex method, or algebraically through",linear_prog
"duality. These approaches are developed in Sections 8.1, 8.2, and 8.3. Then Section",linear_prog
8.4 is about problems (like marriage) in which the solution is an integer. Section 8.5,linear_prog
discusses poker and other matrix games. The MIT students in Bringing Down the House,linear_prog
"counted high cards to win at blackjack (Las Vegas follows ﬁxed rules, and a true matrix",linear_prog
game involves random strategies).,linear_prog
Section 8.3 has something new in this fourth edition. The simplex method is now,linear_prog
"in a lively competition with a completely different way to do the computations, called",linear_prog
an interior point method. The excitement began when Karmarkar claimed that his,linear_prog
"version was 50 times faster than the simplex method. (His algorithm, outlined in 8.2,",linear_prog
"was one of the ﬁrst to be patented—something we then believed impossible, and not",linear_prog
really desirable.) That claim brought a burst of research into methods that approach,linear_prog
the solution from the “interior” where all inequalities are strict: x ≥ 0 becomes x > 0.,linear_prog
The result is now a great way to get help from the dual problem in solving the primal,linear_prog
One key to this chapter is to see the geometric meaning of linear inequalities. An,linear_prog
inequality divides n-dimensional space into a halfspace in which the inequality is satis-,linear_prog
"ﬁed, and a halfspace in which it is not. A typical example is x +2y ≥ 4. The boundary",linear_prog
"between the two halfspaces is the line x+2y = 4, where the inequality is “tight.” Figure",linear_prog
8.1 would look almost the same in three dimensions. The boundary becomes a plane,linear_prog
"like x + 2y + z = 4, and above it is the halfspace x + 2y + z ≥ 4. In n dimensions, the",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
“plane” has dimension n−1.,linear_prog
x + 2y ≥ 4,linear_prog
x + 2y = 4,linear_prog
x + 2y = 0,linear_prog
Figure 8.1: Equations give lines and planes. Inequalities give halfspaces.,linear_prog
Another constraint is fundamental to linear programming: x and y are required to be,linear_prog
nonnegative. This pair of inequalities x ≥ 0 and y ≥ 0 produces two more halfspaces.,linear_prog
Figure 8.2 is bounded by the coordinate axes: x ≥ 0 admits all points to the right of,linear_prog
"x = 0, and y ≥ 0 is the halfspace above y = 0.",linear_prog
The Feasible Set and the Cost Function,linear_prog
The important step is to impose all three inequalities at once. They combine to give the,linear_prog
shaded region in Figure 8.2. This feasible set is the intersection of the three halfspaces,linear_prog
"x +2y ≥ 4, x ≥ 0, and y ≥ 0. A feasible set is composed of the solutions to a family of",linear_prog
linear inequalities like Ax ≥ b (the intersection of m halfspaces). When we also require,linear_prog
"that every component of x is nonnegative (the vector inequality x ≥ 0), this adds n more",linear_prog
"halfspaces. The more constraints we impose, the smaller the feasible set.",linear_prog
It can easily happen that a feasible set is bounded or even empty. If we switch our,linear_prog
"example to the halfspace x+2y ≤ 4, keeping x ≥ 0 and y ≥ 0, we get the small triangle",linear_prog
"OAB. By combining both inequalities x+2y ≥ 4 and x+2y ≤ 4, the set shrinks to a line",linear_prog
"where x+2y = 4. If we add a contradictory constraint like x+2y ≤ −2, the feasible set",linear_prog
The algebra of linear inequalities (or feasible sets) is one part of our subject. But,linear_prog
linear programming has another essential ingredient: It looks for the feasible point that,linear_prog
maximizes or minimizes a certain cost function like 2x + 3y. The problem in linear,linear_prog
programming is to ﬁnd the point that lies in the feasible set and minimizes the cost.,linear_prog
"The problem is illustrated by the geometry of Figure 8,2. The family of costs 2x+3y",linear_prog
gives a family of parallel lines. The minimum cost comes when the ﬁrst line intersects,linear_prog
"the feasible set. That intersection occurs at B, where x∗ = 0 and y∗ = 2; the minimum",linear_prog
"cost is 2x∗ +3y∗ = 6. The vector (0,2) is feasible because it lies in the feasible set, it is",linear_prog
"optimal because it minimizes the cost function, and the minimum cost 6 is the value of",linear_prog
the program. We denote optimal vectors by an asterisk.,linear_prog
x + 2y ≥ 4,linear_prog
2x + 3y = 6,linear_prog
2x + 3y = 0,linear_prog
"Figure 8.2: The feasible set with ﬂat sides, and the costs 2x+3y, touching at B.",linear_prog
The optimal vector occurs at a corner of the feasible set. This is guaranteed by the,linear_prog
"geometry, because the lines that give the cost function (or the planes, when we get to",linear_prog
more unknowns) move steadily up until they intersect the feasible set. The ﬁrst contact,linear_prog
must occur along its boundary! The “simplex method” will go from one corner of the,linear_prog
"feasible set to the next until it ﬁnds the corner with lowest cost. In contrast, “interior",linear_prog
point methods” approach that optimal solution from inside the feasible set.,linear_prog
"Note. With a different cost function, the intersection might not be just a single point. If",linear_prog
"the cost happened to be x+2y, the whole edge between B and A would be optimal. The",linear_prog
"minimum cost is x∗ +2y∗, which equals 4 for all these optimal vectors. On our feasible",linear_prog
"set, the maximum problem would have no solution! The cost can go arbitrarily high and",linear_prog
the maximum cost is inﬁnite.,linear_prog
Every linear programming problem falls into one of three possible categories:,linear_prog
1. The feasible set is empty.,linear_prog
2. The cost function is unbounded on the feasible set.,linear_prog
3. The cost reaches its minimum (or maximum) on the feasible set: the good case.,linear_prog
The empty and unbounded cases should be very uncommon for a genuine problem in,linear_prog
economics or engineering. We expect a solution.,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
There is a simple way to change the inequality x+2y ≥ 4 to an equation. Just introduce,linear_prog
the difference as a slack variable w = x + 2y − 4. This is our equation! The old con-,linear_prog
"straint x+2y ≥ 4 is converted into w ≥ 0, which matches perfectly the other inequality",linear_prog
"constraints x ≥ 0, y ≥ 0. Then we have only equations and simple nonnegativity con-",linear_prog
"straints on x, y, w. The variables w that “take up the slack” are now included in the vector",linear_prog
Minimize cx subject to Ax = b and x ≥ 0.,linear_prog
"The row vector c contains the costs; in our example, c = [2 3 0]. The condition x ≥ 0",linear_prog
puts the problem into the nonnegative part of Rn. Those inequalities cut back on the,linear_prog
"solutions to Ax = b. Elimination is in danger, and a completely new idea is needed.",linear_prog
The Diet Problem and Its Dual,linear_prog
Our example with cost 2x + 3y can be put into words. It illustrates the “diet problem”,linear_prog
"in linear programming, with two sources of protein—say steak and peanut butter. Each",linear_prog
"pound of peanut butter gives a unit of protein, and each steak gives two units. At least",linear_prog
four units are required in the diet. Therefore a diet containing x pounds of peanut butter,linear_prog
"and y steaks is constrained by x+2y ≥ 4, as well as by x ≥ 0 and y ≥ 0. (We cannot have",linear_prog
"negative steak or peanut butter.) This is the feasible set, and me p1001cm is to minimize",linear_prog
the cost. If a pound of peanut butter costs $2 and a steak is $3. then the cost of the whole,linear_prog
"diet is 2x+3y. Fortunately, the optimal diet is two steaks: x∗ = 0 and y∗ = 2.",linear_prog
"Every linear program, including this one, has a dual. If the original prohe1v a min-",linear_prog
"imization, its dual is a maximization. The minimum in the given “primal problem”",linear_prog
"equals the maximum in its dual. This is the key to linear programming, and it will be",linear_prog
explained in Section 8.3. Here we stay with the diet problem and try to interpret its dual.,linear_prog
"In place of the shopper, who buys enough protein at minimal cost, the dual problem",linear_prog
is faced by a druggist. Protein pills compete with steak and peanut butter. Immediately,linear_prog
we meet the two ingredients of a typical linear program: The druggist maximizes the,linear_prog
"pill price p, but that price is subject to linear constraints. Synthetic protein must not",linear_prog
cost more than the protein in peanut butter ($2 a unit) or the protein in steak ($3 for two,linear_prog
units). The price must be nonnegative or the druggist will not sell. Since four units of,linear_prog
"protein are required, the income to the druggist will be 4p:",linear_prog
"Maximize 4p, subject to p ≤ 2, 2p ≤ 3, and p ≥ 0.",linear_prog
In this example the dual is easier to solve than the primal; it has only one unknown,linear_prog
"p. The constraint 2p ≤ 3 is the tight one that is really active, and the maximum price",linear_prog
"of synthetic protein is p = $1.50. The maximum revenue is 4p = $6, and the shopper",linear_prog
ends up paying the same for natural and synthetic protein. That is the duality theorem:,linear_prog
The next section will concentrate on solving linear programs. This is the time to describe,linear_prog
two practical situations in which we minimize or maximize a linear cost function subject,linear_prog
Suppose General Motors makes a proﬁt of $200 on each,linear_prog
"Chevrolet, $300 on each Buick, and $500 on each Cadillac. These get 20, 17, and 14",linear_prog
"miles per gallon, respectively, and Congress insists that the average car must get 18. The",linear_prog
"plant can assemble a Chevrolet in 1 minute, a Buick in 2 minutes, and a Cadillac in 3",linear_prog
minutes. What is the maximum proﬁt in 8 hours (480 minutes)?,linear_prog
Problem Maximize the proﬁt 200x+300y+500z subject to,linear_prog
"2. Portfolio Selection. Federal bonds pay 5%, municipals pay 6%, and junk bonds pay",linear_prog
"9%. We can buy amounts x, y, z not exceeding a total of $100,000. The problem is to",linear_prog
"maximize the interest, with two constraints:",linear_prog
"(i) no more than $20,000 can be invested in junk bonds, and",linear_prog
"(ii) the portfolio’s average quality must be no lower than municipals, so x ≥ z.",linear_prog
Problem Maximize 5x+6y+9z subject to,linear_prog
"The three inequalities give three slack variables, with new equations like w = x −z and",linear_prog
inequalities w ≥ 0.,linear_prog
"1. Sketch the feasible set with constraints x + 2y ≥ 6, 2x + y ≥ 6, x ≥ 0, y ≥ 0. What",linear_prog
points lie at the three “corners” of this set?,linear_prog
"2. (Recommended) On the preceding feasible set, what is the minimum value of the",linear_prog
cost function x+y? Draw the line x+y = constant that ﬁrst touches the feasible set.,linear_prog
What points minimize the cost functions 3x+y and x−y?,linear_prog
"3. Show that the feasible set constrained by 2x+5y ≤ 3, −3x+8y ≤ −5, x ≥ 0, y ≥ 0,",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"4. Show that the following problem is feasible but unbounded, so it has no optimal",linear_prog
"solution: Maximize x+y, subject to x ≥ 0, y ≥ 0, −3x+2y ≤ −1, x−y ≤ 2.",linear_prog
"5. Add a single inequality constraint to x ≥ 0, y ≥ 0 such that the feasible set contains",linear_prog
"6. What shape is the feasible set x ≥ 0, y ≥ 0, z ≥ 0, x + y + z = 1, and what is the",linear_prog
7. Solve the portfolio problem at the end of the preceding section.,linear_prog
"8. In the feasible set for the General Motors problem, the nonnegativity x,y,z ≥ 0 leaves",linear_prog
an eighth of three-dimensional space (the positive octant). How is this cut by the two,linear_prog
"planes from the constraints, and what shape is the feasible set? How do its corners",linear_prog
"show that, with only these two constraints, there will be only two kinds of cars in the",linear_prog
"9. (Transportation problem) Suppose Texas, California, and Alaska each produce a mil-",linear_prog
"lion barrels of oil; 800,000 barrels are needed in Chicago at a distance of 1000, 2000,",linear_prog
"and 3000 miles from the three producers, respectively; and 2,200,000 barrels are",linear_prog
"needed in New England 1500, 3000, and 3700 miles away. If shipments cost one",linear_prog
"unit for each barrel-mile, what linear program with ﬁve equality constraints must be",linear_prog
solved to minimize the shipping cost?,linear_prog
This section is about linear programming with n unknowns x ≥ 0 and m constraints,linear_prog
"Ax ≥ b. In the previous section we had two variables, and one constraint x+2y ≥ 4. The",linear_prog
"full problem is not hard to explain, and not easy to solve.",linear_prog
"The best approach is to put the problem into matrix form. We are given A, b, and c:",linear_prog
1. an m by n matrix A.,linear_prog
"2. a column vector b with m components, and",linear_prog
3. a row vector c (cost vector) with n components.,linear_prog
"To be “feasible,” the vector x must satisfy x ≥ 0 and Ax ≥ b. The optimal vector x∗ is the",linear_prog
feasible vector of least cost—and the cost is cx = c1x1 +···+cnxn.,linear_prog
"Minimize the cost cx, subject to x ≥ 0 and Ax ≥ b.",linear_prog
The condition x ≥ 0 restricts x to the positive quadrant in n-dimensional space. In,linear_prog
R2 it is a quarter of the plane; it is an eighth of R3. A random vector has one chance,linear_prog
"in 2n of being nonnegative. Ax ≥ b produces m additional halfspaces, and the feasible",linear_prog
8.2 The Simplex Method,linear_prog
"vectors meet all of the m + n conditions. In other words, x lies in the intersection of",linear_prog
m + n halfspaces. This feasible set has ﬂat sides; it may be unbounded. and it may be,linear_prog
The cost function cx brings to the problem a family of parallel planes. One plane,linear_prog
cx = 0 goes through the origin. The planes cx = constant give all possible costs. As,linear_prog
"the cost varies, these planes sweep out the whole n-dimensional space. The optimal x∗",linear_prog
(lowest cost) occurs at the point where the planes ﬁrst touch the feasible set.,linear_prog
Our aim is to compute x∗. We could do it (in principle) by ﬁnding all the corners,linear_prog
"of the feasible set, and computing their costs. In practice this is impossible. There",linear_prog
"could be billions of corners, and we cannot compute them all. Instead we turn to the",linear_prog
"simplex method, one of the most celebrated ideas in computational mathematics. It was",linear_prog
"developed by Dantzig as a systematic way to solve linear programs, and either by luck",linear_prog
or genius it is an astonishing success. The steps of the simplex method are summarized,linear_prog
"later, and ﬁrst we try to explain them.",linear_prog
The Geometry: Movement Along Edges,linear_prog
I think it is the geometric explanation that gives the method away. Phase I simply locates,linear_prog
one corner of the feasible set. The heart of the method goes from corner to corner,linear_prog
along the edges of the feasible set. At a typical corner there are n edges to choose from.,linear_prog
"Some edges lead away from the optimal but unknown x∗, and others lead gradually",linear_prog
toward it. Dantzig chose an edge that leads to a new corner with a lower cost. There,linear_prog
is no possibility of returning to anything more expensive. Eventually a special corner is,linear_prog
"reached, from which all edges go the wrong way: The cost has been minimized. That",linear_prog
"corner is the optimal vector x∗, and the method stops.",linear_prog
The next problem is to turn the ideas of corner and edge into linear algebra. A corner,linear_prog
is the meeting point of n different planes. Each plane is given by one equation—just,linear_prog
"as three planes (front wall, side wall, and ﬂoor) produce a corner in three dimensions.",linear_prog
Each corner of the feasible set comes from turning n of the n + m inequalities Ax ≥ b,linear_prog
"and x ≥ 0 into equations, and ﬁnding the intersection of these n planes.",linear_prog
"One possibility is to choose the n equations x1 = 0,...,xn = 0, and end up at the",linear_prog
"origin. Like all the other possible choices, this intersection point will only be a genuine",linear_prog
corner if it also satisﬁes the m remaining inequality constraints. Otherwise it is not even,linear_prog
"in the feasible set, and is a complete fake. Our example with n = 2 variables and m = 2",linear_prog
"constraints has six intersections, illustrated in Figure 8.3. Three of them are actually",linear_prog
"corners P, Q, R of the feasible set. They are the vectors (0,6), (2,2), and (6,0), One",linear_prog
"of them must be the optimal vector (unless the minimum cost is −∞). The other three,",linear_prog
"including the origin, are fakes.",linear_prog
In general there are (n+m)!/n!m! possible intersections. That counts the number of,linear_prog
ways to choose n plane equations out of n + m. The size of that binomial coefﬁcient,linear_prog
makes computing all corners totally impractical for large m and n. It is the task of Phase,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
2x + y = 6,linear_prog
x + 2y = 6,linear_prog
"Figure 8.3: The corners P, Q, R, and the edges of the feasible set.",linear_prog
I either to ﬁnd one genuine corner or to establish that the feasible set is empty. We,linear_prog
continue on the assumption that a corner has been found.,linear_prog
Suppose one of the n intersecting planes is removed. The points that satisfy the,linear_prog
remaining n − 1 equations form an edge that comes out of the corner. This edge,linear_prog
"is the intersection of the n − 1 planes. To stay in the feasible set, only one direction is",linear_prog
"allowed along each edge. But we do have a choice of n different edges, and Phase II",linear_prog
must make that choice.,linear_prog
"To describe this phase, rewrite Ax ≥ b in a form completely parallel to the n simple",linear_prog
constraints x j ≥ 0. This is the role of the slack variables w = Ax − b. The constraints,linear_prog
"Ax ≥ b are translated into w1 ≥ 0,...,wm ≥ 0, with one slack variable for every row of",linear_prog
"A. The equation w = Ax−b, or Ax−w = b, goes into matrix form:",linear_prog
Slack variables give m equations,linear_prog
"The feasible set is governed by these m equations and the n+m simple inequalities x ≥ 0,",linear_prog
w ≥ 0. We now have equality constraints and nonnegativity.,linear_prog
"The simplex method notices no difference between x and w, so we simplify:",linear_prog
The equality constraints are now Ax = b. The n + m inequalities become just x ≥ 0.,linear_prog
The only trace left of the slack variable w is in the fact that the new matrix A is m by,linear_prog
"n+m, and the new x has n+m components. We keep this much of the original notation",linear_prog
leaving m and n unchanged as a reminder of what happened. The problem has become:,linear_prog
"Minimize cx, subject to x ≥ 0 and Ax = b.",linear_prog
8.2 The Simplex Method,linear_prog
"Example 1. The problem in Figure 8.3 has constraints x+2y ≥ 6, 2x+y ≥ 6, and cost",linear_prog
"x+y. The new system has four unknowns (x, y, and two slack variables):",linear_prog
1 1 0 0,linear_prog
"With equality constraints, the simplex method can begin. A corner is now a point where",linear_prog
n components of the new vector x (the old x and w) are zero. These n components of x,linear_prog
are the free variables in Ax = b. The remaining m components are the basic variables or,linear_prog
"pivot variables. Setting the n free variables to zero, the m equations Ax = b determine",linear_prog
the m basic variables. This “basic solution” x will be a genuine corner if its m nonzero,linear_prog
components are positive. Then x belongs to the feasible set.,linear_prog
8A The corners of the feasible set are the basic feasible solutions of Ax = b.,linear_prog
"A solution is basic when n of its m+n components are zero, and it is feasible",linear_prog
when it satisﬁes x ≥ 0. Phase I of the simplex method ﬁnds one basic feasible,linear_prog
solution. Phase II moves step by step to the optimal x∗.,linear_prog
The corner point P in Figure 8.3 is the intersection of x = 0 with 2x+y−6 = 0.,linear_prog
Which corner do we go to next? We want to move along an edge to an adjacent,linear_prog
"corner. Since the two corners are neighbors, m − 1 basic variables will remain basic.",linear_prog
"Only one of the 6s will become free (zero). At the same time, one variable will move up",linear_prog
"from zero to become basic. The other m−1 basic components (in this case, the other 6)",linear_prog
will change but stay positive. The choice of edge (see Example 2 below) decides which,linear_prog
variable leaves the basis and which one enters. The basic variables are computed by,linear_prog
solving Ax = b. The free components of x are set to zero.,linear_prog
Example 2. An entering variable and a leaving variable move us to a new corner.,linear_prog
+x3 +6x4 +2x5 = 8,linear_prog
Start from the corner at which x1 = 8 and x2 = 9 are the basic variables. At that corner,linear_prog
"x3 = x4 = x5 = 0. This is feasible, but the zero cost may not be minimal. It would",linear_prog
"be foolish to make x3 positive, because its cost coefﬁcient is +7 and we are trying to",linear_prog
lower the cost. We choose x5 because it has the most negative cost coefﬁcient −3. The,linear_prog
entering variable will be x5.,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"With x5 entering the basis, x1 or x2 must leave. In the ﬁrst equation, increase x5 and",linear_prog
decrease x1 while keeping x1 +2x5 = 8. Then x1 will be down to zero when x5 reaches,linear_prog
4. The second equation keeps x2 + 3x5 = 9. Here x5 can only increase as far as 3. To,linear_prog
"go further would make x2 negative, so the leaving variable is x2. The new corner has",linear_prog
"x = (2,0,0,0,3). The cost is down to −9.",linear_prog
"In Ax = b, the right sides divided by the coefﬁcients of the entering",linear_prog
3. The smallest ratio 9,linear_prog
"3 tells which variable hits zero ﬁrst, and must",linear_prog
"leave. We consider only positive ratios, because if the coefﬁcient of x5 were −3, then",linear_prog
increasing x5 would actually increase x2. (At x5 = 10 the second equation would give,linear_prog
x2 = 39.) The ratio 9,linear_prog
3 says that the second variable leaves. It also gives x5 = 3.,linear_prog
"If all coefﬁcients of x5 had been negative, this would be an unbounded case: we can",linear_prog
"make x5 arbitrarily large, and bring the cost down toward −∞.",linear_prog
"The current step ends at the new corner x = (2,0,0,0,3). The next step will only be",linear_prog
easy if the basic variables x1 and x5 stand by themselves (as x1 and x2 originally did).,linear_prog
"Therefore, we “pivot” by substituting x5 = 1",linear_prog
3(9−x2 −x3) into the cost function and the,linear_prog
"ﬁrst equation. The new problem, starting from the new corner, is:",linear_prog
7x3 −x4 −(9−x2 −x3) = x2 +8x3 −x4 −9,linear_prog
The next step is now easy. The only negative coefﬁcient −1 in the cost makes x4 the,linear_prog
entering variable. The ratios of 2,linear_prog
"0, the right sides divided by the x4 column, make",linear_prog
"x1 the leaving variable. The new corner is x∗ = (0,0,0, 1",linear_prog
"3,3). The new cost −91",linear_prog
"In a large problem, a departing variable might reenter the basis later on. But the cost",linear_prog
keeps going down—except in a degenerate case—so the m basic variables can’t be the,linear_prog
same as before. No corner is ever revisited! The simplex method must end at the optimal,linear_prog
corner (or at −∞ if the cost turns out to be unbounded). What is remarkable is the speed,linear_prog
at which x∗ is found.,linear_prog
"The cost coefﬁcients 7, −1, −3 at the ﬁrst corner and 1, 8, −1 at the",linear_prog
"second corner decided the entering variables. (These numbers go into r, the crucial vec-",linear_prog
tor deﬁned below. When they are all positive we stop.) The ratios decided the leaving,linear_prog
A corner is degenerate if more than the usual n com-,linear_prog
"ponents of x are zero. More than n planes pass through the corner, so a basic variable",linear_prog
"happens to vanish. The ratios that determine the leaving variable will include zeros, and",linear_prog
"the basis might change without actually moving from the corner. In theory, we could",linear_prog
stay at a corner and cycle forever in the choice of basis.,linear_prog
8.2 The Simplex Method,linear_prog
"Fortunately, cycling does not occur. It is so rare that commercial codes ignore it.",linear_prog
"Unfortunately, degeneracy is extremely common in applications—if you print the cost",linear_prog
after each simplex step you see it repeat several times before the simplex method ﬁnds a,linear_prog
good edge. Then the cost decreases again.,linear_prog
Each simplex step involves decisions followed by row operations—the entering and,linear_prog
"leaving variables have to be chosen, and they have to be made to come and go. One",linear_prog
"way to organize the step is to ﬁt A, b, c into a large matrix, or tableau:",linear_prog
Tableau is m+1 by m+n+1,linear_prog
"At the start, the basic variables may be mixed with the free variables. Renumbering if",linear_prog
"necessary, suppose that x1,...,xm are the basic (nonzero) variables at the current corner.",linear_prog
The ﬁrst m columns of A form a square matrix B (the basis matrix for that corner). The,linear_prog
"last n columns give an m by n matrix N. The cost vector c splits into [cB cN], and the",linear_prog
"unknown x into (xB,xN).",linear_prog
"At the corner, the free variables are xN = 0. There, Ax = b turns into BxB = b:",linear_prog
The basic variables will stand alone when elimination multiplies by B−1:,linear_prog
"To reach the fully reduced row echelon form R = rref(T), subtract cB times the top",linear_prog
block row from the bottom row:,linear_prog
0 cN − cBB−1N −cBB−1b,linear_prog
"Let me review the meaning of each entry in this tableau, and also call attention to Ex-",linear_prog
"ample 3 (following, with numbers). Here is the algebra:",linear_prog
xB +B−1NxN = B−1b,linear_prog
The cost cBxB +cNxN has been turned into,linear_prog
cx = (cN −cBB−1N)xN +cBB−1b,linear_prog
Cost at this corner = cBB−1b.,linear_prog
Every important quantity appears in the fully reduced tableau R. We can decide whether,linear_prog
the corner is optimal by looking at r = cN −cBB−1N in the middle of the bottom row. If,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"any entry in r is negative, the cost can still be reduced. We can make rxN negative,",linear_prog
"at the start of equation (2), by increasing a component of xN. That will be our next step.",linear_prog
"But if r ≥ 0, the best corner has been found. This is the stopping test, or optimality",linear_prog
The corner is optimal when r = cN − cBB−1N ≥ 0. Its cost is cBB−1b.,linear_prog
Negative components of r correspond to edges on which the cost goes down.,linear_prog
The entering variable xi corresponds to the most negative component of r.,linear_prog
The components of r are the reduced costs—the cost in cN to use a free variable,linear_prog
minus what it saves. Computing r is called pricing out the variables. If the direct cost,linear_prog
"(in cN) is less than the saving (from reducing basic variables), then ri < 0, and it will pay",linear_prog
to increase that free variable.,linear_prog
Suppose the most negative reduced cost is ri. Then the ith component of xN is the,linear_prog
"entering variable, which increases from zero to a positive value α at the next corner",linear_prog
(the end of the edge).,linear_prog
"As xi is increased, other components of x may decrease (to maintain Ax = b). The xk",linear_prog
that reaches zero ﬁrst becomes the leaving variable—it changes from basic to free. We,linear_prog
reach the next corner when a component of xB drops to zero.,linear_prog
That new corner is feasible because we still have x ≥ 0. It is basic because we again,linear_prog
have n zero components. The ith component of xN went from zero to α. The kth com-,linear_prog
ponent of xB dropped to zero (the other components of xB remain positive). The leaving,linear_prog
xk that drops to zero is the one that gives the minimum ratio in equation (3):,linear_prog
Suppose xi is the entering variable and u is column i of N:,linear_prog
xi = α = smallest ratio(B−1b)j,linear_prog
This minimum is taken only over positive components of B−1u. The kth col-,linear_prog
umn of the old B leaves the basis (xk becomes 0) and the new column u enters.,linear_prog
"B−1u is the column of B−1N in the reduced tableau R, above the most negative entry in",linear_prog
"the bottom row r, If B−1u ≤ 0, the next corner is inﬁnitely far away and the minimal",linear_prog
"cost is −∞ (this doesn’t happen here). Our example will go from the corner P to Q, and",linear_prog
begin again at Q.,linear_prog
"Example 3. The original cost function x+y and constraints Ax = b = (6,6) give",linear_prog
"At the corner P in Figure 8.3, x = 0 intersects 2x+y = 6. To be organized, we exchange",linear_prog
8.2 The Simplex Method,linear_prog
columns 1 and 3 to put basic variables before free variables:,linear_prog
"Then, elimination multiplies the ﬁrst row by −1, to give a unit pivot, and uses the second",linear_prog
row to produce zeros in the second column:,linear_prog
Fully reduced at P,linear_prog
"Look ﬁrst at r = [−1 1] in the bottom row. It has a negative entry in column 3, so the",linear_prog
third variable will enter the basis. The current corner P and its cost +6 are not optimal.,linear_prog
"The column above that negative entry is B−1u = (3,2); its ratios with the last column",linear_prog
"2. Since the ﬁrst ratio is smaller, the ﬁrst unknown w (and the ﬁrst column of",linear_prog
the tableau) is pushed out of the basis. We move along the feasible set from corner P to,linear_prog
corner Q in Figure 8.3.,linear_prog
"The new tableau exchanges columns 1 and 3, and pivoting by elimination gives",linear_prog
"In that new tableau at Q, r = [1",linear_prog
3] is positive. The stopping test is passed. The corner,linear_prog
x = y = 2 and its cost +4 are optimal.,linear_prog
The Organization of a Simplex Step,linear_prog
The geometry of the simplex method is now expressed in algebra—“corners” are “basic,linear_prog
feasible solutions.” The vector r and the ratio α are decisive. Their calculation is the,linear_prog
"heart of the simplex method, and it can be organized in three different ways:",linear_prog
"1. In a tableau, as above.",linear_prog
2. By updating B−1 when column u taken from N replaces column k of B.,linear_prog
"3. By computing B = LU, and updating these LU factors instead of B−1.",linear_prog
"This list is really a brief history of the simplex method, In some ways, the most",linear_prog
fascinating stage was the ﬁrst—the tableau—which dominated the subject for so many,linear_prog
"years. For most of us it brought an aura of mystery to linear programming, chieﬂy",linear_prog
because it managed to avoid matrix notation almost completely (by the skillful device of,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
writing out all matrices in full !). For computational purposes (except for small problems,linear_prog
"in textbooks), the day of the tableau is over.",linear_prog
"To see why, remember that after the most negative coefﬁcient in r indicates which",linear_prog
"column u will enter the basis, none of the other columns above r will be used. It was",linear_prog
"a waste of time to compute them. In a larger problem, hundreds of columns would be",linear_prog
"computed time and time again, just waiting for their turn to enter the basis. It makes",linear_prog
the theory clear to do the eliminations so completely and reach R. But in practice this,linear_prog
"It is quicker, and in the end simpler, to see what calculations are really necessary.",linear_prog
Each simplex step exchanges a column of N for a column of B. Those columns are,linear_prog
decided by r and α. This step begins with the current basis matrix B and the current,linear_prog
solution xB = B−1b.,linear_prog
A Step of the Simplex Method,linear_prog
1. Compute the row vector λ = cBB−1 and the reduced costs r = cN −λN.,linear_prog
"2. If r ≥ 0, stop: the current solution is optimal. Otherwise, if ri is the most",linear_prog
"negative component, choose u = column i of N to enter the basis.",linear_prog
"3. Compute the ratios of B−1b to B−1u, admitting only positive components",linear_prog
"of B−1u. (If B−1u < 0, the minimal cost is −∞.) When the smallest ratio",linear_prog
"occurs at component k, the kth column of the current B will leave.",linear_prog
"4. Update B, B−1, or LU, and the solution xB = B−1b. Return to step 1.",linear_prog
This is sometimes called the revised simplex method to distinguish it from the oper-,linear_prog
"ations on a tableau. It is really the simplex method itself, boiled down.",linear_prog
"This discussion is ﬁnished once we decide how to compute steps 1, 3, and 4:",linear_prog
"The most popular way is to work directly with B−1, calculating it explicitly at the ﬁrst",linear_prog
"corner. At succeeding corners, the pivoting step is simple. When column k of the identity",linear_prog
"matrix is replaced by u, column k of B−1 is replaced by v = B−1u. To recover the identity",linear_prog
"matrix, elimination will multiply the old B−1 by",linear_prog
"Many simplex codes use the product form of the inverse, which saves these simple",linear_prog
"matrices E−1 instead of directly updating B−1. When needed, they are applied to b and",linear_prog
"cB. At regular intervals (maybe every 40 simplex steps), B−1 is recomputed and the E−1",linear_prog
are erased. Equation (5) is checked in Problem 9 at the end of this section.,linear_prog
8.2 The Simplex Method,linear_prog
"A newer approach uses the ordinary methods of numerical linear algebra, regarding",linear_prog
equation (4) as three equations sharing the same matrix B:,linear_prog
"The usual factorization B = LU (or PB = LU, with row exchanges for stability) leads to",linear_prog
the three solutions. L and U can be updated instead of recomputed.,linear_prog
One question remains: How many simplex steps do we have to take? This is impos-,linear_prog
sible to answer in advance. Experience shows that the method touches only about 3m/2,linear_prog
"different corners, which means an operation count of about m2n. That is comparable to",linear_prog
"ordinary elimination for Ax = b, and is the reason for the simplex method’s success. But",linear_prog
mathematics shows that the path length cannot always be bounded by any ﬁxed multiple,linear_prog
or power of m. The worst feasible sets (Klee and Minty invented a lopsided cube) can,linear_prog
force the simplex method to try every corner—at exponential cost.,linear_prog
It was Khachian’s method that showed that linear programming could be solved in,linear_prog
"polynomial time.1 His algorithm stayed inside the feasible set, and captured x∗ in a series",linear_prog
"of shrinking ellipsoids. Linear programming is in the nice class P, not in the dreaded",linear_prog
class NP (like the traveling salesman problem). For NP problems it is believed (but not,linear_prog
"proved) that all deterministic algorithms must take exponentially long to ﬁnish, in the",linear_prog
"All this time, the simplex method was doing the job—in an average time that is now",linear_prog
"proved (for variants of the usual method) to be polynomial. For some reason, hidden in",linear_prog
"the geometry of many-dimensional polyhedra, bad feasible sets are rare and the simplex",linear_prog
We come now to the most sensational event in the recent history of linear programming.,linear_prog
"Karmarkar proposed a method based on two simple ideas, and in his experiments it",linear_prog
defeated the simplex method. The choice of problem and the details of the code are both,linear_prog
"crucial, and the debate is still going on. But Karmarkar’s ideas were so natural, and ﬁt",linear_prog
"so perfectly into the framework of applied linear algebra, that they can be explained in a",linear_prog
The ﬁrst idea is to start from a point inside the feasible set—we will suppose it is,linear_prog
"x0 = (1,1,...,1). Since the cost is cx, the best cost-reducing direction is toward −c.",linear_prog
Normally that takes us off the feasible set; moving in that direction does not maintain,linear_prog
"Ax = b. If Ax0 = b and Ax1 = b, then ∆x = x1 −x0 has to satisfy A∆x = 0. The step ∆x",linear_prog
"must lie in the nullspace of A. Therefore we project −c onto the nullspace, to ﬁnd the",linear_prog
feasible direction closest to the best direction. This is the natural but expensive step in,linear_prog
"1The number of operations is bounded by powers of m and n, as in elimination. For integer programming and",linear_prog
"factoring into primes, all known algorithms can take exponentially long. The celebrated conjecture “P ̸= NP” says",linear_prog
that such problems cannot have polynomial algorithms.,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"The step ∆x is a multiple of the projection −Pc. The longer the step, the more the",linear_prog
cost is reduced—but we cannot go out of the feasible set. The multiple of −Pc is chosen,linear_prog
"so that x1 is close to, but a little inside, the boundary at which a component of x reaches",linear_prog
That completes the ﬁrst idea—the projection that gives the steepest feasible descent.,linear_prog
The second step needs a new idea. since to continue in the same direction is useless.,linear_prog
"Karmarkar’s suggestion is to transform x1 back to (1,1,...,1) at the center. His",linear_prog
"change of variables was nonlinear, but the simplest transformation is just a rescaling by",linear_prog
a diagonal matrix D. Then we have room to move. The rescaling from x to X = D−1x,linear_prog
changes the constraint and the cost:,linear_prog
"Therefore the matrix AD takes the place of A, and the vector cTD takes the place of cT.",linear_prog
The second step projects the new c onto the nullspace of the new A. All the work is in,linear_prog
"this projection, to solve the weighted normal equations:",linear_prog
The normal way to compute y is by elimination. Gram-Schmidt will orthogonalize the,linear_prog
"columns of DAT, which can be expensive (although it makes the rest of the calculation",linear_prog
"easy). The favorite for large sparse problems is the conjugate gradient method, which",linear_prog
"gives the exact answer y more slowly than elimination, but you can go part way and then",linear_prog
stop. In the middle of elimination you cannot stop.,linear_prog
"Like other new ideas in scientiﬁc computing, Karmarkar’s method succeeded on some",linear_prog
problems and not on others. The underlying idea was analyzed and improved. Newer,linear_prog
interior point methods (staying inside the feasible set) are a major success—mentioned,linear_prog
in the next section. And the simplex method remains tremendously valuable. like the,linear_prog
"whole subject of linear programming—which was discovered centuries after Ax = b, but",linear_prog
shares the fundamental ideas of linear algebra. The most far-reaching of those ideas is,linear_prog
"duality, which comes next.",linear_prog
"1. Minimize x1 +x2 −x3, subject to",linear_prog
2x1 −4x2 +x3 +x4,linear_prog
"Which of x1, x2, x3 should enter the basis, and which of x4, x5 should leave? Compute",linear_prog
"the new pair of basic variables, and ﬁnd the cost at the new corner.",linear_prog
"2. After the preceding simplex step, prepare for and decide on the next step.",linear_prog
8.2 The Simplex Method,linear_prog
"3. In Example 3, suppose the cost is 3x + y. With rearrangement, the cost vector is",linear_prog
"c = (0,1,3,0). Show that r ≥ 0 and, therefore, that corner P is optimal.",linear_prog
"4. Suppose the cost function in Example 3 is x − y, so that after rearrangement c =",linear_prog
"(0,−1,1,0) at the corner P. Compute r and decide which column u should enter the",linear_prog
basis. Then compute B−1u and show from its sign that you will never meet another,linear_prog
"corner. We are climbing the y-axis in Figure 8.3, and x−y goes to −∞.",linear_prog
"5. Again in Example 3, change the cost to x+3y. Verify that the simplex method takes",linear_prog
"you from P to Q to R, and that the corner R is optimal.",linear_prog
6. Phase I ﬁnds a basic feasible solution to Ax = b (a corner). After changing signs,linear_prog
"to make b ≥ 0, consider the auxiliary problem of minimizing w1 + w2 + ··· + wm,",linear_prog
"subject to x ≥ 0, w ≥ 0, Ax + w = b. Whenever Ax = b has a nonnegative solution,",linear_prog
the minimum cost in this problem will be zero—with w∗ = 0.,linear_prog
"(a) Show that, for this new problem, the corner x = 0, w = b is both basic and fea-",linear_prog
"sible. Therefore its Phase I is already set, and the simplex method can proceed",linear_prog
"to ﬁnd the optimal pair x∗, w∗. If w∗ = 0, then x∗ is the required corner in the",linear_prog
"(b) With A = [1 1] and b = [3], write out the auxiliary problem, its Phase I vector",linear_prog
"x = 0, w = b, and its optimal vector. Find the corner of the feasible set x1−x2 = 3,",linear_prog
"x1 ≥ x2 ≥ 0, and draw a picture of this set.",linear_prog
"7. If we wanted to maximize instead of minimize the cost (with Ax = b and x ≥ 0),",linear_prog
"what would be the stopping test on r, and what rules would choose the column of N",linear_prog
to make basic and the column of B to make free?,linear_prog
"8. Minimize 2x1 +x2, subject to x1 +x2 ≥ 4, x1 +3x2 ≥ 12, x1 −x2 ≥ 0, x ≥ 0.",linear_prog
"9. Verify the inverse in equation (5), and show that BE has Bv = u in its kth column.",linear_prog
"Then BE is the correct basis matrix for the next stop, E−1B−1 is its inverse, and E−1",linear_prog
updates the basis matrix correctly.,linear_prog
"10. Suppose we want to minimize cx = x1 −x2, subject to",linear_prog
"(all x1,x2,x3,x4 ≥ 0).",linear_prog
"Starting from x = (0,0,6,12), should x1 or x2 be increased from its current value of",linear_prog
zero? How far can it be increased until the equations force x3 or x4 down to zero? At,linear_prog
"that point, what is the new x?",linear_prog
"11. For the matrix P = I − AT(AAT)−1A, show that if x is in the nullspace of A, then",linear_prog
Px = x. The nullspace stays unchanged under this projection.,linear_prog
"12. (a) Minimize the cost cTx = 5x1 +4x2 +8x3 on the plane x1 +x2 +x3 = 3, by testing",linear_prog
"the vertices P, Q, R, where the triangle is cut off by the requirement x ≥ 0.",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"(b) Project c = (5,4,8) onto the nullspace of A = [1 1 1], and ﬁnd the maximum",linear_prog
step s that keeps e−sPc nonnegative.,linear_prog
"Elimination can solve Ax = b, but the four fundamental subspaces showed that a different",linear_prog
and deeper understanding is possible. It is exactly the same for linear programming. The,linear_prog
"mechanics of the simplex method will solve a linear program, but duality is really at the",linear_prog
"center of the underlying theory. Introducing the dual problem is an elegant idea, and",linear_prog
at the same time fundamental for the applications. We shall explain as much as we,linear_prog
The theory begins with the given primal problem:,linear_prog
"Minimize cx, subject to x ≥ 0 and Ax ≥ b.",linear_prog
"The dual problem starts from the same A, b, and c, and reverses everything. In the",linear_prog
"primal, c is in the cost function and b is in the constraint, In the dual, b and c are",linear_prog
"switched, The dual unknown y is a row vector with m components, and the feasible set",linear_prog
has yA ≤ c instead of Ax ≥ b.,linear_prog
"In short, the dual of a minimum problem is a maximum problem. Now y ≥ 0:",linear_prog
"Maximize yb, subject to y ≥ 0 and yA ≤ c.",linear_prog
The dual of this problem is the original minimum problem. There is complete symmetry,linear_prog
between the primal and dual problems. The simplex method applies equally well to a,linear_prog
"maximization—anyway, both problems get solved at once.",linear_prog
I have to give you some interpretation of all these reversals. They conceal a competi-,linear_prog
"tion between the minimizer and the maximizer. In the diet problem, the minimizer has n",linear_prog
"foods (peanut butter and steak, in Section 8.1). They enter the diet in the (nonnegative)",linear_prog
"amounts x1,...,xn. The constraints represent m required vitamins, in place of the one",linear_prog
earlier constraint of sufﬁcient protein. The entry aij measures the ith vitamin in the jth,linear_prog
"food, and the ith row of Ax ≥ b forces the diet to include at least bi of that vitamin. If ci",linear_prog
"is the cost of the jth food, then c1x1 +···+cnxn = cx is the cost of the diet. That cost is",linear_prog
"In the dual, the druggist is selling vitamin pills at prices yi ≥ 0. Since food j",linear_prog
"contains vitamins in the amounts aij, the druggist’s price for the vitamin equivalent",linear_prog
cannot exceed the grocer’s price c j. That is the jth constraint in yA ≤ c. Working within,linear_prog
"this constraint on vitamin prices, the druggist can sell the required amount bi of each",linear_prog
vitamin for a total income of y1b1 +···+ymbm = yb—to be maximized.,linear_prog
The feasible sets for the primal and dual problems look completely different. The,linear_prog
"ﬁrst is a subset of Rn, marked out by x ≥ 0 and Ax ≥ b. The second is a subset of Rm,",linear_prog
8.3 The Dual Problem,linear_prog
determined by y ≥ 0 and AT and c. The whole theory of linear programming hinges on,linear_prog
the relation between primal and dual. Here is the fundamental result:,linear_prog
"Duality Theorem When both problems have feasible vectors, they have",linear_prog
optimal x∗ and y∗. The minimum cost cx∗ equals the maximum income y∗b.,linear_prog
"If optimal vectors do not exist, there are two possibilities: Either both feasible sets are",linear_prog
"empty, or one is empty and the other problem is unbounded (the maximum is +∞ or the",linear_prog
The duality theorem settles the competition between the grocer and the druggist. The,linear_prog
result is always a tie. We will ﬁnd a similar “minimax theorem” in game theory. The,linear_prog
"customer has no economic reason to prefer vitamins over food, even though the druggist",linear_prog
guarantees to match the grocer on every food—and even undercuts on expensive foods,linear_prog
"(like peanut butter). We will show that expensive foods are kept out of the optimal diet,",linear_prog
so the outcome can be (and is) a tie.,linear_prog
"This may seem like a total stalemate, but I hope you will not be fooled. The optimal",linear_prog
"vectors contain the crucial information. In the primal problem, x∗ tells the purchaser",linear_prog
"what to buy. In the dual, y∗ ﬁxes the natural prices (shadow prices) at which the economy",linear_prog
should run. Insofar as our linear model reﬂects the true economy. x∗ and y∗ represent,linear_prog
the essential decisions to be made.,linear_prog
We want to prove that c∗x = y∗b. It may seem obvious that the druggist can raise,linear_prog
"the vitamin prices y∗ to meet the grocer, hut only one thing is truly clear: Since each",linear_prog
"food can be replaced by its vitamin equivalent, with no increase in cost, all adequate",linear_prog
"food diets must cost at least as much as vitamins. This is only a one-sided inequality,",linear_prog
"druggist’s price ≤ grocer’s price. It is called weak duality, and it is easy to prove for",linear_prog
any linear program and its dual:,linear_prog
"If x and y are feasible in the primal and dual problems, then yb ≤ cx.",linear_prog
"Proof. Since the vectors are feasible, they satisfy Ax ≥ b and yA ≤ c. Because feasi-",linear_prog
"bility also includes x ≥ 0 and y ≥ 0, we can take inner products without spoiling those",linear_prog
inequalities (multiplying by negative numbers would reverse them):,linear_prog
"Since the left-hand sides are identical, we have weak duality yb ≤ cx.",linear_prog
This one-sided inequality prohibits the possibility that both problems are unbounded.,linear_prog
"If yb is arbitrarily large, a feasible x would contradict yb ≤ cx. Similarly, if cx can go",linear_prog
"down to −∞, the dual cannot admit a feasible y.",linear_prog
"Equally important, any vectors that achieve yb = cx must be optimal. At that point",linear_prog
the grocer’s price equals the druggist’s price. We recognize an optimal food diet and,linear_prog
optimal vitamin prices by the fact that the consumer has nothing to choose:,linear_prog
"If the vectors x and y are feasible and cx = yb, then x and y are optimal.",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"Since no feasible y can make yb larger than cx, our y that achieves this value is opti-",linear_prog
"mal. Similarly, any x that achieves the cost cx = yb must be an optimal x∗.",linear_prog
We give an example with two foods and two vitamins. Note how AT appears when,linear_prog
"we write out the dual, since yA ≤ c for row vectors means ATyT ≤ cT for columns.",linear_prog
"x1 ≥ 0, x2 ≥ 0",linear_prog
"y1 ≥ 0, y2 ≥ 0",linear_prog
2x1 +x2 ≥ 6,linear_prog
2y1 +5y2 ≤ 1,linear_prog
5x1 +3x2 ≥ 7.,linear_prog
y1 +3x2 ≤ 4.,linear_prog
"Solution x1 = 3 and x2 = 0 are feasible, with cost x1 +4x2 = 3. In the dual, y1 = 1",linear_prog
y2 = 0 give the same value 6y1 +7y2 = 3. These vectors must be optimal.,linear_prog
Please look closely to see what actually happens at the moment when yb = cx. Some,linear_prog
"of the inequality constraints are tight, meaning that equality holds. Other constraints are",linear_prog
"loose, and the key rule makes economic sense:",linear_prog
(i) The diet has x∗,linear_prog
j = 0 when food j is priced above its vitamin equivalent.,linear_prog
(ii) The price is y∗,linear_prog
i = 0 when vitamin i is oversupplied in the diet x∗.,linear_prog
"In the example, x2 = 0 because the second food is too expensive. Its price exceeds the",linear_prog
"druggist’s price, since y1 + 3y2 ≤ 4 is a strict inequality 1",linear_prog
"2 + 0 < 4. Similarly, the diet",linear_prog
"required seven units of the second vitamin, but actually supplied 5x1 +3x2 = 15. So we",linear_prog
"found y2 = 0, and that vitamin is a free good. You can see how the duality has become",linear_prog
These optimality conditions are easy to understand in matrix terms. From equation,linear_prog
"(1) we want y∗Ax∗ = y∗b at the optimum. Feasibility requires Ax∗ ≥ b, and we look for",linear_prog
any components in which equality fails. This corresponds to a vitamin that is oversup-,linear_prog
"plied, so its price is y∗",linear_prog
"At the same time, we have y∗A ≤ c. All strict inequalities (expensive foods) corre-",linear_prog
"j = 0 (omission from the diet). That is the key to y∗Ax∗ = cx∗, which we",linear_prog
"need. These are the complementary slackness conditions of linear programming, and",linear_prog
the Kuhn-Tucker conditions of nonlinear programming:,linear_prog
The optimal vectors x∗ and y∗ satisfy complementary slackness:,linear_prog
(y∗A)j > c j,linear_prog
Let me repeat the proof. Any feasible vectors x and y satisfy weak duality:,linear_prog
yb ≤ y(Ax) = (yA)x ≤ cx.,linear_prog
"We need equality, and there is only one way in which y∗b can equal y∗(Ax∗). Any time",linear_prog
"bi < (Ax∗)i, the factor y∗",linear_prog
i that multiplies these components must be zero.,linear_prog
8.3 The Dual Problem,linear_prog
"Similarly, feasibility gives yAx ≤ cx. We get equality only when the second slackness",linear_prog
"condition is fulﬁlled. If there is an overpricing (y∗A)j < c j, it must be canceled through",linear_prog
j = 0. This leaves us with y∗b = cx∗ in equation (3). This equality,linear_prog
guarantees the optimality of x∗ and y∗.,linear_prog
The Proof of Duality,linear_prog
The one-sided inequality yb ≤ cx was easy to prove; it gave a quick test for optimal,linear_prog
vectors (they turn it into an equality); and now it has given the slackness conditions in,linear_prog
equation (2). The only thing it has not done is to show that y∗b = cx∗ is really possible.,linear_prog
"Until those optimal vectors are actually produced, the duality theorem is not complete.",linear_prog
To produce y∗ we return to the simplex method—which has already computed x∗. Our,linear_prog
problem is to show that the method stopped in the right place for the dual problem (even,linear_prog
though it was constructed to solve the primal). Recall that the m inequalities Ax ≥ b,linear_prog
were changed to equations by introducing the slack variables w = Ax−b:,linear_prog
"Every simplex step picked m columns of the long matrix [A −I] to be basic, and shifted",linear_prog
them (theoretically) to the front. This produced [B N]. The same shift reordered the,linear_prog
"long cost vector [c 0] into [cB cN]. The stopping condition, which brought the simplex",linear_prog
"method to an end, was r = cN −cBB−1N ≥ 0.",linear_prog
"This condition r ≥ 0 was ﬁnally met, since the number of corners is ﬁnite. At that",linear_prog
moment the cost was as low as possible:,linear_prog
"If we can choose y∗ = cBB−1 in the dual, we certainly have y∗b = cx∗. The minimum",linear_prog
and maximum will be equal. We have to show that this y∗ satisﬁes the dual constraints,linear_prog
yA ≤ c and y ≥ 0:,linear_prog
When the simplex method reshufﬂes the long matrix and vector to put the basic variables,linear_prog
"ﬁrst, this rearranges the constraints in equation (6) into",linear_prog
"For y∗ = cBB−1, the ﬁrst half is an equality and the second half is cBB−1N ≤ cN. This is",linear_prog
"the stopping condition r ≥ 0 that we know to be satisﬁed! Therefore our y∗ is feasible,",linear_prog
"and the duality theorem is proved. By locating the critical m by m matrix B, which is",linear_prog
"nonsingular as long as degeneracy is forbidden, the simplex method has produced the",linear_prog
optimal y∗ as well as x∗.,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"In calculus, everybody knows the condition for a maximum or a minimum: The ﬁrst",linear_prog
derivatives are zero. But this is completely changed by constraints. The simplest exam-,linear_prog
"ple is the line y = x. Its derivative is never zero, calculus looks useless, and the largest",linear_prog
y is certain to occur at the end of the interval. That is exactly the situation in linear pro-,linear_prog
"gramming! There are more variables, and an interval is replaced by a feasible set, but",linear_prog
still the maximum is always found at a corner of the feasible set (with only m nonzero,linear_prog
"The problem in linear programming is to locate that cornet For this, calculus is not",linear_prog
"completely helpless. Far from it, because “Lagrange multipliers” will bring back zero",linear_prog
derivatives at the maximum and minimum. The dual variables y are exactly the La-,linear_prog
grange multipliers. And they answer the key question: How does the minimum cost,linear_prog
"cx∗ = y∗b change, if we change b or c?",linear_prog
This is a question in sensitivity analysis. It allows us to squeeze extra information out,linear_prog
"of the dual problem. For an economist or an executive, these questions about marginal",linear_prog
cost are the most important.,linear_prog
"If we allow large changes in b or c, the solution behaves in a very jumpy way. As",linear_prog
"the price of eggs increases, there will be a point at which they disappear from the diet.",linear_prog
"The variable xegg will jump from basic to free. To follow it properly, we would have",linear_prog
"to introduce “parametric” programming. But if the changes are small, the corner that",linear_prog
was optimal remains optimal. The choice of basic variables does not change; B and N,linear_prog
"stay the same. Geometrically, we shifted the feasible set a little (by changing b), and we",linear_prog
"tilted the planes that come up to meet it (by changing c). When these changes are small,",linear_prog
contact occurs at the same (slightly moved) corner.,linear_prog
"At the end of the simplex method, when the right basic variables are known, the",linear_prog
"corresponding m columns of A make up the basis matrix B. At that corner, a shift of size",linear_prog
∆b changes the minimum cost by y∗∆b. The dual solution y∗ gives the rate of change,linear_prog
of minimum cost (its derivative) with respect to changes in b. The components of y∗,linear_prog
"are the shadow prices. If the requirement for a vitamin goes up by ∆, and the druggist’s",linear_prog
"1, then the diet cost (from druggist or grocer) will go up by y∗",linear_prog
1∆. In the case,linear_prog
"1 is zero, that vitamin is a free good and the small change has no effect. The diet",linear_prog
already contained more than b1.,linear_prog
We now ask a different question. Suppose we insist that the diet contain some small,linear_prog
edible amount of egg. The condition xegg ≥ 0 is changed to xegg ≥ δ. How does this,linear_prog
"If eggs were in the diet x∗, there is no change. But if x∗",linear_prog
"egg = 0, it will cost extra to add",linear_prog
"in the amount δ. The increase will not be the full price ceggδ, since we can cut down on",linear_prog
"other foods. The reduced cost of eggs is their own price, minus the price we are paying",linear_prog
for the equivalent in cheaper foods. To compute it we return to equation (2) of Section,linear_prog
8.3 The Dual Problem,linear_prog
cost = (cN −cBB−1N)xN +cBB−1b = rxN +cBB−1b.,linear_prog
"If egg is the ﬁrst free variable, then increasing the ﬁrst component of xN to δ will increase",linear_prog
the cost by r1δ. The real cost of egg is r1. This is the change in diet cost as the zero lower,linear_prog
"bound (nonnegativity constraint) moves upwards. We know that r ≥ 0, and economics",linear_prog
tells us the same thing: The reduced cost of eggs cannot be negative or they would have,linear_prog
"The simplex method moves along edges of the feasible set, eventually reaching the opti-",linear_prog
mal corner x∗. Interior point methods start inside the feasible set (where the constraints,linear_prog
are all inequalities). These methods hope to move more directly to x∗ (and also ﬁnd y∗).,linear_prog
"When they are very close to the answer, they stop.",linear_prog
One way to stay inside is to put a barrier at the boundary. Add an extra cost in the,linear_prog
form of a logarithm that blows up when any variable x or any slack variable w = Ax−b,linear_prog
touches zero. The number θ is a small parameter to be chosen:,linear_prog
"This cost is nonlinear (but linear programming is already nonlinear, from inequalities).",linear_prog
"The notation is simpler if the long vector (x,w) is renamed x and [A −I] is renamed A.",linear_prog
The primal constraints are now x ≥ 0 and Ax = b. The sum of lnxi in the barrier now,linear_prog
The dual constraints are yA ≤ c. (We don’t need y ≥ 0 when we have Ax = b in,linear_prog
"the primal.) The slack variable is s = c − yA, with s ≥ 0. What are the Kuhn-Tucker",linear_prog
conditions for x and y to be the optimal x∗ and y∗? Along with the constraints we require,linear_prog
duality: cx∗ = y∗b.,linear_prog
Including the barrier gives an approximate problem P(θ). For its Kuhn-Tucker op-,linear_prog
"timality conditions, the derivative of lnxi gives 1/xi. If we create a diagonal matrix X",linear_prog
"from those positive numbers xi, and use e = [1 ··· 1] for the row vector of n+m ones,",linear_prog
then optimality in P(θ) is as follows:,linear_prog
"As θ → 0, we expect those optimal x and y to approach x∗ and y∗ for the original no-",linear_prog
"barrier problem, and θeX−1 will stay nonnegative. The plan is to solve equations (9a–",linear_prog
"9b) with smaller and smaller barriers, given by the size of θ.",linear_prog
"In reality, those nonlinear equations are approximately solved by Newton’s method",linear_prog
"(which means they are linearized). The nonlinear term is s = θeX−1. To avoid 1/xi,",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"rewrite that as sX = θe. Creating the diagonal matrix S from s, this is eSX = θe. If we",linear_prog
"change e, y, c, and s to column vectors, and transpose, optimality now has three parts:",linear_prog
"Newton’s method takes a step ∆x, ∆y, ∆s from the current x, y, s. (Those solve equa-",linear_prog
"tions (10a) and (10b), but not (10c).) By ignoring the second-order term ∆X∆Se, the",linear_prog
corrections come from linear equations!,linear_prog
Robert Freund’s notes for his MIT class pin down the (quadratic) convergence rate and,linear_prog
"the computational complexity of this algorithm. Regardless of the dimensions m and n,",linear_prog
the duality gap sx is generally below 10−8 after 20–80 Newton steps. This algorithm,linear_prog
"is used almost “as is” in commercial interior-point software, and for a large class of",linear_prog
nonlinear optimization problems as well.,linear_prog
The Theory of Inequalities,linear_prog
"There is more than one way to study duality. We quickly proved yb ≤ cx, and then",linear_prog
used the simplex method to get equality. This was a constructive proof; x∗ and y∗ were,linear_prog
"actually computed. Now we look brieﬂy at a different approach, which omits the simplex",linear_prog
algorithm and looks more directly at the geometry. I think the key ideas will be just as,linear_prog
"clear (in fact, probably clearer) if we omit some of the details.",linear_prog
The best illustration of this approach came in the Fundamental Theorem of Linear,linear_prog
Algebra. The problem in Chapter 2 was to ﬁnd b in the column space of A. After elim-,linear_prog
"ination and the four subspaces, this solvability question was answered in a completely",linear_prog
different way by Problem 11 in Section 3.1:,linear_prog
Ax = b has a solution,linear_prog
there is a y such that yA = 0 and yb ̸= 0.,linear_prog
"This is the theorem of the alternative, because to ﬁnd both x and y is impossible: If Ax =",linear_prog
"b then yAx = yb ̸= 0, and this contradicts yAx = 0x = 0. In the language of subspaces,",linear_prog
"either b is in the column space, or it has a component sticking into the left nullspace.",linear_prog
That component is the required y.,linear_prog
"For inequalities, we want to ﬁnd a theorem of exactly the same kind. Start with the",linear_prog
"same system Ax = b, but add the constraint x ≥ 0. When does there exist a nonnegative",linear_prog
solution to Ax = b?,linear_prog
"In Chapter 2, b was anywhere in the column space. Now we allow only nonnegative",linear_prog
"combinations, and the b’s no longer ﬁll out a subspace. Instead, they ﬁll a cone-shaped",linear_prog
8.3 The Dual Problem,linear_prog
"Figure 8.4: The cone of nonnegative combinations of the columns: b = Ax with x ≥ 0. When b is outside the cone,",linear_prog
it is separated by a hyperplane (perpendicular to y).,linear_prog
"region. For n columns in Rm, the cone becomes an open-ended pyramid. Figure 8.4 has",linear_prog
"four vectors in R2, and A is 2 by 4. If b lies in this cone, there is a nonnegative solution",linear_prog
to Ax = b; otherwise not.,linear_prog
What is the alternative if b lies outside the cone? Figure 8.4 also shows a “separating,linear_prog
"hyperplane,” which has the vector b on one side and the whole cone on the other side.",linear_prog
The plane consists of all vectors perpendicular to a ﬁxed vector y. The angle between,linear_prog
"y and b is greater than 90°, so yb < 0. The angle between y and every column of A is",linear_prog
"less than 90°, so yA ≥ 0. This is the alternative we are looking for. This theorem of the",linear_prog
separating hyperplane is fundamental to mathematical economics.,linear_prog
Ax = b has a nonnegative solution,linear_prog
there is a y with yA ≥ 0 and,linear_prog
Example 1. The nonnegative combinations of the columns of A = I ﬁll the positive,linear_prog
"quadrant b ≥ 0. For every other b, the alternative must hold for some y:",linear_prog
"The x-axis, perpendicular to y = [0 1], separates b from the cone = quadrant.",linear_prog
Here is a curious pair of alternatives. It is impossible for a subspace S and its or-,linear_prog
thogonal complement S⊥ both to contain positive vectors. Their inner product would,linear_prog
"be positive, not zero. But S might be the x-axis and S⊥ the y-axis, in which case they",linear_prog
contain the “semipositive” vectors [1 0] and [0 1]. This slightly weaker alternative does,linear_prog
"work: Either S contains a positive vector x > 0, or S⊥ contains a nonzero y ≥ 0. When S",linear_prog
"and S⊥ are perpendicular lines in the plane, one or the other must enter the ﬁrst quadrant.",linear_prog
I can’t see this clearly in three or four dimensions.,linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
"For linear programming, the important alternatives come when the constraints are",linear_prog
inequalities. When is the feasible set empty (no x)?,linear_prog
8J Ax ≥ b has a solution x ≥ 0,linear_prog
there is a y ≤ 0 with yA ≥ 0 and yb < 0.,linear_prog
Proof. The slack variables w = Ax−b change Ax ≥ b into an equation. Use 8I:,linear_prog
for some y with,linear_prog
It is this result that leads to a “nonconstructive proof” of the duality theorem.,linear_prog
"1. What is the dual of the following problem: Minimize x1 + x2, subject to x1 ≥ 0,",linear_prog
"x2 ≥ 0, 2x1 ≥ 4, x1 +3x2 ≥ 11? Find the solution to both this problem and its dual,",linear_prog
and verify that minimum equals maximum.,linear_prog
"2. What is the dual of the following problem: Maximize y2 subject to y1 ≥ 0, y2 ≥ 0,",linear_prog
y1 +y2 ≤ 3? Solve both this problem and its dual.,linear_prog
"3. Suppose A is the identity matrix (so that m = n), and the vectors b and c are nonnega-",linear_prog
"tive. Explain why x∗ = b is optimal in the minimum problem, ﬁnd y∗ in the maximum",linear_prog
"problem, and verify that the two values are the same. If the ﬁrst component of b is",linear_prog
"negative, what are x∗ and y∗?",linear_prog
"4. Construct a 1 by 1 example in which Ax ≥ b, x ≥ 0 is unfeasible, and the dual problem",linear_prog
5. Starting with the 2 by 2 matrix A =,linear_prog
", choose b and c so that both of the feasible",linear_prog
"sets Ax ≥ b, x ≥ 0 and yA ≤ c, y ≥ 0 are empty.",linear_prog
"6. If all entries of A, b, and c are positive, show that both the primal and the dual are",linear_prog
"7. Show that x = (1,1,1,0) and y = (1,1,0,1) are feasible in the primal and dual, with",linear_prog
0 0 1 0,linear_prog
0 1 0 0,linear_prog
1 1 1 1,linear_prog
1 0 0 1,linear_prog
"Then, after computing cx and yb, explain how you know they are optimal.",linear_prog
8.3 The Dual Problem,linear_prog
8. Verify that the vectors in the previous exercise satisfy the complementary slackness,linear_prog
"conditions in equation (2), and ﬁnd the one slack inequality in both the primal and",linear_prog
9. Suppose that A =,linear_prog
", and c =",linear_prog
". Find the optimal x and y, and verify",linear_prog
the complementary slackness conditions (as well as yb = cx).,linear_prog
10. If the primal problem is constrained by equations instead of inequalities—Minimize,linear_prog
cx subject to Ax = b and x ≥ 0—then the requirement y ≥ 0 is left out of the dual:,linear_prog
Maximize yb subject to yA ≤ c. Show that the one-sided inequality yb ≤ cx still,linear_prog
holds. Why was y ≥ 0 needed in equation (1) but not here? This weak duality can be,linear_prog
completed to full duality.,linear_prog
"11. (a) Without the simplex method, minimize the cost 5x1 +3x2 +4x3, subject to x1 +",linear_prog
"x2 +x3 ≥ 1, x1 ≥ 0, x2 ≥ 0, x3 ≥ 0.",linear_prog
(b) What is the shape of the feasible set?,linear_prog
"(c) What is the dual problem, and what is its solution y?",linear_prog
"12. If the primal has a unique optimal solution x∗, and then c is changed a little, explain",linear_prog
why x∗ still remains the optimal solution.,linear_prog
13. Write the dual of the following problem: Maximize x1+x2+x3 subject to 2x1+x2 ≤,linear_prog
"4, x3 ≤ 6. What are the optimal x∗ and y∗ (if they exist!)?",linear_prog
14. If A =,linear_prog
", describe the cone of nonnegative combinations of the columns. If b lies",linear_prog
"inside that cone, say b = (3,2), what is the feasible vector x? If b lies outside, say",linear_prog
"b = (0,1), what vector y will satisfy the alternative?",linear_prog
"15. In three dimensions, can you ﬁnd a set of six vectors whose cone of nonnegative",linear_prog
combinations ﬁlls the whole space? What about four vectors?,linear_prog
"16. Use 8H to show that the following equation has no solution, because the alternative",linear_prog
17. Use 8I to show that there is no solution x ≥ 0 (the alternative holds):,linear_prog
"18. Show that the alternatives in 8J (Ax ≥ b, x ≥ 0, yA ≥ 0, yb < 0, y ≤ 0) cannot both",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
Some linear problems have a structure that makes their solution very quick. Band ma-,linear_prog
"trices have all nonzeros close to the main diagonal, and Ax = b is easy to solve. In linear",linear_prog
"programming, we are interested in the special class for which A is an incidence matrix.",linear_prog
"Its entries are −1 or +1 or (mostly) zero, and pivot steps involve only additions and",linear_prog
subtractions. Much larger problems than usual can be solved.,linear_prog
Networks enter all kinds of applications. Trafﬁc through an intersection satisﬁes,linear_prog
"Kirchhoff’s current law: ﬂow in equals ﬂow out. For gas and oil, network programming",linear_prog
has designed pipeline systems that are millions of dollars cheaper than the intuitive (not,linear_prog
optimized) designs. Scheduling pilots and crews and airplanes has become a signiﬁcant,linear_prog
problem in applied mathematics! We even solve the marriage problem—to maximize,linear_prog
"the number of marriages when brides have a veto. That may not be the real problem, but",linear_prog
it is the one that network programming solves.,linear_prog
The problem in Figure 8.5 is to maximize the ﬂow from the source to the sink. The,linear_prog
"ﬂows cannot exceed the capacities marked on the edges, and the directions given by",linear_prog
the arrows cannot be reversed. The ﬂow on the two edges into the sink cannot exceed,linear_prog
6+1 = 7. Is this total of 7 achievable? What is the maximal ﬂow from left to right?,linear_prog
The unknowns are the ﬂows xij from node i to node j. The capacity constraints are,linear_prog
xij ≤ cij. The ﬂows are nonnegative: xij ≥ 0 going with the arrows. By maximizing the,linear_prog
"return ﬂow x61 (dotted line), we maximize the total ﬂow into the sink.",linear_prog
Figure 8.5: A 6-node network with edge capacities: the maximal ﬂow problem.,linear_prog
"Another constraint is still to be heard from. It is the “conservation law,” that the ﬂow",linear_prog
into each node equals the ﬂow out. That is Kirchhoff’s current law:,linear_prog
x jk = 0,linear_prog
The ﬂows xij enter node j from earlier nodes i. The ﬂows x jk leave node j to later,linear_prog
"nodes k. The balance in equation (1) can be written as Ax = 0, where A is a node-edge",linear_prog
"incidence matrix (the transpose of Section 2.5). A has a row for every node and a +1,",linear_prog
−1 column for every edge:,linear_prog
Maximize x61 subject to Ax = 0 and 0 ≤ xij ≤ cij.,linear_prog
A ﬂow of 2 can go on the path 1-2-4-6-1. A ﬂow of 3 can go along 1-3-4-6-1. An,linear_prog
"additional ﬂow of 1 can take the lowest path 1-3-5-6-1. The total is 6, and no more is",linear_prog
possible. How do you prove that the maximal ﬂow is 6 and not 7?,linear_prog
"Trial and error is convincing, but mathematics is conclusive: The key is to ﬁnd a cut in",linear_prog
"the network, across which all capacities are ﬁlled. That cut separates nodes 5 and 6 from",linear_prog
the others. The edges that go forward across the cut have total capacity 2+3+1 = 6—,linear_prog
and no more can get across! Weak duality says that every cut gives a bound to the total,linear_prog
"ﬂow, and full duality says that the cut of smallest capacity (the minimal cut) is ﬁlled by",linear_prog
Max ﬂow-min cut theorem. The maximal ﬂow in a network equals the,linear_prog
total capacity across the minimal cut.,linear_prog
A “cut” splits the nodes into two groups S and T (source in S and sink in T). Its capacity,linear_prog
is the sum of the capacities of all edges crossing the cut (from S to T). Several cuts,linear_prog
might have the same capacity. Certainly the total ﬂow can never be greater than the total,linear_prog
"capacity across the minimal cut. The problem, here and in all of duality, is to show that",linear_prog
equality is achieved by the right ﬂow and the right cut.,linear_prog
Proof that max ﬂow = min cut. Suppose a ﬂow is maximal. Some nodes might still be,linear_prog
"reached from the source by additional ﬂow, without exceeding any capacities. Those",linear_prog
"nodes go with the source into the set S. The sink must lie in the remaining set T, or it",linear_prog
"could have received more ﬂow! Every edge across the cut must he ﬁlled, or extra ﬂow",linear_prog
could have gone further forward to a node in T. Thus the maximal ﬂow does ﬁll this cut,linear_prog
to capacity. and equality has been achieved.,linear_prog
This suggests a way to construct the maximal ﬂow: Check whether any path has,linear_prog
"unused capacity. If so, add ﬂow along that “augmenting path.” Then compute the re-",linear_prog
"maining capacities and decide whether the sink is cut off from the source, or additional",linear_prog
ﬂow is possible. If you label each node in S by the previous node that ﬂow could come,linear_prog
"from, you can backtrack to ﬁnd the path for extra ﬂow.",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
Suppose we have four women and four men. Some of those sixteen couples are compat-,linear_prog
"ible, others regrettably are not. When is it possible to ﬁnd a complete matching, with",linear_prog
"everyone married? If linear algebra can work in 20-dimensional space, it can certainly",linear_prog
handle the trivial problem of marriage.,linear_prog
There are two ways to present the problem—in a matrix or on a graph. The matrix,linear_prog
"contains aij = 0 if the ith woman and jth man are not compatible, and aij = 1 if they are",linear_prog
"willing to try. Thus row i gives the choices of the ith woman, and column j corresponds",linear_prog
to the jth man:,linear_prog
1 0 0 0,linear_prog
1 1 1 0,linear_prog
0 0 0 1,linear_prog
0 0 0 1,linear_prog
has 6 compatible pairs.,linear_prog
The left graph in Figure 8.6 shows two possible marriages. Ignoring the source s and,linear_prog
"sink t, it has four women on the left and four men on the right. The edges correspond",linear_prog
"to the 1s in the matrix, and the capacities are 1 marriage. There is no edge between the",linear_prog
"ﬁrst woman and fourth man, because the matrix has a14 = 0.",linear_prog
"Figure 8.6: Two marriages on the left, three (maximum) on the right. The third is created by adding two new",linear_prog
marriages and one divorce (backward ﬂow).,linear_prog
It might seem that node M2 can’t be reached by more ﬂow—but that is not so! The,linear_prog
extra ﬂow on the right goes backward to cancel an existing marriage. This extra ﬂow,linear_prog
"makes 3 marriages, which is maximal. The minimal cut is crossed by 3 edges.",linear_prog
A complete matching (if it is possible) is a set of four is in the matrix. They would,linear_prog
"come from four different rows and four different columns, since bigamy is not allowed.",linear_prog
"It is like ﬁnding a permutation matrix within the nonzero entries of A. On the graph, this",linear_prog
means four edges with no nodes in common. The maximal ﬂow is less than 4 exactly,linear_prog
when a complete matching is impossible.,linear_prog
"In our example the maximal ﬂow is 3, not 4. The marriages 1–1, 2–2, 4–4 are allowed",linear_prog
"(and several other sets of three marriages), but there is no way to reach four. The minimal",linear_prog
cut on the right separates the two women at the bottom from the three men at the top.,linear_prog
The two women have only one man left to choose—not enough. The capacity across the,linear_prog
cut is only 3.,linear_prog
"Whenever there is a subset of k women who among them like fewer than k men, a",linear_prog
complete matching is impossible.,linear_prog
That test is decisive. The same impossibility can be expressed in different ways:,linear_prog
"1. (For Chess) It is impossible to put four rooks on squares with 1s in A, so that no",linear_prog
rook can take any other rook.,linear_prog
2. (For Marriage Matrices) The 1s in the matrix can be covered by three horizontal,linear_prog
or vertical lines. That equals the maximum number of marriages.,linear_prog
3. (For Linear Algebra) Every matrix with the same zeros as A is singular.,linear_prog
Remember that the determinant is a sum of 4! = 24 terms. Each term uses all four rows,linear_prog
and columns. The zeros in A make all 24 terms zero.,linear_prog
"A block of zeros is preventing a complete matching! The 2 by 3 submatrix in rows 3,",linear_prog
"4 and columns 1, 2, 3 of A is entirely zero. The general rule for an n by n matrix is that",linear_prog
"a p by q block of zeros prevents a matching if p+q > n. Here women 3, 4 could marry",linear_prog
only the man 4. If p women can marry only n−q men and p > n−q (which is the same,linear_prog
"as a zero block with p+q > n), then a complete matching is impossible.",linear_prog
The mathematical problem is to prove the following: If every set of p women does,linear_prog
"like at least p men, a complete matching is possible. That is Hall’s condition. No block",linear_prog
"of zeros is too large. Each woman must like at least one man, each two women must",linear_prog
"between them like at least two men, and so on, to p = n.",linear_prog
A complete matching is possible if (and only if) Hall’s condition holds.,linear_prog
"The proof is simplest if the capacities are n, instead of 1, on all edges across the",linear_prog
middle. The capacities out of the source and into the sink are still 1. If the maximal ﬂow,linear_prog
"is n, all those edges from the source and into the sink are ﬁlled—and the ﬂow produces",linear_prog
"n marriages. When a complete matching is impossible, and the maximal ﬂow is below",linear_prog
"n, some cut must be responsible.",linear_prog
"That cut will have capacity below n, so no middle edges cross it. Suppose p nodes on",linear_prog
the left and r nodes on the right are in the set S with the source. The capacity across that,linear_prog
"cut is n− p from the source to the remaining women, and r from these men to the sink.",linear_prog
"Since the cut capacity is below n, the p women like only the r men and no others. But",linear_prog
"the capacity n− p+r is below n exactly when p > r, and Hall’s condition fails.",linear_prog
Chapter 8 Linear Programming and Game Theory,linear_prog
Spanning Trees and the Greedy Algorithm,linear_prog
A fundamental network model is the shortest path problem—in which the edges have,linear_prog
lengths instead of capacities. We want the shortest path from source to sink. If the edges,linear_prog
"are telephone lines and the lengths are delay times, we are ﬁnding the quickest route",linear_prog
"for a call, If the nodes are computers, we are looking for the perfect message-passing",linear_prog
A closely related problem ﬁnds the shortest spanning tree—a set of n − 1 edges,linear_prog
connecting all the nodes of the network. Instead of getting quickly between a source,linear_prog
"and a sink, we are now minimizing the cost of connecting all the nodes. There are no",linear_prog
"loops, because the cost to close a loop is unnecessary. A spanning tree connects the",linear_prog
"nodes without loops, and we want the shortest one. Here is one possible algorithm:",linear_prog
1. Start from any node s and repeat the following step:,linear_prog
Add the shortest edge that connects the current tree to a new node.,linear_prog
"In Figure 8.7, the edge lengths would come in the order 1, 2, 7, 4, 3, 6. The last step",linear_prog
"skips the edge of length 5, which closes a loop. The total length is 23—but is it minimal?",linear_prog
"We accepted the edge of length 7 very early, and the second algorithm holds out longer.",linear_prog
Figure 8.7: A network and a shortest spanning tree of length 23.,linear_prog
"2. Accept edges in increasing order of length, rejecting edges that complete a loop.",linear_prog
"Now the edges come in the order 1, 2, 3, 4, 6 (again rejecting 5), and 7. They are the",linear_prog
same edges—although that will not always happen. Their total length is the same—and,linear_prog
"that does always happen. The spanning tree problem is exceptional, because it can be",linear_prog
solved in one pass.,linear_prog
"In the language of linear programming, we are ﬁnding the optimal corner ﬁrst. The",linear_prog
"spanning tree problem is being solved like back-substitution, with no false steps. This",linear_prog
general approach is called the greedy algorithm. Here is another greedy idea:,linear_prog
"3. Build trees from all n nodes, by repeating the following step:",linear_prog
Select any tree and add the minimum-length edge going out from that tree.,linear_prog
The steps depend on the selection order of the trees. To stay with the same tree is,linear_prog
algorithm 1. To take the lengths in order is algorithm 2. To sweep through all the trees,linear_prog
